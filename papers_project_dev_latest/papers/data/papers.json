[
    {
        "new_title": "Contrastive Learning of Asset Embeddings from Financial Time Series",
        "new_link": "http://arxiv.org/abs/2407.18645v1",
        "new_summary": "  Representation learning has emerged as a powerful paradigm for extracting\nvaluable latent features from complex, high-dimensional data. In financial\ndomains, learning informative representations for assets can be used for tasks\nlike sector classification, and risk management. However, the complex and\nstochastic nature of financial markets poses unique challenges. We propose a\nnovel contrastive learning framework to generate asset embeddings from\nfinancial time series data. Our approach leverages the similarity of asset\nreturns over many subwindows to generate informative positive and negative\nsamples, using a statistical sampling strategy based on hypothesis testing to\naddress the noisy nature of financial data. We explore various contrastive loss\nfunctions that capture the relationships between assets in different ways to\nlearn a discriminative representation space. Experiments on real-world datasets\ndemonstrate the effectiveness of the learned asset embeddings on benchmark\nindustry classification and portfolio optimization tasks. In each case our\nnovel approaches significantly outperform existing baselines highlighting the\npotential for contrastive learning to capture meaningful and actionable\nrelationships in financial data.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18645v1.pdf",
        "similarity": 0.9999999970600049,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Predicting Stock Prices with FinBERT-LSTM: Integrating News Sentiment\n  Analysis",
        "new_link": "http://arxiv.org/abs/2407.16150v1",
        "new_summary": "  The stock market's ascent typically mirrors the flourishing state of the\neconomy, whereas its decline is often an indicator of an economic downturn.\nTherefore, for a long time, significant correlation elements for predicting\ntrends in financial stock markets have been widely discussed, and people are\nbecoming increasingly interested in the task of financial text mining. The\ninherent instability of stock prices makes them acutely responsive to\nfluctuations within the financial markets. In this article, we use deep\nlearning networks, based on the history of stock prices and articles of\nfinancial, business, technical news that introduce market information to\npredict stock prices. We illustrate the enhancement of predictive precision by\nintegrating weighted news categories into the forecasting model. We developed a\npre-trained NLP model known as FinBERT, designed to discern the sentiments\nwithin financial texts. Subsequently, we advanced this model by incorporating\nthe sophisticated Long Short Term Memory (LSTM) architecture, thus constructing\nthe innovative FinBERT-LSTM model. This model utilizes news categories related\nto the stock market structure hierarchy, namely market, industry, and stock\nrelated news categories, combined with the stock market's stock price situation\nin the previous week for prediction. We selected NASDAQ-100 index stock data\nand trained the model on Benzinga news articles, and utilized Mean Absolute\nError (MAE), Mean Absolute Percentage Error (MAPE), and Accuracy as the key\nmetrics for the assessment and comparative analysis of the model's performance.\nThe results indicate that FinBERT-LSTM performs the best, followed by LSTM, and\nDNN model ranks third in terms of effectiveness.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.16150v1.pdf",
        "similarity": 0.5890686809214012,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-23"
    },
    {
        "new_title": "Embedding And Clustering Your Data Can Improve Contrastive Pretraining",
        "new_link": "http://arxiv.org/abs/2407.18887v1",
        "new_summary": "  Recent studies of large-scale contrastive pretraining in the text embedding\ndomain show that using single-source minibatches, rather than mixed-source\nminibatches, can substantially improve overall model accuracy. In this work, we\nexplore extending training data stratification beyond source granularity by\nleveraging a pretrained text embedding model and the classic k-means clustering\nalgorithm to further split training data apart by the semantic clusters within\neach source. Experimentally, we observe a notable increase in NDCG@10 when\npretraining a BERT-based text embedding model on query-passage pairs from the\nMSMARCO passage retrieval dataset. Additionally, we conceptually connect our\nclustering approach to both the Topic Aware Sampling (TAS) aspect of the TAS-B\nmethodology and the nearest-neighbor-based hard-negative mining aspect of the\nANCE methodology and discuss how this unified view motivates future lines of\nresearch on the organization of contrastive pretraining data.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18887v1.pdf",
        "similarity": 0.48623362191132413,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Learning production functions for supply chains with graph neural\n  networks",
        "new_link": "http://arxiv.org/abs/2407.18772v1",
        "new_summary": "  The global economy relies on the flow of goods over supply chain networks,\nwith nodes as firms and edges as transactions between firms. While we may\nobserve these external transactions, they are governed by unseen production\nfunctions, which determine how firms internally transform the input products\nthey receive into output products that they sell. In this setting, it can be\nextremely valuable to infer these production functions, to better understand\nand improve supply chains, and to forecast future transactions more accurately.\nHowever, existing graph neural networks (GNNs) cannot capture these hidden\nrelationships between nodes' inputs and outputs. Here, we introduce a new class\nof models for this setting, by combining temporal GNNs with a novel inventory\nmodule, which learns production functions via attention weights and a special\nloss function. We evaluate our models extensively on real supply chains data,\nalong with data generated from our new open-source simulator, SupplySim. Our\nmodels successfully infer production functions, with a 6-50% improvement over\nbaselines, and forecast future transactions on real and synthetic data,\noutperforming baselines by 11-62%.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18772v1.pdf",
        "similarity": 0.47725816943658256,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Achieving interpretable machine learning by functional decomposition of\n  black-box models into explainable predictor effects",
        "new_link": "http://arxiv.org/abs/2407.18650v1",
        "new_summary": "  Machine learning (ML) has seen significant growth in both popularity and\nimportance. The high prediction accuracy of ML models is often achieved through\ncomplex black-box architectures that are difficult to interpret. This\ninterpretability problem has been hindering the use of ML in fields like\nmedicine, ecology and insurance, where an understanding of the inner workings\nof the model is paramount to ensure user acceptance and fairness. The need for\ninterpretable ML models has boosted research in the field of interpretable\nmachine learning (IML). Here we propose a novel approach for the functional\ndecomposition of black-box predictions, which is considered a core concept of\nIML. The idea of our method is to replace the prediction function by a\nsurrogate model consisting of simpler subfunctions. Similar to additive\nregression models, these functions provide insights into the direction and\nstrength of the main feature contributions and their interactions. Our method\nis based on a novel concept termed stacked orthogonality, which ensures that\nthe main effects capture as much functional behavior as possible and do not\ncontain information explained by higher-order interactions. Unlike earlier\nfunctional IML approaches, it is neither affected by extrapolation nor by\nhidden feature interactions. To compute the subfunctions, we propose an\nalgorithm based on neural additive modeling and an efficient post-hoc\northogonalization procedure.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18650v1.pdf",
        "similarity": 0.4626214269586701,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Pre-training with Fractional Denoising to Enhance Molecular Property\n  Prediction",
        "new_link": "http://arxiv.org/abs/2407.11086v1",
        "new_summary": "  Deep learning methods have been considered promising for accelerating\nmolecular screening in drug discovery and material design. Due to the limited\navailability of labelled data, various self-supervised molecular pre-training\nmethods have been presented. While many existing methods utilize common\npre-training tasks in computer vision (CV) and natural language processing\n(NLP), they often overlook the fundamental physical principles governing\nmolecules. In contrast, applying denoising in pre-training can be interpreted\nas an equivalent force learning, but the limited noise distribution introduces\nbias into the molecular distribution. To address this issue, we introduce a\nmolecular pre-training framework called fractional denoising (Frad), which\ndecouples noise design from the constraints imposed by force learning\nequivalence. In this way, the noise becomes customizable, allowing for\nincorporating chemical priors to significantly improve molecular distribution\nmodeling. Experiments demonstrate that our framework consistently outperforms\nexisting methods, establishing state-of-the-art results across force\nprediction, quantum chemical properties, and binding affinity tasks. The\nrefined noise design enhances force accuracy and sampling coverage, which\ncontribute to the creation of physically consistent molecular representations,\nultimately leading to superior predictive performance.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.11086v1.pdf",
        "similarity": 0.4582284222093418,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-14"
    },
    {
        "new_title": "Application of interpretable machine learning for cross-diagnostic\n  inference on the ST40 spherical tokamak",
        "new_link": "http://arxiv.org/abs/2407.18741v1",
        "new_summary": "  Machine learning models are exceptionally effective in capturing complex\nnon-linear relationships of high-dimensional datasets and making accurate\npredictions. However, their intrinsic ``black-box'' nature makes it difficult\nto interpret them or guarantee ``safe behavior'' when deployed in high-risk\napplications such as feedback control, healthcare and finance. This drawback\nacts as a significant barrier to their wider application across many scientific\nand industrial domains where the interpretability of the model predictions is\nas important as accuracy. Leveraging the latest developments in interpretable\nmachine learning, we develop a method to parameterise ``black-box'' models,\neffectively transforming them into ``grey-box'' models. We apply this approach\nto plasma diagnostics by creating a parameterised synthetic Soft X-Ray imaging\n$-$ Thomson Scattering diagnostic, which predicts high temporal resolution\nelectron temperature and density profiles from the measured soft X-ray\nemission. The ``grey-box'' model predictions are benchmarked against the\ntrained ``black-box'' models as well as a diverse range of plasma conditions.\nOur model-agnostic approach can be applied to various machine learning\narchitectures, enabling direct comparisons of model interpretations.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18741v1.pdf",
        "similarity": 0.4570248855553253,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Unifying Visual and Semantic Feature Spaces with Diffusion Models for\n  Enhanced Cross-Modal Alignment",
        "new_link": "http://arxiv.org/abs/2407.18854v1",
        "new_summary": "  Image classification models often demonstrate unstable performance in\nreal-world applications due to variations in image information, driven by\ndiffering visual perspectives of subject objects and lighting discrepancies. To\nmitigate these challenges, existing studies commonly incorporate additional\nmodal information matching the visual data to regularize the model's learning\nprocess, enabling the extraction of high-quality visual features from complex\nimage regions. Specifically, in the realm of multimodal learning, cross-modal\nalignment is recognized as an effective strategy, harmonizing different modal\ninformation by learning a domain-consistent latent feature space for visual and\nsemantic features. However, this approach may face limitations due to the\nheterogeneity between multimodal information, such as differences in feature\ndistribution and structure. To address this issue, we introduce a Multimodal\nAlignment and Reconstruction Network (MARNet), designed to enhance the model's\nresistance to visual noise. Importantly, MARNet includes a cross-modal\ndiffusion reconstruction module for smoothly and stably blending information\nacross different domains. Experiments conducted on two benchmark datasets,\nVireo-Food172 and Ingredient-101, demonstrate that MARNet effectively improves\nthe quality of image information extracted by the model. It is a plug-and-play\nframework that can be rapidly integrated into various image classification\nframeworks, boosting model performance.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18854v1.pdf",
        "similarity": 0.4484840955984701,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Capturing Style in Author and Document Representation",
        "new_link": "http://arxiv.org/abs/2407.13358v1",
        "new_summary": "  A wide range of Deep Natural Language Processing (NLP) models integrates\ncontinuous and low dimensional representations of words and documents.\nSurprisingly, very few models study representation learning for authors. These\nrepresentations can be used for many NLP tasks, such as author identification\nand classification, or in recommendation systems. A strong limitation of\nexisting works is that they do not explicitly capture writing style, making\nthem hardly applicable to literary data. We therefore propose a new\narchitecture based on Variational Information Bottleneck (VIB) that learns\nembeddings for both authors and documents with a stylistic constraint. Our\nmodel fine-tunes a pre-trained document encoder. We stimulate the detection of\nwriting style by adding predefined stylistic features making the representation\naxis interpretable with respect to writing style indicators. We evaluate our\nmethod on three datasets: a literary corpus extracted from the Gutenberg\nProject, the Blog Authorship Corpus and IMDb62, for which we show that it\nmatches or outperforms strong/recent baselines in authorship attribution while\ncapturing much more accurately the authors stylistic aspects.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.13358v1.pdf",
        "similarity": 0.44288492523080264,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-18"
    },
    {
        "new_title": "Learn from the Learnt: Source-Free Active Domain Adaptation via\n  Contrastive Sampling and Visual Persistence",
        "new_link": "http://arxiv.org/abs/2407.18899v1",
        "new_summary": "  Domain Adaptation (DA) facilitates knowledge transfer from a source domain to\na related target domain. This paper investigates a practical DA paradigm,\nnamely Source data-Free Active Domain Adaptation (SFADA), where source data\nbecomes inaccessible during adaptation, and a minimum amount of annotation\nbudget is available in the target domain. Without referencing the source data,\nnew challenges emerge in identifying the most informative target samples for\nlabeling, establishing cross-domain alignment during adaptation, and ensuring\ncontinuous performance improvements through the iterative query-and-adaptation\nprocess. In response, we present learn from the learnt (LFTL), a novel paradigm\nfor SFADA to leverage the learnt knowledge from the source pretrained model and\nactively iterated models without extra overhead. We propose Contrastive Active\nSampling to learn from the hypotheses of the preceding model, thereby querying\ntarget samples that are both informative to the current model and persistently\nchallenging throughout active learning. During adaptation, we learn from\nfeatures of actively selected anchors obtained from previous intermediate\nmodels, so that the Visual Persistence-guided Adaptation can facilitate feature\ndistribution alignment and active sample exploitation. Extensive experiments on\nthree widely-used benchmarks show that our LFTL achieves state-of-the-art\nperformance, superior computational efficiency and continuous improvements as\nthe annotation budget increases. Our code is available at\nhttps://github.com/lyumengyao/lftl.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18899v1.pdf",
        "similarity": 0.4415150007099349,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Benchmarking Dependence Measures to Prevent Shortcut Learning in Medical\n  Imaging",
        "new_link": "http://arxiv.org/abs/2407.18792v1",
        "new_summary": "  Medical imaging cohorts are often confounded by factors such as acquisition\ndevices, hospital sites, patient backgrounds, and many more. As a result, deep\nlearning models tend to learn spurious correlations instead of causally related\nfeatures, limiting their generalizability to new and unseen data. This problem\ncan be addressed by minimizing dependence measures between intermediate\nrepresentations of task-related and non-task-related variables. These measures\ninclude mutual information, distance correlation, and the performance of\nadversarial classifiers. Here, we benchmark such dependence measures for the\ntask of preventing shortcut learning. We study a simplified setting using\nMorpho-MNIST and a medical imaging task with CheXpert chest radiographs. Our\nresults provide insights into how to mitigate confounding factors in medical\nimaging.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18792v1.pdf",
        "similarity": 0.43520847764514053,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "ZZU-NLP at SIGHAN-2024 dimABSA Task: Aspect-Based Sentiment Analysis\n  with Coarse-to-Fine In-context Learning",
        "new_link": "http://arxiv.org/abs/2407.15341v1",
        "new_summary": "  The DimABSA task requires fine-grained sentiment intensity prediction for\nrestaurant reviews, including scores for Valence and Arousal dimensions for\neach Aspect Term. In this study, we propose a Coarse-to-Fine In-context\nLearning(CFICL) method based on the Baichuan2-7B model for the DimABSA task in\nthe SIGHAN 2024 workshop. Our method improves prediction accuracy through a\ntwo-stage optimization process. In the first stage, we use fixed in-context\nexamples and prompt templates to enhance the model's sentiment recognition\ncapability and provide initial predictions for the test data. In the second\nstage, we encode the Opinion field using BERT and select the most similar\ntraining data as new in-context examples based on similarity. These examples\ninclude the Opinion field and its scores, as well as related opinion words and\ntheir average scores. By filtering for sentiment polarity, we ensure that the\nexamples are consistent with the test data. Our method significantly improves\nprediction accuracy and consistency by effectively utilizing training data and\noptimizing in-context examples, as validated by experimental results.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.15341v1.pdf",
        "similarity": 0.43463089617307105,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-22"
    },
    {
        "new_title": "ViTime: A Visual Intelligence-Based Foundation Model for Time Series\n  Forecasting",
        "new_link": "http://arxiv.org/abs/2407.07311v1",
        "new_summary": "  The success of large pretrained models in natural language processing (NLP)\nand computer vision (CV) has opened new avenues for constructing foundation\nmodels for time series forecasting (TSF). Traditional TSF foundation models\nrely heavily on numerical data fitting. In contrast, the human brain is\ninherently skilled at processing visual information, prefer predicting future\ntrends by observing visualized sequences. From a biomimetic perspective,\nutilizing models to directly process numerical sequences might not be the most\neffective route to achieving Artificial General Intelligence (AGI). This paper\nproposes ViTime, a novel Visual Intelligence-based foundation model for TSF.\nViTime overcomes the limitations of numerical time series data fitting by\nutilizing visual data processing paradigms and employs a innovative data\nsynthesis method during training, called Real Time Series (RealTS). Experiments\non a diverse set of previously unseen forecasting datasets demonstrate that\nViTime achieves state-of-the-art zero-shot performance, even surpassing the\nbest individually trained supervised models in some situations. These findings\nsuggest that visual intelligence can significantly enhance time series analysis\nand forecasting, paving the way for more advanced and versatile models in the\nfield. The code for our framework is accessible at\nhttps://github.com/IkeYang/ViTime.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.07311v1.pdf",
        "similarity": 0.43141021885594383,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-10"
    },
    {
        "new_title": "Deep Companion Learning: Enhancing Generalization Through Historical\n  Consistency",
        "new_link": "http://arxiv.org/abs/2407.18821v1",
        "new_summary": "  We propose Deep Companion Learning (DCL), a novel training method for Deep\nNeural Networks (DNNs) that enhances generalization by penalizing inconsistent\nmodel predictions compared to its historical performance. To achieve this, we\ntrain a deep-companion model (DCM), by using previous versions of the model to\nprovide forecasts on new inputs. This companion model deciphers a meaningful\nlatent semantic structure within the data, thereby providing targeted\nsupervision that encourages the primary model to address the scenarios it finds\nmost challenging. We validate our approach through both theoretical analysis\nand extensive experimentation, including ablation studies, on a variety of\nbenchmark datasets (CIFAR-100, Tiny-ImageNet, ImageNet-1K) using diverse\narchitectural models (ShuffleNetV2, ResNet, Vision Transformer, etc.),\ndemonstrating state-of-the-art performance.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18821v1.pdf",
        "similarity": 0.43070237861824684,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Weak-to-Strong Reasoning",
        "new_link": "http://arxiv.org/abs/2407.13647v1",
        "new_summary": "  When large language models (LLMs) exceed human-level capabilities, it becomes\nincreasingly challenging to provide full-scale and accurate supervisions for\nthese models. Weak-to-strong learning, which leverages a less capable model to\nunlock the latent abilities of a stronger model, proves valuable in this\ncontext. Yet, the efficacy of this approach for complex reasoning tasks is\nstill untested. Furthermore, tackling reasoning tasks under the weak-to-strong\nsetting currently lacks efficient methods to avoid blindly imitating the weak\nsupervisor including its errors. In this paper, we introduce a progressive\nlearning framework that enables the strong model to autonomously refine its\ntraining data, without requiring input from either a more advanced model or\nhuman-annotated data. This framework begins with supervised fine-tuning on a\nselective small but high-quality dataset, followed by preference optimization\non contrastive samples identified by the strong model itself. Extensive\nexperiments on the GSM8K and MATH datasets demonstrate that our method\nsignificantly enhances the reasoning capabilities of Llama2-70b using three\nseparate weak models. This method is further validated in a forward-looking\nexperimental setup, where Llama3-8b-instruct effectively supervises Llama3-70b\non the highly challenging OlympicArena dataset. This work paves the way for a\nmore scalable and sophisticated strategy to enhance AI reasoning powers. All\nrelevant code and resources are available in\n\\url{https://github.com/GAIR-NLP/weak-to-strong-reasoning}.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.13647v1.pdf",
        "similarity": 0.4226928188586669,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-18"
    },
    {
        "new_title": "BCTR: Bidirectional Conditioning Transformer for Scene Graph Generation",
        "new_link": "http://arxiv.org/abs/2407.18715v1",
        "new_summary": "  Scene Graph Generation (SGG) remains a challenging task due to its\ncompositional property. Previous approaches improve prediction efficiency by\nlearning in an end-to-end manner. However, these methods exhibit limited\nperformance as they assume unidirectional conditioning between entities and\npredicates, leading to insufficient information interaction. To address this\nlimitation, we propose a novel bidirectional conditioning factorization for\nSGG, introducing efficient interaction between entities and predicates.\nSpecifically, we develop an end-to-end scene graph generation model,\nBidirectional Conditioning Transformer (BCTR), to implement our factorization.\nBCTR consists of two key modules. First, the Bidirectional Conditioning\nGenerator (BCG) facilitates multi-stage interactive feature augmentation\nbetween entities and predicates, enabling mutual benefits between the two\npredictions. Second, Random Feature Alignment (RFA) regularizes the feature\nspace by distilling multi-modal knowledge from pre-trained models, enhancing\nBCTR's ability on tailed categories without relying on statistical priors. We\nconduct a series of experiments on Visual Genome and Open Image V6,\ndemonstrating that BCTR achieves state-of-the-art performance on both\nbenchmarks. The code will be available upon acceptance of the paper.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18715v1.pdf",
        "similarity": 0.418951672822535,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Cluster-norm for Unsupervised Probing of Knowledge",
        "new_link": "http://arxiv.org/abs/2407.18712v1",
        "new_summary": "  The deployment of language models brings challenges in generating reliable\ninformation, especially when these models are fine-tuned using human\npreferences. To extract encoded knowledge without (potentially) biased human\nlabels, unsupervised probing techniques like Contrast-Consistent Search (CCS)\nhave been developed (Burns et al., 2022). However, salient but unrelated\nfeatures in a given dataset can mislead these probes (Farquhar et al., 2023).\nAddressing this, we propose a cluster normalization method to minimize the\nimpact of such features by clustering and normalizing activations of contrast\npairs before applying unsupervised probing techniques. While this approach does\nnot address the issue of differentiating between knowledge in general and\nsimulated knowledge - a major issue in the literature of latent knowledge\nelicitation (Christiano et al., 2021) - it significantly improves the ability\nof unsupervised probes to identify the intended knowledge amidst distractions.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18712v1.pdf",
        "similarity": 0.4179203981631817,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Learning Spectral-Decomposed Tokens for Domain Generalized Semantic\n  Segmentation",
        "new_link": "http://arxiv.org/abs/2407.18568v1",
        "new_summary": "  The rapid development of Vision Foundation Model (VFM) brings inherent\nout-domain generalization for a variety of down-stream tasks. Among them,\ndomain generalized semantic segmentation (DGSS) holds unique challenges as the\ncross-domain images share common pixel-wise content information but vary\ngreatly in terms of the style. In this paper, we present a novel\nSpectral-dEcomposed Token (SET) learning framework to advance the frontier.\nDelving into further than existing fine-tuning token & frozen backbone\nparadigm, the proposed SET especially focuses on the way learning\nstyle-invariant features from these learnable tokens. Particularly, the frozen\nVFM features are first decomposed into the phase and amplitude components in\nthe frequency space, which mainly contain the information of content and style,\nrespectively, and then separately processed by learnable tokens for\ntask-specific information extraction. After the decomposition, style variation\nprimarily impacts the token-based feature enhancement within the amplitude\nbranch. To address this issue, we further develop an attention optimization\nmethod to bridge the gap between style-affected representation and static\ntokens during inference. Extensive cross-domain experiments show its\nstate-of-the-art performance.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18568v1.pdf",
        "similarity": 0.41791774614687954,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Retrieval-Enhanced Machine Learning: Synthesis and Opportunities",
        "new_link": "http://arxiv.org/abs/2407.12982v1",
        "new_summary": "  In the field of language modeling, models augmented with retrieval components\nhave emerged as a promising solution to address several challenges faced in the\nnatural language processing (NLP) field, including knowledge grounding,\ninterpretability, and scalability. Despite the primary focus on NLP, we posit\nthat the paradigm of retrieval-enhancement can be extended to a broader\nspectrum of machine learning (ML) such as computer vision, time series\nprediction, and computational biology. Therefore, this work introduces a formal\nframework of this paradigm, Retrieval-Enhanced Machine Learning (REML), by\nsynthesizing the literature in various domains in ML with consistent notations\nwhich is missing from the current literature. Also, we found that while a\nnumber of studies employ retrieval components to augment their models, there is\na lack of integration with foundational Information Retrieval (IR) research. We\nbridge this gap between the seminal IR research and contemporary REML studies\nby investigating each component that comprises the REML framework. Ultimately,\nthe goal of this work is to equip researchers across various disciplines with a\ncomprehensive, formally structured framework of retrieval-enhanced models,\nthereby fostering interdisciplinary future research.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.12982v1.pdf",
        "similarity": 0.41782823881958686,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-17"
    },
    {
        "new_title": "Climbing the Complexity Ladder with Expressive Attention",
        "new_link": "http://arxiv.org/abs/2407.18601v1",
        "new_summary": "  Attention involves comparing query and key vectors in terms of a scalar\nproduct, $\\mathbf{Q}^T\\mathbf{K}$, together with a subsequent softmax\nnormalization. Classicaly, parallel/orthogonal/antiparallel queries and keys\nlead to large/intermediate/small attention weights. Here we study expressive\nattention (EA), which is based on $(\\mathbf{Q}^T\\mathbf{K})^2$, the squared dot\nproduct. In this case attention is enhanced when query and key are either\nparallel or antiparallel, and suppressed for orthogonal configurations. For a\nseries of autoregressive prediction tasks, we find that EA performs at least as\nwell as the standard mechanism, dot-product attention (DPA). Increasing task\ncomplexity, EA is observed to outperform DPA with increasing margins, which\nalso holds for multi-task settings. For a given model size, EA manages to\nachieve 100\\% performance for a range of complexity levels not accessible to\nDPA.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18601v1.pdf",
        "similarity": 0.4171189027780283,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Content-driven Magnitude-Derivative Spectrum Complementary Learning for\n  Hyperspectral Image Classification",
        "new_link": "http://arxiv.org/abs/2407.18593v1",
        "new_summary": "  Extracting discriminative information from complex spectral details in\nhyperspectral image (HSI) for HSI classification is pivotal. While current\nprevailing methods rely on spectral magnitude features, they could cause\nconfusion in certain classes, resulting in misclassification and decreased\naccuracy. We find that the derivative spectrum proves more adept at capturing\nconcealed information, thereby offering a distinct advantage in separating\nthese confusion classes. Leveraging the complementarity between spectral\nmagnitude and derivative features, we propose a Content-driven Spectrum\nComplementary Network based on Magnitude-Derivative Dual Encoder, employing\nthese two features as combined inputs. To fully utilize their complementary\ninformation, we raise a Content-adaptive Point-wise Fusion Module, enabling\nadaptive fusion of dual-encoder features in a point-wise selective manner,\ncontingent upon feature representation. To preserve a rich source of\ncomplementary information while extracting more distinguishable features, we\nintroduce a Hybrid Disparity-enhancing Loss that enhances the differential\nexpression of the features from the two branches and increases the inter-class\ndistance. As a result, our method achieves state-of-the-art results on the\nextensive WHU-OHS dataset and eight other benchmark datasets.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18593v1.pdf",
        "similarity": 0.41662907949712913,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Dual-Decoupling Learning and Metric-Adaptive Thresholding for\n  Semi-Supervised Multi-Label Learning",
        "new_link": "http://arxiv.org/abs/2407.18624v1",
        "new_summary": "  Semi-supervised multi-label learning (SSMLL) is a powerful framework for\nleveraging unlabeled data to reduce the expensive cost of collecting precise\nmulti-label annotations. Unlike semi-supervised learning, one cannot select the\nmost probable label as the pseudo-label in SSMLL due to multiple semantics\ncontained in an instance. To solve this problem, the mainstream method\ndeveloped an effective thresholding strategy to generate accurate\npseudo-labels. Unfortunately, the method neglected the quality of model\npredictions and its potential impact on pseudo-labeling performance. In this\npaper, we propose a dual-perspective method to generate high-quality\npseudo-labels. To improve the quality of model predictions, we perform\ndual-decoupling to boost the learning of correlative and discriminative\nfeatures, while refining the generation and utilization of pseudo-labels. To\nobtain proper class-wise thresholds, we propose the metric-adaptive\nthresholding strategy to estimate the thresholds, which maximize the\npseudo-label performance for a given metric on labeled data. Experiments on\nmultiple benchmark datasets show the proposed method can achieve the\nstate-of-the-art performance and outperform the comparative methods with a\nsignificant margin.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18624v1.pdf",
        "similarity": 0.41529337711245345,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Robust VAEs via Generating Process of Noise Augmented Data",
        "new_link": "http://arxiv.org/abs/2407.18632v1",
        "new_summary": "  Advancing defensive mechanisms against adversarial attacks in generative\nmodels is a critical research topic in machine learning. Our study focuses on a\nspecific type of generative models - Variational Auto-Encoders (VAEs). Contrary\nto common beliefs and existing literature which suggest that noise injection\ntowards training data can make models more robust, our preliminary experiments\nrevealed that naive usage of noise augmentation technique did not substantially\nimprove VAE robustness. In fact, it even degraded the quality of learned\nrepresentations, making VAEs more susceptible to adversarial perturbations.\nThis paper introduces a novel framework that enhances robustness by\nregularizing the latent space divergence between original and noise-augmented\ndata. Through incorporating a paired probabilistic prior into the standard\nvariational lower bound, our method significantly boosts defense against\nadversarial attacks. Our empirical evaluations demonstrate that this approach,\ntermed Robust Augmented Variational Auto-ENcoder (RAVEN), yields superior\nperformance in resisting adversarial inputs on widely-recognized benchmark\ndatasets.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18632v1.pdf",
        "similarity": 0.4116809765637444,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Towards More Accurate Prediction of Human Empathy and Emotion in Text\n  and Multi-turn Conversations by Combining Advanced NLP, Transformers-based\n  Networks, and Linguistic Methodologies",
        "new_link": "http://arxiv.org/abs/2407.18496v1",
        "new_summary": "  Based on the WASSA 2022 Shared Task on Empathy Detection and Emotion\nClassification, we predict the level of empathic concern and personal distress\ndisplayed in essays. For the first stage of this project we implemented a\nFeed-Forward Neural Network using sentence-level embeddings as features. We\nexperimented with four different embedding models for generating the inputs to\nthe neural network. The subsequent stage builds upon the previous work and we\nhave implemented three types of revisions. The first revision focuses on the\nenhancements to the model architecture and the training approach. The second\nrevision focuses on handling class imbalance using stratified data sampling.\nThe third revision focuses on leveraging lexical resources, where we apply four\ndifferent resources to enrich the features associated with the dataset. During\nthe final stage of this project, we have created the final end-to-end system\nfor the primary task using an ensemble of models to revise primary task\nperformance. Additionally, as part of the final stage, these approaches have\nbeen adapted to the WASSA 2023 Shared Task on Empathy Emotion and Personality\nDetection in Interactions, in which the empathic concern, emotion polarity, and\nemotion intensity in dyadic text conversations are predicted.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18496v1.pdf",
        "similarity": 0.40787833305066884,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Enhancing material property prediction with ensemble deep graph\n  convolutional networks",
        "new_link": "http://arxiv.org/abs/2407.18847v1",
        "new_summary": "  Machine learning (ML) models have emerged as powerful tools for accelerating\nmaterials discovery and design by enabling accurate predictions of properties\nfrom compositional and structural data. These capabilities are vital for\ndeveloping advanced technologies across fields such as energy, electronics, and\nbiomedicine, potentially reducing the time and resources needed for new\nmaterial exploration and promoting rapid innovation cycles. Recent efforts have\nfocused on employing advanced ML algorithms, including deep learning - based\ngraph neural network, for property prediction. Additionally, ensemble models\nhave proven to enhance the generalizability and robustness of ML and DL.\nHowever, the use of such ensemble strategies in deep graph networks for\nmaterial property prediction remains underexplored. Our research provides an\nin-depth evaluation of ensemble strategies in deep learning - based graph\nneural network, specifically targeting material property prediction tasks. By\ntesting the Crystal Graph Convolutional Neural Network (CGCNN) and its\nmultitask version, MT-CGCNN, we demonstrated that ensemble techniques,\nespecially prediction averaging, substantially improve precision beyond\ntraditional metrics for key properties like formation energy per atom ($\\Delta\nE^{f}$), band gap ($E_{g}$) and density ($\\rho$) in 33,990 stable inorganic\nmaterials. These findings support the broader application of ensemble methods\nto enhance predictive accuracy in the field.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18847v1.pdf",
        "similarity": 0.406463616298856,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Any four real numbers are on all fours with analogy",
        "new_link": "http://arxiv.org/abs/2407.18770v1",
        "new_summary": "  This work presents a formalization of analogy on numbers that relies on\ngeneralized means. It is motivated by recent advances in artificial\nintelligence and applications of machine learning, where the notion of analogy\nis used to infer results, create data and even as an assessment tool of object\nrepresentations, or embeddings, that are basically collections of numbers\n(vectors, matrices, tensors). This extended analogy use asks for mathematical\nfoundations and clear understanding of the notion of analogy between numbers.\nWe propose a unifying view of analogies that relies on generalized means\ndefined in terms of a power parameter. In particular, we show that any four\nincreasing positive real numbers is an analogy in a unique suitable power. In\naddition, we show that any such analogy can be reduced to an equivalent\narithmetic analogy and that any analogical equation has a solution for\nincreasing numbers, which generalizes without restriction to complex numbers.\nThese foundational results provide a better understanding of analogies in areas\nwhere representations are numerical.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18770v1.pdf",
        "similarity": 0.4029681296322554,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Economy Watchers Survey provides Datasets and Tasks for Japanese\n  Financial Domain",
        "new_link": "http://arxiv.org/abs/2407.14727v1",
        "new_summary": "  Many natural language processing (NLP) tasks in English or general domains\nare widely available and are often used to evaluate pre-trained language\nmodels. In contrast, there are fewer tasks available for languages other than\nEnglish and for the financial domain. In particular, tasks in Japanese and the\nfinancial domain are limited. We construct two large datasets using materials\npublished by a Japanese central government agency. The datasets provide three\nJapanese financial NLP tasks, which include a 3-class and 12-class\nclassification for categorizing sentences, as well as a 5-class classification\ntask for sentiment analysis. Our datasets are designed to be comprehensive and\nup-to-date, leveraging an automatic update framework that ensures the latest\ntask datasets are publicly available anytime.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.14727v1.pdf",
        "similarity": 0.4008539802305077,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-20"
    },
    {
        "new_title": "Empowering Few-Shot Relation Extraction with The Integration of\n  Traditional RE Methods and Large Language Models",
        "new_link": "http://arxiv.org/abs/2407.08967v1",
        "new_summary": "  Few-Shot Relation Extraction (FSRE), a subtask of Relation Extraction (RE)\nthat utilizes limited training instances, appeals to more researchers in\nNatural Language Processing (NLP) due to its capability to extract textual\ninformation in extremely low-resource scenarios. The primary methodologies\nemployed for FSRE have been fine-tuning or prompt tuning techniques based on\nPre-trained Language Models (PLMs). Recently, the emergence of Large Language\nModels (LLMs) has prompted numerous researchers to explore FSRE through\nIn-Context Learning (ICL). However, there are substantial limitations\nassociated with methods based on either traditional RE models or LLMs.\nTraditional RE models are hampered by a lack of necessary prior knowledge,\nwhile LLMs fall short in their task-specific capabilities for RE. To address\nthese shortcomings, we propose a Dual-System Augmented Relation Extractor\n(DSARE), which synergistically combines traditional RE models with LLMs.\nSpecifically, DSARE innovatively injects the prior knowledge of LLMs into\ntraditional RE models, and conversely enhances LLMs' task-specific aptitude for\nRE through relation extraction augmentation. Moreover, an Integrated Prediction\nmodule is employed to jointly consider these two respective predictions and\nderive the final results. Extensive experiments demonstrate the efficacy of our\nproposed method.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.08967v1.pdf",
        "similarity": 0.40024724484034024,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-12"
    },
    {
        "new_title": "ROSA: Random Subspace Adaptation for Efficient Fine-Tuning",
        "new_link": "http://arxiv.org/abs/2407.07802v1",
        "new_summary": "  Model training requires significantly more memory, compared with inference.\nParameter efficient fine-tuning (PEFT) methods provide a means of adapting\nlarge models to downstream tasks using less memory. However, existing methods\nsuch as adapters, prompt tuning or low-rank adaptation (LoRA) either introduce\nlatency overhead at inference time or achieve subpar downstream performance\ncompared with full fine-tuning. In this work we propose Random Subspace\nAdaptation (ROSA), a method that outperforms previous PEFT methods by a\nsignificant margin, while maintaining a zero latency overhead during inference\ntime. In contrast to previous methods, ROSA is able to adapt subspaces of\narbitrarily large dimension, better approximating full-finetuning. We\ndemonstrate both theoretically and experimentally that this makes ROSA strictly\nmore expressive than LoRA, without consuming additional memory during runtime.\nAs PEFT methods are especially useful in the natural language processing\ndomain, where models operate on scales that make full fine-tuning very\nexpensive, we evaluate ROSA in two common NLP scenarios: natural language\ngeneration (NLG) and natural language understanding (NLU) with GPT-2 and\nRoBERTa, respectively. We show that on almost every GLUE task ROSA outperforms\nLoRA by a significant margin, while also outperforming LoRA on NLG tasks. Our\ncode is available at https://github.com/rosa-paper/rosa\n",
        "pdf_link": "https://arxiv.org/pdf/2407.07802v1.pdf",
        "similarity": 0.39317261866515496,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-10"
    },
    {
        "new_title": "Whitening Not Recommended for Classification Tasks in LLMs",
        "new_link": "http://arxiv.org/abs/2407.12886v1",
        "new_summary": "  Sentence embedding is a cornerstone in NLP. Whitening has been claimed to be\nan effective operation to improve embedding quality obtained from Large\nLanguage Models (LLMs). However, we find that the efficacy of whitening is\nmodel-dependent and task-dependent. In particular, whitening degenerates\nembeddings for classification tasks. The conclusion is supported by extensive\nexperiments. We also explored a variety of whitening operations, including PCA,\nZCA, PCA-Cor, ZCA-Cor and Cholesky whitenings. A by-product of our research is\nembedding evaluation platform for LLMs called SentEval+.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.12886v1.pdf",
        "similarity": 0.3914064426657975,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-16"
    },
    {
        "new_title": "Latent Causal Probing: A Formal Perspective on Probing with Causal\n  Models of Data",
        "new_link": "http://arxiv.org/abs/2407.13765v1",
        "new_summary": "  As language models (LMs) deliver increasing performance on a range of NLP\ntasks, probing classifiers have become an indispensable technique in the effort\nto better understand their inner workings. A typical setup involves (1)\ndefining an auxiliary task consisting of a dataset of text annotated with\nlabels, then (2) supervising small classifiers to predict the labels from the\nrepresentations of a pretrained LM as it processed the dataset. A high probing\naccuracy is interpreted as evidence that the LM has learned to perform the\nauxiliary task as an unsupervised byproduct of its original pretraining\nobjective. Despite the widespread usage of probes, however, the robust design\nand analysis of probing experiments remains a challenge. We develop a formal\nperspective on probing using structural causal models (SCM). Specifically,\ngiven an SCM which explains the distribution of tokens observed during\ntraining, we frame the central hypothesis as whether the LM has learned to\nrepresent the latent variables of the SCM. Empirically, we extend a recent\nstudy of LMs in the context of a synthetic grid-world navigation task, where\nhaving an exact model of the underlying causal structure allows us to draw\nstrong inferences from the result of probing experiments. Our techniques\nprovide robust empirical evidence for the ability of LMs to learn the latent\ncausal concepts underlying text.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.13765v1.pdf",
        "similarity": 0.3894608497797129,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-18"
    },
    {
        "new_title": "Adversarial Robustification via Text-to-Image Diffusion Models",
        "new_link": "http://arxiv.org/abs/2407.18658v1",
        "new_summary": "  Adversarial robustness has been conventionally believed as a challenging\nproperty to encode for neural networks, requiring plenty of training data. In\nthe recent paradigm of adopting off-the-shelf models, however, access to their\ntraining data is often infeasible or not practical, while most of such models\nare not originally trained concerning adversarial robustness. In this paper, we\ndevelop a scalable and model-agnostic solution to achieve adversarial\nrobustness without using any data. Our intuition is to view recent\ntext-to-image diffusion models as \"adaptable\" denoisers that can be optimized\nto specify target tasks. Based on this, we propose: (a) to initiate a\ndenoise-and-classify pipeline that offers provable guarantees against\nadversarial attacks, and (b) to leverage a few synthetic reference images\ngenerated from the text-to-image model that enables novel adaptation schemes.\nOur experiments show that our data-free scheme applied to the pre-trained CLIP\ncould improve the (provable) adversarial robustness of its diverse zero-shot\nclassification derivatives (while maintaining their accuracy), significantly\nsurpassing prior approaches that utilize the full training data. Not only for\nCLIP, we also demonstrate that our framework is easily applicable for\nrobustifying other visual classifiers efficiently.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18658v1.pdf",
        "similarity": 0.38833917763052755,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Improving ICD coding using Chapter based Named Entities and Attentional\n  Models",
        "new_link": "http://arxiv.org/abs/2407.17230v1",
        "new_summary": "  Recent advancements in natural language processing (NLP) have led to\nautomation in various domains. However, clinical NLP often relies on benchmark\ndatasets that may not reflect real-world scenarios accurately. Automatic ICD\ncoding, a vital NLP task, typically uses outdated and imbalanced datasets like\nMIMIC-III, with existing methods yielding micro-averaged F1 scores between 0.4\nand 0.7 due to many false positives. Our research introduces an enhanced\napproach to ICD coding that improves F1 scores by using chapter-based named\nentities and attentional models. This method categorizes discharge summaries\ninto ICD-9 Chapters and develops attentional models with chapter-specific data,\neliminating the need to consider external data for code identification. For\ncategorization, we use Chapter-IV to de-bias and influence key entities and\nweights without neural networks, creating accurate thresholds and providing\ninterpretability for human validation. Post-validation, we develop attentional\nmodels for three frequent and three non-frequent codes from Chapter-IV using\nBidirectional-Gated Recurrent Units (GRUs) with Attention and Transformer with\nMulti-head Attention architectures. The average Micro-F1 scores of 0.79 and\n0.81 from these models demonstrate significant performance improvements in ICD\ncoding.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.17230v1.pdf",
        "similarity": 0.38527483481535013,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-24"
    },
    {
        "new_title": "CXR-Agent: Vision-language models for chest X-ray interpretation with\n  uncertainty aware radiology reporting",
        "new_link": "http://arxiv.org/abs/2407.08811v1",
        "new_summary": "  Recently large vision-language models have shown potential when interpreting\ncomplex images and generating natural language descriptions using advanced\nreasoning. Medicine's inherently multimodal nature incorporating scans and\ntext-based medical histories to write reports makes it conducive to benefit\nfrom these leaps in AI capabilities. We evaluate the publicly available, state\nof the art, foundational vision-language models for chest X-ray interpretation\nacross several datasets and benchmarks. We use linear probes to evaluate the\nperformance of various components including CheXagent's vision transformer and\nQ-former, which outperform the industry-standard Torch X-ray Vision models\nacross many different datasets showing robust generalisation capabilities.\nImportantly, we find that vision-language models often hallucinate with\nconfident language, which slows down clinical interpretation. Based on these\nfindings, we develop an agent-based vision-language approach for report\ngeneration using CheXagent's linear probes and BioViL-T's phrase grounding\ntools to generate uncertainty-aware radiology reports with pathologies\nlocalised and described based on their likelihood. We thoroughly evaluate our\nvision-language agents using NLP metrics, chest X-ray benchmarks and clinical\nevaluations by developing an evaluation platform to perform a user study with\nrespiratory specialists. Our results show considerable improvements in\naccuracy, interpretability and safety of the AI-generated reports. We stress\nthe importance of analysing results for normal and abnormal scans separately.\nFinally, we emphasise the need for larger paired (scan and report) datasets\nalongside data augmentation to tackle overfitting seen in these large\nvision-language models.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.08811v1.pdf",
        "similarity": 0.38366774856275826,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-11"
    },
    {
        "new_title": "KaPQA: Knowledge-Augmented Product Question-Answering",
        "new_link": "http://arxiv.org/abs/2407.16073v1",
        "new_summary": "  Question-answering for domain-specific applications has recently attracted\nmuch interest due to the latest advancements in large language models (LLMs).\nHowever, accurately assessing the performance of these applications remains a\nchallenge, mainly due to the lack of suitable benchmarks that effectively\nsimulate real-world scenarios. To address this challenge, we introduce two\nproduct question-answering (QA) datasets focused on Adobe Acrobat and Photoshop\nproducts to help evaluate the performance of existing models on domain-specific\nproduct QA tasks. Additionally, we propose a novel knowledge-driven RAG-QA\nframework to enhance the performance of the models in the product QA task. Our\nexperiments demonstrated that inducing domain knowledge through query\nreformulation allowed for increased retrieval and generative performance when\ncompared to standard RAG-QA methods. This improvement, however, is slight, and\nthus illustrates the challenge posed by the datasets introduced.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.16073v1.pdf",
        "similarity": 0.38249240316582106,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-22"
    },
    {
        "new_title": "HRP: Human Affordances for Robotic Pre-Training",
        "new_link": "http://arxiv.org/abs/2407.18911v1",
        "new_summary": "  In order to *generalize* to various tasks in the wild, robotic agents will\nneed a suitable representation (i.e., vision network) that enables the robot to\npredict optimal actions given high dimensional vision inputs. However, learning\nsuch a representation requires an extreme amount of diverse training data,\nwhich is prohibitively expensive to collect on a real robot. How can we\novercome this problem? Instead of collecting more robot data, this paper\nproposes using internet-scale, human videos to extract \"affordances,\" both at\nthe environment and agent level, and distill them into a pre-trained\nrepresentation. We present a simple framework for pre-training representations\non hand, object, and contact \"affordance labels\" that highlight relevant\nobjects in images and how to interact with them. These affordances are\nautomatically extracted from human video data (with the help of off-the-shelf\ncomputer vision modules) and used to fine-tune existing representations. Our\napproach can efficiently fine-tune *any* existing representation, and results\nin models with stronger downstream robotic performance across the board. We\nexperimentally demonstrate (using 3000+ robot trials) that this affordance\npre-training scheme boosts performance by a minimum of 15% on 5 real-world\ntasks, which consider three diverse robot morphologies (including a dexterous\nhand). Unlike prior works in the space, these representations improve\nperformance across 3 different camera views. Quantitatively, we find that our\napproach leads to higher levels of generalization in out-of-distribution\nsettings. For code, weights, and data check: https://hrp-robot.github.io\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18911v1.pdf",
        "similarity": 0.38049078890148147,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Score matching through the roof: linear, nonlinear, and latent variables\n  causal discovery",
        "new_link": "http://arxiv.org/abs/2407.18755v1",
        "new_summary": "  Causal discovery from observational data holds great promise, but existing\nmethods rely on strong assumptions about the underlying causal structure, often\nrequiring full observability of all relevant variables. We tackle these\nchallenges by leveraging the score function $\\nabla \\log p(X)$ of observed\nvariables for causal discovery and propose the following contributions. First,\nwe generalize the existing results of identifiability with the score to\nadditive noise models with minimal requirements on the causal mechanisms.\nSecond, we establish conditions for inferring causal relations from the score\neven in the presence of hidden variables; this result is two-faced: we\ndemonstrate the score's potential as an alternative to conditional independence\ntests to infer the equivalence class of causal graphs with hidden variables,\nand we provide the necessary conditions for identifying direct causes in latent\nvariable models. Building on these insights, we propose a flexible algorithm\nfor causal discovery across linear, nonlinear, and latent variable models,\nwhich we empirically validate.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18755v1.pdf",
        "similarity": 0.38011060255969653,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "A Universal Prompting Strategy for Extracting Process Model Information\n  from Natural Language Text using Large Language Models",
        "new_link": "http://arxiv.org/abs/2407.18540v1",
        "new_summary": "  Over the past decade, extensive research efforts have been dedicated to the\nextraction of information from textual process descriptions. Despite the\nremarkable progress witnessed in natural language processing (NLP), information\nextraction within the Business Process Management domain remains predominantly\nreliant on rule-based systems and machine learning methodologies. Data scarcity\nhas so far prevented the successful application of deep learning techniques.\nHowever, the rapid progress in generative large language models (LLMs) makes it\npossible to solve many NLP tasks with very high quality without the need for\nextensive data. Therefore, we systematically investigate the potential of LLMs\nfor extracting information from textual process descriptions, targeting the\ndetection of process elements such as activities and actors, and relations\nbetween them. Using a heuristic algorithm, we demonstrate the suitability of\nthe extracted information for process model generation. Based on a novel\nprompting strategy, we show that LLMs are able to outperform state-of-the-art\nmachine learning approaches with absolute performance improvements of up to 8\\%\n$F_1$ score across three different datasets. We evaluate our prompting strategy\non eight different LLMs, showing it is universally applicable, while also\nanalyzing the impact of certain prompt parts on extraction quality. The number\nof example texts, the specificity of definitions, and the rigour of format\ninstructions are identified as key for improving the accuracy of extracted\ninformation. Our code, prompts, and data are publicly available.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18540v1.pdf",
        "similarity": 0.3782380196108729,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Deep Learning-based Sentiment Analysis of Olympics Tweets",
        "new_link": "http://arxiv.org/abs/2407.12376v1",
        "new_summary": "  Sentiment analysis (SA), is an approach of natural language processing (NLP)\nfor determining a text's emotional tone by analyzing subjective information\nsuch as views, feelings, and attitudes toward specific topics, products,\nservices, events, or experiences. This study attempts to develop an advanced\ndeep learning (DL) model for SA to understand global audience emotions through\ntweets in the context of the Olympic Games. The findings represent global\nattitudes around the Olympics and contribute to advancing the SA models. We\nhave used NLP for tweet pre-processing and sophisticated DL models for arguing\nwith SA, this research enhances the reliability and accuracy of sentiment\nclassification. The study focuses on data selection, preprocessing,\nvisualization, feature extraction, and model building, featuring a baseline\nNa\\\"ive Bayes (NB) model and three advanced DL models: Convolutional Neural\nNetwork (CNN), Bidirectional Long Short-Term Memory (BiLSTM), and Bidirectional\nEncoder Representations from Transformers (BERT). The results of the\nexperiments show that the BERT model can efficiently classify sentiments\nrelated to the Olympics, achieving the highest accuracy of 99.23%.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.12376v1.pdf",
        "similarity": 0.374135301458561,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-17"
    },
    {
        "new_title": "Automatic Detection of Moral Values in Music Lyrics",
        "new_link": "http://arxiv.org/abs/2407.18787v1",
        "new_summary": "  Moral values play a fundamental role in how we evaluate information, make\ndecisions, and form judgements around important social issues. The possibility\nto extract morality rapidly from lyrics enables a deeper understanding of our\nmusic-listening behaviours. Building on the Moral Foundations Theory (MFT), we\ntasked a set of transformer-based language models (BERT) fine-tuned on 2,721\nsynthetic lyrics generated by a large language model (GPT-4) to detect moral\nvalues in 200 real music lyrics annotated by two experts.We evaluate their\npredictive capabilities against a series of baselines including out-of-domain\n(BERT fine-tuned on MFT-annotated social media texts) and zero-shot (GPT-4)\nclassification. The proposed models yielded the best accuracy across\nexperiments, with an average F1 weighted score of 0.8. This performance is, on\naverage, 5% higher than out-of-domain and zero-shot models. When examining\nprecision in binary classification, the proposed models perform on average 12%\nhigher than the baselines.Our approach contributes to annotation-free and\neffective lyrics morality learning, and provides useful insights into the\nknowledge distillation of LLMs regarding moral expression in music, and the\npotential impact of these technologies on the creative industries and musical\nculture.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18787v1.pdf",
        "similarity": 0.36987011031897726,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Exploring Description-Augmented Dataless Intent Classification",
        "new_link": "http://arxiv.org/abs/2407.17862v1",
        "new_summary": "  In this work, we introduce several schemes to leverage description-augmented\nembedding similarity for dataless intent classification using current\nstate-of-the-art (SOTA) text embedding models. We report results of our methods\non four commonly used intent classification datasets and compare against\nprevious works of a similar nature. Our work shows promising results for\ndataless classification scaling to a large number of unseen intents. We show\ncompetitive results and significant improvements (+6.12\\% Avg.) over strong\nzero-shot baselines, all without training on labelled or task-specific data.\nFurthermore, we provide qualitative error analysis of the shortfalls of this\nmethodology to help guide future research in this area.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.17862v1.pdf",
        "similarity": 0.3655392865480664,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-25"
    },
    {
        "new_title": "Robustness of LLMs to Perturbations in Text",
        "new_link": "http://arxiv.org/abs/2407.08989v1",
        "new_summary": "  Having a clean dataset has been the foundational assumption of most natural\nlanguage processing (NLP) systems. However, properly written text is rarely\nfound in real-world scenarios and hence, oftentimes invalidates the\naforementioned foundational assumption. Recently, Large language models (LLMs)\nhave shown impressive performance, but can they handle the inevitable noise in\nreal-world data? This work tackles this critical question by investigating\nLLMs' resilience against morphological variations in text. To that end, we\nartificially introduce varying levels of noise into a diverse set of datasets\nand systematically evaluate LLMs' robustness against the corrupt variations of\nthe original text. Our findings show that contrary to popular beliefs,\ngenerative LLMs are quiet robust to noisy perturbations in text. This is a\ndeparture from pre-trained models like BERT or RoBERTa whose performance has\nbeen shown to be sensitive to deteriorating noisy text. Additionally, we test\nLLMs' resilience on multiple real-world benchmarks that closely mimic commonly\nfound errors in the wild. With minimal prompting, LLMs achieve a new\nstate-of-the-art on the benchmark tasks of Grammar Error Correction (GEC) and\nLexical Semantic Change (LSC). To empower future research, we also release a\ndataset annotated by humans stating their preference for LLM vs.\nhuman-corrected outputs along with the code to reproduce our results.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.08989v1.pdf",
        "similarity": 0.3635654030182605,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-12"
    },
    {
        "new_title": "Deep learning for predicting the occurrence of tipping points",
        "new_link": "http://arxiv.org/abs/2407.18693v1",
        "new_summary": "  Tipping points occur in many real-world systems, at which the system shifts\nsuddenly from one state to another. The ability to predict the occurrence of\ntipping points from time series data remains an outstanding challenge and a\nmajor interest in a broad range of research fields. Particularly, the widely\nused methods based on bifurcation theory are neither reliable in prediction\naccuracy nor applicable for irregularly-sampled time series which are commonly\nobserved from real-world systems. Here we address this challenge by developing\na deep learning algorithm for predicting the occurrence of tipping points in\nuntrained systems, by exploiting information about normal forms. Our algorithm\nnot only outperforms traditional methods for regularly-sampled model time\nseries but also achieves accurate predictions for irregularly-sampled model\ntime series and empirical time series. Our ability to predict tipping points\nfor complex systems paves the way for mitigation risks, prevention of\ncatastrophic failures, and restoration of degraded systems, with broad\napplications in social science, engineering, and biology.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18693v1.pdf",
        "similarity": 0.3621396398478134,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "R-SFLLM: Jamming Resilient Framework for Split Federated Learning with\n  Large Language Models",
        "new_link": "http://arxiv.org/abs/2407.11654v1",
        "new_summary": "  Split federated learning (SFL) is a compute-efficient paradigm in distributed\nmachine learning (ML), where components of large ML models are outsourced to\nremote servers. A significant challenge in SFL, particularly when deployed over\nwireless channels, is the susceptibility of transmitted model parameters to\nadversarial jamming that could jeopardize the learning process. This is\nparticularly pronounced for word embedding parameters in large language models\n(LLMs), which are crucial for language understanding. In this paper, rigorous\ninsights are provided into the influence of jamming LLM word embeddings in SFL\nby deriving an expression for the ML training loss divergence and showing that\nit is upper-bounded by the mean squared error (MSE). Based on this analysis, a\nphysical layer framework is developed for resilient SFL with LLMs (R-SFLLM)\nover wireless networks. R-SFLLM leverages wireless sensing data to gather\ninformation on the jamming directions-of-arrival (DoAs) for the purpose of\ndevising a novel, sensing-assisted anti-jamming strategy while jointly\noptimizing beamforming, user scheduling, and resource allocation. Extensive\nexperiments using BERT and RoBERTa models demonstrate R-SFLLM's effectiveness,\nachieving close-to-baseline performance across various natural language\nprocessing (NLP) tasks and datasets. The proposed methodology further\nintroduces an adversarial training component, where controlled noise exposure\nsignificantly enhances the LLM's resilience to perturbed parameters during\ntraining. The results show that more noise-sensitive models, such as RoBERTa,\nbenefit from this feature, especially when resource allocation is unfair. It is\nalso shown that worst-case jamming in particular translates into worst-case\nmodel outcomes, thereby necessitating the need for jamming-resilient SFL\nprotocols.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.11654v1.pdf",
        "similarity": 0.3615947979273669,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-16"
    },
    {
        "new_title": "CVA Sensitivities, Hedging and Risk",
        "new_link": "http://arxiv.org/abs/2407.18583v1",
        "new_summary": "  We present a unified framework for computing CVA sensitivities, hedging the\nCVA, and assessing CVA risk, using probabilistic machine learning meant as\nrefined regression tools on simulated data, validatable by low-cost companion\nMonte Carlo procedures. Various notions of sensitivities are introduced and\nbenchmarked numerically. We identify the sensitivities representing the best\npractical trade-offs in downstream tasks including CVA hedging and risk\nassessment.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18583v1.pdf",
        "similarity": 0.36124257083305006,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Text-to-Battery Recipe: A language modeling-based protocol for automatic\n  battery recipe extraction and retrieval",
        "new_link": "http://arxiv.org/abs/2407.15459v1",
        "new_summary": "  Recent studies have increasingly applied natural language processing (NLP) to\nautomatically extract experimental research data from the extensive battery\nmaterials literature. Despite the complex process involved in battery\nmanufacturing -- from material synthesis to cell assembly -- there has been no\ncomprehensive study systematically organizing this information. In response, we\npropose a language modeling-based protocol, Text-to-Battery Recipe (T2BR), for\nthe automatic extraction of end-to-end battery recipes, validated using a case\nstudy on batteries containing LiFePO4 cathode material. We report machine\nlearning-based paper filtering models, screening 2,174 relevant papers from the\nkeyword-based search results, and unsupervised topic models to identify 2,876\nparagraphs related to cathode synthesis and 2,958 paragraphs related to cell\nassembly. Then, focusing on the two topics, two deep learning-based named\nentity recognition models are developed to extract a total of 30 entities --\nincluding precursors, active materials, and synthesis methods -- achieving F1\nscores of 88.18% and 94.61%. The accurate extraction of entities enables the\nsystematic generation of 165 end-toend recipes of LiFePO4 batteries. Our\nprotocol and results offer valuable insights into specific trends, such as\nassociations between precursor materials and synthesis methods, or combinations\nbetween different precursor materials. We anticipate that our findings will\nserve as a foundational knowledge base for facilitating battery-recipe\ninformation retrieval. The proposed protocol will significantly accelerate the\nreview of battery material literature and catalyze innovations in battery\ndesign and development.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.15459v1.pdf",
        "similarity": 0.36074259383563817,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-22"
    },
    {
        "new_title": "Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended\n  Text Generation",
        "new_link": "http://arxiv.org/abs/2407.18698v1",
        "new_summary": "  Decoding from the output distributions of large language models to produce\nhigh-quality text is a complex challenge in language modeling. Various\napproaches, such as beam search, sampling with temperature, $k-$sampling,\nnucleus $p-$sampling, typical decoding, contrastive decoding, and contrastive\nsearch, have been proposed to address this problem, aiming to improve\ncoherence, diversity, as well as resemblance to human-generated text. In this\nstudy, we introduce adaptive contrastive search, a novel decoding strategy\nextending contrastive search by incorporating an adaptive degeneration penalty,\nguided by the estimated uncertainty of the model at each generation step. This\nstrategy is designed to enhance both the creativity and diversity of the\nlanguage modeling process while at the same time producing coherent and\nhigh-quality generated text output. Our findings indicate performance\nenhancement in both aspects, across different model architectures and datasets,\nunderscoring the effectiveness of our method in text generation tasks. Our code\nbase, datasets, and models are publicly available.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18698v1.pdf",
        "similarity": 0.36071406523055266,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Knowledge Graph Structure as Prompt: Improving Small Language Models\n  Capabilities for Knowledge-based Causal Discovery",
        "new_link": "http://arxiv.org/abs/2407.18752v1",
        "new_summary": "  Causal discovery aims to estimate causal structures among variables based on\nobservational data. Large Language Models (LLMs) offer a fresh perspective to\ntackle the causal discovery problem by reasoning on the metadata associated\nwith variables rather than their actual data values, an approach referred to as\nknowledge-based causal discovery. In this paper, we investigate the\ncapabilities of Small Language Models (SLMs, defined as LLMs with fewer than 1\nbillion parameters) with prompt-based learning for knowledge-based causal\ndiscovery. Specifically, we present KG Structure as Prompt, a novel approach\nfor integrating structural information from a knowledge graph, such as common\nneighbor nodes and metapaths, into prompt-based learning to enhance the\ncapabilities of SLMs. Experimental results on three types of biomedical and\nopen-domain datasets under few-shot settings demonstrate the effectiveness of\nour approach, surpassing most baselines and even conventional fine-tuning\napproaches trained on full datasets. Our findings further highlight the strong\ncapabilities of SLMs: in combination with knowledge graphs and prompt-based\nlearning, SLMs demonstrate the potential to surpass LLMs with larger number of\nparameters. Our code and datasets are available on GitHub.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18752v1.pdf",
        "similarity": 0.3591090887826828,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "The Foundations of Tokenization: Statistical and Computational Concerns",
        "new_link": "http://arxiv.org/abs/2407.11606v1",
        "new_summary": "  Tokenization - the practice of converting strings of characters over an\nalphabet into sequences of tokens over a vocabulary - is a critical yet\nunder-theorized step in the NLP pipeline. Notably, it remains the only major\nstep not fully integrated into widely used end-to-end neural models. This paper\naims to address this theoretical gap by laying the foundations of tokenization\nfrom a formal perspective. By articulating and extending basic properties about\nthe category of stochastic maps, we propose a unified framework for\nrepresenting and analyzing tokenizer models. This framework allows us to\nestablish general conditions for the use of tokenizers. In particular, we\nformally establish the necessary and sufficient conditions for a tokenizer\nmodel to preserve the consistency of statistical estimators. Additionally, we\ndiscuss statistical and computational concerns crucial for the design and\nimplementation of tokenizer models. The framework and results advanced in this\npaper represent a step toward a robust theoretical foundation for neural\nlanguage modeling.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.11606v1.pdf",
        "similarity": 0.3577262179108841,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-16"
    },
    {
        "new_title": "TVR-Ranking: A Dataset for Ranked Video Moment Retrieval with Imprecise\n  Queries",
        "new_link": "http://arxiv.org/abs/2407.06597v2",
        "new_summary": "  In this paper, we propose the task of \\textit{Ranked Video Moment Retrieval}\n(RVMR) to locate a ranked list of matching moments from a collection of videos,\nthrough queries in natural language. Although a few related tasks have been\nproposed and studied by CV, NLP, and IR communities, RVMR is the task that best\nreflects the practical setting of moment search. To facilitate research in\nRVMR, we develop the TVR-Ranking dataset, based on the raw videos and existing\nmoment annotations provided in the TVR dataset. Our key contribution is the\nmanual annotation of relevance levels for 94,442 query-moment pairs. We then\ndevelop the $NDCG@K, IoU\\geq \\mu$ evaluation metric for this new task and\nconduct experiments to evaluate three baseline models. Our experiments show\nthat the new RVMR task brings new challenges to existing models and we believe\nthis new dataset contributes to the research on multi-modality search. The\ndataset is available at \\url{https://github.com/Ranking-VMR/TVR-Ranking}\n",
        "pdf_link": "https://arxiv.org/pdf/2407.06597v2.pdf",
        "similarity": 0.35673152880363806,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-09"
    },
    {
        "new_title": "Generative Adversarial Networks for Imputing Sparse Learning Performance",
        "new_link": "http://arxiv.org/abs/2407.18875v1",
        "new_summary": "  Learning performance data, such as correct or incorrect responses to\nquestions in Intelligent Tutoring Systems (ITSs) is crucial for tracking and\nassessing the learners' progress and mastery of knowledge. However, the issue\nof data sparsity, characterized by unexplored questions and missing attempts,\nhampers accurate assessment and the provision of tailored, personalized\ninstruction within ITSs. This paper proposes using the Generative Adversarial\nImputation Networks (GAIN) framework to impute sparse learning performance\ndata, reconstructed into a three-dimensional (3D) tensor representation across\nthe dimensions of learners, questions and attempts. Our customized GAIN-based\nmethod computational process imputes sparse data in a 3D tensor space,\nsignificantly enhanced by convolutional neural networks for its input and\noutput layers. This adaptation also includes the use of a least squares loss\nfunction for optimization and aligns the shapes of the input and output with\nthe dimensions of the questions-attempts matrices along the learners'\ndimension. Through extensive experiments on six datasets from various ITSs,\nincluding AutoTutor, ASSISTments and MATHia, we demonstrate that the GAIN\napproach generally outperforms existing methods such as tensor factorization\nand other generative adversarial network (GAN) based approaches in terms of\nimputation accuracy. This finding enhances comprehensive learning data modeling\nand analytics in AI-based education.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18875v1.pdf",
        "similarity": 0.3564937775228912,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Label Alignment and Reassignment with Generalist Large Language Model\n  for Enhanced Cross-Domain Named Entity Recognition",
        "new_link": "http://arxiv.org/abs/2407.17344v1",
        "new_summary": "  Named entity recognition on the in-domain supervised and few-shot settings\nhave been extensively discussed in the NLP community and made significant\nprogress. However, cross-domain NER, a more common task in practical scenarios,\nstill poses a challenge for most NER methods. Previous research efforts in that\narea primarily focus on knowledge transfer such as correlate label information\nfrom source to target domains but few works pay attention to the problem of\nlabel conflict. In this study, we introduce a label alignment and reassignment\napproach, namely LAR, to address this issue for enhanced cross-domain named\nentity recognition, which includes two core procedures: label alignment between\nsource and target domains and label reassignment for type inference. The\nprocess of label reassignment can significantly be enhanced by integrating with\nan advanced large-scale language model such as ChatGPT. We conduct an extensive\nrange of experiments on NER datasets involving both supervised and zero-shot\nscenarios. Empirical experimental results demonstrate the validation of our\nmethod with remarkable performance under the supervised and zero-shot\nout-of-domain settings compared to SOTA methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.17344v1.pdf",
        "similarity": 0.3562727607136421,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-24"
    },
    {
        "new_title": "TAGCOS: Task-agnostic Gradient Clustered Coreset Selection for\n  Instruction Tuning Data",
        "new_link": "http://arxiv.org/abs/2407.15235v1",
        "new_summary": "  Instruction tuning has achieved unprecedented success in NLP, turning large\nlanguage models into versatile chatbots. However, the increasing variety and\nvolume of instruction datasets demand significant computational resources. To\naddress this, it is essential to extract a small and highly informative subset\n(i.e., Coreset) that achieves comparable performance to the full dataset.\nAchieving this goal poses non-trivial challenges: 1) data selection requires\naccurate data representations that reflect the training samples' quality, 2)\nconsidering the diverse nature of instruction datasets, and 3) ensuring the\nefficiency of the coreset selection algorithm for large models. To address\nthese challenges, we propose Task-Agnostic Gradient Clustered COreset Selection\n(TAGCOS). Specifically, we leverage sample gradients as the data\nrepresentations, perform clustering to group similar data, and apply an\nefficient greedy algorithm for coreset selection. Experimental results show\nthat our algorithm, selecting only 5% of the data, surpasses other unsupervised\nmethods and achieves performance close to that of the full dataset.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.15235v1.pdf",
        "similarity": 0.3558061402838068,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-21"
    },
    {
        "new_title": "Learning Chaotic Systems and Long-Term Predictions with Neural Jump ODEs",
        "new_link": "http://arxiv.org/abs/2407.18808v1",
        "new_summary": "  The Path-dependent Neural Jump ODE (PD-NJ-ODE) is a model for online\nprediction of generic (possibly non-Markovian) stochastic processes with\nirregular (in time) and potentially incomplete (with respect to coordinates)\nobservations. It is a model for which convergence to the $L^2$-optimal\npredictor, which is given by the conditional expectation, is established\ntheoretically. Thereby, the training of the model is solely based on a dataset\nof realizations of the underlying stochastic process, without the need of\nknowledge of the law of the process. In the case where the underlying process\nis deterministic, the conditional expectation coincides with the process\nitself. Therefore, this framework can equivalently be used to learn the\ndynamics of ODE or PDE systems solely from realizations of the dynamical system\nwith different initial conditions. We showcase the potential of our method by\napplying it to the chaotic system of a double pendulum. When training the\nstandard PD-NJ-ODE method, we see that the prediction starts to diverge from\nthe true path after about half of the evaluation time. In this work we enhance\nthe model with two novel ideas, which independently of each other improve the\nperformance of our modelling setup. The resulting dynamics match the true\ndynamics of the chaotic system very closely. The same enhancements can be used\nto provably enable the PD-NJ-ODE to learn long-term predictions for general\nstochastic datasets, where the standard model fails. This is verified in\nseveral experiments.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18808v1.pdf",
        "similarity": 0.35528203186642576,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "ARTEMIS: A Mixed Analog-Stochastic In-DRAM Accelerator for Transformer\n  Neural Networks",
        "new_link": "http://arxiv.org/abs/2407.12638v1",
        "new_summary": "  Transformers have emerged as a powerful tool for natural language processing\n(NLP) and computer vision. Through the attention mechanism, these models have\nexhibited remarkable performance gains when compared to conventional approaches\nlike recurrent neural networks (RNNs) and convolutional neural networks (CNNs).\nNevertheless, transformers typically demand substantial execution time due to\ntheir extensive computations and large memory footprint. Processing in-memory\n(PIM) and near-memory computing (NMC) are promising solutions to accelerating\ntransformers as they offer high compute parallelism and memory bandwidth.\nHowever, designing PIM/NMC architectures to support the complex operations and\nmassive amounts of data that need to be moved between layers in transformer\nneural networks remains a challenge. We propose ARTEMIS, a mixed\nanalog-stochastic in-DRAM accelerator for transformer models. Through employing\nminimal changes to the conventional DRAM arrays, ARTEMIS efficiently alleviates\nthe costs associated with transformer model execution by supporting stochastic\ncomputing for multiplications and temporal analog accumulations using a novel\nin-DRAM metal-on-metal capacitor. Our analysis indicates that ARTEMIS exhibits\nat least 3.0x speedup, 1.8x lower energy, and 1.9x better energy efficiency\ncompared to GPU, TPU, CPU, and state-of-the-art PIM transformer hardware\naccelerators.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.12638v1.pdf",
        "similarity": 0.35500477736972014,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-17"
    },
    {
        "new_title": "Hybrid summary statistics: neural weak lensing inference beyond the\n  power spectrum",
        "new_link": "http://arxiv.org/abs/2407.18909v1",
        "new_summary": "  In inference problems, we often have domain knowledge which allows us to\ndefine summary statistics that capture most of the information content in a\ndataset. In this paper, we present a hybrid approach, where such physics-based\nsummaries are augmented by a set of compressed neural summary statistics that\nare optimised to extract the extra information that is not captured by the\npredefined summaries. The resulting statistics are very powerful inputs to\nsimulation-based or implicit inference of model parameters. We apply this\ngeneralisation of Information Maximising Neural Networks (IMNNs) to parameter\nconstraints from tomographic weak gravitational lensing convergence maps to\nfind summary statistics that are explicitly optimised to complement angular\npower spectrum estimates. We study several dark matter simulation resolutions\nin low- and high-noise regimes. We show that i) the information-update\nformalism extracts at least $3\\times$ and up to $8\\times$ as much information\nas the angular power spectrum in all noise regimes, ii) the network summaries\nare highly complementary to existing 2-point summaries, and iii) our formalism\nallows for networks with smaller, physically-informed architectures to match\nmuch larger regression networks with far fewer simulations needed to obtain\nasymptotically optimal inference.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18909v1.pdf",
        "similarity": 0.3543187834354869,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "LookupForensics: A Large-Scale Multi-Task Dataset for Multi-Phase\n  Image-Based Fact Verification",
        "new_link": "http://arxiv.org/abs/2407.18614v1",
        "new_summary": "  Amid the proliferation of forged images, notably the tsunami of deepfake\ncontent, extensive research has been conducted on using artificial intelligence\n(AI) to identify forged content in the face of continuing advancements in\ncounterfeiting technologies. We have investigated the use of AI to provide the\noriginal authentic image after deepfake detection, which we believe is a\nreliable and persuasive solution. We call this \"image-based automated fact\nverification,\" a name that originated from a text-based fact-checking system\nused by journalists. We have developed a two-phase open framework that\nintegrates detection and retrieval components. Additionally, inspired by a\ndataset proposed by Meta Fundamental AI Research, we further constructed a\nlarge-scale dataset that is specifically designed for this task. This dataset\nsimulates real-world conditions and includes both content-preserving and\ncontent-aware manipulations that present a range of difficulty levels and have\npotential for ongoing research. This multi-task dataset is fully annotated,\nenabling it to be utilized for sub-tasks within the forgery identification and\nfact retrieval domains. This paper makes two main contributions: (1) We\nintroduce a new task, \"image-based automated fact verification,\" and present a\nnovel two-phase open framework combining \"forgery identification\" and \"fact\nretrieval.\" (2) We present a large-scale dataset tailored for this new task\nthat features various hand-crafted image edits and machine learning-driven\nmanipulations, with extensive annotations suitable for various sub-tasks.\nExtensive experimental results validate its practicality for fact verification\nresearch and clarify its difficulty levels for various sub-tasks.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18614v1.pdf",
        "similarity": 0.35305012409989006,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Small Molecule Optimization with Large Language Models",
        "new_link": "http://arxiv.org/abs/2407.18897v1",
        "new_summary": "  Recent advancements in large language models have opened new possibilities\nfor generative molecular drug design. We present Chemlactica and Chemma, two\nlanguage models fine-tuned on a novel corpus of 110M molecules with computed\nproperties, totaling 40B tokens. These models demonstrate strong performance in\ngenerating molecules with specified properties and predicting new molecular\ncharacteristics from limited samples. We introduce a novel optimization\nalgorithm that leverages our language models to optimize molecules for\narbitrary properties given limited access to a black box oracle. Our approach\ncombines ideas from genetic algorithms, rejection sampling, and prompt\noptimization. It achieves state-of-the-art performance on multiple molecular\noptimization benchmarks, including an 8% improvement on Practical Molecular\nOptimization compared to previous methods. We publicly release the training\ncorpus, the language models and the optimization algorithm.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18897v1.pdf",
        "similarity": 0.35043682066084625,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Dilated Strip Attention Network for Image Restoration",
        "new_link": "http://arxiv.org/abs/2407.18613v1",
        "new_summary": "  Image restoration is a long-standing task that seeks to recover the latent\nsharp image from its deteriorated counterpart. Due to the robust capacity of\nself-attention to capture long-range dependencies, transformer-based methods or\nsome attention-based convolutional neural networks have demonstrated promising\nresults on many image restoration tasks in recent years. However, existing\nattention modules encounters limited receptive fields or abundant parameters.\nIn order to integrate contextual information more effectively and efficiently,\nin this paper, we propose a dilated strip attention network (DSAN) for image\nrestoration. Specifically, to gather more contextual information for each pixel\nfrom its neighboring pixels in the same row or column, a dilated strip\nattention (DSA) mechanism is elaborately proposed. By employing the DSA\noperation horizontally and vertically, each location can harvest the contextual\ninformation from a much wider region. In addition, we utilize multi-scale\nreceptive fields across different feature groups in DSA to improve\nrepresentation learning. Extensive experiments show that our DSAN outperforms\nstate-of-the-art algorithms on several image restoration tasks.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18613v1.pdf",
        "similarity": 0.34948928751251784,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Aspects of importance sampling in parameter selection for neural\n  networks using ridgelet transform",
        "new_link": "http://arxiv.org/abs/2407.18655v1",
        "new_summary": "  The choice of parameters in neural networks is crucial in the performance,\nand an oracle distribution derived from the ridgelet transform enables us to\nobtain suitable initial parameters. In other words, the distribution of\nparameters is connected to the integral representation of target functions. The\noracle distribution allows us to avoid the conventional backpropagation\nlearning process; only a linear regression is enough to construct the neural\nnetwork in simple cases. This study provides a new look at the oracle\ndistributions and ridgelet transforms, i.e., an aspect of importance sampling.\nIn addition, we propose extensions of the parameter sampling methods. We\ndemonstrate the aspect of importance sampling and the proposed sampling\nalgorithms via one-dimensional and high-dimensional examples; the results imply\nthat the magnitude of weight parameters could be more crucial than the\nintercept parameters.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18655v1.pdf",
        "similarity": 0.34882540418604496,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Unsupervised Reservoir Computing for Multivariate Denoising of Severely\n  Contaminated Signals",
        "new_link": "http://arxiv.org/abs/2407.18759v1",
        "new_summary": "  The interdependence and high dimensionality of multivariate signals present\nsignificant challenges for denoising, as conventional univariate methods often\nstruggle to capture the complex interactions between variables. A successful\napproach must consider not only the multivariate dependencies of the desired\nsignal but also the multivariate dependencies of the interfering noise. In our\nprevious research, we introduced a method using machine learning to extract the\nmaximum portion of ``predictable information\" from univariate signal. We extend\nthis approach to multivariate signals, with the key idea being to properly\nincorporate the interdependencies of the noise back into the interdependent\nreconstruction of the signal. The method works successfully for various\nmultivariate signals, including chaotic signals and highly oscillating\nsinusoidal signals which are corrupted by spatially correlated intensive noise.\nIt consistently outperforms other existing multivariate denoising methods\nacross a wide range of scenarios.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18759v1.pdf",
        "similarity": 0.3481604186737275,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Reinforcement Learning for Sustainable Energy: A Survey",
        "new_link": "http://arxiv.org/abs/2407.18597v1",
        "new_summary": "  The transition to sustainable energy is a key challenge of our time,\nrequiring modifications in the entire pipeline of energy production, storage,\ntransmission, and consumption. At every stage, new sequential decision-making\nchallenges emerge, ranging from the operation of wind farms to the management\nof electrical grids or the scheduling of electric vehicle charging stations.\nAll such problems are well suited for reinforcement learning, the branch of\nmachine learning that learns behavior from data. Therefore, numerous studies\nhave explored the use of reinforcement learning for sustainable energy. This\npaper surveys this literature with the intention of bridging both the\nunderlying research communities: energy and machine learning. After a brief\nintroduction of both fields, we systematically list relevant sustainability\nchallenges, how they can be modeled as a reinforcement learning problem, and\nwhat solution approaches currently exist in the literature. Afterwards, we zoom\nout and identify overarching reinforcement learning themes that appear\nthroughout sustainability, such as multi-agent, offline, and safe reinforcement\nlearning. Lastly, we also cover standardization of environments, which will be\ncrucial for connecting both research fields, and highlight potential directions\nfor future work. In summary, this survey provides an extensive overview of\nreinforcement learning methods for sustainable energy, which may play a vital\nrole in the energy transition.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18597v1.pdf",
        "similarity": 0.3471451585537606,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Do We Really Need Graph Convolution During Training? Light Post-Training\n  Graph-ODE for Efficient Recommendation",
        "new_link": "http://arxiv.org/abs/2407.18910v1",
        "new_summary": "  The efficiency and scalability of graph convolution networks (GCNs) in\ntraining recommender systems (RecSys) have been persistent concerns, hindering\ntheir deployment in real-world applications. This paper presents a critical\nexamination of the necessity of graph convolutions during the training phase\nand introduces an innovative alternative: the Light Post-Training Graph\nOrdinary-Differential-Equation (LightGODE). Our investigation reveals that the\nbenefits of GCNs are more pronounced during testing rather than training.\nMotivated by this, LightGODE utilizes a novel post-training graph convolution\nmethod that bypasses the computation-intensive message passing of GCNs and\nemploys a non-parametric continuous graph ordinary-differential-equation (ODE)\nto dynamically model node representations. This approach drastically reduces\ntraining time while achieving fine-grained post-training graph convolution to\navoid the distortion of the original training embedding space, termed the\nembedding discrepancy issue. We validate our model across several real-world\ndatasets of different scales, demonstrating that LightGODE not only outperforms\nGCN-based models in terms of efficiency and effectiveness but also\nsignificantly mitigates the embedding discrepancy commonly associated with\ndeeper graph convolution layers. Our LightGODE challenges the prevailing\nparadigms in RecSys training and suggests re-evaluating the role of graph\nconvolutions, potentially guiding future developments of efficient large-scale\ngraph-based RecSys.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18910v1.pdf",
        "similarity": 0.34602929917599073,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Vulnerability Detection in Ethereum Smart Contracts via Machine\n  Learning: A Qualitative Analysis",
        "new_link": "http://arxiv.org/abs/2407.18639v1",
        "new_summary": "  Smart contracts are central to a myriad of critical blockchain applications,\nfrom financial transactions to supply chain management. However, their adoption\nis hindered by security vulnerabilities that can result in significant\nfinancial losses. Most vulnerability detection tools and methods available\nnowadays leverage either static analysis methods or machine learning.\nUnfortunately, as valuable as they are, both approaches suffer from limitations\nthat make them only partially effective. In this survey, we analyze the state\nof the art in machine-learning vulnerability detection for Ethereum smart\ncontracts, by categorizing existing tools and methodologies, evaluating them,\nand highlighting their limitations. Our critical assessment unveils issues such\nas restricted vulnerability coverage and dataset construction flaws, providing\nus with new metrics to overcome the difficulties that restrain a sound\ncomparison of existing solutions. Driven by our findings, we discuss best\npractices to enhance the accuracy, scope, and efficiency of vulnerability\ndetection in smart contracts. Our guidelines address the known flaws while at\nthe same time opening new avenues for research and development. By shedding\nlight on current challenges and offering novel directions for improvement, we\ncontribute to the advancement of secure smart contract development and\nblockchain technology as a whole.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18639v1.pdf",
        "similarity": 0.3459496234954802,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Induction Heads as an Essential Mechanism for Pattern Matching in\n  In-context Learning",
        "new_link": "http://arxiv.org/abs/2407.07011v1",
        "new_summary": "  Large language models (LLMs) have shown a remarkable ability to learn and\nperform complex tasks through in-context learning (ICL). However, a\ncomprehensive understanding of its internal mechanisms is still lacking. This\npaper explores the role of induction heads in a few-shot ICL setting. We\nanalyse two state-of-the-art models, Llama-3-8B and InternLM2-20B on abstract\npattern recognition and NLP tasks. Our results show that even a minimal\nablation of induction heads leads to ICL performance decreases of up to ~32%\nfor abstract pattern recognition tasks, bringing the performance close to\nrandom. For NLP tasks, this ablation substantially decreases the model's\nability to benefit from examples, bringing few-shot ICL performance close to\nthat of zero-shot prompts. We further use attention knockout to disable\nspecific induction patterns, and present fine-grained evidence for the role\nthat the induction mechanism plays in ICL.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.07011v1.pdf",
        "similarity": 0.34557042901682156,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-09"
    },
    {
        "new_title": "Denoising L\u00e9vy Probabilistic Models",
        "new_link": "http://arxiv.org/abs/2407.18609v1",
        "new_summary": "  Investigating noise distribution beyond Gaussian in diffusion generative\nmodels is an open problem. The Gaussian case has seen success experimentally\nand theoretically, fitting a unified SDE framework for score-based and\ndenoising formulations. Recent studies suggest heavy-tailed noise distributions\ncan address mode collapse and manage datasets with class imbalance, heavy\ntails, or outliers. Yoon et al. (NeurIPS 2023) introduced the L\\'evy-Ito model\n(LIM), extending the SDE framework to heavy-tailed SDEs with $\\alpha$-stable\nnoise. Despite its theoretical elegance and performance gains, LIM's complex\nmathematics may limit its accessibility and broader adoption. This study takes\na simpler approach by extending the denoising diffusion probabilistic model\n(DDPM) with $\\alpha$-stable noise, creating the denoising L\\'evy probabilistic\nmodel (DLPM). Using elementary proof techniques, we show DLPM reduces to\nrunning vanilla DDPM with minimal changes, allowing the use of existing\nimplementations with minimal changes. DLPM and LIM have different training\nalgorithms and, unlike the Gaussian case, they admit different backward\nprocesses and sampling algorithms. Our experiments demonstrate that DLPM\nachieves better coverage of data distribution tail, improved generation of\nunbalanced datasets, and faster computation times with fewer backward steps.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18609v1.pdf",
        "similarity": 0.345161243194924,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Beyond Generative Artificial Intelligence: Roadmap for Natural Language\n  Generation",
        "new_link": "http://arxiv.org/abs/2407.10554v1",
        "new_summary": "  Generative Artificial Intelligence has grown exponentially as a result of\nLarge Language Models (LLMs). This has been possible because of the impressive\nperformance of deep learning methods created within the field of Natural\nLanguage Processing (NLP) and its subfield Natural Language Generation (NLG),\nwhich is the focus of this paper. Within the growing LLM family are the popular\nGPT-4, Bard and more specifically, tools such as ChatGPT have become a\nbenchmark for other LLMs when solving most of the tasks involved in NLG\nresearch. This scenario poses new questions about the next steps for NLG and\nhow the field can adapt and evolve to deal with new challenges in the era of\nLLMs. To address this, the present paper conducts a review of a representative\nsample of surveys recently published in NLG. By doing so, we aim to provide the\nscientific community with a research roadmap to identify which NLG aspects are\nstill not suitably addressed by LLMs, as well as suggest future lines of\nresearch that should be addressed going forward.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.10554v1.pdf",
        "similarity": 0.3424697990782393,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-15"
    },
    {
        "new_title": "Exploring State Space and Reasoning by Elimination in Tsetlin Machines",
        "new_link": "http://arxiv.org/abs/2407.09162v2",
        "new_summary": "  The Tsetlin Machine (TM) has gained significant attention in Machine Learning\n(ML). By employing logical fundamentals, it facilitates pattern learning and\nrepresentation, offering an alternative approach for developing comprehensible\nArtificial Intelligence (AI) with a specific focus on pattern classification in\nthe form of conjunctive clauses. In the domain of Natural Language Processing\n(NLP), TM is utilised to construct word embedding and describe target words\nusing clauses. To enhance the descriptive capacity of these clauses, we study\nthe concept of Reasoning by Elimination (RbE) in clauses' formulation, which\ninvolves incorporating feature negations to provide a more comprehensive\nrepresentation. In more detail, this paper employs the Tsetlin Machine\nAuto-Encoder (TM-AE) architecture to generate dense word vectors, aiming at\ncapturing contextual information by extracting feature-dense vectors for a\ngiven vocabulary. Thereafter, the principle of RbE is explored to improve\ndescriptivity and optimise the performance of the TM. Specifically, the\nspecificity parameter s and the voting margin parameter T are leveraged to\nregulate feature distribution in the state space, resulting in a dense\nrepresentation of information for each clause. In addition, we investigate the\nstate spaces of TM-AE, especially for the forgotten/excluded features.\nEmpirical investigations on artificially generated data, the IMDB dataset, and\nthe 20 Newsgroups dataset showcase the robustness of the TM, with accuracy\nreaching 90.62\\% for the IMDB.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.09162v2.pdf",
        "similarity": 0.33420391132715466,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-12"
    },
    {
        "new_title": "QT-TDM: Planning with Transformer Dynamics Model and Autoregressive\n  Q-Learning",
        "new_link": "http://arxiv.org/abs/2407.18841v1",
        "new_summary": "  Inspired by the success of the Transformer architecture in natural language\nprocessing and computer vision, we investigate the use of Transformers in\nReinforcement Learning (RL), specifically in modeling the environment's\ndynamics using Transformer Dynamics Models (TDMs). We evaluate the capabilities\nof TDMs for continuous control in real-time planning scenarios with Model\nPredictive Control (MPC). While Transformers excel in long-horizon prediction,\ntheir tokenization mechanism and autoregressive nature lead to costly planning\nover long horizons, especially as the environment's dimensionality increases.\nTo alleviate this issue, we use a TDM for short-term planning, and learn an\nautoregressive discrete Q-function using a separate Q-Transformer (QT) model to\nestimate a long-term return beyond the short-horizon planning. Our proposed\nmethod, QT-TDM, integrates the robust predictive capabilities of Transformers\nas dynamics models with the efficacy of a model-free Q-Transformer to mitigate\nthe computational burden associated with real-time planning. Experiments in\ndiverse state-based continuous control tasks show that QT-TDM is superior in\nperformance and sample efficiency compared to existing Transformer-based RL\nmodels while achieving fast and computationally efficient inference.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18841v1.pdf",
        "similarity": 0.33343160402849353,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Learning the Chaotic and Regular Nature of Trajectories in Hamiltonian\n  Systems with Lagrangian descriptors",
        "new_link": "http://arxiv.org/abs/2407.18831v1",
        "new_summary": "  In this paper, we explore the application of Machine Learning techniques,\nspecifically Support Vector Machines (SVM), to unveil the chaotic and regular\nnature of trajectories in Hamiltonian systems using Lagrangian descriptors.\nTraditional chaos indicators, while effective, are computationally expensive\nand require an exhaustive study of the parameter space to establish the\nclassification thresholds. By using SVMs trained on a dataset obtained from the\nanalysis of the dynamics of the double pendulum Hamiltonian system, we aim at\nreducing the complexity of this process. Our trained SVM models demonstrate\nhigh accuracy when it comes to classifying trajectories in diverse Hamiltonian\nsystems, such as for example in the four-well Hamiltonian, the H\\'enon-Heiles\nsystem and the Chirikov Standard Map. The results indicate that SVMs, when\ncombined with Lagrangian descriptors, offer a robust and efficient method for\nchaos classification across different dynamical systems. Our approach not only\nsimplifies the classification process but also is highlighting the potential of\nMachine Learning algorithms in the study of nonlinear dynamics and chaos.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18831v1.pdf",
        "similarity": 0.3310872167296282,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "RRO: A Regularized Routing Optimization Algorithm for Enhanced\n  Throughput and Low Latency with Efficient Complexity",
        "new_link": "http://arxiv.org/abs/2407.18683v1",
        "new_summary": "  In the rapidly evolving landscape of wireless networks, achieving enhanced\nthroughput with low latency for data transmission is crucial for future\ncommunication systems. While low complexity OSPF-type solutions have shown\neffectiveness in lightly-loaded networks, they often falter in the face of\nincreasing congestion. Recent approaches have suggested utilizing backpressure\nand deep learning techniques for route optimization. However, these approaches\nface challenges due to their high implementation and computational complexity,\nsurpassing the capabilities of networks with limited hardware devices.\n  A key challenge is developing algorithms that improve throughput and reduce\nlatency while keeping complexity levels compatible with OSPF. In this\ncollaborative research between Ben-Gurion University and Ceragon Networks Ltd.,\nwe address this challenge by developing a novel approach, dubbed Regularized\nRouting Optimization (RRO). The RRO algorithm offers both distributed and\ncentralized implementations with low complexity, making it suitable for\nintegration into 5G and beyond technologies, where no significant changes to\nthe existing protocols are needed. It increases throughput while ensuring\nlatency remains sufficiently low through regularized optimization. We analyze\nthe computational complexity of RRO and prove that it converges with a level of\ncomplexity comparable to OSPF. Extensive simulation results across diverse\nnetwork topologies demonstrate that RRO significantly outperforms existing\nmethods.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18683v1.pdf",
        "similarity": 0.3235324403360704,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18625",
        "published": "2024-07-26"
    },
    {
        "new_title": "Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better\n  Together",
        "new_link": "http://arxiv.org/abs/2407.10930v1",
        "new_summary": "  Natural Language Processing (NLP) systems are increasingly taking the form of\nmulti-stage pipelines involving multiple distinct language models (LMs) and\nprompting strategies. Here we address the question of how to fine-tune such\nsystems to improve their performance. We cast this as a problem of optimizing\nthe underlying LM weights and the prompting strategies together, and consider a\nchallenging but highly realistic scenario in which we have no gold labels for\nany intermediate stages in the pipeline. To address this challenge, we evaluate\napproximate optimization strategies in which we bootstrap training labels for\nall pipeline stages and use these to optimize the pipeline's prompts and\nfine-tune its weights alternatingly. In experiments with multi-hop QA,\nmathematical reasoning, and feature-based classification, we find that simple\napproaches for optimizing the prompts and weights together outperform directly\noptimizing weights alone and prompts alone by up to 65% and 5%, respectively,\non average across LMs and tasks. We will release our new optimizers in DSPy at\nhttp://dspy.ai\n",
        "pdf_link": "https://arxiv.org/pdf/2407.10930v1.pdf",
        "similarity": 0.32336675048552765,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-15"
    },
    {
        "new_title": "Adapting LLMs to Hebrew: Unveiling DictaLM 2.0 with Enhanced Vocabulary\n  and Instruction Capabilities",
        "new_link": "http://arxiv.org/abs/2407.07080v1",
        "new_summary": "  Training large language models (LLMs) in low-resource languages such as\nHebrew poses unique challenges. In this paper, we introduce DictaLM2.0 and\nDictaLM2.0-Instruct, two LLMs derived from the Mistral model, trained on a\nsubstantial corpus of approximately 200 billion tokens in both Hebrew and\nEnglish. Adapting a pre-trained model to a new language involves specialized\ntechniques that differ significantly from training a model from scratch or\nfurther training existing models on well-resourced languages such as English.\nWe outline these novel training methodologies, which facilitate effective\nlearning and adaptation to the linguistic properties of Hebrew. Additionally,\nwe fine-tuned DictaLM2.0-Instruct on a comprehensive instruct dataset to\nenhance its performance on task-specific instructions. To rigorously evaluate\nour models, we introduce a new benchmark suite for Hebrew LLM evaluation,\ncovering a diverse set of tasks including Question Answering, Sentiment\nAnalysis, Winograd Schema Challenge, Translation, and Summarization. Our work\nnot only addresses the intricacies of training LLMs in low-resource languages\nbut also proposes a framework that can be leveraged for adapting other LLMs to\nvarious non-English languages, contributing to the broader field of\nmultilingual NLP.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.07080v1.pdf",
        "similarity": 0.32309010648503395,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-09"
    },
    {
        "new_title": "AutoRDF2GML: Facilitating RDF Integration in Graph Machine Learning",
        "new_link": "http://arxiv.org/abs/2407.18735v1",
        "new_summary": "  In this paper, we introduce AutoRDF2GML, a framework designed to convert RDF\ndata into data representations tailored for graph machine learning tasks.\nAutoRDF2GML enables, for the first time, the creation of both content-based\nfeatures -- i.e., features based on RDF datatype properties -- and\ntopology-based features -- i.e., features based on RDF object properties.\nCharacterized by automated feature extraction, AutoRDF2GML makes it possible\neven for users less familiar with RDF and SPARQL to generate data\nrepresentations ready for graph machine learning tasks, such as link\nprediction, node classification, and graph classification. Furthermore, we\npresent four new benchmark datasets for graph machine learning, created from\nlarge RDF knowledge graphs using our framework. These datasets serve as\nvaluable resources for evaluating graph machine learning approaches, such as\ngraph neural networks. Overall, our framework effectively bridges the gap\nbetween the Graph Machine Learning and Semantic Web communities, paving the way\nfor RDF-based machine learning applications.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18735v1.pdf",
        "similarity": 0.32248520877479925,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "A Survey on Cell Nuclei Instance Segmentation and Classification:\n  Leveraging Context and Attention",
        "new_link": "http://arxiv.org/abs/2407.18673v1",
        "new_summary": "  Manually annotating nuclei from the gigapixel Hematoxylin and Eosin\n(H&E)-stained Whole Slide Images (WSIs) is a laborious and costly task, meaning\nautomated algorithms for cell nuclei instance segmentation and classification\ncould alleviate the workload of pathologists and clinical researchers and at\nthe same time facilitate the automatic extraction of clinically interpretable\nfeatures. But due to high intra- and inter-class variability of nuclei\nmorphological and chromatic features, as well as H&E-stains susceptibility to\nartefacts, state-of-the-art algorithms cannot correctly detect and classify\ninstances with the necessary performance. In this work, we hypothesise context\nand attention inductive biases in artificial neural networks (ANNs) could\nincrease the generalization of algorithms for cell nuclei instance segmentation\nand classification. We conduct a thorough survey on context and attention\nmethods for cell nuclei instance segmentation and classification from\nH&E-stained microscopy imaging, while providing a comprehensive discussion of\nthe challenges being tackled with context and attention. Besides, we illustrate\nsome limitations of current approaches and present ideas for future research.\nAs a case study, we extend both a general instance segmentation and\nclassification method (Mask-RCNN) and a tailored cell nuclei instance\nsegmentation and classification model (HoVer-Net) with context- and\nattention-based mechanisms, and do a comparative analysis on a multi-centre\ncolon nuclei identification and counting dataset. Although pathologists rely on\ncontext at multiple levels while paying attention to specific Regions of\nInterest (RoIs) when analysing and annotating WSIs, our findings suggest\ntranslating that domain knowledge into algorithm design is no trivial task, but\nto fully exploit these mechanisms, the scientific understanding of these\nmethods should be addressed.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18673v1.pdf",
        "similarity": 0.3223070448418059,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Log-Concave Coupling for Sampling Neural Net Posteriors",
        "new_link": "http://arxiv.org/abs/2407.18802v1",
        "new_summary": "  In this work, we present a sampling algorithm for single hidden layer neural\nnetworks. This algorithm is built upon a recursive series of Bayesian\nposteriors using a method we call Greedy Bayes. Sampling of the Bayesian\nposterior for neuron weight vectors $w$ of dimension $d$ is challenging because\nof its multimodality. Our algorithm to tackle this problem is based on a\ncoupling of the posterior density for $w$ with an auxiliary random variable\n$\\xi$.\n  The resulting reverse conditional $w|\\xi$ of neuron weights given auxiliary\nrandom variable is shown to be log concave. In the construction of the\nposterior distributions we provide some freedom in the choice of the prior. In\nparticular, for Gaussian priors on $w$ with suitably small variance, the\nresulting marginal density of the auxiliary variable $\\xi$ is proven to be\nstrictly log concave for all dimensions $d$. For a uniform prior on the unit\n$\\ell_1$ ball, evidence is given that the density of $\\xi$ is again strictly\nlog concave for sufficiently large $d$.\n  The score of the marginal density of the auxiliary random variable $\\xi$ is\ndetermined by an expectation over $w|\\xi$ and thus can be computed by various\nrapidly mixing Markov Chain Monte Carlo methods. Moreover, the computation of\nthe score of $\\xi$ permits methods of sampling $\\xi$ by a stochastic diffusion\n(Langevin dynamics) with drift function built from this score. With such\ndynamics, information-theoretic methods pioneered by Bakry and Emery show that\naccurate sampling of $\\xi$ is obtained rapidly when its density is indeed\nstrictly log-concave. After which, one more draw from $w|\\xi$, produces neuron\nweights $w$ whose marginal distribution is from the desired posterior.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18802v1.pdf",
        "similarity": 0.32083400247982946,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Collaborative Evolving Strategy for Automatic Data-Centric Development",
        "new_link": "http://arxiv.org/abs/2407.18690v1",
        "new_summary": "  Artificial Intelligence (AI) significantly influences many fields, largely\nthanks to the vast amounts of high-quality data for machine learning models.\nThe emphasis is now on a data-centric AI strategy, prioritizing data\ndevelopment over model design progress. Automating this process is crucial. In\nthis paper, we serve as the first work to introduce the automatic data-centric\ndevelopment (AD^2) task and outline its core challenges, which require\ndomain-experts-like task scheduling and implementation capability, largely\nunexplored by previous work.\n  By leveraging the strong complex problem-solving capabilities of large\nlanguage models (LLMs), we propose an LLM-based autonomous agent, equipped with\na strategy named Collaborative Knowledge-STudying-Enhanced Evolution by\nRetrieval (Co-STEER), to simultaneously address all the challenges.\nSpecifically, our proposed Co-STEER agent enriches its domain knowledge through\nour proposed evolving strategy and develops both its scheduling and\nimplementation skills by accumulating and retrieving domain-specific practical\nexperience. With an improved schedule, the capability for implementation\naccelerates. Simultaneously, as implementation feedback becomes more thorough,\nthe scheduling accuracy increases. These two capabilities evolve together\nthrough practical feedback, enabling a collaborative evolution process.\n  Extensive experimental results demonstrate that our Co-STEER agent breaks new\nground in AD^2 research, possesses strong evolvable schedule and implementation\nability, and demonstrates the significant effectiveness of its components. Our\nCo-STEER paves the way for AD^2 advancements.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18690v1.pdf",
        "similarity": 0.32078985799235976,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Designing and Implementing a Generator Framework for a SIMD Abstraction\n  Library",
        "new_link": "http://arxiv.org/abs/2407.18728v1",
        "new_summary": "  The Single Instruction Multiple Data (SIMD) parallel paradigm is a\nwell-established and heavily-used hardware-driven technique to increase the\nsingle-thread performance in different system domains such as database or\nmachine learning. Depending on the hardware vendor and the specific processor\ngeneration/version, SIMD capabilities come in different flavors concerning the\nregister size and the supported SIMD instructions. Due to this heterogeneity\nand the lack of standardized calling conventions, building high-performance and\nportable systems is a challenging task. To address this challenge, academia and\nindustry have invested a remarkable effort into creating SIMD abstraction\nlibraries that provide unified access to different SIMD hardware capabilities.\nHowever, those one-size-fits-all library approaches are inherently complex,\nwhich hampers maintainability and extensibility. Furthermore, they assume\nsimilar SIMD hardware designs, which may be invalidated through ARM SVE's\nemergence. Additionally, while existing SIMD abstraction libraries do a great\njob of hiding away the specifics of the underlying hardware, their lack of\nexpressiveness impedes crucial algorithm design decisions for system\ndevelopers. To overcome these limitations, we present TSLGen, a novel\nend-to-end framework approach for generating an SIMD abstraction library in\nthis paper. We have implemented our TSLGen framework and used our generated\nTemplate SIMD Library (TSL) to program various system components from different\ndomains. As we will show, the programming effort is comparable to existing\nlibraries, and we achieve the same performance results. However, our framework\nis easy to maintain and to extend, which simultaneously supports disruptive\nchanges to the interface by design and exposes valuable insights for assessing\nprovided functionality.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18728v1.pdf",
        "similarity": 0.3201370461675763,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18625",
        "published": "2024-07-26"
    },
    {
        "new_title": "Interpreting artificial neural networks to detect genome-wide\n  association signals for complex traits",
        "new_link": "http://arxiv.org/abs/2407.18811v1",
        "new_summary": "  Investigating the genetic architecture of complex diseases is challenging due\nto the highly polygenic and interactive landscape of genetic and environmental\nfactors. Although genome-wide association studies (GWAS) have identified\nthousands of variants for multiple complex phenotypes, conventional statistical\napproaches can be limited by simplified assumptions such as linearity and lack\nof epistasis models. In this work, we trained artificial neural networks for\npredicting complex traits using both simulated and real genotype/phenotype\ndatasets. We extracted feature importance scores via different post hoc\ninterpretability methods to identify potentially associated loci (PAL) for the\ntarget phenotype. Simulations we performed with various parameters demonstrated\nthat associated loci can be detected with good precision using strict selection\ncriteria, but downstream analyses are required for fine-mapping the exact\nvariants due to linkage disequilibrium, similarly to conventional GWAS. By\napplying our approach to the schizophrenia cohort in the Estonian Biobank, we\nwere able to detect multiple PAL related to this highly polygenic and heritable\ndisorder. We also performed enrichment analyses with PAL in genic regions,\nwhich predominantly identified terms associated with brain morphology. With\nfurther improvements in model optimization and confidence measures, artificial\nneural networks can enhance the identification of genomic loci associated with\ncomplex diseases, providing a more comprehensive approach for GWAS and serving\nas initial screening tools for subsequent functional studies.\n  Keywords: Deep learning, interpretability, genome-wide association studies,\ncomplex diseases\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18811v1.pdf",
        "similarity": 0.31773727199502005,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "AlcLaM: Arabic Dialectal Language Model",
        "new_link": "http://arxiv.org/abs/2407.13097v1",
        "new_summary": "  Pre-trained Language Models (PLMs) are integral to many modern natural\nlanguage processing (NLP) systems. Although multilingual models cover a wide\nrange of languages, they often grapple with challenges like high inference\ncosts and a lack of diverse non-English training data. Arabic-specific PLMs are\ntrained predominantly on modern standard Arabic, which compromises their\nperformance on regional dialects. To tackle this, we construct an Arabic\ndialectal corpus comprising 3.4M sentences gathered from social media\nplatforms. We utilize this corpus to expand the vocabulary and retrain a\nBERT-based model from scratch. Named AlcLaM, our model was trained using only\n13 GB of text, which represents a fraction of the data used by existing models\nsuch as CAMeL, MARBERT, and ArBERT, compared to 7.8%, 10.2%, and 21.3%,\nrespectively. Remarkably, AlcLaM demonstrates superior performance on a variety\nof Arabic NLP tasks despite the limited training data. AlcLaM is available at\nGitHub https://github.com/amurtadha/Alclam and HuggingFace\nhttps://huggingface.co/rahbi.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.13097v1.pdf",
        "similarity": 0.31654132987097766,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-18"
    },
    {
        "new_title": "Using GPT-4 to guide causal machine learning",
        "new_link": "http://arxiv.org/abs/2407.18607v1",
        "new_summary": "  Since its introduction to the public, ChatGPT has had an unprecedented\nimpact. While some experts praised AI advancements and highlighted their\npotential risks, others have been critical about the accuracy and usefulness of\nLarge Language Models (LLMs). In this paper, we are interested in the ability\nof LLMs to identify causal relationships. We focus on the well-established\nGPT-4 (Turbo) and evaluate its performance under the most restrictive\nconditions, by isolating its ability to infer causal relationships based solely\non the variable labels without being given any context, demonstrating the\nminimum level of effectiveness one can expect when it is provided with\nlabel-only information. We show that questionnaire participants judge the GPT-4\ngraphs as the most accurate in the evaluated categories, closely followed by\nknowledge graphs constructed by domain experts, with causal Machine Learning\n(ML) far behind. We use these results to highlight the important limitation of\ncausal ML, which often produces causal graphs that violate common sense,\naffecting trust in them. However, we show that pairing GPT-4 with causal ML\novercomes this limitation, resulting in graphical structures learnt from real\ndata that align more closely with those identified by domain experts, compared\nto structures learnt by causal ML alone. Overall, our findings suggest that\ndespite GPT-4 not being explicitly designed to reason causally, it can still be\na valuable tool for causal representation, as it improves the causal discovery\nprocess of causal ML algorithms that are designed to do just that.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18607v1.pdf",
        "similarity": 0.3162696469381161,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Exploring Quantum Active Learning for Materials Design and Discovery",
        "new_link": "http://arxiv.org/abs/2407.18731v1",
        "new_summary": "  The meeting of artificial intelligence (AI) and quantum computing is already\na reality; quantum machine learning (QML) promises the design of better\nregression models. In this work, we extend our previous studies of materials\ndiscovery using classical active learning (AL), which showed remarkable economy\nof data, to explore the use of quantum algorithms within the AL framework (QAL)\nas implemented in the MLChem4D and QMLMaterials codes. The proposed QAL uses\nquantum support vector regressor (QSVR) or a quantum Gaussian process regressor\n(QGPR) with various quantum kernels and different feature maps. Data sets\ninclude perovskite properties (piezoelectric coefficient, band gap, energy\nstorage) and the structure optimization of a doped nanoparticle (3Al@Si11)\nchosen to compare with classical AL results. Our results revealed that the QAL\nmethod improved the searches in most cases, but not all, seemingly correlated\nwith the roughness of the data. QAL has the potential of finding optimum\nsolutions, within chemical space, in materials science and elsewhere in\nchemistry.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18731v1.pdf",
        "similarity": 0.31604899156486965,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval\n  Augmented Question Answering",
        "new_link": "http://arxiv.org/abs/2407.13998v1",
        "new_summary": "  Question answering based on retrieval augmented generation (RAG-QA) is an\nimportant research topic in NLP and has a wide range of real-world\napplications. However, most existing datasets for this task are either\nconstructed using a single source corpus or consist of short extractive\nanswers, which fall short of evaluating large language model (LLM) based RAG-QA\nsystems on cross-domain generalization. To address these limitations, we create\nLong-form RobustQA (LFRQA), a new dataset comprising human-written long-form\nanswers that integrate short extractive answers from multiple documents into a\nsingle, coherent narrative, covering 26K queries and large corpora across seven\ndifferent domains. We further propose RAG-QA Arena by directly comparing\nmodel-generated answers against LFRQA's answers using LLMs as evaluators. We\nshow via extensive experiments that RAG-QA Arena and human judgments on answer\nquality are highly correlated. Moreover, only 41.3% of the most competitive\nLLM's answers are preferred to LFRQA's answers, demonstrating RAG-QA Arena as a\nchallenging evaluation platform for future research.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.13998v1.pdf",
        "similarity": 0.3156905654747184,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-19"
    },
    {
        "new_title": "FairFlow: An Automated Approach to Model-based Counterfactual Data\n  Augmentation For NLP",
        "new_link": "http://arxiv.org/abs/2407.16431v1",
        "new_summary": "  Despite the evolution of language models, they continue to portray harmful\nsocietal biases and stereotypes inadvertently learned from training data. These\ninherent biases often result in detrimental effects in various applications.\nCounterfactual Data Augmentation (CDA), which seeks to balance demographic\nattributes in training data, has been a widely adopted approach to mitigate\nbias in natural language processing. However, many existing CDA approaches rely\non word substitution techniques using manually compiled word-pair dictionaries.\nThese techniques often lead to out-of-context substitutions, resulting in\npotential quality issues. The advancement of model-based techniques, on the\nother hand, has been challenged by the need for parallel training data. Works\nin this area resort to manually generated parallel data that are expensive to\ncollect and are consequently limited in scale. This paper proposes FairFlow, an\nautomated approach to generating parallel data for training counterfactual text\ngenerator models that limits the need for human intervention. Furthermore, we\nshow that FairFlow significantly overcomes the limitations of dictionary-based\nword-substitution approaches whilst maintaining good performance.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.16431v1.pdf",
        "similarity": 0.31470682584637505,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-23"
    },
    {
        "new_title": "MARS: Mixture of Auto-Regressive Models for Fine-grained Text-to-image\n  Synthesis",
        "new_link": "http://arxiv.org/abs/2407.07614v2",
        "new_summary": "  Auto-regressive models have made significant progress in the realm of\nlanguage generation, yet they do not perform on par with diffusion models in\nthe domain of image synthesis. In this work, we introduce MARS, a novel\nframework for T2I generation that incorporates a specially designed Semantic\nVision-Language Integration Expert (SemVIE). This innovative component\nintegrates pre-trained LLMs by independently processing linguistic and visual\ninformation, freezing the textual component while fine-tuning the visual\ncomponent. This methodology preserves the NLP capabilities of LLMs while\nimbuing them with exceptional visual understanding. Building upon the powerful\nbase of the pre-trained Qwen-7B, MARS stands out with its bilingual generative\ncapabilities corresponding to both English and Chinese language prompts and the\ncapacity for joint image and text generation. The flexibility of this framework\nlends itself to migration towards any-to-any task adaptability. Furthermore,\nMARS employs a multi-stage training strategy that first establishes robust\nimage-text alignment through complementary bidirectional tasks and subsequently\nconcentrates on refining the T2I generation process, significantly augmenting\ntext-image synchrony and the granularity of image details. Notably, MARS\nrequires only 9% of the GPU days needed by SD1.5, yet it achieves remarkable\nresults across a variety of benchmarks, illustrating the training efficiency\nand the potential for swift deployment in various applications.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.07614v2.pdf",
        "similarity": 0.3138847855764962,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-10"
    },
    {
        "new_title": "Large Language Model Enabled Semantic Communication Systems",
        "new_link": "http://arxiv.org/abs/2407.14112v1",
        "new_summary": "  Large language models (LLMs) have recently demonstrated state-of-the-art\nperformance across various natural language processing (NLP) tasks, achieving\nnear-human levels in multiple language understanding challenges and aligning\nclosely with the core principles of semantic communication. Inspired by LLMs'\nadvancements in semantic processing, we propose an innovative LLM-enabled\nsemantic communication system framework, named LLM-SC, that applies LLMs\ndirectly to the physical layer coding and decoding for the first time. By\nanalyzing the relationship between the training process of LLMs and the\noptimization objectives of semantic communication, we propose training a\nsemantic encoder through LLMs' tokenizer training and establishing a semantic\nknowledge base via the LLMs' unsupervised pre-training process. This knowledge\nbase aids in constructing the optimal decoder by providing the prior\nprobability of the transmitted language sequence. Based on this foundation, we\nderive the optimal decoding criterion for the receiver and introduce the beam\nsearch algorithm to further reduce the complexity. Furthermore, we assert that\nexisting LLMs can be employed directly for LLM-SC without additional\nre-training or fine-tuning. Simulation results demonstrate that LLM-SC\noutperforms classical DeepSC at signal-to-noise ratios (SNR) exceeding 3 dB,\nenabling error-free transmission of semantic information under high SNR, which\nis unattainable by DeepSC. In addition to semantic-level performance, LLM-SC\ndemonstrates compatibility with technical-level performance, achieving\napproximately 8 dB coding gain for a bit error ratio (BER) of $10^{-3}$ without\nany channel coding while maintaining the same joint source-channel coding rate\nas traditional communication systems.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.14112v1.pdf",
        "similarity": 0.3131166208682926,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-19"
    },
    {
        "new_title": "Right Now, Wrong Then: Non-Stationary Direct Preference Optimization\n  under Preference Drift",
        "new_link": "http://arxiv.org/abs/2407.18676v1",
        "new_summary": "  Reinforcement learning from human feedback (RLHF) aligns Large Language\nModels (LLMs) with human preferences. However, these preferences can often\nchange over time due to external factors (e.g. environment change and societal\ninfluence). Consequently, what was wrong then might be right now. Current\npreference optimization algorithms do not account for temporal preference drift\nin their modeling, which can lead to severe misalignment. To address this\nlimitation, we use a Dynamic Bradley-Terry model that models preferences via\ntime-dependent reward functions, and propose Non-Stationary Direct Preference\nOptimisation (NS-DPO). By introducing a discount parameter in the loss\nfunction, NS-DPO applies exponential weighting, which proportionally focuses\nlearning on more time-relevant datapoints. We theoretically analyse the\nconvergence of NS-DPO in the offline setting, providing upper bounds on the\nestimation error caused by non-stationary preferences. Finally, we demonstrate\nthe effectiveness of NS-DPO1 for fine-tuning LLMs in scenarios with drifting\npreferences. By simulating preference drift using renowned reward models and\nmodifying popular LLM datasets accordingly, we show that NS-DPO fine-tuned LLMs\nremain robust under non-stationarity, significantly outperforming baseline\nalgorithms that ignore temporal preference changes, without sacrificing\nperformance in stationary cases.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18676v1.pdf",
        "similarity": 0.312327476057543,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Fast and Reliable Probabilistic Reflectometry Inversion with\n  Prior-Amortized Neural Posterior Estimation",
        "new_link": "http://arxiv.org/abs/2407.18648v1",
        "new_summary": "  Reconstructing the structure of thin films and multilayers from measurements\nof scattered X-rays or neutrons is key to progress in physics, chemistry, and\nbiology. However, finding all structures compatible with reflectometry data is\ncomputationally prohibitive for standard algorithms, which typically results in\nunreliable analysis with only a single potential solution identified. We\naddress this lack of reliability with a probabilistic deep learning method that\nidentifies all realistic structures in seconds, setting new standards in\nreflectometry. Our method, Prior-Amortized Neural Posterior Estimation (PANPE),\ncombines simulation-based inference with novel adaptive priors that inform the\ninference network about known structural properties and controllable\nexperimental conditions. PANPE networks support key scenarios such as\nhigh-throughput sample characterization, real-time monitoring of evolving\nstructures, or the co-refinement of several experimental data sets, and can be\nadapted to provide fast, reliable, and flexible inference across many other\ninverse problems.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18648v1.pdf",
        "similarity": 0.30953875347804,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "The Role of Temporal Hierarchy in Spiking Neural Networks",
        "new_link": "http://arxiv.org/abs/2407.18838v1",
        "new_summary": "  Spiking Neural Networks (SNNs) have the potential for rich spatio-temporal\nsignal processing thanks to exploiting both spatial and temporal parameters.\nThe temporal dynamics such as time constants of the synapses and neurons and\ndelays have been recently shown to have computational benefits that help reduce\nthe overall number of parameters required in the network and increase the\naccuracy of the SNNs in solving temporal tasks. Optimizing such temporal\nparameters, for example, through gradient descent, gives rise to a temporal\narchitecture for different problems. As has been shown in machine learning, to\nreduce the cost of optimization, architectural biases can be applied, in this\ncase in the temporal domain. Such inductive biases in temporal parameters have\nbeen found in neuroscience studies, highlighting a hierarchy of temporal\nstructure and input representation in different layers of the cortex. Motivated\nby this, we propose to impose a hierarchy of temporal representation in the\nhidden layers of SNNs, highlighting that such an inductive bias improves their\nperformance. We demonstrate the positive effects of temporal hierarchy in the\ntime constants of feed-forward SNNs applied to temporal tasks (Multi-Time-Scale\nXOR and Keyword Spotting, with a benefit of up to 4.1% in classification\naccuracy). Moreover, we show that such architectural biases, i.e. hierarchy of\ntime constants, naturally emerge when optimizing the time constants through\ngradient descent, initialized as homogeneous values. We further pursue this\nproposal in temporal convolutional SNNs, by introducing the hierarchical bias\nin the size and dilation of temporal kernels, giving rise to competitive\nresults in popular temporal spike-based datasets.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18838v1.pdf",
        "similarity": 0.30928518927110343,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "PP-TIL: Personalized Planning for Autonomous Driving with Instance-based\n  Transfer Imitation Learning",
        "new_link": "http://arxiv.org/abs/2407.18569v1",
        "new_summary": "  Personalized motion planning holds significant importance within urban\nautomated driving, catering to the unique requirements of individual users.\nNevertheless, prior endeavors have frequently encountered difficulties in\nsimultaneously addressing two crucial aspects: personalized planning within\nintricate urban settings and enhancing planning performance through data\nutilization. The challenge arises from the expensive and limited nature of user\ndata, coupled with the scene state space tending towards infinity. These\nfactors contribute to overfitting and poor generalization problems during model\ntraining. Henceforth, we propose an instance-based transfer imitation learning\napproach. This method facilitates knowledge transfer from extensive expert\ndomain data to the user domain, presenting a fundamental resolution to these\nissues. We initially train a pre-trained model using large-scale expert data.\nSubsequently, during the fine-tuning phase, we feed the batch data, which\ncomprises expert and user data. Employing the inverse reinforcement learning\ntechnique, we extract the style feature distribution from user demonstrations,\nconstructing the regularization term for the approximation of user style. In\nour experiments, we conducted extensive evaluations of the proposed method.\nCompared to the baseline methods, our approach mitigates the overfitting issue\ncaused by sparse user data. Furthermore, we discovered that integrating the\ndriving model with a differentiable nonlinear optimizer as a safety protection\nlayer for end-to-end personalized fine-tuning results in superior planning\nperformance.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18569v1.pdf",
        "similarity": 0.3053349413502115,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Towards Effective and Efficient Continual Pre-training of Large Language\n  Models",
        "new_link": "http://arxiv.org/abs/2407.18743v1",
        "new_summary": "  Continual pre-training (CPT) has been an important approach for adapting\nlanguage models to specific domains or tasks. To make the CPT approach more\ntraceable, this paper presents a technical report for continually pre-training\nLlama-3 (8B), which significantly enhances the Chinese language ability and\nscientific reasoning ability of the backbone model. To enhance the new\nabilities while retaining the original abilities, we design specific data\nmixture and curriculum strategies by utilizing existing datasets and\nsynthesizing high-quality datasets. Specifically, we synthesize\nmultidisciplinary scientific question and answer (QA) pairs based on related\nweb pages, and subsequently incorporate these synthetic data to improve the\nscientific reasoning ability of Llama-3. We refer to the model after CPT as\nLlama-3-SynE (Synthetic data Enhanced Llama-3). We also present the tuning\nexperiments with a relatively small model -- TinyLlama, and employ the derived\nfindings to train the backbone model. Extensive experiments on a number of\nevaluation benchmarks show that our approach can largely improve the\nperformance of the backbone models, including both the general abilities (+8.81\non C-Eval and +6.31 on CMMLU) and the scientific reasoning abilities (+12.00 on\nMATH and +4.13 on SciEval), without hurting the original capacities. Our model,\ndata, and codes are available at https://github.com/RUC-GSAI/Llama-3-SynE.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18743v1.pdf",
        "similarity": 0.3051747103700251,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "TookaBERT: A Step Forward for Persian NLU",
        "new_link": "http://arxiv.org/abs/2407.16382v1",
        "new_summary": "  The field of natural language processing (NLP) has seen remarkable\nadvancements, thanks to the power of deep learning and foundation models.\nLanguage models, and specifically BERT, have been key players in this progress.\nIn this study, we trained and introduced two new BERT models using Persian\ndata. We put our models to the test, comparing them to seven existing models\nacross 14 diverse Persian natural language understanding (NLU) tasks. The\nresults speak for themselves: our larger model outperforms the competition,\nshowing an average improvement of at least +2.8 points. This highlights the\neffectiveness and potential of our new BERT models for Persian NLU tasks.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.16382v1.pdf",
        "similarity": 0.3037257845047699,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-23"
    },
    {
        "new_title": "Robust multi-mode superconducting qubit designed with evolutionary\n  algorithms",
        "new_link": "http://arxiv.org/abs/2407.18895v1",
        "new_summary": "  Multi-mode superconducting circuits offer a promising platform for\nengineering robust systems for quantum computation. Previous studies have shown\nthat single-mode devices cannot simultaneously exhibit resilience against\nmultiple decoherence sources due to conflicting protection requirements. In\ncontrast, multi-mode systems offer increased flexibility and have proven\ncapable of overcoming these fundamental limitations. Nevertheless, exploring\nmulti-mode architectures is computationally demanding due to the exponential\nscaling of the Hilbert space dimension. Here, we present a multi-mode device\ndesigned using evolutionary optimization techniques, which have been shown to\nbe effective for this computational task. The proposed device was optimized to\nfeature an anharmonicity of a third of the qubit frequency and reduced energy\ndispersion caused by charge and magnetic flux fluctuations. It exhibits\nimprovements over the fundamental errors limiting Transmon and Fluxonium\ncoherence and manipulation, aiming for a balance between low depolarization\nerror and fast manipulation; furthermore demonstrating robustness against\nfabrication errors, a major limitation in many proposed multi-mode devices.\nOverall, by striking a balance between coupling matrix elements and noise\nprotection, we propose a device that paves the way towards finding proper\ncharacteristics for the construction of superconducting quantum processors.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18895v1.pdf",
        "similarity": 0.3034091339074616,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18625",
        "published": "2024-07-26"
    },
    {
        "new_title": "A Labeled Ophthalmic Ultrasound Dataset with Medical Report Generation\n  Based on Cross-modal Deep Learning",
        "new_link": "http://arxiv.org/abs/2407.18667v1",
        "new_summary": "  Ultrasound imaging reveals eye morphology and aids in diagnosing and treating\neye diseases. However, interpreting diagnostic reports requires specialized\nphysicians. We present a labeled ophthalmic dataset for the precise analysis\nand the automated exploration of medical images along with their associated\nreports. It collects three modal data, including the ultrasound images, blood\nflow information and examination reports from 2,417 patients at an\nophthalmology hospital in Shenyang, China, during the year 2018, in which the\npatient information is de-identified for privacy protection. To the best of our\nknowledge, it is the only ophthalmic dataset that contains the three modal\ninformation simultaneously. It incrementally consists of 4,858 images with the\ncorresponding free-text reports, which describe 15 typical imaging findings of\nintraocular diseases and the corresponding anatomical locations. Each image\nshows three kinds of blood flow indices at three specific arteries, i.e., nine\nparameter values to describe the spectral characteristics of blood flow\ndistribution. The reports were written by ophthalmologists during the clinical\ncare. The proposed dataset is applied to generate medical report based on the\ncross-modal deep learning model. The experimental results demonstrate that our\ndataset is suitable for training supervised models concerning cross-modal\nmedical data.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18667v1.pdf",
        "similarity": 0.3029946872049527,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Restore-RWKV: Efficient and Effective Medical Image Restoration with\n  RWKV",
        "new_link": "http://arxiv.org/abs/2407.11087v1",
        "new_summary": "  Transformers have revolutionized medical image restoration, but the quadratic\ncomplexity still poses limitations for their application to high-resolution\nmedical images. The recent advent of RWKV in the NLP field has attracted much\nattention as it can process long sequences efficiently. To leverage its\nadvanced design, we propose Restore-RWKV, the first RWKV-based model for\nmedical image restoration. Since the original RWKV model is designed for 1D\nsequences, we make two necessary modifications for modeling spatial relations\nin 2D images. First, we present a recurrent WKV (Re-WKV) attention mechanism\nthat captures global dependencies with linear computational complexity. Re-WKV\nincorporates bidirectional attention as basic for a global receptive field and\nrecurrent attention to effectively model 2D dependencies from various scan\ndirections. Second, we develop an omnidirectional token shift (Omni-Shift)\nlayer that enhances local dependencies by shifting tokens from all directions\nand across a wide context range. These adaptations make the proposed\nRestore-RWKV an efficient and effective model for medical image restoration.\nExtensive experiments demonstrate that Restore-RWKV achieves superior\nperformance across various medical image restoration tasks, including MRI image\nsuper-resolution, CT image denoising, PET image synthesis, and all-in-one\nmedical image restoration. Code is available at:\n\\href{https://github.com/Yaziwel/Restore-RWKV.git}{https://github.com/Yaziwel/Restore-RWKV}.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.11087v1.pdf",
        "similarity": 0.3018624180888777,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-14"
    },
    {
        "new_title": "Learning a Shape-Conditioned Agent for Purely Tactile In-Hand\n  Manipulation of Various Objects",
        "new_link": "http://arxiv.org/abs/2407.18834v1",
        "new_summary": "  Reorienting diverse objects with a multi-fingered hand is a challenging task.\nCurrent methods in robotic in-hand manipulation are either object-specific or\nrequire permanent supervision of the object state from visual sensors. This is\nfar from human capabilities and from what is needed in real-world applications.\nIn this work, we address this gap by training shape-conditioned agents to\nreorient diverse objects in hand, relying purely on tactile feedback (via\ntorque and position measurements of the fingers' joints). To achieve this, we\npropose a learning framework that exploits shape information in a reinforcement\nlearning policy and a learned state estimator. We find that representing 3D\nshapes by vectors from a fixed set of basis points to the shape's surface,\ntransformed by its predicted 3D pose, is especially helpful for learning\ndexterous in-hand manipulation. In simulation and real-world experiments, we\nshow the reorientation of many objects with high success rates, on par with\nstate-of-the-art results obtained with specialized single-object agents.\nMoreover, we show generalization to novel objects, achieving success rates of\n$\\sim$90% even for non-convex shapes.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18834v1.pdf",
        "similarity": 0.30065885717814017,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Wolf: Captioning Everything with a World Summarization Framework",
        "new_link": "http://arxiv.org/abs/2407.18908v1",
        "new_summary": "  We propose Wolf, a WOrLd summarization Framework for accurate video\ncaptioning. Wolf is an automated captioning framework that adopts a\nmixture-of-experts approach, leveraging complementary strengths of Vision\nLanguage Models (VLMs). By utilizing both image and video models, our framework\ncaptures different levels of information and summarizes them efficiently. Our\napproach can be applied to enhance video understanding, auto-labeling, and\ncaptioning. To evaluate caption quality, we introduce CapScore, an LLM-based\nmetric to assess the similarity and quality of generated captions compared to\nthe ground truth captions. We further build four human-annotated datasets in\nthree domains: autonomous driving, general scenes, and robotics, to facilitate\ncomprehensive comparisons. We show that Wolf achieves superior captioning\nperformance compared to state-of-the-art approaches from the research community\n(VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For\ninstance, in comparison with GPT-4V, Wolf improves CapScore both quality-wise\nby 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally,\nwe establish a benchmark for video captioning and introduce a leaderboard,\naiming to accelerate advancements in video understanding, captioning, and data\nalignment. Leaderboard: https://wolfv0.github.io/leaderboard.html.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18908v1.pdf",
        "similarity": 0.3004596949667578,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Enhancing Electrocardiogram Signal Analysis Using NLP-Inspired\n  Techniques: A Novel Approach with Embedding and Self-Attention",
        "new_link": "http://arxiv.org/abs/2407.11102v1",
        "new_summary": "  A language is made up of an infinite/finite number of sentences, which in\nturn is composed of a number of words. The Electrocardiogram (ECG) is the most\npopular noninvasive medical tool for studying heart function and diagnosing\nvarious irregular cardiac rhythms. Intuitive inspection of the ECG reveals a\nmarked similarity between ECG signals and the spoken language. As a result, the\nECG signal may be thought of as a series of heartbeats (similar to sentences in\na spoken language), with each heartbeat consisting of a collection of waves\n(similar to words in a sentence) with varying morphologies. Just as natural\nlanguage processing (NLP) is used to help computers comprehend and interpret\nhuman natural language, it is conceivable to create NLP-inspired algorithms to\nhelp computers comprehend the electrocardiogram data more efficiently. In this\nstudy, we propose a novel ECG analysis technique, based on embedding and self\nattention, to capture the spatial as well as the temporal dependencies of the\nECG data. To generate the embedding, an encoder-decoder network was proposed to\ncapture the temporal dependencies of the ECG signal and perform data\ncompression. The compressed and encoded data was fed to the embedding layer as\nits weights. Finally, the proposed CNN-LSTM-Self Attention classifier works on\nthe embedding layer and classifies the signal as normal or anomalous. The\napproach was tested using the PTB-xl dataset, which is severely imbalanced. Our\nemphasis was to appropriately recognise the disease classes present in minority\nnumbers, in order to limit the detection of False Negative cases. An accuracy\nof 91% was achieved with a good F1-score for all the disease classes.\nAdditionally, the the size of the model was reduced by 34% due to compression,\nmaking it suitable for deployment in real time applications\n",
        "pdf_link": "https://arxiv.org/pdf/2407.11102v1.pdf",
        "similarity": 0.30024496116798627,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-15"
    },
    {
        "new_title": "Finite Neural Networks as Mixtures of Gaussian Processes: From Provable\n  Error Bounds to Prior Selection",
        "new_link": "http://arxiv.org/abs/2407.18707v1",
        "new_summary": "  Infinitely wide or deep neural networks (NNs) with independent and\nidentically distributed (i.i.d.) parameters have been shown to be equivalent to\nGaussian processes. Because of the favorable properties of Gaussian processes,\nthis equivalence is commonly employed to analyze neural networks and has led to\nvarious breakthroughs over the years. However, neural networks and Gaussian\nprocesses are equivalent only in the limit; in the finite case there are\ncurrently no methods available to approximate a trained neural network with a\nGaussian model with bounds on the approximation error. In this work, we present\nan algorithmic framework to approximate a neural network of finite width and\ndepth, and with not necessarily i.i.d. parameters, with a mixture of Gaussian\nprocesses with error bounds on the approximation error. In particular, we\nconsider the Wasserstein distance to quantify the closeness between\nprobabilistic models and, by relying on tools from optimal transport and\nGaussian processes, we iteratively approximate the output distribution of each\nlayer of the neural network as a mixture of Gaussian processes. Crucially, for\nany NN and $\\epsilon >0$ our approach is able to return a mixture of Gaussian\nprocesses that is $\\epsilon$-close to the NN at a finite set of input points.\nFurthermore, we rely on the differentiability of the resulting error bound to\nshow how our approach can be employed to tune the parameters of a NN to mimic\nthe functional behavior of a given Gaussian process, e.g., for prior selection\nin the context of Bayesian inference. We empirically investigate the\neffectiveness of our results on both regression and classification problems\nwith various neural network architectures. Our experiments highlight how our\nresults can represent an important step towards understanding neural network\npredictions and formally quantifying their uncertainty.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18707v1.pdf",
        "similarity": 0.2979565740628505,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Every Part Matters: Integrity Verification of Scientific Figures Based\n  on Multimodal Large Language Models",
        "new_link": "http://arxiv.org/abs/2407.18626v1",
        "new_summary": "  This paper tackles a key issue in the interpretation of scientific figures:\nthe fine-grained alignment of text and figures. It advances beyond prior\nresearch that primarily dealt with straightforward, data-driven visualizations\nsuch as bar and pie charts and only offered a basic understanding of diagrams\nthrough captioning and classification. We introduce a novel task, Figure\nIntegrity Verification, designed to evaluate the precision of technologies in\naligning textual knowledge with visual elements in scientific figures. To\nsupport this, we develop a semi-automated method for constructing a large-scale\ndataset, Figure-seg, specifically designed for this task. Additionally, we\npropose an innovative framework, Every Part Matters (EPM), which leverages\nMultimodal Large Language Models (MLLMs) to not only incrementally improve the\nalignment and verification of text-figure integrity but also enhance integrity\nthrough analogical reasoning. Our comprehensive experiments show that these\ninnovations substantially improve upon existing methods, allowing for more\nprecise and thorough analysis of complex scientific figures. This progress not\nonly enhances our understanding of multimodal technologies but also stimulates\nfurther research and practical applications across fields requiring the\naccurate interpretation of complex visual data.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18626v1.pdf",
        "similarity": 0.2949204584732507,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "A Taxonomy for Data Contamination in Large Language Models",
        "new_link": "http://arxiv.org/abs/2407.08716v1",
        "new_summary": "  Large language models pretrained on extensive web corpora demonstrate\nremarkable performance across a wide range of downstream tasks. However, a\ngrowing concern is data contamination, where evaluation datasets may be\ncontained in the pretraining corpus, inflating model performance.\nDecontamination, the process of detecting and removing such data, is a\npotential solution; yet these contaminants may originate from altered versions\nof the test set, evading detection during decontamination. How different types\nof contamination impact the performance of language models on downstream tasks\nis not fully understood. We present a taxonomy that categorizes the various\ntypes of contamination encountered by LLMs during the pretraining phase and\nidentify which types pose the highest risk. We analyze the impact of\ncontamination on two key NLP tasks -- summarization and question answering --\nrevealing how different types of contamination influence task performance\nduring evaluation.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.08716v1.pdf",
        "similarity": 0.2942806715352617,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-11"
    },
    {
        "new_title": "A Survey of Prompt Engineering Methods in Large Language Models for\n  Different NLP Tasks",
        "new_link": "http://arxiv.org/abs/2407.12994v2",
        "new_summary": "  Large language models (LLMs) have shown remarkable performance on many\ndifferent Natural Language Processing (NLP) tasks. Prompt engineering plays a\nkey role in adding more to the already existing abilities of LLMs to achieve\nsignificant performance gains on various NLP tasks. Prompt engineering requires\ncomposing natural language instructions called prompts to elicit knowledge from\nLLMs in a structured way. Unlike previous state-of-the-art (SoTA) models,\nprompt engineering does not require extensive parameter re-training or\nfine-tuning based on the given NLP task and thus solely operates on the\nembedded knowledge of LLMs. Additionally, LLM enthusiasts can intelligently\nextract LLMs' knowledge through a basic natural language conversational\nexchange or prompt engineering, allowing more and more people even without deep\nmathematical machine learning background to experiment with LLMs. With prompt\nengineering gaining popularity in the last two years, researchers have come up\nwith numerous engineering techniques around designing prompts to improve\naccuracy of information extraction from the LLMs. In this paper, we summarize\ndifferent prompting techniques and club them together based on different NLP\ntasks that they have been used for. We further granularly highlight the\nperformance of these prompting strategies on various datasets belonging to that\nNLP task, talk about the corresponding LLMs used, present a taxonomy diagram\nand discuss the possible SoTA for specific datasets. In total, we read and\npresent a survey of 44 research papers which talk about 39 different prompting\nmethods on 29 different NLP tasks of which most of them have been published in\nthe last two years.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.12994v2.pdf",
        "similarity": 0.2942152767195629,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-17"
    },
    {
        "new_title": "MOoSE: Multi-Orientation Sharing Experts for Open-set Scene Text\n  Recognition",
        "new_link": "http://arxiv.org/abs/2407.18616v1",
        "new_summary": "  Open-set text recognition, which aims to address both novel characters and\npreviously seen ones, is one of the rising subtopics in the text recognition\nfield. However, the current open-set text recognition solutions only focuses on\nhorizontal text, which fail to model the real-life challenges posed by the\nvariety of writing directions in real-world scene text. Multi-orientation text\nrecognition, in general, faces challenges from the diverse image aspect ratios,\nsignificant imbalance in data amount, and domain gaps between orientations. In\nthis work, we first propose a Multi-Oriented Open-Set Text Recognition task\n(MOOSTR) to model the challenges of both novel characters and writing direction\nvariety. We then propose a Multi-Orientation Sharing Experts (MOoSE) framework\nas a strong baseline solution. MOoSE uses a mixture-of-experts scheme to\nalleviate the domain gaps between orientations, while exploiting common\nstructural knowledge among experts to alleviate the data scarcity that some\nexperts face. The proposed MOoSE framework is validated by ablative\nexperiments, and also tested for feasibility on the existing open-set\nbenchmark. Code, models, and documents are available at:\nhttps://github.com/lancercat/Moose/\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18616v1.pdf",
        "similarity": 0.2941504173184885,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Absence of gapless Majorana edge modes in few-leg bosonic flux ladders",
        "new_link": "http://arxiv.org/abs/2407.18873v1",
        "new_summary": "  The search for Majorana excitations has seen tremendous efforts in recent\nyears, ultimately aiming for their individual controllability in future\ntopological quantum computers. A promising framework to realize such exotic\nMajorana fermions are topologically ordered non-Abelian phases of matter, such\nas certain fractional quantum Hall states. Quantum simulators provide\nunprecedented controllability and versatility to investigate such states, and\ndeveloping experimentally feasible schemes to realize and identify them is of\nimmediate relevance. Motivated by recent experiments, we consider bosons on\ncoupled chains, subjected to a magnetic flux and experiencing Hubbard\nrepulsion. At magnetic filling factor $\\nu=1$, similar systems on cylinders\nhave been found to host the non-Abelian Moore-Read Pfaffian state in the bulk.\nHere, we address the question whether more realistic few-leg ladders can host\nthis exotic state and its chiral Majorana edge states. To this end, we perform\nextensive DMRG simulations and determine the central charge of the ground\nstate. While we do not find any evidence of gapless Majorana edge modes in\nsystems of up to six legs, exact diagonalization of small systems reveals\nevidence for the Pfaffian state in the entanglement structure. By\nsystematically varying the number of legs and monitoring the appearance and\ndisappearance of this signal, our work highlights the importance of finite-size\neffects for the realization of exotic states in experimentally realistic\nsystems.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18873v1.pdf",
        "similarity": 0.29141863281349484,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18625",
        "published": "2024-07-26"
    },
    {
        "new_title": "On the Pros and Cons of Active Learning for Moral Preference Elicitation",
        "new_link": "http://arxiv.org/abs/2407.18889v1",
        "new_summary": "  Computational preference elicitation methods are tools used to learn people's\npreferences quantitatively in a given context. Recent works on preference\nelicitation advocate for active learning as an efficient method to iteratively\nconstruct queries (framed as comparisons between context-specific cases) that\nare likely to be most informative about an agent's underlying preferences. In\nthis work, we argue that the use of active learning for moral preference\nelicitation relies on certain assumptions about the underlying moral\npreferences, which can be violated in practice. Specifically, we highlight the\nfollowing common assumptions (a) preferences are stable over time and not\nsensitive to the sequence of presented queries, (b) the appropriate hypothesis\nclass is chosen to model moral preferences, and (c) noise in the agent's\nresponses is limited. While these assumptions can be appropriate for preference\nelicitation in certain domains, prior research on moral psychology suggests\nthey may not be valid for moral judgments. Through a synthetic simulation of\npreferences that violate the above assumptions, we observe that active learning\ncan have similar or worse performance than a basic random query selection\nmethod in certain settings. Yet, simulation results also demonstrate that\nactive learning can still be viable if the degree of instability or noise is\nrelatively small and when the agent's preferences can be approximately\nrepresented with the hypothesis class used for learning. Our study highlights\nthe nuances associated with effective moral preference elicitation in practice\nand advocates for the cautious use of active learning as a methodology to learn\nmoral preferences.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18889v1.pdf",
        "similarity": 0.29097791789253125,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Can Open-Source LLMs Compete with Commercial Models? Exploring the\n  Few-Shot Performance of Current GPT Models in Biomedical Tasks",
        "new_link": "http://arxiv.org/abs/2407.13511v1",
        "new_summary": "  Commercial large language models (LLMs), like OpenAI's GPT-4 powering ChatGPT\nand Anthropic's Claude 3 Opus, have dominated natural language processing (NLP)\nbenchmarks across different domains. New competing Open-Source alternatives\nlike Mixtral 8x7B or Llama 3 have emerged and seem to be closing the gap while\noften offering higher throughput and being less costly to use. Open-Source LLMs\ncan also be self-hosted, which makes them interesting for enterprise and\nclinical use cases where sensitive data should not be processed by third\nparties. We participated in the 12th BioASQ challenge, which is a retrieval\naugmented generation (RAG) setting, and explored the performance of current GPT\nmodels Claude 3 Opus, GPT-3.5-turbo and Mixtral 8x7b with in-context learning\n(zero-shot, few-shot) and QLoRa fine-tuning. We also explored how additional\nrelevant knowledge from Wikipedia added to the context-window of the LLM might\nimprove their performance. Mixtral 8x7b was competitive in the 10-shot setting,\nboth with and without fine-tuning, but failed to produce usable results in the\nzero-shot setting. QLoRa fine-tuning and Wikipedia context did not lead to\nmeasurable performance gains. Our results indicate that the performance gap\nbetween commercial and open-source models in RAG setups exists mainly in the\nzero-shot setting and can be closed by simply collecting few-shot examples for\ndomain-specific use cases. The code needed to rerun these experiments is\navailable through GitHub.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.13511v1.pdf",
        "similarity": 0.2909754944004223,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-18"
    },
    {
        "new_title": "Is larger always better? Evaluating and prompting large language models\n  for non-generative medical tasks",
        "new_link": "http://arxiv.org/abs/2407.18525v1",
        "new_summary": "  The use of Large Language Models (LLMs) in medicine is growing, but their\nability to handle both structured Electronic Health Record (EHR) data and\nunstructured clinical notes is not well-studied. This study benchmarks various\nmodels, including GPT-based LLMs, BERT-based models, and traditional clinical\npredictive models, for non-generative medical tasks utilizing renowned\ndatasets. We assessed 14 language models (9 GPT-based and 5 BERT-based) and 7\ntraditional predictive models using the MIMIC dataset (ICU patient records) and\nthe TJH dataset (early COVID-19 EHR data), focusing on tasks such as mortality\nand readmission prediction, disease hierarchy reconstruction, and biomedical\nsentence matching, comparing both zero-shot and finetuned performance. Results\nindicated that LLMs exhibited robust zero-shot predictive capabilities on\nstructured EHR data when using well-designed prompting strategies, frequently\nsurpassing traditional models. However, for unstructured medical texts, LLMs\ndid not outperform finetuned BERT models, which excelled in both supervised and\nunsupervised tasks. Consequently, while LLMs are effective for zero-shot\nlearning on structured data, finetuned BERT models are more suitable for\nunstructured texts, underscoring the importance of selecting models based on\nspecific task requirements and data characteristics to optimize the application\nof NLP technology in healthcare.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18525v1.pdf",
        "similarity": 0.2905149938963939,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "MASIVE: Open-Ended Affective State Identification in English and Spanish",
        "new_link": "http://arxiv.org/abs/2407.12196v1",
        "new_summary": "  In the field of emotion analysis, much NLP research focuses on identifying a\nlimited number of discrete emotion categories, often applied across languages.\nThese basic sets, however, are rarely designed with textual data in mind, and\nculture, language, and dialect can influence how particular emotions are\ninterpreted. In this work, we broaden our scope to a practically unbounded set\nof \\textit{affective states}, which includes any terms that humans use to\ndescribe their experiences of feeling. We collect and publish MASIVE, a dataset\nof Reddit posts in English and Spanish containing over 1,000 unique affective\nstates each. We then define the new problem of \\textit{affective state\nidentification} for language generation models framed as a masked span\nprediction task. On this task, we find that smaller finetuned multilingual\nmodels outperform much larger LLMs, even on region-specific Spanish affective\nstates. Additionally, we show that pretraining on MASIVE improves model\nperformance on existing emotion benchmarks. Finally, through machine\ntranslation experiments, we find that native speaker-written data is vital to\ngood performance on this task.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.12196v1.pdf",
        "similarity": 0.29049239962079454,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-16"
    },
    {
        "new_title": "HICEScore: A Hierarchical Metric for Image Captioning Evaluation",
        "new_link": "http://arxiv.org/abs/2407.18589v1",
        "new_summary": "  Image captioning evaluation metrics can be divided into two categories,\nreference-based metrics and reference-free metrics. However, reference-based\napproaches may struggle to evaluate descriptive captions with abundant visual\ndetails produced by advanced multimodal large language models, due to their\nheavy reliance on limited human-annotated references. In contrast, previous\nreference-free metrics have been proven effective via CLIP cross-modality\nsimilarity. Nonetheless, CLIP-based metrics, constrained by their solution of\nglobal image-text compatibility, often have a deficiency in detecting local\ntextual hallucinations and are insensitive to small visual objects. Besides,\ntheir single-scale designs are unable to provide an interpretable evaluation\nprocess such as pinpointing the position of caption mistakes and identifying\nvisual regions that have not been described. To move forward, we propose a\nnovel reference-free metric for image captioning evaluation, dubbed\nHierarchical Image Captioning Evaluation Score (HICE-S). By detecting local\nvisual regions and textual phrases, HICE-S builds an interpretable hierarchical\nscoring mechanism, breaking through the barriers of the single-scale structure\nof existing reference-free metrics. Comprehensive experiments indicate that our\nproposed metric achieves the SOTA performance on several benchmarks,\noutperforming existing reference-free metrics like CLIP-S and PAC-S, and\nreference-based metrics like METEOR and CIDEr. Moreover, several case studies\nreveal that the assessment process of HICE-S on detailed captions closely\nresembles interpretable human judgments.Our code is available at\nhttps://github.com/joeyz0z/HICE.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18589v1.pdf",
        "similarity": 0.28864537967027254,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "PEFT-U: Parameter-Efficient Fine-Tuning for User Personalization",
        "new_link": "http://arxiv.org/abs/2407.18078v1",
        "new_summary": "  The recent emergence of Large Language Models (LLMs) has heralded a new era\nof human-AI interaction. These sophisticated models, exemplified by Chat-GPT\nand its successors, have exhibited remarkable capabilities in language\nunderstanding. However, as these LLMs have undergone exponential growth, a\ncrucial dimension that remains understudied is the personalization of these\nmodels. Large foundation models such as GPT-3 etc. focus on creating a\nuniversal model that serves a broad range of tasks and users. This approach\nemphasizes the model's generalization capabilities, treating users as a\ncollective rather than as distinct individuals. While practical for many common\napplications, this one-size-fits-all approach often fails to address the rich\ntapestry of human diversity and individual needs. To explore this issue we\nintroduce the PEFT-U Benchmark: a new dataset for building and evaluating NLP\nmodels for user personalization. \\datasetname{} consists of a series of\nuser-centered tasks containing diverse and individualized expressions where the\npreferences of users can potentially differ for the same input. Using PEFT-U,\nwe explore the challenge of efficiently personalizing LLMs to accommodate\nuser-specific preferences in the context of diverse user-centered tasks.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18078v1.pdf",
        "similarity": 0.2863035936934224,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-25"
    },
    {
        "new_title": "ANHALTEN: Cross-Lingual Transfer for German Token-Level Reference-Free\n  Hallucination Detection",
        "new_link": "http://arxiv.org/abs/2407.13702v1",
        "new_summary": "  Research on token-level reference-free hallucination detection has\npredominantly focused on English, primarily due to the scarcity of robust\ndatasets in other languages. This has hindered systematic investigations into\nthe effectiveness of cross-lingual transfer for this important NLP application.\nTo address this gap, we introduce ANHALTEN, a new evaluation dataset that\nextends the English hallucination detection dataset to German. To the best of\nour knowledge, this is the first work that explores cross-lingual transfer for\ntoken-level reference-free hallucination detection. ANHALTEN contains gold\nannotations in German that are parallel (i.e., directly comparable to the\noriginal English instances). We benchmark several prominent cross-lingual\ntransfer approaches, demonstrating that larger context length leads to better\nhallucination detection in German, even without succeeding context.\nImportantly, we show that the sample-efficient few-shot transfer is the most\neffective approach in most setups. This highlights the practical benefits of\nminimal annotation effort in the target language for reference-free\nhallucination detection. Aiming to catalyze future research on cross-lingual\ntoken-level reference-free hallucination detection, we make ANHALTEN publicly\navailable: https://github.com/janekh24/anhalten\n",
        "pdf_link": "https://arxiv.org/pdf/2407.13702v1.pdf",
        "similarity": 0.2849032940337825,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-18"
    },
    {
        "new_title": "SHANGUS: Deep Reinforcement Learning Meets Heuristic Optimization for\n  Speedy Frontier-Based Exploration of Autonomous Vehicles in Unknown Spaces",
        "new_link": "http://arxiv.org/abs/2407.18892v1",
        "new_summary": "  This paper introduces SHANGUS, an advanced framework combining Deep\nReinforcement Learning (DRL) with heuristic optimization to improve\nfrontier-based exploration efficiency in unknown environments, particularly for\nintelligent vehicles in autonomous air services, search and rescue operations,\nand space exploration robotics. SHANGUS harnesses DRL's adaptability and\nheuristic prioritization, markedly enhancing exploration efficiency, reducing\ncompletion time, and minimizing travel distance. The strategy involves a\nfrontier selection node to identify unexplored areas and a DRL navigation node\nusing the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm for\nrobust path planning and dynamic obstacle avoidance. Extensive experiments in\nROS2 and Gazebo simulation environments show SHANGUS surpasses representative\ntraditional methods like the Nearest Frontier (NF), Novel Frontier-Based\nExploration Algorithm (CFE), and Goal-Driven Autonomous Exploration (GDAE)\nalgorithms, especially in complex scenarios, excelling in completion time,\ntravel distance, and exploration rate. This scalable solution is suitable for\nreal-time autonomous navigation in fields such as industrial automation,\nautonomous driving, household robotics, and space exploration. Future research\nwill integrate additional sensory inputs and refine heuristic functions to\nfurther boost SHANGUS's efficiency and robustness.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18892v1.pdf",
        "similarity": 0.2844380742879429,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18625",
        "published": "2024-07-26"
    },
    {
        "new_title": "Lectures on Parallel Computing",
        "new_link": "http://arxiv.org/abs/2407.18795v1",
        "new_summary": "  These lecture notes are designed to accompany an imaginary, virtual,\nundergraduate, one or two semester course on fundamentals of Parallel Computing\nas well as to serve as background and reference for graduate courses on\nHigh-Performance Computing, parallel algorithms and shared-memory\nmultiprocessor programming. They introduce theoretical concepts and tools for\nexpressing, analyzing and judging parallel algorithms and, in detail, cover the\ntwo most widely used concrete frameworks OpenMP and MPI as well as the\nthreading interface pthreads for writing parallel programs for either shared or\ndistributed memory parallel computers with emphasis on general concepts and\nprinciples. Code examples are given in a C-like style and many are actual,\ncorrect C code. The lecture notes deliberately do not cover GPU architectures\nand GPU programming, but the general concerns, guidelines and principles (time,\nwork, cost, efficiency, scalability, memory structure and bandwidth) will be\njust as relevant for efficiently utilizing various GPU architectures. Likewise,\nthe lecture notes focus on deterministic algorithms only and do not use\nrandomization. The student of this material will find it instructive to take\nthe time to understand concepts and algorithms visually. The exercises can be\nused for self-study and as inspiration for small implementation projects in\nOpenMP and MPI that can and should accompany any serious course on Parallel\nComputing. The student will benefit from actually implementing and carefully\nbenchmarking the suggested algorithms on the parallel computing system that may\nor should be made available as part of such a Parallel Computing course. In\nclass, the exercises can be used as basis for hand-ins and small programming\nprojects for which sufficient, additional detail and precision should be\nprovided by the instructor.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18795v1.pdf",
        "similarity": 0.28432527096092514,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18625",
        "published": "2024-07-26"
    },
    {
        "new_title": "Self-Evolving GPT: A Lifelong Autonomous Experiential Learner",
        "new_link": "http://arxiv.org/abs/2407.08937v1",
        "new_summary": "  To improve the performance of large language models (LLMs), researchers have\nexplored providing LLMs with textual task-solving experience via prompts.\nHowever, they rely on manual efforts to acquire and apply such experience for\neach task, which is not feasible for the growing demand for LLMs and the\nvariety of user questions. To address this issue, we design a lifelong\nautonomous experiential learning framework based on LLMs to explore whether\nLLMs can imitate human ability for learning and utilizing experience. It\nautonomously learns and accumulates experience through experience transfer and\ninduction, categorizing the types of input questions to select which\naccumulated experience to employ for them. Experimental results on six widely\nused NLP datasets show that our framework performs reliably in each\nintermediate step and effectively improves the performance of GPT-3.5 and\nGPT-4. This validates the feasibility of using LLMs to mimic human experiential\nlearning and application capabilities. Additionally, we provide a detailed\nanalysis of the behavior of our framework at each step.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.08937v1.pdf",
        "similarity": 0.2842497255330565,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-12"
    },
    {
        "new_title": "Open (Clinical) LLMs are Sensitive to Instruction Phrasings",
        "new_link": "http://arxiv.org/abs/2407.09429v1",
        "new_summary": "  Instruction-tuned Large Language Models (LLMs) can perform a wide range of\ntasks given natural language instructions to do so, but they are sensitive to\nhow such instructions are phrased. This issue is especially concerning in\nhealthcare, as clinicians are unlikely to be experienced prompt engineers and\nthe potential consequences of inaccurate outputs are heightened in this domain.\n  This raises a practical question: How robust are instruction-tuned LLMs to\nnatural variations in the instructions provided for clinical NLP tasks? We\ncollect prompts from medical doctors across a range of tasks and quantify the\nsensitivity of seven LLMs -- some general, others specialized -- to natural\n(i.e., non-adversarial) instruction phrasings. We find that performance varies\nsubstantially across all models, and that -- perhaps surprisingly --\ndomain-specific models explicitly trained on clinical data are especially\nbrittle, compared to their general domain counterparts. Further, arbitrary\nphrasing differences can affect fairness, e.g., valid but distinct instructions\nfor mortality prediction yield a range both in overall performance, and in\nterms of differences between demographic groups.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.09429v1.pdf",
        "similarity": 0.28368218573543114,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-12"
    },
    {
        "new_title": "Relational Database Augmented Large Language Model",
        "new_link": "http://arxiv.org/abs/2407.15071v1",
        "new_summary": "  Large language models (LLMs) excel in many natural language processing (NLP)\ntasks. However, since LLMs can only incorporate new knowledge through training\nor supervised fine-tuning processes, they are unsuitable for applications that\ndemand precise, up-to-date, and private information not available in the\ntraining corpora. This precise, up-to-date, and private information is\ntypically stored in relational databases. Thus, a promising solution is to\naugment LLMs with the inclusion of relational databases as external memory.\nThis can ensure the timeliness, correctness, and consistency of data, and\nassist LLMs in performing complex arithmetic operations beyond their inherent\ncapabilities. However, bridging the gap between LLMs and relational databases\nis challenging. It requires the awareness of databases and data values stored\nin databases to select correct databases and issue correct SQL queries.\nBesides, it is necessary for the external memory to be independent of the LLM\nto meet the needs of real-world applications. We introduce a novel LLM-agnostic\nmemory architecture comprising a database selection memory, a data value\nmemory, and relational databases. And we design an elegant pipeline to retrieve\ninformation from it. Besides, we carefully design the prompts to instruct the\nLLM to maximize the framework's potential. To evaluate our method, we compose a\nnew dataset with various types of questions. Experimental results show that our\nframework enables LLMs to effectively answer database-related questions, which\nis beyond their direct ability.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.15071v1.pdf",
        "similarity": 0.2826815419611623,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-21"
    },
    {
        "new_title": "Multi-Agent Deep Reinforcement Learning for Energy Efficient Multi-Hop\n  STAR-RIS-Assisted Transmissions",
        "new_link": "http://arxiv.org/abs/2407.18627v1",
        "new_summary": "  Simultaneously transmitting and reflecting reconfigurable intelligent surface\n(STAR-RIS) provides a promising way to expand coverage in wireless\ncommunications. However, limitation of single STAR-RIS inspire us to integrate\nthe concept of multi-hop transmissions, as focused on RIS in existing research.\nTherefore, we propose the novel architecture of multi-hop STAR-RISs to achieve\na wider range of full-plane service coverage. In this paper, we intend to solve\nactive beamforming of the base station and passive beamforming of STAR-RISs,\naiming for maximizing the energy efficiency constrained by hardware limitation\nof STAR-RISs. Furthermore, we investigate the impact of the on-off state of\nSTAR-RIS elements on energy efficiency. To tackle the complex problem, a\nMulti-Agent Global and locAl deep Reinforcement learning (MAGAR) algorithm is\ndesigned. The global agent elevates the collaboration among local agents, which\nfocus on individual learning. In numerical results, we observe the significant\nimprovement of MAGAR compared to the other benchmarks, including Q-learning,\nmulti-agent deep Q network (DQN) with golbal reward, and multi-agent DQN with\nlocal rewards. Moreover, the proposed architecture of multi-hop STAR-RISs\nachieves the highest energy efficiency compared to mode switching based\nSTAR-RISs, conventional RISs and deployment without RISs or STAR-RISs.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18627v1.pdf",
        "similarity": 0.280926378195469,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18625",
        "published": "2024-07-26"
    },
    {
        "new_title": "Revisiting Who's Harry Potter: Towards Targeted Unlearning from a Causal\n  Intervention Perspective",
        "new_link": "http://arxiv.org/abs/2407.16997v1",
        "new_summary": "  This paper investigates Who's Harry Potter (WHP), a pioneering yet\ninsufficiently understood method for LLM unlearning. We explore it in two\nsteps. First, we introduce a new task of LLM targeted unlearning, where given\nan unlearning target (e.g., a person) and some unlearning documents, we aim to\nunlearn only the information about the target, rather than everything in the\nunlearning documents. We further argue that a successful unlearning should\nsatisfy criteria such as not outputting gibberish, not fabricating facts about\nthe unlearning target, and not releasing factual information under jailbreak\nattacks. Second, we construct a causal intervention framework for targeted\nunlearning, where the knowledge of the unlearning target is modeled as a\nconfounder between LLM input and output, and the unlearning process as a\ndeconfounding process. This framework justifies and extends WHP, deriving a\nsimple unlearning algorithm that includes WHP as a special case. Experiments on\nexisting and new datasets show that our approach, without explicitly optimizing\nfor the aforementioned criteria, achieves competitive performance in all of\nthem. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/causal_unlearn.git.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.16997v1.pdf",
        "similarity": 0.2808480418091496,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-24"
    },
    {
        "new_title": "Halu-J: Critique-Based Hallucination Judge",
        "new_link": "http://arxiv.org/abs/2407.12943v1",
        "new_summary": "  Large language models (LLMs) frequently generate non-factual content, known\nas hallucinations. Existing retrieval-augmented-based hallucination detection\napproaches typically address this by framing it as a classification task,\nevaluating hallucinations based on their consistency with retrieved evidence.\nHowever, this approach usually lacks detailed explanations for these\nevaluations and does not assess the reliability of these explanations.\nFurthermore, deficiencies in retrieval systems can lead to irrelevant or\npartially relevant evidence retrieval, impairing the detection process.\nMoreover, while real-world hallucination detection requires analyzing multiple\npieces of evidence, current systems usually treat all evidence uniformly\nwithout considering its relevance to the content. To address these challenges,\nwe introduce Halu-J, a critique-based hallucination judge with 7 billion\nparameters. Halu-J enhances hallucination detection by selecting pertinent\nevidence and providing detailed critiques. Our experiments indicate that Halu-J\noutperforms GPT-4o in multiple-evidence hallucination detection and matches its\ncapability in critique generation and evidence selection. We also introduce\nME-FEVER, a new dataset designed for multiple-evidence hallucination detection.\nOur code and dataset can be found in https://github.com/GAIR-NLP/factool .\n",
        "pdf_link": "https://arxiv.org/pdf/2407.12943v1.pdf",
        "similarity": 0.28017257938707346,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-17"
    },
    {
        "new_title": "Utilizing TTS Synthesized Data for Efficient Development of Keyword\n  Spotting Model",
        "new_link": "http://arxiv.org/abs/2407.18879v1",
        "new_summary": "  This paper explores the use of TTS synthesized training data for KWS (keyword\nspotting) task while minimizing development cost and time. Keyword spotting\nmodels require a huge amount of training data to be accurate, and obtaining\nsuch training data can be costly. In the current state of the art, TTS models\ncan generate large amounts of natural-sounding data, which can help reducing\ncost and time for KWS model development. Still, TTS generated data can be\nlacking diversity compared to real data. To pursue maximizing KWS model\naccuracy under the constraint of limited resources and current TTS capability,\nwe explored various strategies to mix TTS data and real human speech data, with\na focus on minimizing real data use and maximizing diversity of TTS output. Our\nexperimental results indicate that relatively small amounts of real audio data\nwith speaker diversity (100 speakers, 2k utterances) and large amounts of TTS\nsynthesized data can achieve reasonably high accuracy (within 3x error rate of\nbaseline), compared to the baseline (trained with 3.8M real positive\nutterances).\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18879v1.pdf",
        "similarity": 0.27964230588802025,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Constructing the CORD-19 Vaccine Dataset",
        "new_link": "http://arxiv.org/abs/2407.18471v1",
        "new_summary": "  We introduce new dataset 'CORD-19-Vaccination' to cater to scientists\nspecifically looking into COVID-19 vaccine-related research. This dataset is\nextracted from CORD-19 dataset [Wang et al., 2020] and augmented with new\ncolumns for language detail, author demography, keywords, and topic per paper.\nFacebook's fastText model is used to identify languages [Joulin et al., 2016].\nTo establish author demography (author affiliation, lab/institution location,\nand lab/institution country columns) we processed the JSON file for each paper\nand then further enhanced using Google's search API to determine country\nvalues. 'Yake' was used to extract keywords from the title, abstract, and body\nof each paper and the LDA (Latent Dirichlet Allocation) algorithm was used to\nadd topic information [Campos et al., 2020, 2018a,b]. To evaluate the dataset,\nwe demonstrate a question-answering task like the one used in the CORD-19\nKaggle challenge [Goldbloom et al., 2022]. For further evaluation, sequential\nsentence classification was performed on each paper's abstract using the model\nfrom Dernoncourt et al. [2016]. We partially hand annotated the training\ndataset and used a pre-trained BERT-PubMed layer. 'CORD- 19-Vaccination'\ncontains 30k research papers and can be immensely valuable for NLP research\nsuch as text mining, information extraction, and question answering, specific\nto the domain of COVID-19 vaccine research.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18471v1.pdf",
        "similarity": 0.27905776795371223,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Recent Advances in Generative AI and Large Language Models: Current\n  Status, Challenges, and Perspectives",
        "new_link": "http://arxiv.org/abs/2407.14962v2",
        "new_summary": "  The emergence of Generative Artificial Intelligence (AI) and Large Language\nModels (LLMs) has marked a new era of Natural Language Processing (NLP),\nintroducing unprecedented capabilities that are revolutionizing various\ndomains. This paper explores the current state of these cutting-edge\ntechnologies, demonstrating their remarkable advancements and wide-ranging\napplications. Our paper contributes to providing a holistic perspective on the\ntechnical foundations, practical applications, and emerging challenges within\nthe evolving landscape of Generative AI and LLMs. We believe that understanding\nthe generative capabilities of AI systems and the specific context of LLMs is\ncrucial for researchers, practitioners, and policymakers to collaboratively\nshape the responsible and ethical integration of these technologies into\nvarious domains. Furthermore, we identify and address main research gaps,\nproviding valuable insights to guide future research endeavors within the AI\nresearch community.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.14962v2.pdf",
        "similarity": 0.27848122794298275,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-20"
    },
    {
        "new_title": "A Scalable Quantum Non-local Neural Network for Image Classification",
        "new_link": "http://arxiv.org/abs/2407.18906v1",
        "new_summary": "  Non-local operations play a crucial role in computer vision enabling the\ncapture of long-range dependencies through weighted sums of features across the\ninput, surpassing the constraints of traditional convolution operations that\nfocus solely on local neighborhoods. Non-local operations typically require\ncomputing pairwise relationships between all elements in a set, leading to\nquadratic complexity in terms of time and memory. Due to the high computational\nand memory demands, scaling non-local neural networks to large-scale problems\ncan be challenging. This article introduces a hybrid quantum-classical scalable\nnon-local neural network, referred to as Quantum Non-Local Neural Network\n(QNL-Net), to enhance pattern recognition. The proposed QNL-Net relies on\ninherent quantum parallelism to allow the simultaneous processing of a large\nnumber of input features enabling more efficient computations in\nquantum-enhanced feature space and involving pairwise relationships through\nquantum entanglement. We benchmark our proposed QNL-Net with other quantum\ncounterparts to binary classification with datasets MNIST and CIFAR-10. The\nsimulation findings showcase our QNL-Net achieves cutting-edge accuracy levels\nin binary image classification among quantum classifiers while utilizing fewer\nqubits.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18906v1.pdf",
        "similarity": 0.277690115464521,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Beyond Text: Leveraging Multi-Task Learning and Cognitive Appraisal\n  Theory for Post-Purchase Intention Analysis",
        "new_link": "http://arxiv.org/abs/2407.08182v1",
        "new_summary": "  Supervised machine-learning models for predicting user behavior offer a\nchallenging classification problem with lower average prediction performance\nscores than other text classification tasks. This study evaluates multi-task\nlearning frameworks grounded in Cognitive Appraisal Theory to predict user\nbehavior as a function of users' self-expression and psychological attributes.\nOur experiments show that users' language and traits improve predictions above\nand beyond models predicting only from text. Our findings highlight the\nimportance of integrating psychological constructs into NLP to enhance the\nunderstanding and prediction of user actions. We close with a discussion of the\nimplications for future applications of large language models for computational\npsychology.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.08182v1.pdf",
        "similarity": 0.2754139994709684,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-11"
    },
    {
        "new_title": "Intelligent Multi-Document Summarisation for Extracting Insights on\n  Racial Inequalities from Maternity Incident Investigation Reports",
        "new_link": "http://arxiv.org/abs/2407.08322v1",
        "new_summary": "  In healthcare, thousands of safety incidents occur every year, but learning\nfrom these incidents is not effectively aggregated. Analysing incident reports\nusing AI could uncover critical insights to prevent harm by identifying\nrecurring patterns and contributing factors. To aggregate and extract valuable\ninformation, natural language processing (NLP) and machine learning techniques\ncan be employed to summarise and mine unstructured data, potentially surfacing\nsystemic issues and priority areas for improvement. This paper presents\nI-SIRch:CS, a framework designed to facilitate the aggregation and analysis of\nsafety incident reports while ensuring traceability throughout the process. The\nframework integrates concept annotation using the Safety Intelligence Research\n(SIRch) taxonomy with clustering, summarisation, and analysis capabilities.\nUtilising a dataset of 188 anonymised maternity investigation reports annotated\nwith 27 SIRch human factors concepts, I-SIRch:CS groups the annotated sentences\ninto clusters using sentence embeddings and k-means clustering, maintaining\ntraceability via file and sentence IDs. Summaries are generated for each\ncluster using offline state-of-the-art abstractive summarisation models (BART,\nDistilBART, T5), which are evaluated and compared using metrics assessing\nsummary quality attributes. The generated summaries are linked back to the\noriginal file and sentence IDs, ensuring traceability and allowing for\nverification of the summarised information. Results demonstrate BART's\nstrengths in creating informative and concise summaries.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.08322v1.pdf",
        "similarity": 0.27312222493598165,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-11"
    },
    {
        "new_title": "AraFinNLP 2024: The First Arabic Financial NLP Shared Task",
        "new_link": "http://arxiv.org/abs/2407.09818v1",
        "new_summary": "  The expanding financial markets of the Arab world require sophisticated\nArabic NLP tools. To address this need within the banking domain, the Arabic\nFinancial NLP (AraFinNLP) shared task proposes two subtasks: (i) Multi-dialect\nIntent Detection and (ii) Cross-dialect Translation and Intent Preservation.\nThis shared task uses the updated ArBanking77 dataset, which includes about 39k\nparallel queries in MSA and four dialects. Each query is labeled with one or\nmore of a common 77 intents in the banking domain. These resources aim to\nfoster the development of robust financial Arabic NLP, particularly in the\nareas of machine translation and banking chat-bots. A total of 45 unique teams\nregistered for this shared task, with 11 of them actively participated in the\ntest phase. Specifically, 11 teams participated in Subtask 1, while only 1 team\nparticipated in Subtask 2. The winning team of Subtask 1 achieved F1 score of\n0.8773, and the only team submitted in Subtask 2 achieved a 1.667 BLEU score.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.09818v1.pdf",
        "similarity": 0.2714855766481441,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-13"
    },
    {
        "new_title": "Graph Neural Networks for Virtual Sensing in Complex Systems: Addressing\n  Heterogeneous Temporal Dynamics",
        "new_link": "http://arxiv.org/abs/2407.18691v1",
        "new_summary": "  Real-time condition monitoring is crucial for the reliable and efficient\noperation of complex systems. However, relying solely on physical sensors can\nbe limited due to their cost, placement constraints, or inability to directly\nmeasure certain critical parameters. Virtual sensing addresses these\nlimitations by leveraging readily available sensor data and system knowledge to\nestimate inaccessible parameters or infer system states. The increasing\ncomplexity of industrial systems necessitates deployments of sensors with\ndiverse modalities to provide a comprehensive understanding of system states.\nThese sensors capture data at varying frequencies to monitor both rapid and\nslowly varying system dynamics, as well as local and global state evolutions of\nthe systems. This leads to heterogeneous temporal dynamics, which, particularly\nunder varying operational end environmental conditions, pose a significant\nchallenge for accurate virtual sensing. To address this, we propose a\nHeterogeneous Temporal Graph Neural Network (HTGNN) framework. HTGNN explicitly\nmodels signals from diverse sensors and integrates operating conditions into\nthe model architecture. We evaluate HTGNN using two newly released datasets: a\nbearing dataset with diverse load conditions for bearing load prediction and a\nyear-long simulated dataset for predicting bridge live loads. Our results\ndemonstrate that HTGNN significantly outperforms established baseline methods\nin both tasks, particularly under highly varying operating conditions. These\nresults highlight HTGNN's potential as a robust and accurate virtual sensing\napproach for complex systems, paving the way for improved monitoring,\npredictive maintenance, and enhanced system performance.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18691v1.pdf",
        "similarity": 0.26997798069152856,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "SOAP-RL: Sequential Option Advantage Propagation for Reinforcement\n  Learning in POMDP Environments",
        "new_link": "http://arxiv.org/abs/2407.18913v1",
        "new_summary": "  This work compares ways of extending Reinforcement Learning algorithms to\nPartially Observed Markov Decision Processes (POMDPs) with options. One view of\noptions is as temporally extended action, which can be realized as a memory\nthat allows the agent to retain historical information beyond the policy's\ncontext window. While option assignment could be handled using heuristics and\nhand-crafted objectives, learning temporally consistent options and associated\nsub-policies without explicit supervision is a challenge. Two algorithms, PPOEM\nand SOAP, are proposed and studied in depth to address this problem. PPOEM\napplies the forward-backward algorithm (for Hidden Markov Models) to optimize\nthe expected returns for an option-augmented policy. However, this learning\napproach is unstable during on-policy rollouts. It is also unsuited for\nlearning causal policies without the knowledge of future trajectories, since\noption assignments are optimized for offline sequences where the entire episode\nis available. As an alternative approach, SOAP evaluates the policy gradient\nfor an optimal option assignment. It extends the concept of the generalized\nadvantage estimation (GAE) to propagate option advantages through time, which\nis an analytical equivalent to performing temporal back-propagation of option\npolicy gradients. This option policy is only conditional on the history of the\nagent, not future actions. Evaluated against competing baselines, SOAP\nexhibited the most robust performance, correctly discovering options for POMDP\ncorridor environments, as well as on standard benchmarks including Atari and\nMuJoCo, outperforming PPOEM, as well as LSTM and Option-Critic baselines. The\nopen-sourced code is available at https://github.com/shuishida/SoapRL.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18913v1.pdf",
        "similarity": 0.2685755085654547,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Topology Optimization of Random Memristors for Input-Aware Dynamic SNN",
        "new_link": "http://arxiv.org/abs/2407.18625v1",
        "new_summary": "  There is unprecedented development in machine learning, exemplified by recent\nlarge language models and world simulators, which are artificial neural\nnetworks running on digital computers. However, they still cannot parallel\nhuman brains in terms of energy efficiency and the streamlined adaptability to\ninputs of different difficulties, due to differences in signal representation,\noptimization, run-time reconfigurability, and hardware architecture. To address\nthese fundamental challenges, we introduce pruning optimization for input-aware\ndynamic memristive spiking neural network (PRIME). Signal representation-wise,\nPRIME employs leaky integrate-and-fire neurons to emulate the brain's inherent\nspiking mechanism. Drawing inspiration from the brain's structural plasticity,\nPRIME optimizes the topology of a random memristive spiking neural network\nwithout expensive memristor conductance fine-tuning. For runtime\nreconfigurability, inspired by the brain's dynamic adjustment of computational\ndepth, PRIME employs an input-aware dynamic early stop policy to minimize\nlatency during inference, thereby boosting energy efficiency without\ncompromising performance. Architecture-wise, PRIME leverages memristive\nin-memory computing, mirroring the brain and mitigating the von Neumann\nbottleneck. We validated our system using a 40 nm 256 Kb memristor-based\nin-memory computing macro on neuromorphic image classification and image\ninpainting. Our results demonstrate the classification accuracy and Inception\nScore are comparable to the software baseline, while achieving maximal\n62.50-fold improvements in energy efficiency, and maximal 77.0% computational\nload savings. The system also exhibits robustness against stochastic synaptic\nnoise of analogue memristors. Our software-hardware co-designed model paves the\nway to future brain-inspired neuromorphic computing with brain-like energy\nefficiency and adaptivity.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18625v1.pdf",
        "similarity": 0.26814156136977874,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "A Flexible and Scalable Approach for Collecting Wildlife Advertisements\n  on the Web",
        "new_link": "http://arxiv.org/abs/2407.18898v1",
        "new_summary": "  Wildlife traffickers are increasingly carrying out their activities in\ncyberspace. As they advertise and sell wildlife products in online\nmarketplaces, they leave digital traces of their activity. This creates a new\nopportunity: by analyzing these traces, we can obtain insights into how\ntrafficking networks work as well as how they can be disrupted. However,\ncollecting such information is difficult. Online marketplaces sell a very large\nnumber of products and identifying ads that actually involve wildlife is a\ncomplex task that is hard to automate. Furthermore, given that the volume of\ndata is staggering, we need scalable mechanisms to acquire, filter, and store\nthe ads, as well as to make them available for analysis. In this paper, we\npresent a new approach to collect wildlife trafficking data at scale. We\npropose a data collection pipeline that combines scoped crawlers for data\ndiscovery and acquisition with foundational models and machine learning\nclassifiers to identify relevant ads. We describe a dataset we created using\nthis pipeline which is, to the best of our knowledge, the largest of its kind:\nit contains almost a million ads obtained from 41 marketplaces, covering 235\nspecies and 20 languages. The source code is publicly available at\n\\url{https://github.com/VIDA-NYU/wildlife_pipeline}.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18898v1.pdf",
        "similarity": 0.2677258885068129,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Large Language Models as Reliable Knowledge Bases?",
        "new_link": "http://arxiv.org/abs/2407.13578v1",
        "new_summary": "  The NLP community has recently shown a growing interest in leveraging Large\nLanguage Models (LLMs) for knowledge-intensive tasks, viewing LLMs as potential\nknowledge bases (KBs). However, the reliability and extent to which LLMs can\nfunction as KBs remain underexplored. While previous studies suggest LLMs can\nencode knowledge within their parameters, the amount of parametric knowledge\nalone is not sufficient to evaluate their effectiveness as KBs. This study\ndefines criteria that a reliable LLM-as-KB should meet, focusing on factuality\nand consistency, and covering both seen and unseen knowledge. We develop\nseveral metrics based on these criteria and use them to evaluate 26 popular\nLLMs, while providing a comprehensive analysis of the effects of model size,\ninstruction tuning, and in-context learning (ICL). Our results paint a worrying\npicture. Even a high-performant model like GPT-3.5-turbo is not factual or\nconsistent, and strategies like ICL and fine-tuning are unsuccessful at making\nLLMs better KBs.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.13578v1.pdf",
        "similarity": 0.26671348185375077,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-18"
    },
    {
        "new_title": "A Deep Reinforcement Learning Approach to Wavefront Control for\n  Exoplanet Imaging",
        "new_link": "http://arxiv.org/abs/2407.18733v1",
        "new_summary": "  Exoplanet imaging uses coronagraphs to block out the bright light from a\nstar, allowing astronomers to observe the much fainter light from planets\norbiting the star. However, these instruments are heavily impacted by small\nwavefront aberrations and require the minimization of starlight residuals\ndirectly in the focal plane. State-of-the art wavefront control methods suffer\nfrom errors in the underlying physical models, and often require several\niterations to minimize the intensity in the dark hole, limiting performance and\nreducing effective observation time. This study aims at developing a\ndata-driven method to create a dark hole in post-coronagraphic images. For this\npurpose, we leverage the model-free capabilities of reinforcement learning to\ntrain an agent to learn a control strategy directly from phase diversity images\nacquired around the focal plane. Initial findings demonstrate successful\naberration correction in non-coronagraphic simulations and promising results\nfor dark hole creation in post-coronagraphic scenarios. These results highlight\nthe potential of model-free reinforcement learning for dark-hole creation,\njustifying further investigation and eventually experimental validation on a\ndedicated testbed.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18733v1.pdf",
        "similarity": 0.2664131798169926,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Robust Learning in Bayesian Parallel Branching Graph Neural Networks:\n  The Narrow Width Limit",
        "new_link": "http://arxiv.org/abs/2407.18807v1",
        "new_summary": "  The infinite width limit of random neural networks is known to result in\nNeural Networks as Gaussian Process (NNGP) (Lee et al. [2018]), characterized\nby task-independent kernels. It is widely accepted that larger network widths\ncontribute to improved generalization (Park et al. [2019]). However, this work\nchallenges this notion by investigating the narrow width limit of the Bayesian\nParallel Branching Graph Neural Network (BPB-GNN), an architecture that\nresembles residual networks. We demonstrate that when the width of a BPB-GNN is\nsignificantly smaller compared to the number of training examples, each branch\nexhibits more robust learning due to a symmetry breaking of branches in kernel\nrenormalization. Surprisingly, the performance of a BPB-GNN in the narrow width\nlimit is generally superior or comparable to that achieved in the wide width\nlimit in bias-limited scenarios. Furthermore, the readout norms of each branch\nin the narrow width limit are mostly independent of the architectural\nhyperparameters but generally reflective of the nature of the data. Our results\ncharacterize a newly defined narrow-width regime for parallel branching\nnetworks in general.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18807v1.pdf",
        "similarity": 0.2656649381283352,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "PassTSL: Modeling Human-Created Passwords through Two-Stage Learning",
        "new_link": "http://arxiv.org/abs/2407.14145v1",
        "new_summary": "  Textual passwords are still the most widely used user authentication\nmechanism. Due to the close connections between textual passwords and natural\nlanguages, advanced technologies in natural language processing (NLP) and\nmachine learning (ML) could be used to model passwords for different purposes\nsuch as studying human password-creation behaviors and developing more advanced\npassword cracking methods for informing better defence mechanisms. In this\npaper, we propose PassTSL (modeling human-created Passwords through Two-Stage\nLearning), inspired by the popular pretraining-finetuning framework in NLP and\ndeep learning (DL). We report how different pretraining settings affected\nPassTSL and proved its effectiveness by applying it to six large leaked\npassword databases. Experimental results showed that it outperforms five\nstate-of-the-art (SOTA) password cracking methods on password guessing by a\nsignificant margin ranging from 4.11% to 64.69% at the maximum point. Based on\nPassTSL, we also implemented a password strength meter (PSM), and our\nexperiments showed that it was able to estimate password strength more\naccurately, causing fewer unsafe errors (overestimating the password strength)\nthan two other SOTA PSMs when they produce the same rate of safe errors\n(underestimating the password strength): a neural-network based method and\nzxcvbn. Furthermore, we explored multiple finetuning settings, and our\nevaluations showed that, even a small amount of additional training data, e.g.,\nonly 0.1% of the pretrained data, can lead to over 3% improvement in password\nguessing on average. We also proposed a heuristic approach to selecting\nfinetuning passwords based on JS (Jensen-Shannon) divergence and experimental\nresults validated its usefulness. In summary, our contributions demonstrate the\npotential and feasibility of applying advanced NLP and ML methods to password\nmodeling and cracking.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.14145v1.pdf",
        "similarity": 0.2647994388513219,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-19"
    },
    {
        "new_title": "Combining Knowledge Graphs and Large Language Models",
        "new_link": "http://arxiv.org/abs/2407.06564v1",
        "new_summary": "  In recent years, Natural Language Processing (NLP) has played a significant\nrole in various Artificial Intelligence (AI) applications such as chatbots,\ntext generation, and language translation. The emergence of large language\nmodels (LLMs) has greatly improved the performance of these applications,\nshowing astonishing results in language understanding and generation. However,\nthey still show some disadvantages, such as hallucinations and lack of\ndomain-specific knowledge, that affect their performance in real-world tasks.\nThese issues can be effectively mitigated by incorporating knowledge graphs\n(KGs), which organise information in structured formats that capture\nrelationships between entities in a versatile and interpretable fashion.\nLikewise, the construction and validation of KGs present challenges that LLMs\ncan help resolve. The complementary relationship between LLMs and KGs has led\nto a trend that combines these technologies to achieve trustworthy results.\nThis work collected 28 papers outlining methods for KG-powered LLMs, LLM-based\nKGs, and LLM-KG hybrid approaches. We systematically analysed and compared\nthese approaches to provide a comprehensive overview highlighting key trends,\ninnovative techniques, and common challenges. This synthesis will benefit\nresearchers new to the field and those seeking to deepen their understanding of\nhow KGs and LLMs can be effectively combined to enhance AI applications\ncapabilities.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.06564v1.pdf",
        "similarity": 0.2639751484003917,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-09"
    },
    {
        "new_title": "InstructAV: Instruction Fine-tuning Large Language Models for Authorship\n  Verification",
        "new_link": "http://arxiv.org/abs/2407.12882v1",
        "new_summary": "  Large Language Models (LLMs) have demonstrated remarkable proficiency in a\nwide range of NLP tasks. However, when it comes to authorship verification (AV)\ntasks, which involve determining whether two given texts share the same\nauthorship, even advanced models like ChatGPT exhibit notable limitations. This\npaper introduces a novel approach, termed InstructAV, for authorship\nverification. This approach utilizes LLMs in conjunction with a\nparameter-efficient fine-tuning (PEFT) method to simultaneously improve\naccuracy and explainability. The distinctiveness of InstructAV lies in its\nability to align classification decisions with transparent and understandable\nexplanations, representing a significant progression in the field of authorship\nverification. Through comprehensive experiments conducted across various\ndatasets, InstructAV demonstrates its state-of-the-art performance on the AV\ntask, offering high classification accuracy coupled with enhanced explanation\nreliability.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.12882v1.pdf",
        "similarity": 0.26391014395901063,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-16"
    },
    {
        "new_title": "PERCORE: A Deep Learning-Based Framework for Persian Spelling Correction\n  with Phonetic Analysis",
        "new_link": "http://arxiv.org/abs/2407.14789v1",
        "new_summary": "  This research introduces a state-of-the-art Persian spelling correction\nsystem that seamlessly integrates deep learning techniques with phonetic\nanalysis, significantly enhancing the accuracy and efficiency of natural\nlanguage processing (NLP) for Persian. Utilizing a fine-tuned language\nrepresentation model, our methodology effectively combines deep contextual\nanalysis with phonetic insights, adeptly correcting both non-word and real-word\nspelling errors. This strategy proves particularly effective in tackling the\nunique complexities of Persian spelling, including its elaborate morphology and\nthe challenge of homophony. A thorough evaluation on a wide-ranging dataset\nconfirms our system's superior performance compared to existing methods, with\nimpressive F1-Scores of 0.890 for detecting real-word errors and 0.905 for\ncorrecting them. Additionally, the system demonstrates a strong capability in\nnon-word error correction, achieving an F1-Score of 0.891. These results\nillustrate the significant benefits of incorporating phonetic insights into\ndeep learning models for spelling correction. Our contributions not only\nadvance Persian language processing by providing a versatile solution for a\nvariety of NLP applications but also pave the way for future research in the\nfield, emphasizing the critical role of phonetic analysis in developing\neffective spelling correction system.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.14789v1.pdf",
        "similarity": 0.26386605513217787,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-20"
    },
    {
        "new_title": "Domain-specific or Uncertainty-aware models: Does it really make a\n  difference for biomedical text classification?",
        "new_link": "http://arxiv.org/abs/2407.12626v1",
        "new_summary": "  The success of pretrained language models (PLMs) across a spate of use-cases\nhas led to significant investment from the NLP community towards building\ndomain-specific foundational models. On the other hand, in mission critical\nsettings such as biomedical applications, other aspects also factor in-chief of\nwhich is a model's ability to produce reasonable estimates of its own\nuncertainty. In the present study, we discuss these two desiderata through the\nlens of how they shape the entropy of a model's output probability\ndistribution. We find that domain specificity and uncertainty awareness can\noften be successfully combined, but the exact task at hand weighs in much more\nstrongly.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.12626v1.pdf",
        "similarity": 0.2625978069056728,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-17"
    },
    {
        "new_title": "Combining Constraint Programming Reasoning with Large Language Model\n  Predictions",
        "new_link": "http://arxiv.org/abs/2407.13490v1",
        "new_summary": "  Constraint Programming (CP) and Machine Learning (ML) face challenges in text\ngeneration due to CP's struggle with implementing \"meaning'' and ML's\ndifficulty with structural constraints. This paper proposes a solution by\ncombining both approaches and embedding a Large Language Model (LLM) in CP. The\nLLM handles word generation and meaning, while CP manages structural\nconstraints. This approach builds on GenCP, an improved version of On-the-fly\nConstraint Programming Search (OTFS) using LLM-generated domains. Compared to\nBeam Search (BS), a standard NLP method, this combined approach (GenCP with\nLLM) is faster and produces better results, ensuring all constraints are\nsatisfied. This fusion of CP and ML presents new possibilities for enhancing\ntext generation under constraints.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.13490v1.pdf",
        "similarity": 0.2624670684743738,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-18"
    },
    {
        "new_title": "Morphosyntactic Analysis for CHILDES",
        "new_link": "http://arxiv.org/abs/2407.12389v1",
        "new_summary": "  Language development researchers are interested in comparing the process of\nlanguage learning across languages. Unfortunately, it has been difficult to\nconstruct a consistent quantitative framework for such comparisons. However,\nrecent advances in AI (Artificial Intelligence) and ML (Machine Learning) are\nproviding new methods for ASR (automatic speech recognition) and NLP (natural\nlanguage processing) that can be brought to bear on this problem. Using the\nBatchalign2 program (Liu et al., 2023), we have been transcribing and linking\ndata for the CHILDES database and have applied the UD (Universal Dependencies)\nframework to provide a consistent and comparable morphosyntactic analysis for\n27 languages. These new resources open possibilities for deeper crosslinguistic\nstudy of language learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.12389v1.pdf",
        "similarity": 0.26175118082738724,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-17"
    },
    {
        "new_title": "Two eyes, Two views, and finally, One summary! Towards Multi-modal\n  Multi-tasking Knowledge-Infused Medical Dialogue Summarization",
        "new_link": "http://arxiv.org/abs/2407.15237v1",
        "new_summary": "  We often summarize a multi-party conversation in two stages: chunking with\nhomogeneous units and summarizing the chunks. Thus, we hypothesize that there\nexists a correlation between homogeneous speaker chunking and overall\nsummarization tasks. In this work, we investigate the effectiveness of a\nmulti-faceted approach that simultaneously produces summaries of medical\nconcerns, doctor impressions, and an overall view. We introduce a multi-modal,\nmulti-tasking, knowledge-infused medical dialogue summary generation\n(MMK-Summation) model, which is incorporated with adapter-based fine-tuning\nthrough a gated mechanism for multi-modal information integration. The model,\nMMK-Summation, takes dialogues as input, extracts pertinent external knowledge\nbased on the context, integrates the knowledge and visual cues from the\ndialogues into the textual content, and ultimately generates concise summaries\nencompassing medical concerns, doctor impressions, and a comprehensive\noverview. The introduced model surpasses multiple baselines and traditional\nsummarization models across all evaluation metrics (including human\nevaluation), which firmly demonstrates the efficacy of the knowledge-guided\nmulti-tasking, multimodal medical conversation summarization. The code is\navailable at https://github.com/NLP-RL/MMK-Summation.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.15237v1.pdf",
        "similarity": 0.2602487016065828,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-21"
    },
    {
        "new_title": "Downlink CCM Estimation via Representation Learning with Graph\n  Regularization",
        "new_link": "http://arxiv.org/abs/2407.18865v1",
        "new_summary": "  In this paper, we propose an algorithm for downlink (DL) channel covariance\nmatrix (CCM) estimation for frequency division duplexing (FDD) massive\nmultiple-input multiple-output (MIMO) communication systems with base station\n(BS) possessing a uniform linear array (ULA) antenna structure. We make use of\nthe inherent similarity between the uplink (UL) CCM and the DL CCM due to\nangular reciprocity. We consider a setting where the UL CCM is mapped to DL CCM\nby a mapping function. We first present a theoretical error analysis of\nlearning a nonlinear embedding by constructing a mapping function, which points\nto the importance of the Lipschitz regularity of the mapping function for\nachieving high estimation performance. Then, based on the theoretical ground,\nwe propose a representation learning algorithm as a solution for the estimation\nproblem, where Gaussian RBF kernel interpolators are chosen to map UL CCMs to\ntheir DL counterparts. The proposed algorithm is based on the optimization of\nan objective function that fits a regression model between the DL CCM and UL\nCCM samples in the training dataset and preserves the local geometric structure\nof the data in the UL CCM space, while explicitly regulating the Lipschitz\ncontinuity of the mapping function in light of our theoretical findings. The\nproposed algorithm surpasses benchmark methods in terms of three error metrics\nas shown by simulations.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18865v1.pdf",
        "similarity": 0.2601868514411951,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense\n  Persona Knowledge Linking",
        "new_link": "http://arxiv.org/abs/2407.15281v1",
        "new_summary": "  Understanding rich dialogues often requires NLP systems to access relevant\ncommonsense persona knowledge, but retrieving this knowledge is challenging due\nto complex contexts and the implicit nature of commonsense. This paper presents\nour approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,\naddressing the critical need for integrating persona and commonsense knowledge\nin open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that\nleverages Large Language Models to generate high-quality synthetic datasets for\ntraining commonsense persona knowledge linkers. To demonstrate the efficacy of\nour approach, we present SynCPKL, a new dataset specifically designed for this\ntask. Our experiments validate the effectiveness of SynCPKL for training\ncommonsense persona knowledge linkers. Additionally, our top-performing model,\nDerberta-SynCPKL, secured first place in the CPKL challenge by a 16%\nimprovement in F1 score. We released both SynCPKL and Derberta-SynCPKL at\nhttps://github.com/irislin1006/CPKL.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.15281v1.pdf",
        "similarity": 0.259952726268317,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-21"
    },
    {
        "new_title": "Arabic Automatic Story Generation with Large Language Models",
        "new_link": "http://arxiv.org/abs/2407.07551v1",
        "new_summary": "  Large language models (LLMs) have recently emerged as a powerful tool for a\nwide range of language generation tasks. Nevertheless, this progress has been\nslower in Arabic. In this work, we focus on the task of generating stories from\nLLMs. For our training, we use stories acquired through machine translation\n(MT) as well as GPT-4. For the MT data, we develop a careful pipeline that\nensures we acquire high-quality stories. For our GPT-41 data, we introduce\ncrafted prompts that allow us to generate data well-suited to the Arabic\ncontext in both Modern Standard Arabic (MSA) and two Arabic dialects (Egyptian\nand Moroccan). For example, we generate stories tailored to various Arab\ncountries on a wide host of topics. Our manual evaluation shows that our model\nfine-tuned on these training datasets can generate coherent stories that adhere\nto our instructions. We also conduct an extensive automatic and human\nevaluation comparing our models against state-of-the-art proprietary and\nopen-source models. Our datasets and models will be made publicly available at\nhttps: //github.com/UBC-NLP/arastories.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.07551v1.pdf",
        "similarity": 0.25664230213947886,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-10"
    },
    {
        "new_title": "LIONs: An Empirically Optimized Approach to Align Language Models",
        "new_link": "http://arxiv.org/abs/2407.06542v1",
        "new_summary": "  Alignment is a crucial step to enhance the instruction-following and\nconversational abilities of language models. Despite many recent work proposing\nnew algorithms, datasets, and training pipelines, there is a lack of\ncomprehensive studies measuring the impact of various design choices throughout\nthe whole training process. We first conduct a rigorous analysis over a\nthree-stage training pipeline consisting of supervised fine-tuning, offline\npreference learning, and online preference learning. We have found that using\ntechniques like sequence packing, loss masking in SFT, increasing the\npreference dataset size in DPO, and online DPO training can significantly\nimprove the performance of language models. We then train from Gemma-2b-base\nand LLama-3-8b-base, and find that our best models exceed the performance of\nthe official instruct models tuned with closed-source data and algorithms. Our\ncode and models can be found at\nhttps://github.com/Columbia-NLP-Lab/LionAlignment.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.06542v1.pdf",
        "similarity": 0.25571986561172005,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-09"
    },
    {
        "new_title": "SpeciaLex: A Benchmark for In-Context Specialized Lexicon Learning",
        "new_link": "http://arxiv.org/abs/2407.13297v1",
        "new_summary": "  Specialized lexicons are collections of words with associated constraints\nsuch as special definitions, specific roles, and intended target audiences.\nThese constraints are necessary for content generation and documentation tasks\n(e.g., writing technical manuals or children's books), where the goal is to\nreduce the ambiguity of text content and increase its overall readability for a\nspecific group of audience. Understanding how large language models can capture\nthese constraints can help researchers build better, more impactful tools for\nwider use beyond the NLP community. Towards this end, we introduce SpeciaLex, a\nbenchmark for evaluating a language model's ability to follow specialized\nlexicon-based constraints across 18 diverse subtasks with 1,285 test instances\ncovering core tasks of Checking, Identification, Rewriting, and Open\nGeneration. We present an empirical evaluation of 15 open and closed-source\nLLMs and discuss insights on how factors such as model scale, openness, setup,\nand recency affect performance upon evaluating with the benchmark.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.13297v1.pdf",
        "similarity": 0.2547206126630709,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-18"
    },
    {
        "new_title": "Enhancing LLM's Cognition via Structurization",
        "new_link": "http://arxiv.org/abs/2407.16434v1",
        "new_summary": "  When reading long-form text, human cognition is complex and structurized.\nWhile large language models (LLMs) process input contexts through a causal and\nsequential perspective, this approach can potentially limit their ability to\nhandle intricate and complex inputs effectively. To enhance LLM's cognition\ncapability, this paper presents a novel concept of context structurization.\nSpecifically, we transform the plain, unordered contextual sentences into\nwell-ordered and hierarchically structurized elements. By doing so, LLMs can\nbetter grasp intricate and extended contexts through precise attention and\ninformation-seeking along the organized structures. Extensive evaluations are\nconducted across various model architectures and sizes (including several 7B-\nto 72B-size auto-regressive LLMs as well as BERT-like masking models) on a\ndiverse set of NLP tasks (e.g., context-based question-answering, exhaustive\nhallucination evaluation, and passage-level dense retrieval). Empirical results\nshow consistent and significant performance gains afforded by a single-round\nstructurization. In particular, we boost a 72B-parameter open-source model to\nachieve comparable performance against GPT-3.5-Turbo as the hallucination\nevaluator. Besides, we show the feasibility of distilling advanced LLMs'\nlanguage processing abilities to a smaller yet effective StruXGPT-7B to execute\nstructurization, addressing the practicality of our approach. Code will be made\npublic soon.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.16434v1.pdf",
        "similarity": 0.2532089736671734,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-23"
    },
    {
        "new_title": "Deep learning for dynamic modeling and coded information storage of\n  vector-soliton pulsations in mode-locked fiber lasers",
        "new_link": "http://arxiv.org/abs/2407.18725v1",
        "new_summary": "  Soliton pulsations are ubiquitous feature of non-stationary soliton dynamics\nin mode-locked lasers and many other physical systems. To overcome difficulties\nrelated to huge amount of necessary computations and low efficiency of\ntraditional numerical methods in modeling the evolution of non-stationary\nsolitons, we propose a two-parallel bidirectional long short-term memory\nrecurrent neural network, with the main objective to predict dynamics of\nvector-soliton pulsations in various complex states, whose real-time dynamics\nis verified by experiments. Besides, the scheme of coded information storage\nbased on the TP-Bi_LSTM RNN, instead of actual pulse signals, is realized too.\nThe findings offer new applications of deep learning to ultrafast optics and\ninformation storage.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18725v1.pdf",
        "similarity": 0.25310950542254773,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "A Comparative Study on Automatic Coding of Medical Letters with\n  Explainability",
        "new_link": "http://arxiv.org/abs/2407.13638v1",
        "new_summary": "  This study aims to explore the implementation of Natural Language Processing\n(NLP) and machine learning (ML) techniques to automate the coding of medical\nletters with visualised explainability and light-weighted local computer\nsettings. Currently in clinical settings, coding is a manual process that\ninvolves assigning codes to each condition, procedure, and medication in a\npatient's paperwork (e.g., 56265001 heart disease using SNOMED CT code). There\nare preliminary research on automatic coding in this field using\nstate-of-the-art ML models; however, due to the complexity and size of the\nmodels, the real-world deployment is not achieved. To further facilitate the\npossibility of automatic coding practice, we explore some solutions in a local\ncomputer setting; in addition, we explore the function of explainability for\ntransparency of AI models. We used the publicly available MIMIC-III database\nand the HAN/HLAN network models for ICD code prediction purposes. We also\nexperimented with the mapping between ICD and SNOMED CT knowledge bases. In our\nexperiments, the models provided useful information for 97.98\\% of codes. The\nresult of this investigation can shed some light on implementing automatic\nclinical coding in practice, such as in hospital settings, on the local\ncomputers used by clinicians , project page\n\\url{https://github.com/Glenj01/Medical-Coding}.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.13638v1.pdf",
        "similarity": 0.25100040700931947,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-18"
    },
    {
        "new_title": "VeriCHERI: Exhaustive Formal Security Verification of CHERI at the RTL",
        "new_link": "http://arxiv.org/abs/2407.18679v1",
        "new_summary": "  Protecting data in memory from attackers continues to be a concern in\ncomputing systems. CHERI is a promising approach to achieve such protection, by\nproviding and enforcing fine-grained memory protection directly in the\nhardware. Creating trust for the entire system stack, however, requires a\ngap-free verification of CHERI's hardware-based protection mechanisms. Existing\nverification methods for CHERI target the abstract ISA model rather than the\nunderlying hardware implementation. Fully ensuring the CHERI security\nguarantees for a concrete RTL implementation is a challenge in previous flows\nand demands high manual efforts. This paper presents VeriCHERI, a novel\napproach to security verification. It is conceptionally different from previous\nworks in that it does not require any ISA specification. Instead of checking\ncompliance with a golden ISA model, we check against well-established global\nsecurity objectives of confidentiality and integrity. Fully covering these\nobjectives, VeriCHERI uses as few as four unbounded properties to exhaustively\nprove or disprove any vulnerability. We demonstrate the effectiveness and\nscalability of VeriCHERI on a RISC-V based processor implementing a CHERI\nvariant.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18679v1.pdf",
        "similarity": 0.2508620578453711,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18625",
        "published": "2024-07-26"
    },
    {
        "new_title": "FLUE: Federated Learning with Un-Encrypted model weights",
        "new_link": "http://arxiv.org/abs/2407.18750v1",
        "new_summary": "  Federated Learning enables diverse devices to collaboratively train a shared\nmodel while keeping training data locally stored, avoiding the need for\ncentralized cloud storage. Despite existing privacy measures, concerns arise\nfrom potential reverse engineering of gradients, even with added noise,\nrevealing private data. To address this, recent research emphasizes using\nencrypted model parameters during training. This paper introduces a novel\nfederated learning algorithm, leveraging coded local gradients without\nencryption, exchanging coded proxies for model parameters, and injecting\nsurplus noise for enhanced privacy. Two algorithm variants are presented,\nshowcasing convergence and learning rates adaptable to coding schemes and raw\ndata characteristics. Two encryption-free implementations with fixed and random\ncoding matrices are provided, demonstrating promising simulation results from\nboth federated optimization and machine learning perspectives.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18750v1.pdf",
        "similarity": 0.24875524952441225,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "The BIAS Detection Framework: Bias Detection in Word Embeddings and\n  Language Models for European Languages",
        "new_link": "http://arxiv.org/abs/2407.18689v1",
        "new_summary": "  The project BIAS: Mitigating Diversity Biases of AI in the Labor Market is a\nfour-year project funded by the European commission and supported by the Swiss\nState Secretariat for Education, Research and Innovation (SERI). As part of the\nproject, novel bias detection methods to identify societal bias in language\nmodels and word embeddings in European languages are developed, with particular\nattention to linguistic and geographic particularities. This technical report\ndescribes the overall architecture and components of the BIAS Detection\nFramework. The code described in this technical report is available and will be\nupdated and expanded continuously with upcoming results from the BIAS project.\nThe details about the datasets for the different languages are described in\ncorresponding papers at scientific venues.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18689v1.pdf",
        "similarity": 0.2475435298131016,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Graphusion: Leveraging Large Language Models for Scientific Knowledge\n  Graph Fusion and Construction in NLP Education",
        "new_link": "http://arxiv.org/abs/2407.10794v1",
        "new_summary": "  Knowledge graphs (KGs) are crucial in the field of artificial intelligence\nand are widely applied in downstream tasks, such as enhancing Question\nAnswering (QA) systems. The construction of KGs typically requires significant\neffort from domain experts. Recently, Large Language Models (LLMs) have been\nused for knowledge graph construction (KGC), however, most existing approaches\nfocus on a local perspective, extracting knowledge triplets from individual\nsentences or documents. In this work, we introduce Graphusion, a zero-shot KGC\nframework from free text. The core fusion module provides a global view of\ntriplets, incorporating entity merging, conflict resolution, and novel triplet\ndiscovery. We showcase how Graphusion could be applied to the natural language\nprocessing (NLP) domain and validate it in the educational scenario.\nSpecifically, we introduce TutorQA, a new expert-verified benchmark for graph\nreasoning and QA, comprising six tasks and a total of 1,200 QA pairs. Our\nevaluation demonstrates that Graphusion surpasses supervised baselines by up to\n10% in accuracy on link prediction. Additionally, it achieves average scores of\n2.92 and 2.37 out of 3 in human evaluations for concept entity extraction and\nrelation recognition, respectively.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.10794v1.pdf",
        "similarity": 0.24695689888416095,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-15"
    },
    {
        "new_title": "The Cross-environment Hyperparameter Setting Benchmark for Reinforcement\n  Learning",
        "new_link": "http://arxiv.org/abs/2407.18840v1",
        "new_summary": "  This paper introduces a new empirical methodology, the Cross-environment\nHyperparameter Setting Benchmark, that compares RL algorithms across\nenvironments using a single hyperparameter setting, encouraging algorithmic\ndevelopment which is insensitive to hyperparameters. We demonstrate that this\nbenchmark is robust to statistical noise and obtains qualitatively similar\nresults across repeated applications, even when using few samples. This\nrobustness makes the benchmark computationally cheap to apply, allowing\nstatistically sound insights at low cost. We demonstrate two example\ninstantiations of the CHS, on a set of six small control environments (SC-CHS)\nand on the entire DM Control suite of 28 environments (DMC-CHS). Finally, to\nillustrate the applicability of the CHS to modern RL algorithms on challenging\nenvironments, we conduct a novel empirical study of an open question in the\ncontinuous control literature. We show, with high confidence, that there is no\nmeaningful difference in performance between Ornstein-Uhlenbeck noise and\nuncorrelated Gaussian noise for exploration with the DDPG algorithm on the\nDMC-CHS.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18840v1.pdf",
        "similarity": 0.24545179377594795,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Deep learning interpretable analysis for carbon star identification in\n  Gaia DR3",
        "new_link": "http://arxiv.org/abs/2407.18754v1",
        "new_summary": "  Context. A large fraction of Asymptotic Giant Branch (AGB) stars develop\ncarbon-rich atmospheres during their evolution. Based on their color and\nluminosity, these carbon stars can be easily distinguished from many other\nkinds of stars. However, a large number of G, K, and M giants are also\ndistributed in the same region as carbon stars on the HR diagram. Their spectra\nhave differences,especially in the prominent CN molecular bands. Aims. We aim\nto distinguish carbon stars from other kinds of stars using Gaia's XP spectra,\nwhile providing attribution explanations of key features necessary for\nidentification, and even discovering additional new spectral key features.\nMethods. We proposed a classification model named `GaiaNet', an improved\none-dimensional convolutional neural network specifically designed for handling\nGaia's XP spectra. We utilized the SHAP interpretability model to calculate the\nSHAP value for each feature point in a spectrum, enabling us to explain the\noutput of the `GaiaNet' model and provide further meaningful analysis Results.\nCompared to four traditional machine-learning methods, the `GaiaNet' model\nexhibits an average classification accuracy improvement of approximately 0.3%\non the validation set, with the highest accuracy even reaching 100%. Utilizing\nthe SHAP algorithm, we present a clear spectroscopic heatmap highlighting\nmolecular band absorption features primarily distributed mainly around CN773.3\nand CN895.0, and summarize five crucial feature regions for carbon star\nidentification. Upon applying the trained classification model to the CSTAR\nsample with Gaia `xp_sampled_mean' spectra, we obtained 451 new candidate\ncarbon stars as a by-product.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18754v1.pdf",
        "similarity": 0.24488287669806091,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Auto DragGAN: Editing the Generative Image Manifold in an Autoregressive\n  Manner",
        "new_link": "http://arxiv.org/abs/2407.18656v1",
        "new_summary": "  Pixel-level fine-grained image editing remains an open challenge. Previous\nworks fail to achieve an ideal trade-off between control granularity and\ninference speed. They either fail to achieve pixel-level fine-grained control,\nor their inference speed requires optimization. To address this, this paper for\nthe first time employs a regression-based network to learn the variation\npatterns of StyleGAN latent codes during the image dragging process. This\nmethod enables pixel-level precision in dragging editing with little time cost.\nUsers can specify handle points and their corresponding target points on any\nGAN-generated images, and our method will move each handle point to its\ncorresponding target point. Through experimental analysis, we discover that a\nshort movement distance from handle points to target points yields a\nhigh-fidelity edited image, as the model only needs to predict the movement of\na small portion of pixels. To achieve this, we decompose the entire movement\nprocess into multiple sub-processes. Specifically, we develop a transformer\nencoder-decoder based network named 'Latent Predictor' to predict the latent\ncode motion trajectories from handle points to target points in an\nautoregressive manner. Moreover, to enhance the prediction stability, we\nintroduce a component named 'Latent Regularizer', aimed at constraining the\nlatent code motion within the distribution of natural images. Extensive\nexperiments demonstrate that our method achieves state-of-the-art (SOTA)\ninference speed and image editing performance at the pixel-level granularity.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18656v1.pdf",
        "similarity": 0.24430459328389828,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Lessons from Learning to Spin \"Pens\"",
        "new_link": "http://arxiv.org/abs/2407.18902v1",
        "new_summary": "  In-hand manipulation of pen-like objects is an important skill in our daily\nlives, as many tools such as hammers and screwdrivers are similarly shaped.\nHowever, current learning-based methods struggle with this task due to a lack\nof high-quality demonstrations and the significant gap between simulation and\nthe real world. In this work, we push the boundaries of learning-based in-hand\nmanipulation systems by demonstrating the capability to spin pen-like objects.\nWe first use reinforcement learning to train an oracle policy with privileged\ninformation and generate a high-fidelity trajectory dataset in simulation. This\nserves two purposes: 1) pre-training a sensorimotor policy in simulation; 2)\nconducting open-loop trajectory replay in the real world. We then fine-tune the\nsensorimotor policy using these real-world trajectories to adapt it to the real\nworld dynamics. With less than 50 trajectories, our policy learns to rotate\nmore than ten pen-like objects with different physical properties for multiple\nrevolutions. We present a comprehensive analysis of our design choices and\nshare the lessons learned during development.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18902v1.pdf",
        "similarity": 0.24150761526522915,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "An Accelerated Multi-level Monte Carlo Approach for Average Reward\n  Reinforcement Learning with General Policy Parametrization",
        "new_link": "http://arxiv.org/abs/2407.18878v1",
        "new_summary": "  In our study, we delve into average-reward reinforcement learning with\ngeneral policy parametrization. Within this domain, current guarantees either\nfall short with suboptimal guarantees or demand prior knowledge of mixing time.\nTo address these issues, we introduce Randomized Accelerated Natural Actor\nCritic, a method that integrates Multi-level Monte-Carlo and Natural Actor\nCritic. Our approach is the first to achieve global convergence rate of\n$\\tilde{\\mathcal{O}}(1/\\sqrt{T})$ without requiring knowledge of mixing time,\nsignificantly surpassing the state-of-the-art bound of\n$\\tilde{\\mathcal{O}}(1/T^{1/4})$.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18878v1.pdf",
        "similarity": 0.2414554656616295,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "A Lyapunov Analysis of Accelerated PDHG Algorithms",
        "new_link": "http://arxiv.org/abs/2407.18681v1",
        "new_summary": "  The generalized Lasso is a remarkably versatile and extensively utilized\nmodel across a broad spectrum of domains, including statistics, machine\nlearning, and image science. Among the optimization techniques employed to\naddress the challenges posed by this model, saddle-point methods stand out for\ntheir effectiveness. In particular, the primal-dual hybrid gradient (PDHG)\nalgorithm has emerged as a highly popular choice, celebrated for its robustness\nand efficiency in finding optimal solutions. Recently, the underlying mechanism\nof the PDHG algorithm has been elucidated through the high-resolution ordinary\ndifferential equation (ODE) and the implicit-Euler scheme as detailed in [Li\nand Shi, 2024a]. This insight has spurred the development of several\naccelerated variants of the PDHG algorithm, originally proposed by [Chambolle\nand Pock, 2011]. By employing discrete Lyapunov analysis, we establish that the\nPDHG algorithm with iteration-varying step sizes, converges at a rate near\n$O(1/k^2)$. Furthermore, for the specific setting where $\\tau_{k+1}\\sigma_k =\ns^2$ and $\\theta_k = \\tau_{k+1}/\\tau_k \\in (0, 1)$ as proposed in [Chambolle\nand Pock, 2011], an even faster convergence rate of $O(1/k^2)$ can be achieved.\nTo substantiate these findings, we design a novel discrete Lyapunov function.\nThis function is distinguished by its succinctness and straightforwardness,\nproviding a clear and elegant proof of the enhanced convergence properties of\nthe PDHG algorithm under the specified conditions. Finally, we utilize the\ndiscrete Lyapunov function to establish the optimal linear convergence rate\nwhen both the objective functions are strongly convex.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18681v1.pdf",
        "similarity": 0.24144948190727028,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18625",
        "published": "2024-07-26"
    },
    {
        "new_title": "Methods to achieve near-millisecond energy relaxation and dephasing\n  times for a superconducting transmon qubit",
        "new_link": "http://arxiv.org/abs/2407.18778v1",
        "new_summary": "  Superconducting qubits are one of the most promising physical systems for\nimplementing a quantum computer. However, executing quantum algorithms of\npractical computational advantage requires further improvements in the\nfidelities of qubit operations, which are currently limited by the energy\nrelaxation and dephasing times of the qubits. Here, we report our measurement\nresults of a high-coherence transmon qubit with energy relaxation and echo\ndephasing times surpassing those in the existing literature. We measure a qubit\nfrequency of 2.890 GHz, an energy relaxation time with a median of 502 us and a\nmaximum of (765 +/- 82.6) us, and an echo dephasing time with a median of 541\nus and a maximum of (1057 +/- 138) us. We report details of our design,\nfabrication process, and measurement setup to facilitate the reproduction and\nwide adoption of high-coherence transmon qubits in the academia and industry.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18778v1.pdf",
        "similarity": 0.23924685817968838,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18625",
        "published": "2024-07-26"
    },
    {
        "new_title": "Self-Prompt Tuning: Enable Autonomous Role-Playing in LLMs",
        "new_link": "http://arxiv.org/abs/2407.08995v1",
        "new_summary": "  Recent advancements in LLMs have showcased their remarkable role-playing\ncapabilities, able to accurately simulate the dialogue styles and cognitive\nprocesses of various roles based on different instructions and contexts.\nStudies indicate that assigning LLMs the roles of experts, a strategy known as\nrole-play prompting, can enhance their performance in the corresponding\ndomains. However, the prompt needs to be manually designed for the given\nproblem, requiring certain expertise and iterative modifications. To this end,\nwe propose self-prompt tuning, making LLMs themselves generate role-play\nprompts through fine-tuning. Leveraging the LIMA dataset as our foundational\ncorpus, we employ GPT-4 to annotate role-play prompts for each data points,\nresulting in the creation of the LIMA-Role dataset. We then fine-tune LLMs like\nLlama-2-7B and Mistral-7B on LIMA-Role. Consequently, the self-prompt tuned\nLLMs can automatically generate expert role prompts for any given question. We\nextensively evaluate self-prompt tuned LLMs on widely used NLP benchmarks and\nopen-ended question test. Our empirical results illustrate that self-prompt\ntuned LLMs outperform standard instruction tuned baselines across most\ndatasets. This highlights the great potential of utilizing fine-tuning to\nenable LLMs to self-prompt, thereby automating complex prompting strategies. We\nrelease the dataset, models, and code at this\n\\href{https://anonymous.4open.science/r/Self-Prompt-Tuning-739E/}{url}.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.08995v1.pdf",
        "similarity": 0.23913090042016624,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-12"
    },
    {
        "new_title": "LLAssist: Simple Tools for Automating Literature Review Using Large\n  Language Models",
        "new_link": "http://arxiv.org/abs/2407.13993v1",
        "new_summary": "  This paper introduces LLAssist, an open-source tool designed to streamline\nliterature reviews in academic research. In an era of exponential growth in\nscientific publications, researchers face mounting challenges in efficiently\nprocessing vast volumes of literature. LLAssist addresses this issue by\nleveraging Large Language Models (LLMs) and Natural Language Processing (NLP)\ntechniques to automate key aspects of the review process. Specifically, it\nextracts important information from research articles and evaluates their\nrelevance to user-defined research questions. The goal of LLAssist is to\nsignificantly reduce the time and effort required for comprehensive literature\nreviews, allowing researchers to focus more on analyzing and synthesizing\ninformation rather than on initial screening tasks. By automating parts of the\nliterature review workflow, LLAssist aims to help researchers manage the\ngrowing volume of academic publications more efficiently.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.13993v1.pdf",
        "similarity": 0.23891611157413842,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-19"
    },
    {
        "new_title": "Distributionally Robust Kalman Filtering over Finite and Infinite\n  Horizon",
        "new_link": "http://arxiv.org/abs/2407.18837v1",
        "new_summary": "  This paper investigates the distributionally robust filtering of signals\ngenerated by state-space models driven by exogenous disturbances with noisy\nobservations in finite and infinite horizon scenarios. The exact joint\nprobability distribution of the disturbances and noise is unknown but assumed\nto reside within a Wasserstein-2 ambiguity ball centered around a given nominal\ndistribution. We aim to derive a causal estimator that minimizes the worst-case\nmean squared estimation error among all possible distributions within this\nambiguity set. We remove the iid restriction in prior works by permitting\narbitrarily time-correlated disturbances and noises. In the finite horizon\nsetting, we reduce this problem to a semi-definite program (SDP), with\ncomputational complexity scaling with the time horizon. For infinite horizon\nsettings, we characterize the optimal estimator using Karush-Kuhn-Tucker (KKT)\nconditions. Although the optimal estimator lacks a rational form, i.e., a\nfinite-dimensional state-space realization, it can be fully described by a\nfinite-dimensional parameter. {Leveraging this parametrization, we propose\nefficient algorithms that compute the optimal estimator with arbitrary fidelity\nin the frequency domain.} Moreover, given any finite degree, we provide an\nefficient convex optimization algorithm that finds the finite-dimensional\nstate-space estimator that best approximates the optimal non-rational filter in\n${\\cal H}_\\infty$ norm. This facilitates the practical implementation of the\ninfinite horizon filter without having to grapple with the ill-scaled SDP from\nfinite time. Finally, numerical simulations demonstrate the effectiveness of\nour approach in practical scenarios.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18837v1.pdf",
        "similarity": 0.23557993405433086,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Two Stacks Are Better Than One: A Comparison of Language Modeling and\n  Translation as Multilingual Pretraining Objectives",
        "new_link": "http://arxiv.org/abs/2407.15489v1",
        "new_summary": "  Pretrained language models (PLMs) display impressive performances and have\ncaptured the attention of the NLP community. Establishing the best practices in\npretraining has therefore become a major point of focus for much of NLP\nresearch -- especially since the insights developed for monolingual English\nmodels need not carry to more complex multilingual. One significant caveat of\nthe current state of the art is that different works are rarely comparable:\nthey often discuss different parameter counts, training data, and evaluation\nmethodology.\n  This paper proposes a comparison of multilingual pretraining objectives in a\ncontrolled methodological environment. We ensure that training data and model\narchitectures are comparable, and discuss the downstream performances across 6\nlanguages that we observe in probing and fine-tuning scenarios. We make two key\nobservations: (1) the architecture dictates which pretraining objective is\noptimal; (2) multilingual translation is a very effective pre-training\nobjective under the right conditions. We make our code, data, and model weights\navailable at \\texttt{\\url{https://github.com/Helsinki-NLP/lm-vs-mt}}.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.15489v1.pdf",
        "similarity": 0.23389766854724195,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-22"
    },
    {
        "new_title": "Learning to Enhance Aperture Phasor Field for Non-Line-of-Sight Imaging",
        "new_link": "http://arxiv.org/abs/2407.18574v1",
        "new_summary": "  This paper aims to facilitate more practical NLOS imaging by reducing the\nnumber of samplings and scan areas. To this end, we introduce a phasor-based\nenhancement network that is capable of predicting clean and full measurements\nfrom noisy partial observations. We leverage a denoising autoencoder scheme to\nacquire rich and noise-robust representations in the measurement space. Through\nthis pipeline, our enhancement network is trained to accurately reconstruct\ncomplete measurements from their corrupted and partial counterparts. However,\nwe observe that the \\naive application of denoising often yields degraded and\nover-smoothed results, caused by unnecessary and spurious frequency signals\npresent in measurements. To address this issue, we introduce a phasor-based\npipeline designed to limit the spectrum of our network to the frequency range\nof interests, where the majority of informative signals are detected. The\nphasor wavefronts at the aperture, which are band-limited signals, are employed\nas inputs and outputs of the network, guiding our network to learn from the\nfrequency range of interests and discard unnecessary information. The\nexperimental results in more practical acquisition scenarios demonstrate that\nwe can look around the corners with $16\\times$ or $64\\times$ fewer samplings\nand $4\\times$ smaller apertures. Our code is available at\n\\url{https://github.com/join16/LEAP}.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18574v1.pdf",
        "similarity": 0.23267228017028077,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Code Structure-Aware through Line-level Semantic Learning for Code\n  Vulnerability Detection",
        "new_link": "http://arxiv.org/abs/2407.18877v1",
        "new_summary": "  Different from the flow semantics of natural languages, programming languages\nare inherently rigid in structure and grammar. Existing fine-tuning\nmethodologies for code vulnerability detection generally treat code as long\ntext sequences, stripping away structural elements such as newlines ('/n') and\nwhitespace. However, this approach inadvertently results in the loss of crucial\nstructural information, diminishing the distinct characteristics of code and\nimpairing the accuracy of vulnerability detection. To address these challenges,\nwe propose a novel network architecture method based on pre-trained code\nmodels, which incorporates structural information awareness. We propose an\nenhanced code text processing workflow that retains structural elements prior\nto modeling. This refinement allows the model to retain and exploit line-level\nstructural information and semantic information during the modeling process.\nFurthermore, we introduce a new network architecture, the Code Structure-Aware\nNetwork through Line-level Semantic Learning (CSLS), which integrates three key\ncomponents: global vulnerability awareness, line-structural awareness, and\nsensitive-line awareness. We have conducted comprehensive experiments using\nvulnerability detection datasets from real-world projects. Extensive\nexperiments were conducted on vulnerability detection datasets derived from\nreal-world projects. The results demonstrate that our new code pre-processing\nflow significantly improves existing baselines (e.g., a 3\\% accuracy\nimprovement on the Devign dataset when applied to popular models such as\nCoderBert and UniXcoder). The proposed network architecture also demonstrates\nsuperior accuracy in detecting vulnerabilities, surpassing newly established\nbenchmarks. These findings underscore the importance of structural information\nin enhancing the efficacy of code vulnerability detection models.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18877v1.pdf",
        "similarity": 0.23247893506364262,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Ensemble Kalman inversion approximate Bayesian computation",
        "new_link": "http://arxiv.org/abs/2407.18721v1",
        "new_summary": "  Approximate Bayesian computation (ABC) is the most popular approach to\ninferring parameters in the case where the data model is specified in the form\nof a simulator. It is not possible to directly implement standard Monte Carlo\nmethods for inference in such a model, due to the likelihood not being\navailable to evaluate pointwise. The main idea of ABC is to perform inference\non an alternative model with an approximate likelihood (the ABC likelihood),\nestimated at each iteration from points simulated from the data model. The\ncentral challenge of ABC is then to trade-off bias (introduced by approximating\nthe model) with the variance introduced by estimating the ABC likelihood.\nStabilising the variance of the ABC likelihood requires a computational cost\nthat is exponential in the dimension of the data, thus the most common approach\nto reducing variance is to perform inference conditional on summary statistics.\nIn this paper we introduce a new approach to estimating the ABC likelihood:\nusing iterative ensemble Kalman inversion (IEnKI) (Iglesias, 2016; Iglesias et\nal., 2018). We first introduce new estimators of the marginal likelihood in the\ncase of a Gaussian data model using the IEnKI output, then show how this may be\nused in ABC. Performance is illustrated on the Lotka-Volterra model, where we\nobserve substantial improvements over standard ABC and other commonly-used\napproaches.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18721v1.pdf",
        "similarity": 0.23223700140184936,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "MNTD: An Efficient Dynamic Community Detector Based on Nonnegative\n  Tensor Decomposition",
        "new_link": "http://arxiv.org/abs/2407.18849v1",
        "new_summary": "  Dynamic community detection is crucial for elucidating the temporal evolution\nof social structures, information dissemination, and interactive behaviors\nwithin complex networks. Nonnegative matrix factorization provides an efficient\nframework for identifying communities in static networks but fall short in\ndepicting temporal variations in community affiliations. To solve this problem,\nthis paper proposes a Modularity maximization-incorporated Nonnegative Tensor\nRESCAL Decomposition (MNTD) model for dynamic community detection. This method\nserves two primary functions: a) Nonnegative tensor RESCAL decomposition\nextracts latent community structures in different time slots, highlighting the\npersistence and transformation of communities; and b) Incorporating an initial\ncommunity structure into the modularity maximization algorithm, facilitating\nmore precise community segmentations. Comparative analysis of real-world\ndatasets shows that the MNTD is superior to state-of-the-art dynamic community\ndetection methods in the accuracy of community detection.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18849v1.pdf",
        "similarity": 0.23138806533856393,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "AppWorld: A Controllable World of Apps and People for Benchmarking\n  Interactive Coding Agents",
        "new_link": "http://arxiv.org/abs/2407.18901v1",
        "new_summary": "  Autonomous agents that address day-to-day digital tasks (e.g., ordering\ngroceries for a household), must not only operate multiple apps (e.g., notes,\nmessaging, shopping app) via APIs, but also generate rich code with complex\ncontrol flow in an iterative manner based on their interaction with the\nenvironment. However, existing benchmarks for tool use are inadequate, as they\nonly cover tasks that require a simple sequence of API calls.\n  To remedy this gap, we built $\\textbf{AppWorld Engine}$, a high-quality\nexecution environment (60K lines of code) of 9 day-to-day apps operable via 457\nAPIs and populated with realistic digital activities simulating the lives of\n~100 fictitious users. We then created $\\textbf{AppWorld Benchmark}$ (40K lines\nof code), a suite of 750 natural, diverse, and challenging autonomous agent\ntasks requiring rich and interactive code generation. It supports robust\nprogrammatic evaluation with state-based unit tests, allowing for different\nways of completing a task while also checking for unexpected changes, i.e.,\ncollateral damage. The state-of-the-art LLM, GPT-4o, solves only ~49% of our\n'normal' tasks and ~30% of 'challenge' tasks, while other models solve at least\n16% fewer. This highlights the benchmark's difficulty and AppWorld's potential\nto push the frontiers of interactive coding agents. The project website is\navailable at https://appworld.dev/.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18901v1.pdf",
        "similarity": 0.23099074302133643,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18625",
        "published": "2024-07-26"
    },
    {
        "new_title": "Simulation Experiment Design for Calibration via Active Learning",
        "new_link": "http://arxiv.org/abs/2407.18885v1",
        "new_summary": "  Simulation models often have parameters as input and return outputs to\nunderstand the behavior of complex systems. Calibration is the process of\nestimating the values of the parameters in a simulation model in light of\nobserved data from the system that is being simulated. When simulation models\nare expensive, emulators are built with simulation data as a computationally\nefficient approximation of an expensive model. An emulator then can be used to\npredict model outputs, instead of repeatedly running an expensive simulation\nmodel during the calibration process. Sequential design with an intelligent\nselection criterion can guide the process of collecting simulation data to\nbuild an emulator, making the calibration process more efficient and effective.\nThis article proposes two novel criteria for sequentially acquiring new\nsimulation data in an active learning setting by considering uncertainties on\nthe posterior density of parameters. Analysis of several simulation experiments\nand real-data simulation experiments from epidemiology demonstrates that\nproposed approaches result in improved posterior and field predictions.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18885v1.pdf",
        "similarity": 0.22932379566974187,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "SHIC: Shape-Image Correspondences with no Keypoint Supervision",
        "new_link": "http://arxiv.org/abs/2407.18907v1",
        "new_summary": "  Canonical surface mapping generalizes keypoint detection by assigning each\npixel of an object to a corresponding point in a 3D template. Popularised by\nDensePose for the analysis of humans, authors have since attempted to apply the\nconcept to more categories, but with limited success due to the high cost of\nmanual supervision. In this work, we introduce SHIC, a method to learn\ncanonical maps without manual supervision which achieves better results than\nsupervised methods for most categories. Our idea is to leverage foundation\ncomputer vision models such as DINO and Stable Diffusion that are open-ended\nand thus possess excellent priors over natural categories. SHIC reduces the\nproblem of estimating image-to-template correspondences to predicting\nimage-to-image correspondences using features from the foundation models. The\nreduction works by matching images of the object to non-photorealistic renders\nof the template, which emulates the process of collecting manual annotations\nfor this task. These correspondences are then used to supervise high-quality\ncanonical maps for any object of interest. We also show that image generators\ncan further improve the realism of the template views, which provide an\nadditional source of supervision for the model.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18907v1.pdf",
        "similarity": 0.22922960348066573,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Automate or Assist? The Role of Computational Models in Identifying\n  Gendered Discourse in US Capital Trial Transcripts",
        "new_link": "http://arxiv.org/abs/2407.12500v1",
        "new_summary": "  The language used by US courtroom actors in criminal trials has long been\nstudied for biases. However, systematic studies for bias in high-stakes court\ntrials have been difficult, due to the nuanced nature of bias and the legal\nexpertise required. New large language models offer the possibility to automate\nannotation, saving time and cost. But validating these approaches requires both\nhigh quantitative performance as well as an understanding of how automated\nmethods fit in existing workflows, and what they really offer. In this paper we\npresent a case study of adding an automated system to a complex and high-stakes\nproblem: identifying gender-biased language in US capital trials for women\ndefendants. Our team of experienced death-penalty lawyers and NLP technologists\npursued a three-phase study: first annotating manually, then training and\nevaluating computational models, and finally comparing human annotations to\nmodel predictions. Unlike many typical NLP tasks, annotating for gender bias in\nmonths-long capital trials was a complicated task that involves with many\nindividual judgment calls. In contrast to standard arguments for automation\nthat are based on efficiency and scalability, legal experts found the\ncomputational models most useful in challenging their personal bias in\nannotation and providing opportunities to refine and build consensus on rules\nfor annotation. This suggests that seeking to replace experts with\ncomputational models is both unrealistic and undesirable. Rather, computational\nmodels offer valuable opportunities to assist the legal experts in\nannotation-based studies.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.12500v1.pdf",
        "similarity": 0.22729288483193946,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-17"
    },
    {
        "new_title": "IOVS4NeRF:Incremental Optimal View Selection for Large-Scale NeRFs",
        "new_link": "http://arxiv.org/abs/2407.18611v1",
        "new_summary": "  Urban-level three-dimensional reconstruction for modern applications demands\nhigh rendering fidelity while minimizing computational costs. The advent of\nNeural Radiance Fields (NeRF) has enhanced 3D reconstruction, yet it exhibits\nartifacts under multiple viewpoints. In this paper, we propose a new NeRF\nframework method to address these issues. Our method uses image content and\npose data to iteratively plan the next best view. A crucial aspect of this\nmethod involves uncertainty estimation, guiding the selection of views with\nmaximum information gain from a candidate set. This iterative process enhances\nrendering quality over time. Simultaneously, we introduce the Vonoroi diagram\nand threshold sampling together with flight classifier to boost the efficiency,\nwhile keep the original NeRF network intact. It can serve as a plug-in tool to\nassist in better rendering, outperforming baselines and similar prior works.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18611v1.pdf",
        "similarity": 0.2261570216540203,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "ChatSchema: A pipeline of extracting structured information with Large\n  Multimodal Models based on schema",
        "new_link": "http://arxiv.org/abs/2407.18716v1",
        "new_summary": "  Objective: This study introduces ChatSchema, an effective method for\nextracting and structuring information from unstructured data in medical paper\nreports using a combination of Large Multimodal Models (LMMs) and Optical\nCharacter Recognition (OCR) based on the schema. By integrating predefined\nschema, we intend to enable LMMs to directly extract and standardize\ninformation according to the schema specifications, facilitating further data\nentry. Method: Our approach involves a two-stage process, including\nclassification and extraction for categorizing report scenarios and structuring\ninformation. We established and annotated a dataset to verify the effectiveness\nof ChatSchema, and evaluated key extraction using precision, recall, F1-score,\nand accuracy metrics. Based on key extraction, we further assessed value\nextraction. We conducted ablation studies on two LMMs to illustrate the\nimprovement of structured information extraction with different input modals\nand methods. Result: We analyzed 100 medical reports from Peking University\nFirst Hospital and established a ground truth dataset with 2,945 key-value\npairs. We evaluated ChatSchema using GPT-4o and Gemini 1.5 Pro and found a\nhigher overall performance of GPT-4o. The results are as follows: For the\nresult of key extraction, key-precision was 98.6%, key-recall was 98.5%,\nkey-F1-score was 98.6%. For the result of value extraction based on correct key\nextraction, the overall accuracy was 97.2%, precision was 95.8%, recall was\n95.8%, and F1-score was 95.8%. An ablation study demonstrated that ChatSchema\nachieved significantly higher overall accuracy and overall F1-score of\nkey-value extraction, compared to the Baseline, with increases of 26.9% overall\naccuracy and 27.4% overall F1-score, respectively.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18716v1.pdf",
        "similarity": 0.22360856124895712,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Do LLMs Know When to NOT Answer? Investigating Abstention Abilities of\n  Large Language Models",
        "new_link": "http://arxiv.org/abs/2407.16221v1",
        "new_summary": "  As Large Language Models (LLMs) achieve remarkable performance across various\nNLP tasks, their reliability becomes essential for widespread adoption. This\npaper focuses on Abstention Ability (AA), a critical yet under explored aspect\nof reliability - the ability of LLMs to refrain from answering questions when\nthey are uncertain or when definitive answer is not possible, while maintaining\nquestion-answering (QA) task performance. While previous works have focused on\nunderstanding the recollection abilities of LLMs or their ability to identify\nimponderable/unanswerable questions, we believe there is a need for an\neffective AA evaluation method. Therefore, we propose a black-box evaluation\nmethodology to examine and understand the AA of LLMs across a variety of\nmultiple-choice QA tasks. We measure AA by rewarding models for abstaining from\nanswering when their predictions are incorrect or when the questions are\ninherently unanswerable. We investigate three strategies, Strict Prompting,\nVerbal Confidence Thresholding, and Chain-of-Thought (CoT), to understand their\nimpact on abstention across different LLMs. Our findings reveal that while even\nstate-of-the-art LLMs like GPT-4 struggle with abstention, strategic prompting\nsuch as CoT, can significantly enhance this ability. Furthermore, we\ndemonstrate that improving AA also leads to better overall QA task performance,\nunderscoring the importance of evaluating AA in LLMs.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.16221v1.pdf",
        "similarity": 0.21904496031281367,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-23"
    },
    {
        "new_title": "Towards Generalized Offensive Language Identification",
        "new_link": "http://arxiv.org/abs/2407.18738v1",
        "new_summary": "  The prevalence of offensive content on the internet, encompassing hate speech\nand cyberbullying, is a pervasive issue worldwide. Consequently, it has\ngarnered significant attention from the machine learning (ML) and natural\nlanguage processing (NLP) communities. As a result, numerous systems have been\ndeveloped to automatically identify potentially harmful content and mitigate\nits impact. These systems can follow two approaches; (1) Use publicly available\nmodels and application endpoints, including prompting large language models\n(LLMs) (2) Annotate datasets and train ML models on them. However, both\napproaches lack an understanding of how generalizable they are. Furthermore,\nthe applicability of these systems is often questioned in off-domain and\npractical environments. This paper empirically evaluates the generalizability\nof offensive language detection models and datasets across a novel generalized\nbenchmark. We answer three research questions on generalizability. Our findings\nwill be useful in creating robust real-world offensive language detection\nsystems.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18738v1.pdf",
        "similarity": 0.21758782126612974,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "SAFETY-J: Evaluating Safety with Critique",
        "new_link": "http://arxiv.org/abs/2407.17075v2",
        "new_summary": "  The deployment of Large Language Models (LLMs) in content generation raises\nsignificant safety concerns, particularly regarding the transparency and\ninterpretability of content evaluations. Current methods, primarily focused on\nbinary safety classifications, lack mechanisms for detailed critique, limiting\ntheir utility for model improvement and user trust. To address these\nlimitations, we introduce SAFETY-J, a bilingual generative safety evaluator for\nEnglish and Chinese with critique-based judgment. SAFETY-J utilizes a robust\ntraining dataset that includes diverse dialogues and augmented query-response\npairs to assess safety across various scenarios comprehensively. We establish\nan automated meta-evaluation benchmark that objectively assesses the quality of\ncritiques with minimal human intervention, facilitating scalable and continuous\nimprovement. Additionally, SAFETY-J employs an iterative preference learning\ntechnique to dynamically refine safety assessments based on meta-evaluations\nand critiques. Our evaluations demonstrate that SAFETY-J provides more nuanced\nand accurate safety evaluations, thereby enhancing both critique quality and\npredictive reliability in complex content scenarios. To facilitate further\nresearch and application, we open-source SAFETY-J's training protocols,\ndatasets, and code at \\url{https://github.com/GAIR-NLP/Safety-J}.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.17075v2.pdf",
        "similarity": 0.21703011364412522,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-24"
    },
    {
        "new_title": "A Physics-Informed Neural Network-Based Approach for the Spatial\n  Upsampling of Spherical Microphone Arrays",
        "new_link": "http://arxiv.org/abs/2407.18732v1",
        "new_summary": "  Spherical microphone arrays are convenient tools for capturing the spatial\ncharacteristics of a sound field. However, achieving superior spatial\nresolution requires arrays with numerous capsules, consequently leading to\nexpensive devices. To address this issue, we present a method for spatially\nupsampling spherical microphone arrays with a limited number of capsules. Our\napproach exploits a physics-informed neural network with Rowdy activation\nfunctions, leveraging physical constraints to provide high-order microphone\narray signals, starting from low-order devices. Results show that, within its\ndomain of application, our approach outperforms a state of the art method based\non signal processing for spherical microphone arrays upsampling.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18732v1.pdf",
        "similarity": 0.21672698037775756,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18625",
        "published": "2024-07-26"
    },
    {
        "new_title": "Online Planning in POMDPs with State-Requests",
        "new_link": "http://arxiv.org/abs/2407.18812v1",
        "new_summary": "  In key real-world problems, full state information is sometimes available but\nonly at a high cost, like activating precise yet energy-intensive sensors or\nconsulting humans, thereby compelling the agent to operate under partial\nobservability. For this scenario, we propose AEMS-SR (Anytime Error\nMinimization Search with State Requests), a principled online planning\nalgorithm tailored for POMDPs with state requests. By representing the search\nspace as a graph instead of a tree, AEMS-SR avoids the exponential growth of\nthe search space originating from state requests. Theoretical analysis\ndemonstrates AEMS-SR's $\\varepsilon$-optimality, ensuring solution quality,\nwhile empirical evaluations illustrate its effectiveness compared with AEMS and\nPOMCP, two SOTA online planning algorithms. AEMS-SR enables efficient planning\nin domains characterized by partial observability and costly state requests\noffering practical benefits across various applications.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18812v1.pdf",
        "similarity": 0.21647730433656603,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18625",
        "published": "2024-07-26"
    },
    {
        "new_title": "Q-gen: A Parameterized Quantum Circuit Generator",
        "new_link": "http://arxiv.org/abs/2407.18697v1",
        "new_summary": "  Unlike most classical algorithms that take an input and give the solution\ndirectly as an output, quantum algorithms produce a quantum circuit that works\nas an indirect solution to computationally hard problems. In the full quantum\ncomputing workflow, most data processing remains in the classical domain except\nfor running the quantum circuit in the quantum processor. This leaves massive\nopportunities for classical automation and optimization toward future\nutilization of quantum computing. We kickstart the first step in this direction\nby introducing Q-gen, a high-level, parameterized quantum circuit generator\nincorporating 15 realistic quantum algorithms. Each customized generation\nfunction comes with algorithm-specific parameters beyond the number of qubits,\nproviding a large generation volume with high circuit variability. To\ndemonstrate the functionality of Q-gen, we organize the algorithms into 5\nhierarchical systems and generate a quantum circuit dataset accompanied by\ntheir measurement histograms and state vectors. This dataset enables\nresearchers to statistically analyze the structure, complexity, and performance\nof large-scale quantum circuits, or quickly train novel machine learning models\nwithout worrying about the exponentially growing simulation time. Q-gen is an\nopen-source and multipurpose project that serves as the entrance for users with\na classical computer science background to dive into the world of quantum\ncomputing.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18697v1.pdf",
        "similarity": 0.2150146286608016,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Scalable Group Choreography via Variational Phase Manifold Learning",
        "new_link": "http://arxiv.org/abs/2407.18839v1",
        "new_summary": "  Generating group dance motion from the music is a challenging task with\nseveral industrial applications. Although several methods have been proposed to\ntackle this problem, most of them prioritize optimizing the fidelity in dancing\nmovement, constrained by predetermined dancer counts in datasets. This\nlimitation impedes adaptability to real-world applications. Our study addresses\nthe scalability problem in group choreography while preserving naturalness and\nsynchronization. In particular, we propose a phase-based variational generative\nmodel for group dance generation on learning a generative manifold. Our method\nachieves high-fidelity group dance motion and enables the generation with an\nunlimited number of dancers while consuming only a minimal and constant amount\nof memory. The intensive experiments on two public datasets show that our\nproposed method outperforms recent state-of-the-art approaches by a large\nmargin and is scalable to a great number of dancers beyond the training data.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18839v1.pdf",
        "similarity": 0.21476237665094142,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "ADMM Based Semi-Structured Pattern Pruning Framework For Transformer",
        "new_link": "http://arxiv.org/abs/2407.08334v3",
        "new_summary": "  NLP(natural language processsing) has achieved great success through the\ntransformer model.However, the model has hundreds of millions or billions\nparameters,which is huge burden for its deployment on personal computer or\nsmall scale of server.To deal with it, we either make the model's weight matrix\nrelatively sparser, or compress attention layer. Pattern pruning ,one of the\nmost important pruning methods, permits selecting fixed number of parameters in\neach divided pattern block and prunes it. However, the effect of pattern\npruning is strictly limited by the sparsity within a region of weights in each\nlayer. In this paper,we first introduced Alternating Direction Method of\nMultipliers(ADMM) based pattern pruning framework to reshape the distribution\nof activation map. Specifically, we propose to formulate the pattern pruning on\ntransformer as a constrained optimization and use ADMM to optimize the problem.\nIn this way, the initial dense feature maps is transformed to rather regionally\nsparsified ones.Therefore, we can then achieve higher compression ratio with\nbetter performance based on pattern pruning method. Additionally, this paper\nprovides a theoretical derivations of the ADMM with local sparsity. Finally, we\nalso extend the proposed ADMM based framework with SR-STE to demonstrate its\ngeneralization and to avoid gradient vanishing problem. We conduct extensive\nexperiments on classification tasks over GLUE datasets. Significantly, we\nachieve 50% percent compression ratio while maintaining overall score 80.1 on\nGLUE dataset.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.08334v3.pdf",
        "similarity": 0.21235070783110585,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-11"
    },
    {
        "new_title": "Tuning the kinetics of intracellular transport",
        "new_link": "http://arxiv.org/abs/2407.18784v1",
        "new_summary": "  A variety of complex mechanisms, from chemical reaction pathways to active\nfluctuations, orchestrate molecular transport in intracellular environments.\nDespite significant recent progress in visualizing and probing these processes,\nlittle is known about how tunable the resulting dynamics is through external\nphysical controllers. Here, we demonstrate that coarse-grained, reinforcement\nlearning-based protocols can be developed to achieve highly localized and\ntargeted cargo transport by kinesin motors on intracellular tracks. These\nprotocols can be implemented in practice using optical tweezers, and their\nfeasibility is showcased within experimentally relevant parameter regimes. Our\nresults open new avenues for targeted control of intracellular transport\nprocesses, especially when opportunities for control are limited.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18784v1.pdf",
        "similarity": 0.21219500049050746,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18625",
        "published": "2024-07-26"
    },
    {
        "new_title": "A data balancing approach designing of an expert system for Heart\n  Disease Prediction",
        "new_link": "http://arxiv.org/abs/2407.18606v1",
        "new_summary": "  Heart disease is a major global health concern that results in millions of\ndeaths annually. Prevention and effective treatment of heart-related problems\ndepend heavily on early detection and accurate prediction. It was previously\npredicted accurately with machine learning methods. This innovative development\nin healthcare has the power to transform preventative care and save a great\ndeal of lives. The study starts with a thorough assessment of the literature\nthat covers a wide range of topics, including pre-processing techniques,\nperformance evaluation measures, datasets used in heart disease research,\npredictive modeling strategies, diagnostic methodologies, and current issues in\nthe field. Building on these fundamental understandings, the background section\ndescribes the particular actions conducted in this investigation, such as the\ndescription of the dataset, data pre-treatment techniques, label encoding,\nfeature selection methodology, algorithm selection tactics, and stringent\nperformance evaluation techniques.The results indicate that ensemble methods,\nparticularly random forests, outperformed individual classifiers in predicting\nheart disease. Key predictors identified included hypertension, cholesterol\nlevels, smoking status, and physical inactivity. The Decision Tree and Random\nForest model achieved an accuracy of 99.83%. This work demonstrates how machine\nlearning models, particularly ensemble approaches, can increase the precision\nof heart disease prediction. In comparison to conventional techniques, the\nmodels offer a more reliable risk assessment since they integrate a wide range\nof variables and sophisticated algorithms. The results open the door to\ntailored healthcare treatments that facilitate early identification and\ntreatment of cardiac disease.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18606v1.pdf",
        "similarity": 0.21171177399057237,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "DynamicTrack: Advancing Gigapixel Tracking in Crowded Scenes",
        "new_link": "http://arxiv.org/abs/2407.18637v1",
        "new_summary": "  Tracking in gigapixel scenarios holds numerous potential applications in\nvideo surveillance and pedestrian analysis. Existing algorithms attempt to\nperform tracking in crowded scenes by utilizing multiple cameras or group\nrelationships. However, their performance significantly degrades when\nconfronted with complex interaction and occlusion inherent in gigapixel images.\nIn this paper, we introduce DynamicTrack, a dynamic tracking framework designed\nto address gigapixel tracking challenges in crowded scenes. In particular, we\npropose a dynamic detector that utilizes contrastive learning to jointly detect\nthe head and body of pedestrians. Building upon this, we design a dynamic\nassociation algorithm that effectively utilizes head and body information for\nmatching purposes. Extensive experiments show that our tracker achieves\nstate-of-the-art performance on widely used tracking benchmarks specifically\ndesigned for gigapixel crowded scenes.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18637v1.pdf",
        "similarity": 0.21053362181134083,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Decoding Climate Disagreement: A Graph Neural Network-Based Approach to\n  Understanding Social Media Dynamics",
        "new_link": "http://arxiv.org/abs/2407.07038v1",
        "new_summary": "  This work introduces the ClimateSent-GAT Model, an innovative method that\nintegrates Graph Attention Networks (GATs) with techniques from natural\nlanguage processing to accurately identify and predict disagreements within\nReddit comment-reply pairs. Our model classifies disagreements into three\ncategories: agree, disagree, and neutral. Leveraging the inherent graph\nstructure of Reddit comment-reply pairs, the model significantly outperforms\nexisting benchmarks by capturing complex interaction patterns and sentiment\ndynamics. This research advances graph-based NLP methodologies and provides\nactionable insights for policymakers and educators in climate science\ncommunication.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.07038v1.pdf",
        "similarity": 0.20893546143425662,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-09"
    },
    {
        "new_title": "Granularity is crucial when applying differential privacy to text: An\n  investigation for neural machine translation",
        "new_link": "http://arxiv.org/abs/2407.18789v1",
        "new_summary": "  Applying differential privacy (DP) by means of the DP-SGD algorithm to\nprotect individual data points during training is becoming increasingly popular\nin NLP. However, the choice of granularity at which DP is applied is often\nneglected. For example, neural machine translation (NMT) typically operates on\nthe sentence-level granularity. From the perspective of DP, this setup assumes\nthat each sentence belongs to a single person and any two sentences in the\ntraining dataset are independent. This assumption is however violated in many\nreal-world NMT datasets, e.g. those including dialogues. For proper application\nof DP we thus must shift from sentences to entire documents. In this paper, we\ninvestigate NMT at both the sentence and document levels, analyzing the\nprivacy/utility trade-off for both scenarios, and evaluating the risks of not\nusing the appropriate privacy granularity in terms of leaking personally\nidentifiable information (PII). Our findings indicate that the document-level\nNMT system is more resistant to membership inference attacks, emphasizing the\nsignificance of using the appropriate granularity when working with DP.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18789v1.pdf",
        "similarity": 0.20698783942943574,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "Electron Mobilities in SrTiO$_3$ and KTaO$_3$: Role of Phonon\n  Anharmonicity, Mass Renormalization and Disorder",
        "new_link": "http://arxiv.org/abs/2407.18771v1",
        "new_summary": "  Accurately predicting carrier mobility in strongly anharmonic solids\nnecessitates a precise characterization of lattice dyndamics as a function of\ntemperature. We achieve consistency with experimental electron mobility data\nfor bulk KTaO$_3$ and SrTiO$_3$ above 150 K by refining the Boltzmann transport\nequations. This refinement includes incorporating temperature-dependent\nanharmonic phonon eigenfrequencies and eigenmodes into the electron-phonon\ninteraction tensor, while maintaining the derivatives of the Kohn-Sham\npotential as computed in density functional perturbation theory. Using\nefficient machine-learned force fields and the stochastic self-consistent\nharmonic approximation, we accurately compute the dynamical matrices. At room\ntemperature, the calculated mobility for SrTiO$_3$ exceeds experimental values\nby an order of magnitude, whereas the overestimation for KTaO$_3$ is way less\npronounced. This discrepancy is explained through the more significant electron\nmass renormalization near the conduction band bottom due to anharmonic\nelectron-phonon coupling and the presence of local disorder in SrTiO$_3$.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18771v1.pdf",
        "similarity": 0.20691147142319405,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18625",
        "published": "2024-07-26"
    },
    {
        "new_title": "LinguaLinker: Audio-Driven Portraits Animation with Implicit Facial\n  Control Enhancement",
        "new_link": "http://arxiv.org/abs/2407.18595v1",
        "new_summary": "  This study delves into the intricacies of synchronizing facial dynamics with\nmultilingual audio inputs, focusing on the creation of visually compelling,\ntime-synchronized animations through diffusion-based techniques. Diverging from\ntraditional parametric models for facial animation, our approach, termed\nLinguaLinker, adopts a holistic diffusion-based framework that integrates\naudio-driven visual synthesis to enhance the synergy between auditory stimuli\nand visual responses. We process audio features separately and derive the\ncorresponding control gates, which implicitly govern the movements in the\nmouth, eyes, and head, irrespective of the portrait's origin. The advanced\naudio-driven visual synthesis mechanism provides nuanced control but keeps the\ncompatibility of output video and input audio, allowing for a more tailored and\neffective portrayal of distinct personas across different languages. The\nsignificant improvements in the fidelity of animated portraits, the accuracy of\nlip-syncing, and the appropriate motion variations achieved by our method\nrender it a versatile tool for animating any portrait in any language.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18595v1.pdf",
        "similarity": 0.2043006648104583,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18625",
        "published": "2024-07-26"
    },
    {
        "new_title": "LLASP: Fine-tuning Large Language Models for Answer Set Programming",
        "new_link": "http://arxiv.org/abs/2407.18723v1",
        "new_summary": "  Recently, Large Language Models (LLMs) have showcased their potential in\nvarious natural language processing tasks, including code generation. However,\nwhile significant progress has been made in adapting LLMs to generate code for\nseveral imperative programming languages and tasks, there remains a notable gap\nin their application to declarative formalisms, such as Answer Set Programming\n(ASP). In this paper, we move a step towards exploring the capabilities of LLMs\nfor ASP code generation. First, we perform a systematic evaluation of several\nstate-of-the-art LLMs. Despite their power in terms of number of parameters,\ntraining data and computational resources, empirical results demonstrate\ninadequate performances in generating correct ASP programs. Therefore, we\npropose LLASP, a fine-tuned lightweight model specifically trained to encode\nfundamental ASP program patterns. To this aim, we create an ad-hoc dataset\ncovering a wide variety of fundamental problem specifications that can be\nencoded in ASP. Our experiments demonstrate that the quality of ASP programs\ngenerated by LLASP is remarkable. This holds true not only when compared to the\nnon-fine-tuned counterpart but also when compared to the majority of eager LLM\ncandidates, particularly from a semantic perspective. All the code and data\nused to perform the experiments are publicly available at\nhttps://anonymous.4open.science/r/LLASP-D86C/.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.18723v1.pdf",
        "similarity": 0.20374974603598767,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-26"
    },
    {
        "new_title": "ILiAD: An Interactive Corpus for Linguistic Annotated Data from Twitter\n  Posts",
        "new_link": "http://arxiv.org/abs/2407.15374v1",
        "new_summary": "  Social Media platforms have offered invaluable opportunities for linguistic\nresearch. The availability of up-to-date data, coming from any part in the\nworld, and coming from natural contexts, has allowed researchers to study\nlanguage in real time. One of the fields that has made great use of social\nmedia platforms is Corpus Linguistics. There is currently a wide range of\nprojects which have been able to successfully create corpora from social media.\nIn this paper, we present the development and deployment of a linguistic corpus\nfrom Twitter posts in English, coming from 26 news agencies and 27 individuals.\nThe main goal was to create a fully annotated English corpus for linguistic\nanalysis. We include information on morphology and syntax, as well as NLP\nfeatures such as tokenization, lemmas, and n- grams. The information is\npresented through a range of powerful visualisations for users to explore\nlinguistic patterns in the corpus. With this tool, we aim to contribute to the\narea of language technologies applied to linguistic research.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.15374v1.pdf",
        "similarity": 0.20374399789531686,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-22"
    },
    {
        "new_title": "QuOp: A Quantum Operator Representation for Nodes",
        "new_link": "http://arxiv.org/abs/2407.14281v1",
        "new_summary": "  We derive an intuitive and novel method to represent nodes in a graph with\nspecial unitary operators, or quantum operators, which does not require\nparameter training and is competitive with classical methods on scoring\nsimilarity between nodes. This method opens up future possibilities to apply\nquantum algorithms for NLP or other applications that need to detect anomalies\nwithin a network structure. Specifically, this technique leverages the\nadvantage of quantum computation, representing nodes in higher dimensional\nHilbert spaces. To create the representations, the local topology around each\nnode with a predetermined number of hops is calculated and the respective\nadjacency matrix is used to derive the Hamiltonian. While using the local\ntopology of a node to derive a Hamiltonian is a natural extension of a graph\ninto a quantum circuit, our method differs by not assuming the quantum\noperators in the representation a priori, but letting the adjacency matrix\ndictate the representation. As a consequence of this simplicity, the set of\nadjacency matrices of size $2^n \\times 2^n$ generates a sub-vector space of the\nLie algebra of the special unitary operators, $\\mathfrak{su}(2^n)$. This\nsub-vector space in turn generates a subgroup of the Lie group of special\nunitary operators, $\\mathrm{SU}(2^n)$. Applications of our quantum embedding\nmethod, in comparison with the classical algorithms GloVe (a natural language\nprocessing embedding method) and FastRP (a general graph embedding method,\ndisplay superior performance in measuring similarity between nodes in graph\nstructures.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.14281v1.pdf",
        "similarity": 0.20240170993504777,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2407.18645v1",
        "published": "2024-07-19"
    }
]