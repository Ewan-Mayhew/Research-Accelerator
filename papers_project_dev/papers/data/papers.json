[
    {
        "new_title": "A Closer Look at Deep Learning on Tabular Data",
        "new_link": "http://arxiv.org/abs/2407.00956v1",
        "new_summary": "  Tabular data is prevalent across various domains in machine learning.\nAlthough Deep Neural Network (DNN)-based methods have shown promising\nperformance comparable to tree-based ones, in-depth evaluation of these methods\nis challenging due to varying performance ranks across diverse datasets. In\nthis paper, we propose a comprehensive benchmark comprising 300 tabular\ndatasets, covering a wide range of task types, size distributions, and domains.\nWe perform an extensive comparison between state-of-the-art deep tabular\nmethods and tree-based methods, revealing the average rank of all methods and\nhighlighting the key factors that influence the success of deep tabular\nmethods. Next, we analyze deep tabular methods based on their training\ndynamics, including changes in validation metrics and other statistics. For\neach dataset-method pair, we learn a mapping from both the meta-features of\ndatasets and the first part of the validation curve to the final validation set\nperformance and even the evolution of validation curves. This mapping extracts\nessential meta-features that influence prediction accuracy, helping the\nanalysis of tabular methods from novel aspects. Based on the performance of all\nmethods on this large benchmark, we identify two subsets of 45 datasets each.\nThe first subset contains datasets that favor either tree-based methods or\nDNN-based methods, serving as effective analysis tools to evaluate strategies\n(e.g., attribute encoding strategies) for improving deep tabular models. The\nsecond subset contains datasets where the ranks of methods are consistent with\nthe overall benchmark, acting as a probe for tabular analysis. These ``tiny\ntabular benchmarks'' will facilitate further studies on tabular data.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.00956v1.pdf",
        "similarity": 0.6335940471343524,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-01"
    },
    {
        "new_title": "Arithmetic Feature Interaction Is Necessary for Deep Tabular Learning",
        "new_link": "http://arxiv.org/abs/2402.02334v2",
        "new_summary": "  Until recently, the question of the effective inductive bias of deep models\non tabular data has remained unanswered. This paper investigates the hypothesis\nthat arithmetic feature interaction is necessary for deep tabular learning. To\ntest this point, we create a synthetic tabular dataset with a mild feature\ninteraction assumption and examine a modified transformer architecture enabling\narithmetical feature interactions, referred to as AMFormer. Results show that\nAMFormer outperforms strong counterparts in fine-grained tabular data modeling,\ndata efficiency in training, and generalization. This is attributed to its\nparallel additive and multiplicative attention operators and prompt-based\noptimization, which facilitate the separation of tabular samples in an extended\nspace with arithmetically-engineered features. Our extensive experiments on\nreal-world data also validate the consistent effectiveness, efficiency, and\nrationale of AMFormer, suggesting it has established a strong inductive bias\nfor deep learning on tabular data. Code is available at\nhttps://github.com/aigc-apps/AMFormer.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.02334v2.pdf",
        "similarity": 0.6282230288894539,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-04"
    },
    {
        "new_title": "PTaRL: Prototype-based Tabular Representation Learning via Space\n  Calibration",
        "new_link": "http://arxiv.org/abs/2407.05364v2",
        "new_summary": "  Tabular data have been playing a mostly important role in diverse real-world\nfields, such as healthcare, engineering, finance, etc. With the recent success\nof deep learning, many tabular machine learning (ML) methods based on deep\nnetworks (e.g., Transformer, ResNet) have achieved competitive performance on\ntabular benchmarks. However, existing deep tabular ML methods suffer from the\nrepresentation entanglement and localization, which largely hinders their\nprediction performance and leads to performance inconsistency on tabular tasks.\nTo overcome these problems, we explore a novel direction of applying prototype\nlearning for tabular ML and propose a prototype-based tabular representation\nlearning framework, PTaRL, for tabular prediction tasks. The core idea of PTaRL\nis to construct prototype-based projection space (P-Space) and learn the\ndisentangled representation around global data prototypes. Specifically, PTaRL\nmainly involves two stages: (i) Prototype Generation, that constructs global\nprototypes as the basis vectors of P-Space for representation, and (ii)\nPrototype Projection, that projects the data samples into P-Space and keeps the\ncore global data information via Optimal Transport. Then, to further acquire\nthe disentangled representations, we constrain PTaRL with two strategies: (i)\nto diversify the coordinates towards global prototypes of different\nrepresentations within P-Space, we bring up a diversification constraint for\nrepresentation calibration; (ii) to avoid prototype entanglement in P-Space, we\nintroduce a matrix orthogonalization constraint to ensure the independence of\nglobal prototypes. Finally, we conduct extensive experiments in PTaRL coupled\nwith state-of-the-art deep tabular ML models on various tabular benchmarks and\nthe results have shown our consistent superiority.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.05364v2.pdf",
        "similarity": 0.6266570344553437,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-07"
    },
    {
        "new_title": "TALENT: A Tabular Analytics and Learning Toolbox",
        "new_link": "http://arxiv.org/abs/2407.04057v1",
        "new_summary": "  Tabular data is one of the most common data sources in machine learning.\nAlthough a wide range of classical methods demonstrate practical utilities in\nthis field, deep learning methods on tabular data are becoming promising\nalternatives due to their flexibility and ability to capture complex\ninteractions within the data. Considering that deep tabular methods have\ndiverse design philosophies, including the ways they handle features, design\nlearning objectives, and construct model architectures, we introduce a\nversatile deep-learning toolbox called TALENT (Tabular Analytics and LEarNing\nToolbox) to utilize, analyze, and compare tabular methods. TALENT encompasses\nan extensive collection of more than 20 deep tabular prediction methods,\nassociated with various encoding and normalization modules, and provides a\nunified interface that is easily integrable with new methods as they emerge. In\nthis paper, we present the design and functionality of the toolbox, illustrate\nits practical application through several case studies, and investigate the\nperformance of various methods fairly based on our toolbox. Code is available\nat https://github.com/qile2000/LAMDA-TALENT.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.04057v1.pdf",
        "similarity": 0.6207078979721714,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-04"
    },
    {
        "new_title": "Attention versus Contrastive Learning of Tabular Data -- A Data-centric\n  Benchmarking",
        "new_link": "http://arxiv.org/abs/2401.04266v1",
        "new_summary": "  Despite groundbreaking success in image and text learning, deep learning has\nnot achieved significant improvements against traditional machine learning (ML)\nwhen it comes to tabular data. This performance gap underscores the need for\ndata-centric treatment and benchmarking of learning algorithms. Recently,\nattention and contrastive learning breakthroughs have shifted computer vision\nand natural language processing paradigms. However, the effectiveness of these\nadvanced deep models on tabular data is sparsely studied using a few data sets\nwith very large sample sizes, reporting mixed findings after benchmarking\nagainst a limited number of baselines. We argue that the heterogeneity of\ntabular data sets and selective baselines in the literature can bias the\nbenchmarking outcomes. This article extensively evaluates state-of-the-art\nattention and contrastive learning methods on a wide selection of 28 tabular\ndata sets (14 easy and 14 hard-to-classify) against traditional deep and\nmachine learning. Our data-centric benchmarking demonstrates when traditional\nML is preferred over deep learning and vice versa because no best learning\nmethod exists for all tabular data sets. Combining between-sample and\nbetween-feature attentions conquers the invincible traditional ML on tabular\ndata sets by a significant margin but fails on high dimensional data, where\ncontrastive learning takes a robust lead. While a hybrid attention-contrastive\nlearning strategy mostly wins on hard-to-classify data sets, traditional\nmethods are frequently superior on easy-to-classify data sets with presumably\nsimpler decision boundaries. To the best of our knowledge, this is the first\nbenchmarking paper with statistical analyses of attention and contrastive\nlearning performances on a diverse selection of tabular data sets against\ntraditional deep and machine learning baselines to facilitate further advances\nin this field.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.04266v1.pdf",
        "similarity": 0.6191859377307396,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-08"
    },
    {
        "new_title": "CLoVe: Encoding Compositional Language in Contrastive Vision-Language\n  Models",
        "new_link": "http://arxiv.org/abs/2402.15021v2",
        "new_summary": "  Recent years have witnessed a significant increase in the performance of\nVision and Language tasks. Foundational Vision-Language Models (VLMs), such as\nCLIP, have been leveraged in multiple settings and demonstrated remarkable\nperformance across several tasks. Such models excel at object-centric\nrecognition yet learn text representations that seem invariant to word order,\nfailing to compose known concepts in novel ways. However, no evidence exists\nthat any VLM, including large-scale single-stream models such as GPT-4V,\nidentifies compositions successfully. In this paper, we introduce a framework\nto significantly improve the ability of existing models to encode compositional\nlanguage, with over 10% absolute improvement on compositionality benchmarks,\nwhile maintaining or improving the performance on standard object-recognition\nand retrieval benchmarks. Our code and pre-trained models are publicly\navailable at https://github.com/netflix/clove.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.15021v2.pdf",
        "similarity": 0.6057799747748451,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-22"
    },
    {
        "new_title": "Text Serialization and Their Relationship with the Conventional\n  Paradigms of Tabular Machine Learning",
        "new_link": "http://arxiv.org/abs/2406.13846v1",
        "new_summary": "  Recent research has explored how Language Models (LMs) can be used for\nfeature representation and prediction in tabular machine learning tasks. This\ninvolves employing text serialization and supervised fine-tuning (SFT)\ntechniques. Despite the simplicity of these techniques, significant gaps remain\nin our understanding of the applicability and reliability of LMs in this\ncontext. Our study assesses how emerging LM technologies compare with\ntraditional paradigms in tabular machine learning and evaluates the feasibility\nof adopting similar approaches with these advanced technologies. At the data\nlevel, we investigate various methods of data representation and curation of\nserialized tabular data, exploring their impact on prediction performance. At\nthe classification level, we examine whether text serialization combined with\nLMs enhances performance on tabular datasets (e.g. class imbalance,\ndistribution shift, biases, and high dimensionality), and assess whether this\nmethod represents a state-of-the-art (SOTA) approach for addressing tabular\nmachine learning challenges. Our findings reveal current pre-trained models\nshould not replace conventional approaches.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.13846v1.pdf",
        "similarity": 0.5978146842129548,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-19"
    },
    {
        "new_title": "Modern Neighborhood Components Analysis: A Deep Tabular Baseline Two\n  Decades Later",
        "new_link": "http://arxiv.org/abs/2407.03257v1",
        "new_summary": "  The growing success of deep learning in various domains has prompted\ninvestigations into its application to tabular data, where deep models have\nshown promising results compared to traditional tree-based methods. In this\npaper, we revisit Neighborhood Component Analysis (NCA), a classic tabular\nprediction method introduced in 2004, designed to learn a linear projection\nthat captures semantic similarities between instances. We find that minor\nmodifications, such as adjustments to the learning objectives and the\nintegration of deep learning architectures, significantly enhance NCA's\nperformance, enabling it to surpass most modern deep tabular models.\nAdditionally, we introduce a stochastic neighbor sampling strategy that\nimproves both the efficiency and predictive accuracy of our proposed ModernNCA\n-- sampling only a subset of neighbors during training, while utilizing the\nentire neighborhood during inference. Extensive experiments demonstrate that\nour ModernNCA achieves state-of-the-art results in both classification and\nregression tasks across various tabular datasets, outperforming both tree-based\nand other deep tabular models, while also reducing training time and model\nsize.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.03257v1.pdf",
        "similarity": 0.5950530769019142,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-03"
    },
    {
        "new_title": "Transformers with Stochastic Competition for Tabular Data Modelling",
        "new_link": "http://arxiv.org/abs/2407.13238v1",
        "new_summary": "  Despite the prevalence and significance of tabular data across numerous\nindustries and fields, it has been relatively underexplored in the realm of\ndeep learning. Even today, neural networks are often overshadowed by techniques\nsuch as gradient boosted decision trees (GBDT). However, recent models are\nbeginning to close this gap, outperforming GBDT in various setups and garnering\nincreased attention in the field. Inspired by this development, we introduce a\nnovel stochastic deep learning model specifically designed for tabular data.\nThe foundation of this model is a Transformer-based architecture, carefully\nadapted to cater to the unique properties of tabular data through strategic\narchitectural modifications and leveraging two forms of stochastic competition.\nFirst, we employ stochastic \"Local Winner Takes All\" units to promote\ngeneralization capacity through stochasticity and sparsity. Second, we\nintroduce a novel embedding layer that selects among alternative linear\nembedding layers through a mechanism of stochastic competition. The\neffectiveness of the model is validated on a variety of widely-used, publicly\navailable datasets. We demonstrate that, through the incorporation of these\nelements, our model yields high performance and marks a significant advancement\nin the application of deep learning to tabular data.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.13238v1.pdf",
        "similarity": 0.5789402963117933,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-18"
    },
    {
        "new_title": "BiSHop: Bi-Directional Cellular Learning for Tabular Data with\n  Generalized Sparse Modern Hopfield Model",
        "new_link": "http://arxiv.org/abs/2404.03830v2",
        "new_summary": "  We introduce the \\textbf{B}i-Directional \\textbf{S}parse \\textbf{Hop}field\nNetwork (\\textbf{BiSHop}), a novel end-to-end framework for deep tabular\nlearning. BiSHop handles the two major challenges of deep tabular learning:\nnon-rotationally invariant data structure and feature sparsity in tabular data.\nOur key motivation comes from the recent established connection between\nassociative memory and attention mechanisms. Consequently, BiSHop uses a\ndual-component approach, sequentially processing data both column-wise and\nrow-wise through two interconnected directional learning modules.\nComputationally, these modules house layers of generalized sparse modern\nHopfield layers, a sparse extension of the modern Hopfield model with adaptable\nsparsity. Methodologically, BiSHop facilitates multi-scale representation\nlearning, capturing both intra-feature and inter-feature interactions, with\nadaptive sparsity at each scale. Empirically, through experiments on diverse\nreal-world datasets, we demonstrate that BiSHop surpasses current SOTA methods\nwith significantly less HPO runs, marking it a robust solution for deep tabular\nlearning.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.03830v2.pdf",
        "similarity": 0.5728021428299643,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-04"
    },
    {
        "new_title": "Mixture of In-Context Prompters for Tabular PFNs",
        "new_link": "http://arxiv.org/abs/2405.16156v1",
        "new_summary": "  Recent benchmarks found In-Context Learning (ICL) outperforms both deep\nlearning and tree-based algorithms on small tabular datasets. However, on\nlarger datasets, ICL for tabular learning cannot run without severely\ncompromising performance, due to its quadratic space and time complexity w.r.t.\ndataset size. We propose MIXTUREPFN, which both extends nearest-neighbor\nsampling to the state-of-the-art ICL for tabular learning model and uses\nbootstrapping to finetune said model on the inference-time dataset. MIXTUREPFN\nis the Condorcet winner across 36 diverse tabular datasets against 19 strong\ndeep learning and tree-based baselines, achieving the highest mean rank among\nTop-10 aforementioned algorithms with statistical significance.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.16156v1.pdf",
        "similarity": 0.5630000532136559,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-25"
    },
    {
        "new_title": "An In-Depth Analysis of Data Reduction Methods for Sustainable Deep\n  Learning",
        "new_link": "http://arxiv.org/abs/2403.15150v1",
        "new_summary": "  In recent years, Deep Learning has gained popularity for its ability to solve\ncomplex classification tasks, increasingly delivering better results thanks to\nthe development of more accurate models, the availability of huge volumes of\ndata and the improved computational capabilities of modern computers. However,\nthese improvements in performance also bring efficiency problems, related to\nthe storage of datasets and models, and to the waste of energy and time\ninvolved in both the training and inference processes. In this context, data\nreduction can help reduce energy consumption when training a deep learning\nmodel. In this paper, we present up to eight different methods to reduce the\nsize of a tabular training dataset, and we develop a Python package to apply\nthem. We also introduce a representativeness metric based on topology to\nmeasure how similar are the reduced datasets and the full training dataset.\nAdditionally, we develop a methodology to apply these data reduction methods to\nimage datasets for object detection tasks. Finally, we experimentally compare\nhow these data reduction methods affect the representativeness of the reduced\ndataset, the energy consumption and the predictive performance of the model.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.15150v1.pdf",
        "similarity": 0.5598498471870441,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-22"
    },
    {
        "new_title": "Vision-Language Meets the Skeleton: Progressively Distillation with\n  Cross-Modal Knowledge for 3D Action Representation Learning",
        "new_link": "http://arxiv.org/abs/2405.20606v1",
        "new_summary": "  Supervised and self-supervised learning are two main training paradigms for\nskeleton-based human action recognition. However, the former one-hot\nclassification requires labor-intensive predefined action categories\nannotations, while the latter involves skeleton transformations (e.g.,\ncropping) in the pretext tasks that may impair the skeleton structure. To\naddress these challenges, we introduce a novel skeleton-based training\nframework (C$^2$VL) based on Cross-modal Contrastive learning that uses the\nprogressive distillation to learn task-agnostic human skeleton action\nrepresentation from the Vision-Language knowledge prompts. Specifically, we\nestablish the vision-language action concept space through vision-language\nknowledge prompts generated by pre-trained large multimodal models (LMMs),\nwhich enrich the fine-grained details that the skeleton action space lacks.\nMoreover, we propose the intra-modal self-similarity and inter-modal\ncross-consistency softened targets in the cross-modal contrastive process to\nprogressively control and guide the degree of pulling vision-language knowledge\nprompts and corresponding skeletons closer. These soft instance discrimination\nand self-knowledge distillation strategies contribute to the learning of better\nskeleton-based action representations from the noisy skeleton-vision-language\npairs. During the inference phase, our method requires only the skeleton data\nas the input for action recognition and no longer for vision-language prompts.\nExtensive experiments show that our method achieves state-of-the-art results on\nNTU RGB+D 60, NTU RGB+D 120, and PKU-MMD datasets. The code will be available\nin the future.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.20606v1.pdf",
        "similarity": 0.5560831048989522,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-31"
    },
    {
        "new_title": "Do deep neural networks utilize the weight space efficiently?",
        "new_link": "http://arxiv.org/abs/2401.16438v1",
        "new_summary": "  Deep learning models like Transformers and Convolutional Neural Networks\n(CNNs) have revolutionized various domains, but their parameter-intensive\nnature hampers deployment in resource-constrained settings. In this paper, we\nintroduce a novel concept utilizes column space and row space of weight\nmatrices, which allows for a substantial reduction in model parameters without\ncompromising performance. Leveraging this paradigm, we achieve\nparameter-efficient deep learning models.. Our approach applies to both\nBottleneck and Attention layers, effectively halving the parameters while\nincurring only minor performance degradation. Extensive experiments conducted\non the ImageNet dataset with ViT and ResNet50 demonstrate the effectiveness of\nour method, showcasing competitive performance when compared to traditional\nmodels. This approach not only addresses the pressing demand for parameter\nefficient deep learning solutions but also holds great promise for practical\ndeployment in real-world scenarios.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.16438v1.pdf",
        "similarity": 0.5528990238760223,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-26"
    },
    {
        "new_title": "Exploring the Spectrum of Visio-Linguistic Compositionality and\n  Recognition",
        "new_link": "http://arxiv.org/abs/2406.09388v1",
        "new_summary": "  Vision and language models (VLMs) such as CLIP have showcased remarkable\nzero-shot recognition abilities yet face challenges in visio-linguistic\ncompositionality, particularly in linguistic comprehension and fine-grained\nimage-text alignment. This paper explores the intricate relationship between\ncompositionality and recognition -- two pivotal aspects of VLM capability. We\nconduct a comprehensive evaluation of existing VLMs, covering both pre-training\napproaches aimed at recognition and the fine-tuning methods designed to improve\ncompositionality. Our evaluation employs 12 benchmarks for compositionality,\nalong with 21 zero-shot classification and two retrieval benchmarks for\nrecognition. In our analysis from 274 CLIP model checkpoints, we reveal\npatterns and trade-offs that emerge between compositional understanding and\nrecognition accuracy. Ultimately, this necessitates strategic efforts towards\ndeveloping models that improve both capabilities, as well as the meticulous\nformulation of benchmarks for compositionality. We open our evaluation\nframework at https://github.com/ytaek-oh/vl_compo.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.09388v1.pdf",
        "similarity": 0.5488481778746034,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-13"
    },
    {
        "new_title": "Unveiling Encoder-Free Vision-Language Models",
        "new_link": "http://arxiv.org/abs/2406.11832v1",
        "new_summary": "  Existing vision-language models (VLMs) mostly rely on vision encoders to\nextract visual features followed by large language models (LLMs) for\nvisual-language tasks. However, the vision encoders set a strong inductive bias\nin abstracting visual representation, e.g., resolution, aspect ratio, and\nsemantic priors, which could impede the flexibility and efficiency of the VLMs.\nTraining pure VLMs that accept the seamless vision and language inputs, i.e.,\nwithout vision encoders, remains challenging and rarely explored. Empirical\nobservations reveal that direct training without encoders results in slow\nconvergence and large performance gaps. In this work, we bridge the gap between\nencoder-based and encoder-free models, and present a simple yet effective\ntraining recipe towards pure VLMs. Specifically, we unveil the key aspects of\ntraining encoder-free VLMs efficiently via thorough experiments: (1) Bridging\nvision-language representation inside one unified decoder; (2) Enhancing visual\nrecognition capability via extra supervision. With these strategies, we launch\nEVE, an encoder-free vision-language model that can be trained and forwarded\nefficiently. Notably, solely utilizing 35M publicly accessible data, EVE can\nimpressively rival the encoder-based VLMs of similar capacities across multiple\nvision-language benchmarks. It significantly outperforms the counterpart\nFuyu-8B with mysterious training procedures and undisclosed training data. We\nbelieve that EVE provides a transparent and efficient route for developing a\npure decoder-only architecture across modalities. Our code and models are\npublicly available at: https://github.com/baaivision/EVE.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.11832v1.pdf",
        "similarity": 0.5480607160639757,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-17"
    },
    {
        "new_title": "Bullion: A Column Store for Machine Learning",
        "new_link": "http://arxiv.org/abs/2404.08901v1",
        "new_summary": "  The past two decades have witnessed columnar storage revolutionizing data\nwarehousing and analytics. However, the rapid growth of machine learning poses\nnew challenges to this domain. This paper presents Bullion, a columnar storage\nsystem tailored for machine learning workloads. Bullion addresses the\ncomplexities of data compliance, optimizes the encoding of long sequence sparse\nfeatures, efficiently manages wide-table projections, and introduces feature\nquantization in storage. By aligning with the evolving requirements of ML\napplications, Bullion extends columnar storage to various scenarios, from\nadvertising and recommendation systems to the expanding realm of Generative AI.\n  Preliminary experimental results and theoretical analysis demonstrate\nBullion's superior performance in handling the unique demands of machine\nlearning workloads compared to existing columnar storage solutions. Bullion\nsignificantly reduces I/O costs for deletion compliance, achieves substantial\nstorage savings with its optimized encoding scheme for sparse features, and\ndrastically improves metadata parsing speed for wide-table projections. These\nadvancements position Bullion as a critical component in the future of machine\nlearning infrastructure, enabling organizations to efficiently manage and\nprocess the massive volumes of data required for training and inference in\nmodern AI applications.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.08901v1.pdf",
        "similarity": 0.5476685576103437,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-13"
    },
    {
        "new_title": "GPT4Ego: Unleashing the Potential of Pre-trained Models for Zero-Shot\n  Egocentric Action Recognition",
        "new_link": "http://arxiv.org/abs/2401.10039v2",
        "new_summary": "  Vision-Language Models (VLMs), pre-trained on large-scale datasets, have\nshown impressive performance in various visual recognition tasks. This\nadvancement paves the way for notable performance in Zero-Shot Egocentric\nAction Recognition (ZS-EAR). Typically, VLMs handle ZS-EAR as a global\nvideo-text matching task, which often leads to suboptimal alignment of vision\nand linguistic knowledge. We propose a refined approach for ZS-EAR using VLMs,\nemphasizing fine-grained concept-description alignment that capitalizes on the\nrich semantic and contextual details in egocentric videos. In this paper, we\nintroduce GPT4Ego, a straightforward yet remarkably potent VLM framework for\nZS-EAR, designed to enhance the fine-grained alignment of concept and\ndescription between vision and language. Extensive experiments demonstrate\nGPT4Ego significantly outperforms existing VLMs on three large-scale egocentric\nvideo benchmarks, i.e., EPIC-KITCHENS-100 (33.2%, +9.4%), EGTEA (39.6%, +5.5%),\nand CharadesEgo (31.5%, +2.6%).\n",
        "pdf_link": "https://arxiv.org/pdf/2401.10039v2.pdf",
        "similarity": 0.5380074985010238,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-18"
    },
    {
        "new_title": "Enhancing Compositional Generalization via Compositional Feature\n  Alignment",
        "new_link": "http://arxiv.org/abs/2402.02851v2",
        "new_summary": "  Real-world applications of machine learning models often confront data\ndistribution shifts, wherein discrepancies exist between the training and test\ndata distributions. In the common multi-domain multi-class setup, as the number\nof classes and domains scales up, it becomes infeasible to gather training data\nfor every domain-class combination. This challenge naturally leads the quest\nfor models with Compositional Generalization (CG) ability, where models can\ngeneralize to unseen domain-class combinations. To delve into the CG challenge,\nwe develop CG-Bench, a suite of CG benchmarks derived from existing real-world\nimage datasets, and observe that the prevalent pretraining-finetuning paradigm\non foundational models, such as CLIP and DINOv2, struggles with the challenge.\nTo address this challenge, we propose Compositional Feature Alignment (CFA), a\nsimple two-stage finetuning technique that i) learns two orthogonal linear\nheads on a pretrained encoder with respect to class and domain labels, and ii)\nfine-tunes the encoder with the newly learned head frozen. We theoretically and\nempirically justify that CFA encourages compositional feature learning of\npretrained models. We further conduct extensive experiments on CG-Bench for\nCLIP and DINOv2, two powerful pretrained vision foundation models. Experiment\nresults show that CFA outperforms common finetuning techniques in compositional\ngeneralization, corroborating CFA's efficacy in compositional feature learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.02851v2.pdf",
        "similarity": 0.536197727191195,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-05"
    },
    {
        "new_title": "On the Effectiveness of Supervision in Asymmetric Non-Contrastive\n  Learning",
        "new_link": "http://arxiv.org/abs/2406.10815v1",
        "new_summary": "  Supervised contrastive representation learning has been shown to be effective\nin various transfer learning scenarios. However, while asymmetric\nnon-contrastive learning (ANCL) often outperforms its contrastive learning\ncounterpart in self-supervised representation learning, the extension of ANCL\nto supervised scenarios is less explored. To bridge the gap, we study ANCL for\nsupervised representation learning, coined SupSiam and SupBYOL, leveraging\nlabels in ANCL to achieve better representations. The proposed supervised ANCL\nframework improves representation learning while avoiding collapse. Our\nanalysis reveals that providing supervision to ANCL reduces intra-class\nvariance, and the contribution of supervision should be adjusted to achieve the\nbest performance. Experiments demonstrate the superiority of supervised ANCL\nacross various datasets and tasks. The code is available at:\nhttps://github.com/JH-Oh-23/Sup-ANCL.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.10815v1.pdf",
        "similarity": 0.530779268003097,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-16"
    },
    {
        "new_title": "Supervised Fine-tuning in turn Improves Visual Foundation Models",
        "new_link": "http://arxiv.org/abs/2401.10222v2",
        "new_summary": "  Image-text training like CLIP has dominated the pretraining of vision\nfoundation models in recent years. Subsequent efforts have been made to\nintroduce region-level visual learning into CLIP's pretraining but face\nscalability challenges due to the lack of large-scale region-level datasets.\nDrawing inspiration from supervised fine-tuning (SFT) in natural language\nprocessing such as instruction tuning, we explore the potential of fine-grained\nSFT in enhancing the generation of vision foundation models after their\npretraining. Thus a two-stage method ViSFT (Vision SFT) is proposed to unleash\nthe fine-grained knowledge of vision foundation models. In ViSFT, the vision\nfoundation model is enhanced by performing visual joint learning on some\nin-domain tasks and then tested on out-of-domain benchmarks. With updating\nusing ViSFT on 8 V100 GPUs in less than 2 days, a vision transformer with over\n4.4B parameters shows improvements across various out-of-domain benchmarks\nincluding vision and vision-linguistic scenarios.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.10222v2.pdf",
        "similarity": 0.5256808137365125,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-18"
    },
    {
        "new_title": "Assistive Image Annotation Systems with Deep Learning and Natural\n  Language Capabilities: A Review",
        "new_link": "http://arxiv.org/abs/2407.00252v1",
        "new_summary": "  While supervised learning has achieved significant success in computer vision\ntasks, acquiring high-quality annotated data remains a bottleneck. This paper\nexplores both scholarly and non-scholarly works in AI-assistive deep learning\nimage annotation systems that provide textual suggestions, captions, or\ndescriptions of the input image to the annotator. This potentially results in\nhigher annotation efficiency and quality. Our exploration covers annotation for\na range of computer vision tasks including image classification, object\ndetection, regression, instance, semantic segmentation, and pose estimation. We\nreview various datasets and how they contribute to the training and evaluation\nof AI-assistive annotation systems. We also examine methods leveraging\nneuro-symbolic learning, deep active learning, and self-supervised learning\nalgorithms that enable semantic image understanding and generate free-text\noutput. These include image captioning, visual question answering, and\nmulti-modal reasoning. Despite the promising potential, there is limited\npublicly available work on AI-assistive image annotation with textual output\ncapabilities. We conclude by suggesting future research directions to advance\nthis field, emphasizing the need for more publicly accessible datasets and\ncollaborative efforts between academia and industry.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.00252v1.pdf",
        "similarity": 0.521574098746392,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-28"
    },
    {
        "new_title": "Retrieval Augmented Deep Anomaly Detection for Tabular Data",
        "new_link": "http://arxiv.org/abs/2401.17052v2",
        "new_summary": "  Deep learning for tabular data has garnered increasing attention in recent\nyears, yet employing deep models for structured data remains challenging. While\nthese models excel with unstructured data, their efficacy with structured data\nhas been limited. Recent research has introduced retrieval-augmented models to\naddress this gap, demonstrating promising results in supervised tasks such as\nclassification and regression. In this work, we investigate using\nretrieval-augmented models for anomaly detection on tabular data. We propose a\nreconstruction-based approach in which a transformer model learns to\nreconstruct masked features of \\textit{normal} samples. We test the\neffectiveness of KNN-based and attention-based modules to select relevant\nsamples to help in the reconstruction process of the target sample. Our\nexperiments on a benchmark of 31 tabular datasets reveal that augmenting this\nreconstruction-based anomaly detection (AD) method with sample-sample\ndependencies via retrieval modules significantly boosts performance. The\npresent work supports the idea that retrieval module are useful to augment any\ndeep AD method to enhance anomaly detection on tabular data.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.17052v2.pdf",
        "similarity": 0.5198484592430733,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-30"
    },
    {
        "new_title": "Enhancing Deep Knowledge Tracing via Diffusion Models for Personalized\n  Adaptive Learning",
        "new_link": "http://arxiv.org/abs/2405.05134v1",
        "new_summary": "  In contrast to pedagogies like evidence-based teaching, personalized adaptive\nlearning (PAL) distinguishes itself by closely monitoring the progress of\nindividual students and tailoring the learning path to their unique knowledge\nand requirements. A crucial technique for effective PAL implementation is\nknowledge tracing, which models students' evolving knowledge to predict their\nfuture performance. Based on these predictions, personalized recommendations\nfor resources and learning paths can be made to meet individual needs. Recent\nadvancements in deep learning have successfully enhanced knowledge tracking\nthrough Deep Knowledge Tracing (DKT). This paper introduces generative AI\nmodels to further enhance DKT. Generative AI models, rooted in deep learning,\nare trained to generate synthetic data, addressing data scarcity challenges in\nvarious applications across fields such as natural language processing (NLP)\nand computer vision (CV). This study aims to tackle data shortage issues in\nstudent learning records to enhance DKT performance for PAL. Specifically, it\nemploys TabDDPM, a diffusion model, to generate synthetic educational records\nto augment training data for enhancing DKT. The proposed method's effectiveness\nis validated through extensive experiments on ASSISTments datasets. The\nexperimental results demonstrate that the AI-generated data by TabDDPM\nsignificantly improves DKT performance, particularly in scenarios with small\ndata for training and large data for testing.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.05134v1.pdf",
        "similarity": 0.5130386362072739,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-25"
    },
    {
        "new_title": "Comparing supervised learning dynamics: Deep neural networks match human\n  data efficiency but show a generalisation lag",
        "new_link": "http://arxiv.org/abs/2402.09303v3",
        "new_summary": "  Recent research has seen many behavioral comparisons between humans and deep\nneural networks (DNNs) in the domain of image classification. Often, comparison\nstudies focus on the end-result of the learning process by measuring and\ncomparing the similarities in the representations of object categories once\nthey have been formed. However, the process of how these representations emerge\n-- that is, the behavioral changes and intermediate stages observed during the\nacquisition -- is less often directly and empirically compared. Here we report\na detailed investigation of the learning dynamics in human observers and\nvarious classic and state-of-the-art DNNs. We develop a constrained supervised\nlearning environment to align learning-relevant conditions such as starting\npoint, input modality, available input data and the feedback provided. Across\nthe whole learning process we evaluate and compare how well learned\nrepresentations can be generalized to previously unseen test data. Comparisons\nacross the entire learning process indicate that DNNs demonstrate a level of\ndata efficiency comparable to human learners, challenging some prevailing\nassumptions in the field. However, our results also reveal representational\ndifferences: while DNNs' learning is characterized by a pronounced\ngeneralisation lag, humans appear to immediately acquire generalizable\nrepresentations without a preliminary phase of learning training set-specific\ninformation that is only later transferred to novel data.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.09303v3.pdf",
        "similarity": 0.5116109018201987,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-14"
    },
    {
        "new_title": "Convex space learning for tabular synthetic data generation",
        "new_link": "http://arxiv.org/abs/2407.09789v1",
        "new_summary": "  Generating synthetic samples from the convex space of the minority class is a\npopular oversampling approach for imbalanced classification problems. Recently,\ndeep-learning approaches have been successfully applied to modeling the convex\nspace of minority samples. Beyond oversampling, learning the convex space of\nneighborhoods in training data has not been used to generate entire tabular\ndatasets. In this paper, we introduce a deep learning architecture\n(NextConvGeN) with a generator and discriminator component that can generate\nsynthetic samples by learning to model the convex space of tabular data. The\ngenerator takes data neighborhoods as input and creates synthetic samples\nwithin the convex space of that neighborhood. Thereafter, the discriminator\ntries to classify these synthetic samples against a randomly sampled batch of\ndata from the rest of the data space. We compared our proposed model with five\nstate-of-the-art tabular generative models across ten publicly available\ndatasets from the biomedical domain. Our analysis reveals that synthetic\nsamples generated by NextConvGeN can better preserve classification and\nclustering performance across real and synthetic data than other synthetic data\ngeneration models. Synthetic data generation by deep learning of the convex\nspace produces high scores for popular utility measures. We further compared\nhow diverse synthetic data generation strategies perform in the privacy-utility\nspectrum and produced critical arguments on the necessity of high utility\nmodels. Our research on deep learning of the convex space of tabular data opens\nup opportunities in clinical research, machine learning model development,\ndecision support systems, and clinical data sharing.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.09789v1.pdf",
        "similarity": 0.5108394613253535,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-13"
    },
    {
        "new_title": "Automated data processing and feature engineering for deep learning and\n  big data applications: a survey",
        "new_link": "http://arxiv.org/abs/2403.11395v2",
        "new_summary": "  Modern approach to artificial intelligence (AI) aims to design algorithms\nthat learn directly from data. This approach has achieved impressive results\nand has contributed significantly to the progress of AI, particularly in the\nsphere of supervised deep learning. It has also simplified the design of\nmachine learning systems as the learning process is highly automated. However,\nnot all data processing tasks in conventional deep learning pipelines have been\nautomated. In most cases data has to be manually collected, preprocessed and\nfurther extended through data augmentation before they can be effective for\ntraining. Recently, special techniques for automating these tasks have emerged.\nThe automation of data processing tasks is driven by the need to utilize large\nvolumes of complex, heterogeneous data for machine learning and big data\napplications. Today, end-to-end automated data processing systems based on\nautomated machine learning (AutoML) techniques are capable of taking raw data\nand transforming them into useful features for Big Data tasks by automating all\nintermediate processing stages. In this work, we present a thorough review of\napproaches for automating data processing tasks in deep learning pipelines,\nincluding automated data preprocessing--e.g., data cleaning, labeling, missing\ndata imputation, and categorical data encoding--as well as data augmentation\n(including synthetic data generation using generative AI methods) and feature\nengineering--specifically, automated feature extraction, feature construction\nand feature selection. In addition to automating specific data processing\ntasks, we discuss the use of AutoML methods and tools to simultaneously\noptimize all stages of the machine learning pipeline.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.11395v2.pdf",
        "similarity": 0.5099116331691116,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-18"
    },
    {
        "new_title": "APALU: A Trainable, Adaptive Activation Function for Deep Learning\n  Networks",
        "new_link": "http://arxiv.org/abs/2402.08244v1",
        "new_summary": "  Activation function is a pivotal component of deep learning, facilitating the\nextraction of intricate data patterns. While classical activation functions\nlike ReLU and its variants are extensively utilized, their static nature and\nsimplicity, despite being advantageous, often limit their effectiveness in\nspecialized tasks. The trainable activation functions also struggle sometimes\nto adapt to the unique characteristics of the data. Addressing these\nlimitations, we introduce a novel trainable activation function, adaptive\npiecewise approximated activation linear unit (APALU), to enhance the learning\nperformance of deep learning across a broad range of tasks. It presents a\nunique set of features that enable it to maintain stability and efficiency in\nthe learning process while adapting to complex data representations.\nExperiments reveal significant improvements over widely used activation\nfunctions for different tasks. In image classification, APALU increases\nMobileNet and GoogleNet accuracy by 0.37% and 0.04%, respectively, on the\nCIFAR10 dataset. In anomaly detection, it improves the average area under the\ncurve of One-CLASS Deep SVDD by 0.8% on the MNIST dataset, 1.81% and 1.11%\nimprovements with DifferNet, and knowledge distillation, respectively, on the\nMVTech dataset. Notably, APALU achieves 100% accuracy on a sign language\nrecognition task with a limited dataset. For regression tasks, APALU enhances\nthe performance of deep neural networks and recurrent neural networks on\ndifferent datasets. These improvements highlight the robustness and\nadaptability of APALU across diverse deep-learning applications.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.08244v1.pdf",
        "similarity": 0.5055727102267809,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-13"
    },
    {
        "new_title": "ChimpVLM: Ethogram-Enhanced Chimpanzee Behaviour Recognition",
        "new_link": "http://arxiv.org/abs/2404.08937v1",
        "new_summary": "  We show that chimpanzee behaviour understanding from camera traps can be\nenhanced by providing visual architectures with access to an embedding of text\ndescriptions that detail species behaviours. In particular, we present a\nvision-language model which employs multi-modal decoding of visual features\nextracted directly from camera trap videos to process query tokens representing\nbehaviours and output class predictions. Query tokens are initialised using a\nstandardised ethogram of chimpanzee behaviour, rather than using random or\nname-based initialisations. In addition, the effect of initialising query\ntokens using a masked language model fine-tuned on a text corpus of known\nbehavioural patterns is explored. We evaluate our system on the PanAf500 and\nPanAf20K datasets and demonstrate the performance benefits of our multi-modal\ndecoding approach and query initialisation strategy on multi-class and\nmulti-label recognition tasks, respectively. Results and ablations corroborate\nperformance improvements. We achieve state-of-the-art performance over vision\nand vision-language models in top-1 accuracy (+6.34%) on PanAf500 and overall\n(+1.1%) and tail-class (+2.26%) mean average precision on PanAf20K. We share\ncomplete source code and network weights for full reproducibility of results\nand easy utilisation.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.08937v1.pdf",
        "similarity": 0.505117529118186,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-13"
    },
    {
        "new_title": "DDA: Dimensionality Driven Augmentation Search for Contrastive Learning\n  in Laparoscopic Surgery",
        "new_link": "http://arxiv.org/abs/2406.00907v2",
        "new_summary": "  Self-supervised learning (SSL) has potential for effective representation\nlearning in medical imaging, but the choice of data augmentation is critical\nand domain-specific. It remains uncertain if general augmentation policies suit\nsurgical applications. In this work, we automate the search for suitable\naugmentation policies through a new method called Dimensionality Driven\nAugmentation Search (DDA). DDA leverages the local dimensionality of deep\nrepresentations as a proxy target, and differentiably searches for suitable\ndata augmentation policies in contrastive learning. We demonstrate the\neffectiveness and efficiency of DDA in navigating a large search space and\nsuccessfully identifying an appropriate data augmentation policy for\nlaparoscopic surgery. We systematically evaluate DDA across three laparoscopic\nimage classification and segmentation tasks, where it significantly improves\nover existing baselines. Furthermore, DDA's optimised set of augmentations\nprovides insight into domain-specific dependencies when applying contrastive\nlearning in medical applications. For example, while hue is an effective\naugmentation for natural images, it is not advantageous for laparoscopic\nimages.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.00907v2.pdf",
        "similarity": 0.5044370293899295,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-03"
    },
    {
        "new_title": "VIPriors 4: Visual Inductive Priors for Data-Efficient Deep Learning\n  Challenges",
        "new_link": "http://arxiv.org/abs/2406.18176v2",
        "new_summary": "  The fourth edition of the \"VIPriors: Visual Inductive Priors for\nData-Efficient Deep Learning\" workshop features two data-impaired challenges.\nThese challenges address the problem of training deep learning models for\ncomputer vision tasks with limited data. Participants are limited to training\nmodels from scratch using a low number of training samples and are not allowed\nto use any form of transfer learning. We aim to stimulate the development of\nnovel approaches that incorporate inductive biases to improve the data\nefficiency of deep learning models. Significant advancements are made compared\nto the provided baselines, where winning solutions surpass the baselines by a\nconsiderable margin in both tasks. As in previous editions, these achievements\nare primarily attributed to heavy use of data augmentation policies and large\nmodel ensembles, though novel prior-based methods seem to contribute more to\nsuccessful solutions compared to last year. This report highlights the key\naspects of the challenges and their outcomes.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.18176v2.pdf",
        "similarity": 0.4974334623475828,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-26"
    },
    {
        "new_title": "Promoting AI Equity in Science: Generalized Domain Prompt Learning for\n  Accessible VLM Research",
        "new_link": "http://arxiv.org/abs/2405.08668v1",
        "new_summary": "  Large-scale Vision-Language Models (VLMs) have demonstrated exceptional\nperformance in natural vision tasks, motivating researchers across domains to\nexplore domain-specific VLMs. However, the construction of powerful\ndomain-specific VLMs demands vast amounts of annotated data, substantial\nelectrical energy, and computing resources, primarily accessible to industry,\nyet hindering VLM research in academia. To address this challenge and foster\nsustainable and equitable VLM research, we present the Generalized Domain\nPrompt Learning (GDPL) framework. GDPL facilitates the transfer of VLMs' robust\nrecognition capabilities from natural vision to specialized domains, without\nthe need for extensive data or resources. By leveraging small-scale\ndomain-specific foundation models and minimal prompt samples, GDPL empowers the\nlanguage branch with domain knowledge through quaternion networks, uncovering\ncross-modal relationships between domain-specific vision features and natural\nvision-based contextual embeddings. Simultaneously, GDPL guides the vision\nbranch into specific domains through hierarchical propagation of generated\nvision prompt features, grounded in well-matched vision-language relations.\nFurthermore, to fully harness the domain adaptation potential of VLMs, we\nintroduce a novel low-rank adaptation approach. Extensive experiments across\ndiverse domains like remote sensing, medical imaging, geology, Synthetic\nAperture Radar, and fluid dynamics, validate the efficacy of GDPL,\ndemonstrating its ability to achieve state-of-the-art domain recognition\nperformance in a prompt learning paradigm. Our framework paves the way for\nsustainable and inclusive VLM research, transcending the barriers between\nacademia and industry.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.08668v1.pdf",
        "similarity": 0.49688739324190606,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-14"
    },
    {
        "new_title": "From Forest to Zoo: Great Ape Behavior Recognition with ChimpBehave",
        "new_link": "http://arxiv.org/abs/2405.20025v1",
        "new_summary": "  This paper addresses the significant challenge of recognizing behaviors in\nnon-human primates, specifically focusing on chimpanzees. Automated behavior\nrecognition is crucial for both conservation efforts and the advancement of\nbehavioral research. However, it is significantly hindered by the\nlabor-intensive process of manual video annotation. Despite the availability of\nlarge-scale animal behavior datasets, the effective application of machine\nlearning models across varied environmental settings poses a critical\nchallenge, primarily due to the variability in data collection contexts and the\nspecificity of annotations.\n  In this paper, we introduce ChimpBehave, a novel dataset featuring over 2\nhours of video (approximately 193,000 video frames) of zoo-housed chimpanzees,\nmeticulously annotated with bounding boxes and behavior labels for action\nrecognition. ChimpBehave uniquely aligns its behavior classes with existing\ndatasets, allowing for the study of domain adaptation and cross-dataset\ngeneralization methods between different visual settings. Furthermore, we\nbenchmark our dataset using a state-of-the-art CNN-based action recognition\nmodel, providing the first baseline results for both within and cross-dataset\nsettings. The dataset, models, and code can be accessed at:\nhttps://github.com/MitchFuchs/ChimpBehave\n",
        "pdf_link": "https://arxiv.org/pdf/2405.20025v1.pdf",
        "similarity": 0.49675982168179256,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-30"
    },
    {
        "new_title": "ECOR: Explainable CLIP for Object Recognition",
        "new_link": "http://arxiv.org/abs/2404.12839v1",
        "new_summary": "  Large Vision Language Models (VLMs), such as CLIP, have significantly\ncontributed to various computer vision tasks, including object recognition and\nobject detection. Their open vocabulary feature enhances their value. However,\ntheir black-box nature and lack of explainability in predictions make them less\ntrustworthy in critical domains. Recently, some work has been done to force\nVLMs to provide reasonable rationales for object recognition, but this often\ncomes at the expense of classification accuracy. In this paper, we first\npropose a mathematical definition of explainability in the object recognition\ntask based on the joint probability distribution of categories and rationales,\nthen leverage this definition to fine-tune CLIP in an explainable manner.\nThrough evaluations of different datasets, our method demonstrates\nstate-of-the-art performance in explainable classification. Notably, it excels\nin zero-shot settings, showcasing its adaptability. This advancement improves\nexplainable object recognition, enhancing trust across diverse applications.\nThe code will be made available online upon publication.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.12839v1.pdf",
        "similarity": 0.4938543903746207,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-19"
    },
    {
        "new_title": "You Only Need Less Attention at Each Stage in Vision Transformers",
        "new_link": "http://arxiv.org/abs/2406.00427v1",
        "new_summary": "  The advent of Vision Transformers (ViTs) marks a substantial paradigm shift\nin the realm of computer vision. ViTs capture the global information of images\nthrough self-attention modules, which perform dot product computations among\npatchified image tokens. While self-attention modules empower ViTs to capture\nlong-range dependencies, the computational complexity grows quadratically with\nthe number of tokens, which is a major hindrance to the practical application\nof ViTs. Moreover, the self-attention mechanism in deep ViTs is also\nsusceptible to the attention saturation issue. Accordingly, we argue against\nthe necessity of computing the attention scores in every layer, and we propose\nthe Less-Attention Vision Transformer (LaViT), which computes only a few\nattention operations at each stage and calculates the subsequent feature\nalignments in other layers via attention transformations that leverage the\npreviously calculated attention scores. This novel approach can mitigate two\nprimary issues plaguing traditional self-attention modules: the heavy\ncomputational burden and attention saturation. Our proposed architecture offers\nsuperior efficiency and ease of implementation, merely requiring matrix\nmultiplications that are highly optimized in contemporary deep learning\nframeworks. Moreover, our architecture demonstrates exceptional performance\nacross various vision tasks including classification, detection and\nsegmentation.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.00427v1.pdf",
        "similarity": 0.49365158828654754,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-01"
    },
    {
        "new_title": "NeoNeXt: Novel neural network operator and architecture based on the\n  patch-wise matrix multiplications",
        "new_link": "http://arxiv.org/abs/2403.11251v1",
        "new_summary": "  Most of the computer vision architectures nowadays are built upon the\nwell-known foundation operations: fully-connected layers, convolutions and\nmulti-head self-attention blocks. In this paper we propose a novel foundation\noperation - NeoCell - which learns matrix patterns and performs patchwise\nmatrix multiplications with the input data. The main advantages of the proposed\noperator are (1) simple implementation without need in operations like im2col,\n(2) low computational complexity (especially for large matrices) and (3) simple\nand flexible implementation of up-/down-sampling. We validate NeoNeXt family of\nmodels based on this operation on ImageNet-1K classification task and show that\nthey achieve competitive quality.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.11251v1.pdf",
        "similarity": 0.4928800253277184,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-17"
    },
    {
        "new_title": "Exploring Explainability in Video Action Recognition",
        "new_link": "http://arxiv.org/abs/2404.09067v1",
        "new_summary": "  Image Classification and Video Action Recognition are perhaps the two most\nfoundational tasks in computer vision. Consequently, explaining the inner\nworkings of trained deep neural networks is of prime importance. While numerous\nefforts focus on explaining the decisions of trained deep neural networks in\nimage classification, exploration in the domain of its temporal version, video\naction recognition, has been scant. In this work, we take a deeper look at this\nproblem. We begin by revisiting Grad-CAM, one of the popular feature\nattribution methods for Image Classification, and its extension to Video Action\nRecognition tasks and examine the method's limitations. To address these, we\nintroduce Video-TCAV, by building on TCAV for Image Classification tasks, which\naims to quantify the importance of specific concepts in the decision-making\nprocess of Video Action Recognition models. As the scalable generation of\nconcepts is still an open problem, we propose a machine-assisted approach to\ngenerate spatial and spatiotemporal concepts relevant to Video Action\nRecognition for testing Video-TCAV. We then establish the importance of\ntemporally-varying concepts by demonstrating the superiority of dynamic\nspatiotemporal concepts over trivial spatial concepts. In conclusion, we\nintroduce a framework for investigating hypotheses in action recognition and\nquantitatively testing them, thus advancing research in the explainability of\ndeep neural networks used in video action recognition.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.09067v1.pdf",
        "similarity": 0.49104556120790144,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-13"
    },
    {
        "new_title": "Peeking Behind the Curtains of Residual Learning",
        "new_link": "http://arxiv.org/abs/2402.08645v1",
        "new_summary": "  The utilization of residual learning has become widespread in deep and\nscalable neural nets. However, the fundamental principles that contribute to\nthe success of residual learning remain elusive, thus hindering effective\ntraining of plain nets with depth scalability. In this paper, we peek behind\nthe curtains of residual learning by uncovering the \"dissipating inputs\"\nphenomenon that leads to convergence failure in plain neural nets: the input is\ngradually compromised through plain layers due to non-linearities, resulting in\nchallenges of learning feature representations. We theoretically demonstrate\nhow plain neural nets degenerate the input to random noise and emphasize the\nsignificance of a residual connection that maintains a better lower bound of\nsurviving neurons as a solution. With our theoretical discoveries, we propose\n\"The Plain Neural Net Hypothesis\" (PNNH) that identifies the internal path\nacross non-linear layers as the most critical part in residual learning, and\nestablishes a paradigm to support the training of deep plain neural nets devoid\nof residual connections. We thoroughly evaluate PNNH-enabled CNN architectures\nand Transformers on popular vision benchmarks, showing on-par accuracy, up to\n0.3% higher training throughput, and 2x better parameter efficiency compared to\nResNets and vision Transformers.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.08645v1.pdf",
        "similarity": 0.4906211084578778,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-13"
    },
    {
        "new_title": "A Survey on Transformer Compression",
        "new_link": "http://arxiv.org/abs/2402.05964v2",
        "new_summary": "  Transformer plays a vital role in the realms of natural language processing\n(NLP) and computer vision (CV), specially for constructing large language\nmodels (LLM) and large vision models (LVM). Model compression methods reduce\nthe memory and computational cost of Transformer, which is a necessary step to\nimplement large language/vision models on practical devices. Given the unique\narchitecture of Transformer, featuring alternative attention and feedforward\nneural network (FFN) modules, specific compression techniques are usually\nrequired. The efficiency of these compression methods is also paramount, as\nretraining large models on the entire training dataset is usually impractical.\nThis survey provides a comprehensive review of recent compression methods, with\na specific focus on their application to Transformer-based models. The\ncompression methods are primarily categorized into pruning, quantization,\nknowledge distillation, and efficient architecture design (Mamba, RetNet, RWKV,\netc.). In each category, we discuss compression methods for both language and\nvision tasks, highlighting common underlying principles. Finally, we delve into\nthe relation between various compression methods, and discuss further\ndirections in this domain.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.05964v2.pdf",
        "similarity": 0.4902284322929195,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-05"
    },
    {
        "new_title": "HierCode: A Lightweight Hierarchical Codebook for Zero-shot Chinese Text\n  Recognition",
        "new_link": "http://arxiv.org/abs/2403.13761v1",
        "new_summary": "  Text recognition, especially for complex scripts like Chinese, faces unique\nchallenges due to its intricate character structures and vast vocabulary.\nTraditional one-hot encoding methods struggle with the representation of\nhierarchical radicals, recognition of Out-Of-Vocabulary (OOV) characters, and\non-device deployment due to their computational intensity. To address these\nchallenges, we propose HierCode, a novel and lightweight codebook that exploits\nthe innate hierarchical nature of Chinese characters. HierCode employs a\nmulti-hot encoding strategy, leveraging hierarchical binary tree encoding and\nprototype learning to create distinctive, informative representations for each\ncharacter. This approach not only facilitates zero-shot recognition of OOV\ncharacters by utilizing shared radicals and structures but also excels in\nline-level recognition tasks by computing similarity with visual features, a\nnotable advantage over existing methods. Extensive experiments across diverse\nbenchmarks, including handwritten, scene, document, web, and ancient text, have\nshowcased HierCode's superiority for both conventional and zero-shot Chinese\ncharacter or text recognition, exhibiting state-of-the-art performance with\nsignificantly fewer parameters and fast inference speed.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.13761v1.pdf",
        "similarity": 0.48929514564525933,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-20"
    },
    {
        "new_title": "Machine Unlearning in Contrastive Learning",
        "new_link": "http://arxiv.org/abs/2405.07317v1",
        "new_summary": "  Machine unlearning is a complex process that necessitates the model to\ndiminish the influence of the training data while keeping the loss of accuracy\nto a minimum. Despite the numerous studies on machine unlearning in recent\nyears, the majority of them have primarily focused on supervised learning\nmodels, leaving research on contrastive learning models relatively\nunderexplored. With the conviction that self-supervised learning harbors a\npromising potential, surpassing or rivaling that of supervised learning, we set\nout to investigate methods for machine unlearning centered around contrastive\nlearning models. In this study, we introduce a novel gradient constraint-based\napproach for training the model to effectively achieve machine unlearning. Our\nmethod only necessitates a minimal number of training epochs and the\nidentification of the data slated for unlearning. Remarkably, our approach\ndemonstrates proficient performance not only on contrastive learning models but\nalso on supervised learning models, showcasing its versatility and adaptability\nin various learning paradigms.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.07317v1.pdf",
        "similarity": 0.48920706664083685,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-12"
    },
    {
        "new_title": "DEEP-ICL: Definition-Enriched Experts for Language Model In-Context\n  Learning",
        "new_link": "http://arxiv.org/abs/2403.04233v2",
        "new_summary": "  It has long been assumed that the sheer number of parameters in large\nlanguage models (LLMs) drives in-context learning (ICL) capabilities, enabling\nremarkable performance improvements by leveraging task-specific demonstrations.\nChallenging this hypothesis, we introduce DEEP-ICL, a novel task Definition\nEnriched ExPert Ensembling methodology for ICL. DEEP-ICL explicitly extracts\ntask definitions from given demonstrations and generates responses through\nlearning task-specific examples. We argue that improvement from ICL does not\ndirectly rely on model size, but essentially stems from understanding task\ndefinitions and task-guided learning. Inspired by this, DEEP-ICL combines two\n3B models with distinct roles (one for concluding task definitions and the\nother for learning task demonstrations) and achieves comparable performance to\nLLaMA2-13B. Furthermore, our framework outperforms conventional ICL by\novercoming pretraining sequence length limitations, by supporting unlimited\ndemonstrations. We contend that DEEP-ICL presents a novel alternative for\nachieving efficient few-shot learning, extending beyond the conventional ICL.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.04233v2.pdf",
        "similarity": 0.4891032299911148,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-07"
    },
    {
        "new_title": "Transitive Vision-Language Prompt Learning for Domain Generalization",
        "new_link": "http://arxiv.org/abs/2404.18758v1",
        "new_summary": "  The vision-language pre-training has enabled deep models to make a huge step\nforward in generalizing across unseen domains. The recent learning method based\non the vision-language pre-training model is a great tool for domain\ngeneralization and can solve this problem to a large extent. However, there are\nstill some issues that an advancement still suffers from trading-off between\ndomain invariance and class separability, which are crucial in current DG\nproblems. However, there are still some issues that an advancement still\nsuffers from trading-off between domain invariance and class separability,\nwhich are crucial in current DG problems. In this paper, we introduce a novel\nprompt learning strategy that leverages deep vision prompts to address domain\ninvariance while utilizing language prompts to ensure class separability,\ncoupled with adaptive weighting mechanisms to balance domain invariance and\nclass separability. Extensive experiments demonstrate that deep vision prompts\neffectively extract domain-invariant features, significantly improving the\ngeneralization ability of deep models and achieving state-of-the-art\nperformance on three datasets.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.18758v1.pdf",
        "similarity": 0.4885543705714728,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-29"
    },
    {
        "new_title": "Visual Anchors Are Strong Information Aggregators For Multimodal Large\n  Language Model",
        "new_link": "http://arxiv.org/abs/2405.17815v1",
        "new_summary": "  In the realm of Multimodal Large Language Models (MLLMs), vision-language\nconnector plays a crucial role to link the pre-trained vision encoders with\nLarge Language Models (LLMs). Despite its importance, the vision-language\nconnector has been relatively less explored. In this study, we aim to propose a\nstrong vision-language connector that enables MLLMs to achieve high accuracy\nwhile maintain low computation cost. We first reveal the existence of the\nvisual anchors in Vision Transformer and propose a cost-effective search\nalgorithm to extract them. Building on these findings, we introduce the Anchor\nFormer (AcFormer), a novel vision-language connector designed to leverage the\nrich prior knowledge obtained from these visual anchors during pretraining,\nguiding the aggregation of information. Through extensive experimentation, we\ndemonstrate that the proposed method significantly reduces computational costs\nby nearly two-thirds compared with baseline, while simultaneously outperforming\nbaseline methods. This highlights the effectiveness and efficiency of AcFormer.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.17815v1.pdf",
        "similarity": 0.486516264513616,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-28"
    },
    {
        "new_title": "Deep Support Vectors",
        "new_link": "http://arxiv.org/abs/2403.17329v2",
        "new_summary": "  Deep learning has achieved tremendous success. \\nj{However,} unlike SVMs,\nwhich provide direct decision criteria and can be trained with a small dataset,\nit still has significant weaknesses due to its requirement for massive datasets\nduring training and the black-box characteristics on decision criteria.\n\\nj{This paper addresses} these issues by identifying support vectors in deep\nlearning models. To this end, we propose the DeepKKT condition, an adaptation\nof the traditional Karush-Kuhn-Tucker (KKT) condition for deep learning models,\nand confirm that generated Deep Support Vectors (DSVs) using this condition\nexhibit properties similar to traditional support vectors. This allows us to\napply our method to few-shot dataset distillation problems and alleviate the\nblack-box characteristics of deep learning models. Additionally, we demonstrate\nthat the DeepKKT condition can transform conventional classification models\ninto generative models with high fidelity, particularly as latent\n\\jh{generative} models using class labels as latent variables. We validate the\neffectiveness of DSVs \\nj{using common datasets (ImageNet, CIFAR10 \\nj{and}\nCIFAR100) on the general architectures (ResNet and ConvNet)}, proving their\npractical applicability. (See Fig.~\\ref{fig:generated})\n",
        "pdf_link": "https://arxiv.org/pdf/2403.17329v2.pdf",
        "similarity": 0.48518693660382983,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-26"
    },
    {
        "new_title": "M3H: Multimodal Multitask Machine Learning for Healthcare",
        "new_link": "http://arxiv.org/abs/2404.18975v3",
        "new_summary": "  Developing an integrated many-to-many framework leveraging multimodal data\nfor multiple tasks is crucial to unifying healthcare applications ranging from\ndiagnoses to operations. In resource-constrained hospital environments, a\nscalable and unified machine learning framework that improves previous forecast\nperformances could improve hospital operations and save costs. We introduce\nM3H, an explainable Multimodal Multitask Machine Learning for Healthcare\nframework that consolidates learning from tabular, time-series, language, and\nvision data for supervised binary/multiclass classification, regression, and\nunsupervised clustering. It features a novel attention mechanism balancing\nself-exploitation (learning source-task), and cross-exploration (learning\ncross-tasks), and offers explainability through a proposed TIM score, shedding\nlight on the dynamics of task learning interdependencies. M3H encompasses an\nunprecedented range of medical tasks and machine learning problem classes and\nconsistently outperforms traditional single-task models by on average 11.6%\nacross 40 disease diagnoses from 16 medical departments, three hospital\noperation forecasts, and one patient phenotyping task. The modular design of\nthe framework ensures its generalizability in data processing, task definition,\nand rapid model prototyping, making it production ready for both clinical and\noperational healthcare settings, especially those in constrained environments.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.18975v3.pdf",
        "similarity": 0.48293475472687036,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-29"
    },
    {
        "new_title": "Establishing Deep InfoMax as an effective self-supervised learning\n  methodology in materials informatics",
        "new_link": "http://arxiv.org/abs/2407.00671v1",
        "new_summary": "  The scarcity of property labels remains a key challenge in materials\ninformatics, whereas materials data without property labels are abundant in\ncomparison. By pretraining supervised property prediction models on\nself-supervised tasks that depend only on the \"intrinsic information\" available\nin any Crystallographic Information File (CIF), there is potential to leverage\nthe large amount of crystal data without property labels to improve property\nprediction results on small datasets. We apply Deep InfoMax as a\nself-supervised machine learning framework for materials informatics that\nexplicitly maximises the mutual information between a point set (or graph)\nrepresentation of a crystal and a vector representation suitable for downstream\nlearning. This allows the pretraining of supervised models on large materials\ndatasets without the need for property labels and without requiring the model\nto reconstruct the crystal from a representation vector. We investigate the\nbenefits of Deep InfoMax pretraining implemented on the Site-Net architecture\nto improve the performance of downstream property prediction models with small\namounts (<10^3) of data, a situation relevant to experimentally measured\nmaterials property databases. Using a property label masking methodology, where\nwe perform self-supervised learning on larger supervised datasets and then\ntrain supervised models on a small subset of the labels, we isolate Deep\nInfoMax pretraining from the effects of distributional shift. We demonstrate\nperformance improvements in the contexts of representation learning and\ntransfer learning on the tasks of band gap and formation energy prediction.\nHaving established the effectiveness of Deep InfoMax pretraining in a\ncontrolled environment, our findings provide a foundation for extending the\napproach to address practical challenges in materials informatics.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.00671v1.pdf",
        "similarity": 0.48232702671700084,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-30"
    },
    {
        "new_title": "CSCO: Connectivity Search of Convolutional Operators",
        "new_link": "http://arxiv.org/abs/2404.17152v1",
        "new_summary": "  Exploring dense connectivity of convolutional operators establishes critical\n\"synapses\" to communicate feature vectors from different levels and enriches\nthe set of transformations on Computer Vision applications. Yet, even with\nheavy-machinery approaches such as Neural Architecture Search (NAS),\ndiscovering effective connectivity patterns requires tremendous efforts due to\neither constrained connectivity design space or a sub-optimal exploration\nprocess induced by an unconstrained search space. In this paper, we propose\nCSCO, a novel paradigm that fabricates effective connectivity of convolutional\noperators with minimal utilization of existing design motifs and further\nutilizes the discovered wiring to construct high-performing ConvNets. CSCO\nguides the exploration via a neural predictor as a surrogate of the\nground-truth performance. We introduce Graph Isomorphism as data augmentation\nto improve sample efficiency and propose a Metropolis-Hastings Evolutionary\nSearch (MH-ES) to evade locally optimal architectures and advance search\nquality. Results on ImageNet show ~0.6% performance improvement over\nhand-crafted and NAS-crafted dense connectivity. Our code is publicly\navailable.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.17152v1.pdf",
        "similarity": 0.4815293891683141,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-26"
    },
    {
        "new_title": "Libra: Building Decoupled Vision System on Large Language Models",
        "new_link": "http://arxiv.org/abs/2405.10140v1",
        "new_summary": "  In this work, we introduce Libra, a prototype model with a decoupled vision\nsystem on a large language model (LLM). The decoupled vision system decouples\ninner-modal modeling and cross-modal interaction, yielding unique visual\ninformation modeling and effective cross-modal comprehension. Libra is trained\nthrough discrete auto-regressive modeling on both vision and language inputs.\nSpecifically, we incorporate a routed visual expert with a cross-modal bridge\nmodule into a pretrained LLM to route the vision and language flows during\nattention computing to enable different attention patterns in inner-modal\nmodeling and cross-modal interaction scenarios. Experimental results\ndemonstrate that the dedicated design of Libra achieves a strong MLLM baseline\nthat rivals existing works in the image-to-text scenario with merely 50 million\ntraining data, providing a new perspective for future multimodal foundation\nmodels. Code is available at https://github.com/YifanXu74/Libra.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.10140v1.pdf",
        "similarity": 0.48083130642623356,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-16"
    },
    {
        "new_title": "Linguacodus: A Synergistic Framework for Transformative Code Generation\n  in Machine Learning Pipelines",
        "new_link": "http://arxiv.org/abs/2403.11585v2",
        "new_summary": "  In the ever-evolving landscape of machine learning, seamless translation of\nnatural language descriptions into executable code remains a formidable\nchallenge. This paper introduces Linguacodus, an innovative framework designed\nto tackle this challenge by deploying a dynamic pipeline that iteratively\ntransforms natural language task descriptions into code through high-level\ndata-shaping instructions. The core of Linguacodus is a fine-tuned large\nlanguage model (LLM), empowered to evaluate diverse solutions for various\nproblems and select the most fitting one for a given task. This paper details\nthe fine-tuning process, and sheds light on how natural language descriptions\ncan be translated into functional code. Linguacodus represents a substantial\nleap towards automated code generation, effectively bridging the gap between\ntask descriptions and executable code. It holds great promise for advancing\nmachine learning applications across diverse domains. Additionally, we propose\nan algorithm capable of transforming a natural description of an ML task into\ncode with minimal human interaction. In extensive experiments on a vast machine\nlearning code dataset originating from Kaggle, we showcase the effectiveness of\nLinguacodus. The investigations highlight its potential applications across\ndiverse domains, emphasizing its impact on applied machine learning in various\nscientific fields.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.11585v2.pdf",
        "similarity": 0.48003269185417397,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-18"
    },
    {
        "new_title": "Are Vision xLSTM Embedded UNet More Reliable in Medical 3D Image\n  Segmentation?",
        "new_link": "http://arxiv.org/abs/2406.16993v1",
        "new_summary": "  The advancement of developing efficient medical image segmentation has\nevolved from initial dependence on Convolutional Neural Networks (CNNs) to the\npresent investigation of hybrid models that combine CNNs with Vision\nTransformers. Furthermore, there is an increasing focus on creating\narchitectures that are both high-performing in medical image segmentation tasks\nand computationally efficient to be deployed on systems with limited resources.\nAlthough transformers have several advantages like capturing global\ndependencies in the input data, they face challenges such as high computational\nand memory complexity. This paper investigates the integration of CNNs and\nVision Extended Long Short-Term Memory (Vision-xLSTM) models by introducing a\nnovel approach called UVixLSTM. The Vision-xLSTM blocks captures temporal and\nglobal relationships within the patches extracted from the CNN feature maps.\nThe convolutional feature reconstruction path upsamples the output volume from\nthe Vision-xLSTM blocks to produce the segmentation output. Our primary\nobjective is to propose that Vision-xLSTM forms a reliable backbone for medical\nimage segmentation tasks, offering excellent segmentation performance and\nreduced computational complexity. UVixLSTM exhibits superior performance\ncompared to state-of-the-art networks on the publicly-available Synapse\ndataset. Code is available at: https://github.com/duttapallabi2907/UVixLSTM\n",
        "pdf_link": "https://arxiv.org/pdf/2406.16993v1.pdf",
        "similarity": 0.47917671829489855,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-24"
    },
    {
        "new_title": "Incorporating Geo-Diverse Knowledge into Prompting for Increased\n  Geographical Robustness in Object Recognition",
        "new_link": "http://arxiv.org/abs/2401.01482v2",
        "new_summary": "  Existing object recognition models have been shown to lack robustness in\ndiverse geographical scenarios due to domain shifts in design and context.\nClass representations need to be adapted to more accurately reflect an object\nconcept under these shifts. In the absence of training data from target\ngeographies, we hypothesize that geographically diverse descriptive knowledge\nof categories can enhance robustness. For this purpose, we explore the\nfeasibility of probing a large language model for geography-based object\nknowledge, and we examine the effects of integrating knowledge into zero-shot\nand learnable soft prompting with CLIP. Within this exploration, we propose\ngeography knowledge regularization to ensure that soft prompts trained on a\nsource set of geographies generalize to an unseen target set. Accuracy gains\nover prompting baselines on DollarStreet while training only on Europe data are\nup to +2.8/1.2/1.6 on target data from Africa/Asia/Americas, and +4.6 overall\non the hardest classes. Competitive performance is shown vs. few-shot target\ntraining, and analysis is provided to direct future study of geographical\nrobustness.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.01482v2.pdf",
        "similarity": 0.4789632715700579,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-03"
    },
    {
        "new_title": "Learn and Search: An Elegant Technique for Object Lookup using\n  Contrastive Learning",
        "new_link": "http://arxiv.org/abs/2403.07231v1",
        "new_summary": "  The rapid proliferation of digital content and the ever-growing need for\nprecise object recognition and segmentation have driven the advancement of\ncutting-edge techniques in the field of object classification and segmentation.\nThis paper introduces \"Learn and Search\", a novel approach for object lookup\nthat leverages the power of contrastive learning to enhance the efficiency and\neffectiveness of retrieval systems.\n  In this study, we present an elegant and innovative methodology that\nintegrates deep learning principles and contrastive learning to tackle the\nchallenges of object search. Our extensive experimentation reveals compelling\nresults, with \"Learn and Search\" achieving superior Similarity Grid Accuracy,\nshowcasing its efficacy in discerning regions of utmost similarity within an\nimage relative to a cropped image.\n  The seamless fusion of deep learning and contrastive learning to address the\nintricacies of object identification not only promises transformative\napplications in image recognition, recommendation systems, and content tagging\nbut also revolutionizes content-based search and retrieval. The amalgamation of\nthese techniques, as exemplified by \"Learn and Search,\" represents a\nsignificant stride in the ongoing evolution of methodologies in the dynamic\nrealm of object classification and segmentation.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.07231v1.pdf",
        "similarity": 0.4785454852256317,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-12"
    },
    {
        "new_title": "Comprehensive Study on Performance Evaluation and Optimization of Model\n  Compression: Bridging Traditional Deep Learning and Large Language Models",
        "new_link": "http://arxiv.org/abs/2407.15904v1",
        "new_summary": "  Deep learning models have achieved tremendous success in most of the\nindustries in recent years. The evolution of these models has also led to an\nincrease in the model size and energy requirement, making it difficult to\ndeploy in production on low compute devices. An increase in the number of\nconnected devices around the world warrants compressed models that can be\neasily deployed at the local devices with low compute capacity and power\naccessibility. A wide range of solutions have been proposed by different\nresearchers to reduce the size and complexity of such models, prominent among\nthem are, Weight Quantization, Parameter Pruning, Network Pruning, low-rank\nrepresentation, weights sharing, neural architecture search, knowledge\ndistillation etc. In this research work, we investigate the performance impacts\non various trained deep learning models, compressed using quantization and\npruning techniques. We implemented both, quantization and pruning, compression\ntechniques on popular deep learning models used in the image classification,\nobject detection, language models and generative models-based problem\nstatements. We also explored performance of various large language models\n(LLMs) after quantization and low rank adaptation. We used the standard\nevaluation metrics (model's size, accuracy, and inference time) for all the\nrelated problem statements and concluded this paper by discussing the\nchallenges and future work.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.15904v1.pdf",
        "similarity": 0.47837115817898523,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-22"
    },
    {
        "new_title": "Heterogeneous Contrastive Learning for Foundation Models and Beyond",
        "new_link": "http://arxiv.org/abs/2404.00225v1",
        "new_summary": "  In the era of big data and Artificial Intelligence, an emerging paradigm is\nto utilize contrastive self-supervised learning to model large-scale\nheterogeneous data. Many existing foundation models benefit from the\ngeneralization capability of contrastive self-supervised learning by learning\ncompact and high-quality representations without relying on any label\ninformation. Amidst the explosive advancements in foundation models across\nmultiple domains, including natural language processing and computer vision, a\nthorough survey on heterogeneous contrastive learning for the foundation model\nis urgently needed. In response, this survey critically evaluates the current\nlandscape of heterogeneous contrastive learning for foundation models,\nhighlighting the open challenges and future trends of contrastive learning. In\nparticular, we first present how the recent advanced contrastive learning-based\nmethods deal with view heterogeneity and how contrastive learning is applied to\ntrain and fine-tune the multi-view foundation models. Then, we move to\ncontrastive learning methods for task heterogeneity, including pretraining\ntasks and downstream tasks, and show how different tasks are combined with\ncontrastive learning loss for different purposes. Finally, we conclude this\nsurvey by discussing the open challenges and shedding light on the future\ndirections of contrastive learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.00225v1.pdf",
        "similarity": 0.4779324164533762,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-30"
    },
    {
        "new_title": "The Progression of Transformers from Language to Vision to MOT: A\n  Literature Review on Multi-Object Tracking with Transformers",
        "new_link": "http://arxiv.org/abs/2406.16784v1",
        "new_summary": "  The transformer neural network architecture allows for autoregressive\nsequence-to-sequence modeling through the use of attention layers. It was\noriginally created with the application of machine translation but has\nrevolutionized natural language processing. Recently, transformers have also\nbeen applied across a wide variety of pattern recognition tasks, particularly\nin computer vision. In this literature review, we describe major advances in\ncomputer vision utilizing transformers. We then focus specifically on\nMulti-Object Tracking (MOT) and discuss how transformers are increasingly\nbecoming competitive in state-of-the-art MOT works, yet still lag behind\ntraditional deep learning methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.16784v1.pdf",
        "similarity": 0.4755576132489018,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-24"
    },
    {
        "new_title": "VoCo-LLaMA: Towards Vision Compression with Large Language Models",
        "new_link": "http://arxiv.org/abs/2406.12275v1",
        "new_summary": "  Vision-Language Models (VLMs) have achieved remarkable success in various\nmulti-modal tasks, but they are often bottlenecked by the limited context\nwindow and high computational cost of processing high-resolution image inputs\nand videos. Vision compression can alleviate this problem by reducing the\nvision token count. Previous approaches compress vision tokens with external\nmodules and force LLMs to understand the compressed ones, leading to visual\ninformation loss. However, the LLMs' understanding paradigm of vision tokens is\nnot fully utilised in the compression learning process. We propose VoCo-LLaMA,\nthe first approach to compress vision tokens using LLMs. By introducing Vision\nCompression tokens during the vision instruction tuning phase and leveraging\nattention distillation, our method distill how LLMs comprehend vision tokens\ninto their processing of VoCo tokens. VoCo-LLaMA facilitates effective vision\ncompression and improves the computational efficiency during the inference\nstage. Specifically, our method achieves minimal performance loss with a\ncompression ratio of 576$\\times$, resulting in up to 94.8$\\%$ fewer FLOPs and\n69.6$\\%$ acceleration in inference time. Furthermore, through continuous\ntraining using time-series compressed token sequences of video frames,\nVoCo-LLaMA demonstrates the ability to understand temporal correlations,\noutperforming previous methods on popular video question-answering benchmarks.\nOur approach presents a promising way to unlock the full potential of VLMs'\ncontextual window, enabling more scalable multi-modal applications. The project\npage, along with the associated code, can be accessed via\n$\\href{https://yxxxb.github.io/VoCo-LLaMA-page/}{\\text{this https URL}}$.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.12275v1.pdf",
        "similarity": 0.4744790434533973,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-18"
    },
    {
        "new_title": "Multimodal Structured Generation: CVPR's 2nd MMFM Challenge Technical\n  Report",
        "new_link": "http://arxiv.org/abs/2406.11403v1",
        "new_summary": "  Multimodal Foundation Models (MMFMs) have shown remarkable performance on\nvarious computer vision and natural language processing tasks. However, their\nperformance on particular tasks such as document understanding is still\nlimited. They also require more compute, time, and engineering resources to\nfinetune and deploy compared to traditional, unimodal models. In this report,\nwe present Multimodal Structured Generation, a general framework which\nconstrains the output logits of frozen MMFMs to force them to reason before\nresponding with structured outputs that downstream APIs can parse and use. We\nprovide a detailed account of our approach, including the technical details,\ntheoretical discussions, and final evaluation results in the 2nd Multimodal\nFoundation Models Challenge hosted by the Computer Vision and Pattern\nRecognition (CVPR) conference. Our approach achieved the second highest score\nin the hidden test set for Phase 2 and third highest overall. This shows the\nmethod's ability to generalize to unseen tasks. And that simple engineering can\nbeat expensive & complicated modelling steps as we first discussed in our\npaper, Retrieval Augmented Structured Generation: Business Document Information\nExtraction as Tool Use. All of our scripts, deployment steps, and evaluation\nresults can be accessed in https://github.com/leloykun/MMFM-Challenge\n",
        "pdf_link": "https://arxiv.org/pdf/2406.11403v1.pdf",
        "similarity": 0.4744150397223563,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-17"
    },
    {
        "new_title": "Unmasking Trees for Tabular Data",
        "new_link": "http://arxiv.org/abs/2407.05593v1",
        "new_summary": "  We herein describe UnmaskingTrees, a method and open-source software package\nfor tabular data generation and, especially, imputation. Our experiments\nsuggest that training gradient-boosted trees to incrementally unmask features\noffers a simple, strong baseline for imputation.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.05593v1.pdf",
        "similarity": 0.47252557859668737,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-08"
    },
    {
        "new_title": "Deep Models for Multi-View 3D Object Recognition: A Review",
        "new_link": "http://arxiv.org/abs/2404.15224v1",
        "new_summary": "  Human decision-making often relies on visual information from multiple\nperspectives or views. In contrast, machine learning-based object recognition\nutilizes information from a single image of the object. However, the\ninformation conveyed by a single image may not be sufficient for accurate\ndecision-making, particularly in complex recognition problems. The utilization\nof multi-view 3D representations for object recognition has thus far\ndemonstrated the most promising results for achieving state-of-the-art\nperformance. This review paper comprehensively covers recent progress in\nmulti-view 3D object recognition methods for 3D classification and retrieval\ntasks. Specifically, we focus on deep learning-based and transformer-based\ntechniques, as they are widely utilized and have achieved state-of-the-art\nperformance. We provide detailed information about existing deep learning-based\nand transformer-based multi-view 3D object recognition models, including the\nmost commonly used 3D datasets, camera configurations and number of views, view\nselection strategies, pre-trained CNN architectures, fusion strategies, and\nrecognition performance on 3D classification and 3D retrieval tasks.\nAdditionally, we examine various computer vision applications that use\nmulti-view classification. Finally, we highlight key findings and future\ndirections for developing multi-view 3D object recognition methods to provide\nreaders with a comprehensive understanding of the field.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.15224v1.pdf",
        "similarity": 0.4716841481255238,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-23"
    },
    {
        "new_title": "How Deep Networks Learn Sparse and Hierarchical Data: the Sparse Random\n  Hierarchy Model",
        "new_link": "http://arxiv.org/abs/2404.10727v2",
        "new_summary": "  Understanding what makes high-dimensional data learnable is a fundamental\nquestion in machine learning. On the one hand, it is believed that the success\nof deep learning lies in its ability to build a hierarchy of representations\nthat become increasingly more abstract with depth, going from simple features\nlike edges to more complex concepts. On the other hand, learning to be\ninsensitive to invariances of the task, such as smooth transformations for\nimage datasets, has been argued to be important for deep networks and it\nstrongly correlates with their performance. In this work, we aim to explain\nthis correlation and unify these two viewpoints. We show that by introducing\nsparsity to generative hierarchical models of data, the task acquires\ninsensitivity to spatial transformations that are discrete versions of smooth\ntransformations. In particular, we introduce the Sparse Random Hierarchy Model\n(SRHM), where we observe and rationalize that a hierarchical representation\nmirroring the hierarchical model is learnt precisely when such insensitivity is\nlearnt, thereby explaining the strong correlation between the latter and\nperformance. Moreover, we quantify how the sample complexity of CNNs learning\nthe SRHM depends on both the sparsity and hierarchical structure of the task.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.10727v2.pdf",
        "similarity": 0.47160051577532286,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-16"
    },
    {
        "new_title": "Limits of Deep Learning: Sequence Modeling through the Lens of\n  Complexity Theory",
        "new_link": "http://arxiv.org/abs/2405.16674v1",
        "new_summary": "  Deep learning models have achieved significant success across various\napplications but continue to struggle with tasks requiring complex reasoning\nover sequences, such as function composition and compositional tasks. Despite\nadvancements, models like Structured State Space Models (SSMs) and Transformers\nunderperform in deep compositionality tasks due to inherent architectural and\ntraining limitations. Maintaining accuracy over multiple reasoning steps\nremains a primary challenge, as current models often rely on shortcuts rather\nthan genuine multi-step reasoning, leading to performance degradation as task\ncomplexity increases. Existing research highlights these shortcomings but lacks\ncomprehensive theoretical and empirical analysis for SSMs. Our contributions\naddress this gap by providing a theoretical framework based on complexity\ntheory to explain SSMs' limitations. Moreover, we present extensive empirical\nevidence demonstrating how these limitations impair function composition and\nalgorithmic task performance. Our experiments reveal significant performance\ndrops as task complexity increases, even with Chain-of-Thought (CoT) prompting.\nModels frequently resort to shortcuts, leading to errors in multi-step\nreasoning. This underscores the need for innovative solutions beyond current\ndeep learning paradigms to achieve reliable multi-step reasoning and\ncompositional task-solving in practical applications.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.16674v1.pdf",
        "similarity": 0.47066900807714823,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-26"
    },
    {
        "new_title": "A Survey on Vision-Language-Action Models for Embodied AI",
        "new_link": "http://arxiv.org/abs/2405.14093v1",
        "new_summary": "  Deep learning has demonstrated remarkable success across many domains,\nincluding computer vision, natural language processing, and reinforcement\nlearning. Representative artificial neural networks in these fields span\nconvolutional neural networks, Transformers, and deep Q-networks. Built upon\nunimodal neural networks, numerous multi-modal models have been introduced to\naddress a range of tasks such as visual question answering, image captioning,\nand speech recognition. The rise of instruction-following robotic policies in\nembodied AI has spurred the development of a novel category of multi-modal\nmodels known as vision-language-action models (VLAs). Their multi-modality\ncapability has become a foundational element in robot learning. Various methods\nhave been proposed to enhance traits such as versatility, dexterity, and\ngeneralizability. Some models focus on refining specific components through\npretraining. Others aim to develop control policies adept at predicting\nlow-level actions. Certain VLAs serve as high-level task planners capable of\ndecomposing long-horizon tasks into executable subtasks. Over the past few\nyears, a myriad of VLAs have emerged, reflecting the rapid advancement of\nembodied AI. Therefore, it is imperative to capture the evolving landscape\nthrough a comprehensive survey.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.14093v1.pdf",
        "similarity": 0.46910332243445596,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-23"
    },
    {
        "new_title": "Faces of Experimental Pain: Transferability of Deep Learned Heat Pain\n  Features to Electrical Pain",
        "new_link": "http://arxiv.org/abs/2406.11808v1",
        "new_summary": "  The limited size of pain datasets are a challenge in developing robust deep\nlearning models for pain recognition. Transfer learning approaches are often\nemployed in these scenarios. In this study, we investigate whether deep learned\nfeature representation for one type of experimentally induced pain can be\ntransferred to another. Participating in the AI4Pain challenge, our goal is to\nclassify three levels of pain (No-Pain, Low-Pain, High-Pain). The challenge\ndataset contains data collected from 65 participants undergoing varying\nintensities of electrical pain. We utilize the video recording from the dataset\nto investigate the transferability of deep learned heat pain model to\nelectrical pain. In our proposed approach, we leverage an existing heat pain\nconvolutional neural network (CNN) - trained on BioVid dataset - as a feature\nextractor. The images from the challenge dataset are inputted to the\npre-trained heat pain CNN to obtain feature vectors. These feature vectors are\nused to train two machine learning models: a simple feed-forward neural network\nand a long short-term memory (LSTM) network. Our approach was tested using the\ndataset's predefined training, validation, and testing splits. Our models\noutperformed the baseline of the challenge on both the validation and tests\nsets, highlighting the potential of models trained on other pain datasets for\nreliable feature extraction.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.11808v1.pdf",
        "similarity": 0.46907975865477775,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-17"
    },
    {
        "new_title": "Bone Fracture Classification using Transfer Learning",
        "new_link": "http://arxiv.org/abs/2406.15958v1",
        "new_summary": "  The manual examination of X-ray images for fractures is a time-consuming\nprocess that is prone to human error. In this work, we introduce a robust yet\nsimple training loop for the classification of fractures, which significantly\noutperforms existing methods. Our method achieves superior performance in less\nthan ten epochs and utilizes the latest dataset to deliver the best-performing\nmodel for this task. We emphasize the importance of training deep learning\nmodels responsibly and efficiently, as well as the critical role of selecting\nhigh-quality datasets.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.15958v1.pdf",
        "similarity": 0.46894147762383065,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-22"
    },
    {
        "new_title": "A Survey of Deep Long-Tail Classification Advancements",
        "new_link": "http://arxiv.org/abs/2404.15593v1",
        "new_summary": "  Many data distributions in the real world are hardly uniform. Instead, skewed\nand long-tailed distributions of various kinds are commonly observed. This\nposes an interesting problem for machine learning, where most algorithms assume\nor work well with uniformly distributed data. The problem is further\nexacerbated by current state-of-the-art deep learning models requiring large\nvolumes of training data. As such, learning from imbalanced data remains a\nchallenging research problem and a problem that must be solved as we move\ntowards more real-world applications of deep learning. In the context of class\nimbalance, state-of-the-art (SOTA) accuracies on standard benchmark datasets\nfor classification typically fall less than 75%, even for less challenging\ndatasets such as CIFAR100. Nonetheless, there has been progress in this niche\narea of deep learning. To this end, in this survey, we provide a taxonomy of\nvarious methods proposed for addressing the problem of long-tail\nclassification, focusing on works that happened in the last few years under a\nsingle mathematical framework. We also discuss standard performance metrics,\nconvergence studies, feature distribution and classifier analysis. We also\nprovide a quantitative comparison of the performance of different SOTA methods\nand conclude the survey by discussing the remaining challenges and future\nresearch direction.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.15593v1.pdf",
        "similarity": 0.4682774470158297,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-24"
    },
    {
        "new_title": "Towards evolution of Deep Neural Networks through contrastive\n  Self-Supervised learning",
        "new_link": "http://arxiv.org/abs/2406.14525v1",
        "new_summary": "  Deep Neural Networks (DNNs) have been successfully applied to a wide range of\nproblems. However, two main limitations are commonly pointed out. The first one\nis that they require long time to design. The other is that they heavily rely\non labelled data, which can sometimes be costly and hard to obtain. In order to\naddress the first problem, neuroevolution has been proved to be a plausible\noption to automate the design of DNNs. As for the second problem,\nself-supervised learning has been used to leverage unlabelled data to learn\nrepresentations. Our goal is to study how neuroevolution can help\nself-supervised learning to bridge the gap to supervised learning in terms of\nperformance. In this work, we propose a framework that is able to evolve deep\nneural networks using self-supervised learning. Our results on the CIFAR-10\ndataset show that it is possible to evolve adequate neural networks while\nreducing the reliance on labelled data. Moreover, an analysis to the structure\nof the evolved networks suggests that the amount of labelled data fed to them\nhas less effect on the structure of networks that learned via self-supervised\nlearning, when compared to individuals that relied on supervised learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.14525v1.pdf",
        "similarity": 0.4675982486766901,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-20"
    },
    {
        "new_title": "Generic Knowledge Boosted Pre-training For Remote Sensing Images",
        "new_link": "http://arxiv.org/abs/2401.04614v2",
        "new_summary": "  Deep learning models are essential for scene classification, change\ndetection, land cover segmentation, and other remote sensing image\nunderstanding tasks. Most backbones of existing remote sensing deep learning\nmodels are typically initialized by pre-trained weights obtained from ImageNet\npre-training (IMP). However, domain gaps exist between remote sensing images\nand natural images (e.g., ImageNet), making deep learning models initialized by\npre-trained weights of IMP perform poorly for remote sensing image\nunderstanding. Although some pre-training methods are studied in the remote\nsensing community, current remote sensing pre-training methods face the problem\nof vague generalization by only using remote sensing images. In this paper, we\npropose a novel remote sensing pre-training framework, Generic Knowledge\nBoosted Remote Sensing Pre-training (GeRSP), to learn robust representations\nfrom remote sensing and natural images for remote sensing understanding tasks.\nGeRSP contains two pre-training branches: (1) A self-supervised pre-training\nbranch is adopted to learn domain-related representations from unlabeled remote\nsensing images. (2) A supervised pre-training branch is integrated into GeRSP\nfor general knowledge learning from labeled natural images. Moreover, GeRSP\ncombines two pre-training branches using a teacher-student architecture to\nsimultaneously learn representations with general and special knowledge, which\ngenerates a powerful pre-trained model for deep learning model initialization.\nFinally, we evaluate GeRSP and other remote sensing pre-training methods on\nthree downstream tasks, i.e., object detection, semantic segmentation, and\nscene classification. The extensive experimental results consistently\ndemonstrate that GeRSP can effectively learn robust representations in a\nunified manner, improving the performance of remote sensing downstream tasks.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.04614v2.pdf",
        "similarity": 0.46716656554918096,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-09"
    },
    {
        "new_title": "Verbalized Machine Learning: Revisiting Machine Learning with Language\n  Models",
        "new_link": "http://arxiv.org/abs/2406.04344v1",
        "new_summary": "  Motivated by the large progress made by large language models (LLMs), we\nintroduce the framework of verbalized machine learning (VML). In contrast to\nconventional machine learning models that are typically optimized over a\ncontinuous parameter space, VML constrains the parameter space to be\nhuman-interpretable natural language. Such a constraint leads to a new\nperspective of function approximation, where an LLM with a text prompt can be\nviewed as a function parameterized by the text prompt. Guided by this\nperspective, we revisit classical machine learning problems, such as regression\nand classification, and find that these problems can be solved by an\nLLM-parameterized learner and optimizer. The major advantages of VML include\n(1) easy encoding of inductive bias: prior knowledge about the problem and\nhypothesis class can be encoded in natural language and fed into the\nLLM-parameterized learner; (2) automatic model class selection: the optimizer\ncan automatically select a concrete model class based on data and verbalized\nprior knowledge, and it can update the model class during training; and (3)\ninterpretable learner updates: the LLM-parameterized optimizer can provide\nexplanations for why each learner update is performed. We conduct several\nstudies to empirically evaluate the effectiveness of VML, and hope that VML can\nserve as a stepping stone to stronger interpretability and trustworthiness in\nML.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.04344v1.pdf",
        "similarity": 0.4662185329103836,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-06"
    },
    {
        "new_title": "Image classification network enhancement methods based on knowledge\n  injection",
        "new_link": "http://arxiv.org/abs/2401.04441v1",
        "new_summary": "  The current deep neural network algorithm still stays in the end-to-end\ntraining supervision method like Image-Label pairs, which makes traditional\nalgorithm is difficult to explain the reason for the results, and the\nprediction logic is difficult to understand and analyze. The current algorithm\ndoes not use the existing human knowledge information, which makes the model\nnot in line with the human cognition model and makes the model not suitable for\nhuman use. In order to solve the above problems, the present invention provides\na deep neural network training method based on the human knowledge, which uses\nthe human cognition model to construct the deep neural network training model,\nand uses the existing human knowledge information to construct the deep neural\nnetwork training model. This paper proposes a multi-level hierarchical deep\nlearning algorithm, which is composed of multi-level hierarchical deep neural\nnetwork architecture and multi-level hierarchical deep learning framework. The\nexperimental results show that the proposed algorithm can effectively explain\nthe hidden information of the neural network. The goal of our study is to\nimprove the interpretability of deep neural networks (DNNs) by providing an\nanalysis of the impact of knowledge injection on the classification task. We\nconstructed a knowledge injection dataset with matching knowledge data and\nimage classification data. The knowledge injection dataset is the benchmark\ndataset for the experiments in the paper. Our model expresses the improvement\nin interpretability and classification task performance of hidden layers at\ndifferent scales.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.04441v1.pdf",
        "similarity": 0.4653008782754877,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-09"
    },
    {
        "new_title": "Synthetic Counterfactual Faces",
        "new_link": "http://arxiv.org/abs/2407.13922v1",
        "new_summary": "  Computer vision systems have been deployed in various applications involving\nbiometrics like human faces. These systems can identify social media users,\nsearch for missing persons, and verify identity of individuals. While computer\nvision models are often evaluated for accuracy on available benchmarks, more\nannotated data is necessary to learn about their robustness and fairness\nagainst semantic distributional shifts in input data, especially in face data.\nAmong annotated data, counterfactual examples grant strong explainability\ncharacteristics. Because collecting natural face data is prohibitively\nexpensive, we put forth a generative AI-based framework to construct targeted,\ncounterfactual, high-quality synthetic face data. Our synthetic data pipeline\nhas many use cases, including face recognition systems sensitivity evaluations\nand image understanding system probes. The pipeline is validated with multiple\nuser studies. We showcase the efficacy of our face generation pipeline on a\nleading commercial vision model. We identify facial attributes that cause\nvision systems to fail.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.13922v1.pdf",
        "similarity": 0.46495964735130935,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-18"
    },
    {
        "new_title": "A review on discriminative self-supervised learning methods",
        "new_link": "http://arxiv.org/abs/2405.04969v1",
        "new_summary": "  In the field of computer vision, self-supervised learning has emerged as a\nmethod to extract robust features from unlabeled data, where models derive\nlabels autonomously from the data itself, without the need for manual\nannotation. This paper provides a comprehensive review of discriminative\napproaches of self-supervised learning within the domain of computer vision,\nexamining their evolution and current status. Through an exploration of various\nmethods including contrastive, self-distillation, knowledge distillation,\nfeature decorrelation, and clustering techniques, we investigate how these\napproaches leverage the abundance of unlabeled data. Finally, we have\ncomparison of self-supervised learning methods on the standard ImageNet\nclassification benchmark.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.04969v1.pdf",
        "similarity": 0.4649501022173521,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-08"
    },
    {
        "new_title": "A Survey on Deep Learning and State-of-the-art Applications",
        "new_link": "http://arxiv.org/abs/2403.17561v3",
        "new_summary": "  Deep learning, a branch of artificial intelligence, is a computational model\nthat uses multiple layers of interconnected units (neurons) to learn intricate\npatterns and representations directly from raw input data. Empowered by this\nlearning capability, it has become a powerful tool for solving complex problems\nand is the core driver of many groundbreaking technologies and innovations.\nBuilding a deep learning model is a challenging task due to the algorithm`s\ncomplexity and the dynamic nature of real-world problems. Several studies have\nreviewed deep learning concepts and applications. However, the studies mostly\nfocused on the types of deep learning models and convolutional neural network\narchitectures, offering limited coverage of the state-of-the-art of deep\nlearning models and their applications in solving complex problems across\ndifferent domains. Therefore, motivated by the limitations, this study aims to\ncomprehensively review the state-of-the-art deep learning models in computer\nvision, natural language processing, time series analysis and pervasive\ncomputing. We highlight the key features of the models and their effectiveness\nin solving the problems within each domain. Furthermore, this study presents\nthe fundamentals of deep learning, various deep learning model types and\nprominent convolutional neural network architectures. Finally, challenges and\nfuture directions in deep learning research are discussed to offer a broader\nperspective for future researchers.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.17561v3.pdf",
        "similarity": 0.46445307178045875,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-26"
    },
    {
        "new_title": "SoK: Behind the Accuracy of Complex Human Activity Recognition Using\n  Deep Learning",
        "new_link": "http://arxiv.org/abs/2405.00712v2",
        "new_summary": "  Human Activity Recognition (HAR) is a well-studied field with research dating\nback to the 1980s. Over time, HAR technologies have evolved significantly from\nmanual feature extraction, rule-based algorithms, and simple machine learning\nmodels to powerful deep learning models, from one sensor type to a diverse\narray of sensing modalities. The scope has also expanded from recognising a\nlimited set of activities to encompassing a larger variety of both simple and\ncomplex activities. However, there still exist many challenges that hinder\nadvancement in complex activity recognition using modern deep learning methods.\nIn this paper, we comprehensively systematise factors leading to inaccuracy in\ncomplex HAR, such as data variety and model capacity. Among many sensor types,\nwe give more attention to wearable and camera due to their prevalence. Through\nthis Systematisation of Knowledge (SoK) paper, readers can gain a solid\nunderstanding of the development history and existing challenges of HAR,\ndifferent categorisations of activities, obstacles in deep learning-based\ncomplex HAR that impact accuracy, and potential research directions.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.00712v2.pdf",
        "similarity": 0.4633801448677422,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-25"
    },
    {
        "new_title": "Concrete Surface Crack Detection with Convolutional-based Deep Learning\n  Models",
        "new_link": "http://arxiv.org/abs/2401.07124v1",
        "new_summary": "  Effective crack detection is pivotal for the structural health monitoring and\ninspection of buildings. This task presents a formidable challenge to computer\nvision techniques due to the inherently subtle nature of cracks, which often\nexhibit low-level features that can be easily confounded with background\ntextures, foreign objects, or irregularities in construction. Furthermore, the\npresence of issues like non-uniform lighting and construction irregularities\nposes significant hurdles for autonomous crack detection during building\ninspection and monitoring. Convolutional neural networks (CNNs) have emerged as\na promising framework for crack detection, offering high levels of accuracy and\nprecision. Additionally, the ability to adapt pre-trained networks through\ntransfer learning provides a valuable tool for users, eliminating the need for\nan in-depth understanding of algorithm intricacies. Nevertheless, it is\nimperative to acknowledge the limitations and considerations when deploying\nCNNs, particularly in contexts where the outcomes carry immense significance,\nsuch as crack detection in buildings. In this paper, our approach to surface\ncrack detection involves the utilization of various deep-learning models.\nSpecifically, we employ fine-tuning techniques on pre-trained deep learning\narchitectures: VGG19, ResNet50, Inception V3, and EfficientNetV2. These models\nare chosen for their established performance and versatility in image analysis\ntasks. We compare deep learning models using precision, recall, and F1 scores.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.07124v1.pdf",
        "similarity": 0.4629621221645679,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-13"
    },
    {
        "new_title": "Graph Neural Machine: A New Model for Learning with Tabular Data",
        "new_link": "http://arxiv.org/abs/2402.02862v1",
        "new_summary": "  In recent years, there has been a growing interest in mapping data from\ndifferent domains to graph structures. Among others, neural network models such\nas the multi-layer perceptron (MLP) can be modeled as graphs. In fact, MLPs can\nbe represented as directed acyclic graphs. Graph neural networks (GNNs) have\nrecently become the standard tool for performing machine learning tasks on\ngraphs. In this work, we show that an MLP is equivalent to an asynchronous\nmessage passing GNN model which operates on the MLP's graph representation. We\nthen propose a new machine learning model for tabular data, the so-called Graph\nNeural Machine (GNM), which replaces the MLP's directed acyclic graph with a\nnearly complete graph and which employs a synchronous message passing scheme.\nWe show that a single GNM model can simulate multiple MLP models. We evaluate\nthe proposed model in several classification and regression datasets. In most\ncases, the GNM model outperforms the MLP architecture.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.02862v1.pdf",
        "similarity": 0.46285005127147527,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-05"
    },
    {
        "new_title": "Low-Resource Crop Classification from Multi-Spectral Time Series Using\n  Lossless Compressors",
        "new_link": "http://arxiv.org/abs/2405.18119v2",
        "new_summary": "  Deep learning has significantly improved the accuracy of crop classification\nusing multispectral temporal data. However, these models have complex\nstructures with numerous parameters, requiring large amounts of data and costly\ntraining. In low-resource situations with fewer labeled samples, deep learning\nmodels perform poorly due to insufficient data. Conversely, compressors are\ndata-type agnostic, and non-parametric methods do not bring underlying\nassumptions. Inspired by this insight, we propose a non-training alternative to\ndeep learning models, aiming to address these situations. Specifically, the\nSymbolic Representation Module is proposed to convert the reflectivity into\nsymbolic representations. The symbolic representations are then\ncross-transformed in both the channel and time dimensions to generate symbolic\nembeddings. Next, the Multi-scale Normalised Compression Distance (MNCD) is\ndesigned to measure the correlation between any two symbolic embeddings.\nFinally, based on the MNCDs, high quality crop classification can be achieved\nusing only a k-nearest-neighbor classifier kNN. The entire framework is\nready-to-use and lightweight. Without any training, it outperformed, on\naverage, 7 advanced deep learning models trained at scale on three benchmark\ndatasets. It also outperforms more than half of these models in the few-shot\nsetting with sparse crop labels. Therefore, the high performance and robustness\nof our non-training framework makes it truly applicable to real-world crop\nmapping. Codes are available at:\nhttps://github.com/qinfengsama/Compressor-Based-Crop-Mapping.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.18119v2.pdf",
        "similarity": 0.46170859867350966,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-28"
    },
    {
        "new_title": "Efficient Multi-domain Text Recognition Deep Neural Network\n  Parameterization with Residual Adapters",
        "new_link": "http://arxiv.org/abs/2401.00971v1",
        "new_summary": "  Recent advancements in deep neural networks have markedly enhanced the\nperformance of computer vision tasks, yet the specialized nature of these\nnetworks often necessitates extensive data and high computational power.\nAddressing these requirements, this study presents a novel neural network model\nadept at optical character recognition (OCR) across diverse domains, leveraging\nthe strengths of multi-task learning to improve efficiency and generalization.\nThe model is designed to achieve rapid adaptation to new domains, maintain a\ncompact size conducive to reduced computational resource demand, ensure high\naccuracy, retain knowledge from previous learning experiences, and allow for\ndomain-specific performance improvements without the need to retrain entirely.\nRigorous evaluation on open datasets has validated the model's ability to\nsignificantly lower the number of trainable parameters without sacrificing\nperformance, indicating its potential as a scalable and adaptable solution in\nthe field of computer vision, particularly for applications in optical text\nrecognition.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.00971v1.pdf",
        "similarity": 0.46122022411975905,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-01"
    },
    {
        "new_title": "Enhancing Multimodal Understanding with CLIP-Based Image-to-Text\n  Transformation",
        "new_link": "http://arxiv.org/abs/2401.06167v1",
        "new_summary": "  The process of transforming input images into corresponding textual\nexplanations stands as a crucial and complex endeavor within the domains of\ncomputer vision and natural language processing. In this paper, we propose an\ninnovative ensemble approach that harnesses the capabilities of Contrastive\nLanguage-Image Pretraining models.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.06167v1.pdf",
        "similarity": 0.459350153575787,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-02"
    },
    {
        "new_title": "Meta-Learning Neural Procedural Biases",
        "new_link": "http://arxiv.org/abs/2406.07983v1",
        "new_summary": "  The goal of few-shot learning is to generalize and achieve high performance\non new unseen learning tasks, where each task has only a limited number of\nexamples available. Gradient-based meta-learning attempts to address this\nchallenging task by learning how to learn new tasks by embedding inductive\nbiases informed by prior learning experiences into the components of the\nlearning algorithm. In this work, we build upon prior research and propose\nNeural Procedural Bias Meta-Learning (NPBML), a novel framework designed to\nmeta-learn task-adaptive procedural biases. Our approach aims to consolidate\nrecent advancements in meta-learned initializations, optimizers, and loss\nfunctions by learning them simultaneously and making them adapt to each\nindividual task to maximize the strength of the learned inductive biases. This\nimbues each learning task with a unique set of procedural biases which is\nspecifically designed and selected to attain strong learning performance in\nonly a few gradient steps. The experimental results show that by meta-learning\nthe procedural biases of a neural network, we can induce strong inductive\nbiases towards a distribution of learning tasks, enabling robust learning\nperformance across many well-established few-shot learning benchmarks.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.07983v1.pdf",
        "similarity": 0.4579709145903113,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-12"
    },
    {
        "new_title": "Biathlon: Harnessing Model Resilience for Accelerating ML Inference\n  Pipelines",
        "new_link": "http://arxiv.org/abs/2405.11191v1",
        "new_summary": "  Machine learning inference pipelines commonly encountered in data science and\nindustries often require real-time responsiveness due to their user-facing\nnature. However, meeting this requirement becomes particularly challenging when\ncertain input features require aggregating a large volume of data online.\nRecent literature on interpretable machine learning reveals that most machine\nlearning models exhibit a notable degree of resilience to variations in input.\nThis suggests that machine learning models can effectively accommodate\napproximate input features with minimal discernible impact on accuracy. In this\npaper, we introduce Biathlon, a novel ML serving system that leverages the\ninherent resilience of models and determines the optimal degree of\napproximation for each aggregation feature. This approach enables maximum\nspeedup while ensuring a guaranteed bound on accuracy loss. We evaluate\nBiathlon on real pipelines from both industry applications and data science\ncompetitions, demonstrating its ability to meet real-time latency requirements\nby achieving 5.3x to 16.6x speedup with almost no accuracy loss.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.11191v1.pdf",
        "similarity": 0.457471932972466,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-18"
    },
    {
        "new_title": "A Survey on Deep Active Learning: Recent Advances and New Frontiers",
        "new_link": "http://arxiv.org/abs/2405.00334v2",
        "new_summary": "  Active learning seeks to achieve strong performance with fewer training\nsamples. It does this by iteratively asking an oracle to label new selected\nsamples in a human-in-the-loop manner. This technique has gained increasing\npopularity due to its broad applicability, yet its survey papers, especially\nfor deep learning-based active learning (DAL), remain scarce. Therefore, we\nconduct an advanced and comprehensive survey on DAL. We first introduce\nreviewed paper collection and filtering. Second, we formally define the DAL\ntask and summarize the most influential baselines and widely used datasets.\nThird, we systematically provide a taxonomy of DAL methods from five\nperspectives, including annotation types, query strategies, deep model\narchitectures, learning paradigms, and training processes, and objectively\nanalyze their strengths and weaknesses. Then, we comprehensively summarize main\napplications of DAL in Natural Language Processing (NLP), Computer Vision (CV),\nand Data Mining (DM), etc. Finally, we discuss challenges and perspectives\nafter a detailed analysis of current studies. This work aims to serve as a\nuseful and quick guide for researchers in overcoming difficulties in DAL. We\nhope that this survey will spur further progress in this burgeoning field.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.00334v2.pdf",
        "similarity": 0.4572144087301462,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-01"
    },
    {
        "new_title": "Bridging Operator Learning and Conditioned Neural Fields: A Unifying\n  Perspective",
        "new_link": "http://arxiv.org/abs/2405.13998v1",
        "new_summary": "  Operator learning is an emerging area of machine learning which aims to learn\nmappings between infinite dimensional function spaces. Here we uncover a\nconnection between operator learning architectures and conditioned neural\nfields from computer vision, providing a unified perspective for examining\ndifferences between popular operator learning models. We find that many\ncommonly used operator learning models can be viewed as neural fields with\nconditioning mechanisms restricted to point-wise and/or global information.\nMotivated by this, we propose the Continuous Vision Transformer (CViT), a novel\nneural operator architecture that employs a vision transformer encoder and uses\ncross-attention to modulate a base field constructed with a trainable\ngrid-based positional encoding of query coordinates. Despite its simplicity,\nCViT achieves state-of-the-art results across challenging benchmarks in climate\nmodeling and fluid dynamics. Our contributions can be viewed as a first step\ntowards adapting advanced computer vision architectures for building more\nflexible and accurate machine learning models in physical sciences.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.13998v1.pdf",
        "similarity": 0.4569935619803659,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-22"
    },
    {
        "new_title": "GlycanML: A Multi-Task and Multi-Structure Benchmark for Glycan Machine\n  Learning",
        "new_link": "http://arxiv.org/abs/2405.16206v1",
        "new_summary": "  Glycans are basic biomolecules and perform essential functions within living\norganisms. The rapid increase of functional glycan data provides a good\nopportunity for machine learning solutions to glycan understanding. However,\nthere still lacks a standard machine learning benchmark for glycan function\nprediction. In this work, we fill this blank by building a comprehensive\nbenchmark for Glycan Machine Learning (GlycanML). The GlycanML benchmark\nconsists of diverse types of tasks including glycan taxonomy prediction, glycan\nimmunogenicity prediction, glycosylation type prediction, and protein-glycan\ninteraction prediction. Glycans can be represented by both sequences and graphs\nin GlycanML, which enables us to extensively evaluate sequence-based models and\ngraph neural networks (GNNs) on benchmark tasks. Furthermore, by concurrently\nperforming eight glycan taxonomy prediction tasks, we introduce the\nGlycanML-MTL testbed for multi-task learning (MTL) algorithms. Experimental\nresults show the superiority of modeling glycans with multi-relational GNNs,\nand suitable MTL methods can further boost model performance. We provide all\ndatasets and source codes at https://github.com/GlycanML/GlycanML and maintain\na leaderboard at https://GlycanML.github.io/project\n",
        "pdf_link": "https://arxiv.org/pdf/2405.16206v1.pdf",
        "similarity": 0.456823103013312,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-25"
    },
    {
        "new_title": "CMFN: Cross-Modal Fusion Network for Irregular Scene Text Recognition",
        "new_link": "http://arxiv.org/abs/2401.10041v1",
        "new_summary": "  Scene text recognition, as a cross-modal task involving vision and text, is\nan important research topic in computer vision. Most existing methods use\nlanguage models to extract semantic information for optimizing visual\nrecognition. However, the guidance of visual cues is ignored in the process of\nsemantic mining, which limits the performance of the algorithm in recognizing\nirregular scene text. To tackle this issue, we propose a novel cross-modal\nfusion network (CMFN) for irregular scene text recognition, which incorporates\nvisual cues into the semantic mining process. Specifically, CMFN consists of a\nposition self-enhanced encoder, a visual recognition branch and an iterative\nsemantic recognition branch. The position self-enhanced encoder provides\ncharacter sequence position encoding for both the visual recognition branch and\nthe iterative semantic recognition branch. The visual recognition branch\ncarries out visual recognition based on the visual features extracted by CNN\nand the position encoding information provided by the position self-enhanced\nencoder. The iterative semantic recognition branch, which consists of a\nlanguage recognition module and a cross-modal fusion gate, simulates the way\nthat human recognizes scene text and integrates cross-modal visual cues for\ntext recognition. The experiments demonstrate that the proposed CMFN algorithm\nachieves comparable performance to state-of-the-art algorithms, indicating its\neffectiveness.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.10041v1.pdf",
        "similarity": 0.45634466562154063,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-18"
    },
    {
        "new_title": "Generalizable Semantic Vision Query Generation for Zero-shot Panoptic\n  and Semantic Segmentation",
        "new_link": "http://arxiv.org/abs/2402.13697v1",
        "new_summary": "  Zero-shot Panoptic Segmentation (ZPS) aims to recognize foreground instances\nand background stuff without images containing unseen categories in training.\nDue to the visual data sparsity and the difficulty of generalizing from seen to\nunseen categories, this task remains challenging. To better generalize to\nunseen classes, we propose Conditional tOken aligNment and Cycle trAnsiTion\n(CONCAT), to produce generalizable semantic vision queries. First, a feature\nextractor is trained by CON to link the vision and semantics for providing\ntarget queries. Formally, CON is proposed to align the semantic queries with\nthe CLIP visual CLS token extracted from complete and masked images. To address\nthe lack of unseen categories, a generator is required. However, one of the\ngaps in synthesizing pseudo vision queries, ie, vision queries for unseen\ncategories, is describing fine-grained visual details through semantic\nembeddings. Therefore, we approach CAT to train the generator in\nsemantic-vision and vision-semantic manners. In semantic-vision, visual query\ncontrast is proposed to model the high granularity of vision by pulling the\npseudo vision queries with the corresponding targets containing segments while\npushing those without segments away. To ensure the generated queries retain\nsemantic information, in vision-semantic, the pseudo vision queries are mapped\nback to semantic and supervised by real semantic embeddings. Experiments on ZPS\nachieve a 5.2% hPQ increase surpassing SOTA. We also examine inductive ZPS and\nopen-vocabulary semantic segmentation and obtain comparative results while\nbeing 2 times faster in testing.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.13697v1.pdf",
        "similarity": 0.4562936235203555,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-21"
    },
    {
        "new_title": "Explainable Light-Weight Deep Learning Pipeline for Improved Drought\n  Stress Identification",
        "new_link": "http://arxiv.org/abs/2404.10073v2",
        "new_summary": "  Early identification of drought stress in crops is vital for implementing\neffective mitigation measures and reducing yield loss. Non-invasive imaging\ntechniques hold immense potential by capturing subtle physiological changes in\nplants under water deficit. Sensor based imaging data serves as a rich source\nof information for machine learning and deep learning algorithms, facilitating\nfurther analysis aimed at identifying drought stress. While these approaches\nyield favorable results, real-time field applications requires algorithms\nspecifically designed for the complexities of natural agricultural conditions.\nOur work proposes a novel deep learning framework for classifying drought\nstress in potato crops captured by UAVs in natural settings. The novelty lies\nin the synergistic combination of a pre-trained network with carefully designed\ncustom layers. This architecture leverages feature extraction capabilities of\nthe pre-trained network while the custom layers enable targeted dimensionality\nreduction and enhanced regularization, ultimately leading to improved\nperformance. A key innovation of our work involves the integration of\nGradient-Class Activation Mapping (Grad-CAM), an explainability technique.\nGrad-CAM sheds light on the internal workings of the deep learning model,\ntypically referred to as a black box. By visualizing the focus areas of the\nmodel within the images, Grad-CAM fosters interpretability and builds trust in\nthe decision-making process of the model. Our proposed framework achieves\nsuperior performance, particularly with the DenseNet121 pre-trained network,\nreaching a precision of 97% to identify the stressed class with an overall\naccuracy of 91%. Comparative analysis of existing state-of-the-art object\ndetection algorithms reveals the superiority of our approach in significantly\nhigher precision and accuracy.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.10073v2.pdf",
        "similarity": 0.4557640511907495,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-15"
    },
    {
        "new_title": "Deep Learning without Weight Symmetry",
        "new_link": "http://arxiv.org/abs/2405.20594v1",
        "new_summary": "  Backpropagation (BP), a foundational algorithm for training artificial neural\nnetworks, predominates in contemporary deep learning. Although highly\nsuccessful, it is often considered biologically implausible. A significant\nlimitation arises from the need for precise symmetry between connections in the\nbackward and forward pathways to backpropagate gradient signals accurately,\nwhich is not observed in biological brains. Researchers have proposed several\nalgorithms to alleviate this symmetry constraint, such as feedback alignment\nand direct feedback alignment. However, their divergence from backpropagation\ndynamics presents challenges, particularly in deeper networks and convolutional\nlayers. Here we introduce the Product Feedback Alignment (PFA) algorithm. Our\nfindings demonstrate that PFA closely approximates BP and achieves comparable\nperformance in deep convolutional networks while avoiding explicit weight\nsymmetry. Our results offer a novel solution to the longstanding weight\nsymmetry problem, leading to more biologically plausible learning in deep\nconvolutional networks compared to earlier methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.20594v1.pdf",
        "similarity": 0.45555895304656774,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-31"
    },
    {
        "new_title": "ILPO-NET: Network for the invariant recognition of arbitrary volumetric\n  patterns in 3D",
        "new_link": "http://arxiv.org/abs/2403.19612v3",
        "new_summary": "  Effective recognition of spatial patterns and learning their hierarchy is\ncrucial in modern spatial data analysis. Volumetric data applications seek\ntechniques ensuring invariance not only to shifts but also to pattern\nrotations. While traditional methods can readily achieve translational\ninvariance, rotational invariance possesses multiple challenges and remains an\nactive area of research. Here, we present ILPO-Net (Invariant to Local Patterns\nOrientation Network), a novel approach that handles arbitrarily shaped patterns\nwith the convolutional operation inherently invariant to local spatial pattern\norientations using the Wigner matrix expansions. Our architecture seamlessly\nintegrates the new convolution operator and, when benchmarked on diverse\nvolumetric datasets such as MedMNIST and CATH, demonstrates superior\nperformance over the baselines with significantly reduced parameter counts - up\nto 1000 times fewer in the case of MedMNIST. Beyond these demonstrations,\nILPO-Net's rotational invariance paves the way for other applications across\nmultiple disciplines. Our code is publicly available at\nhttps://gricad-gitlab.univ-grenoble-alpes.fr/GruLab/ILPO/-/tree/main/ILPONet.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.19612v3.pdf",
        "similarity": 0.454839547749903,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-28"
    },
    {
        "new_title": "Tell Me Where You Are: Multimodal LLMs Meet Place Recognition",
        "new_link": "http://arxiv.org/abs/2406.17520v1",
        "new_summary": "  Large language models (LLMs) exhibit a variety of promising capabilities in\nrobotics, including long-horizon planning and commonsense reasoning. However,\ntheir performance in place recognition is still underexplored. In this work, we\nintroduce multimodal LLMs (MLLMs) to visual place recognition (VPR), where a\nrobot must localize itself using visual observations. Our key design is to use\nvision-based retrieval to propose several candidates and then leverage\nlanguage-based reasoning to carefully inspect each candidate for a final\ndecision. Specifically, we leverage the robust visual features produced by\noff-the-shelf vision foundation models (VFMs) to obtain several candidate\nlocations. We then prompt an MLLM to describe the differences between the\ncurrent observation and each candidate in a pairwise manner, and reason about\nthe best candidate based on these descriptions. Our results on three datasets\ndemonstrate that integrating the general-purpose visual features from VFMs with\nthe reasoning capabilities of MLLMs already provides an effective place\nrecognition solution, without any VPR-specific supervised training. We believe\nour work can inspire new possibilities for applying and designing foundation\nmodels, i.e., VFMs, LLMs, and MLLMs, to enhance the localization and navigation\nof mobile robots.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.17520v1.pdf",
        "similarity": 0.4547314393887023,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-25"
    },
    {
        "new_title": "Visual Representation Learning with Stochastic Frame Prediction",
        "new_link": "http://arxiv.org/abs/2406.07398v1",
        "new_summary": "  Self-supervised learning of image representations by predicting future frames\nis a promising direction but still remains a challenge. This is because of the\nunder-determined nature of frame prediction; multiple potential futures can\narise from a single current frame. To tackle this challenge, in this paper, we\nrevisit the idea of stochastic video generation that learns to capture\nuncertainty in frame prediction and explore its effectiveness for\nrepresentation learning. Specifically, we design a framework that trains a\nstochastic frame prediction model to learn temporal information between frames.\nMoreover, to learn dense information within each frame, we introduce an\nauxiliary masked image modeling objective along with a shared decoder\narchitecture. We find this architecture allows for combining both objectives in\na synergistic and compute-efficient manner. We demonstrate the effectiveness of\nour framework on a variety of tasks from video label propagation and\nvision-based robot learning domains, such as video segmentation, pose tracking,\nvision-based robotic locomotion, and manipulation tasks. Code is available on\nthe project webpage: https://sites.google.com/view/2024rsp.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.07398v1.pdf",
        "similarity": 0.4542424036699852,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-11"
    },
    {
        "new_title": "Achieving More Human Brain-Like Vision via Human EEG Representational\n  Alignment",
        "new_link": "http://arxiv.org/abs/2401.17231v2",
        "new_summary": "  Despite advancements in artificial intelligence, object recognition models\nstill lag behind in emulating visual information processing in human brains.\nRecent studies have highlighted the potential of using neural data to mimic\nbrain processing; however, these often rely on invasive neural recordings from\nnon-human subjects, leaving a critical gap in understanding human visual\nperception. Addressing this gap, we present, for the first time,\n'Re(presentational)Al(ignment)net', a vision model aligned with human brain\nactivity based on non-invasive EEG, demonstrating a significantly higher\nsimilarity to human brain representations. Our innovative image-to-brain\nmulti-layer encoding framework advances human neural alignment by optimizing\nmultiple model layers and enabling the model to efficiently learn and mimic\nhuman brain's visual representational patterns across object categories and\ndifferent modalities. Our findings suggest that ReAlnet represents a\nbreakthrough in bridging the gap between artificial and human vision, and\npaving the way for more brain-like artificial intelligence systems.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.17231v2.pdf",
        "similarity": 0.4537916040786451,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-30"
    },
    {
        "new_title": "Compressible Dynamics in Deep Overparameterized Low-Rank Learning &\n  Adaptation",
        "new_link": "http://arxiv.org/abs/2406.04112v2",
        "new_summary": "  While overparameterization in machine learning models offers great benefits\nin terms of optimization and generalization, it also leads to increased\ncomputational requirements as model sizes grow. In this work, we show that by\nleveraging the inherent low-dimensional structures of data and compressible\ndynamics within the model parameters, we can reap the benefits of\noverparameterization without the computational burdens. In practice, we\ndemonstrate the effectiveness of this approach for deep low-rank matrix\ncompletion as well as fine-tuning language models. Our approach is grounded in\ntheoretical findings for deep overparameterized low-rank matrix recovery, where\nwe show that the learning dynamics of each weight matrix are confined to an\ninvariant low-dimensional subspace. Consequently, we can construct and train\ncompact, highly compressed factorizations possessing the same benefits as their\noverparameterized counterparts. In the context of deep matrix completion, our\ntechnique substantially improves training efficiency while retaining the\nadvantages of overparameterization. For language model fine-tuning, we propose\na method called \"Deep LoRA\", which improves the existing low-rank adaptation\n(LoRA) technique, leading to reduced overfitting and a simplified\nhyperparameter setup, while maintaining comparable efficiency. We validate the\neffectiveness of Deep LoRA on natural language tasks, particularly when\nfine-tuning with limited data. Our code is available at\nhttps://github.com/cjyaras/deep-lora-transformers.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.04112v2.pdf",
        "similarity": 0.4533576613990853,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-06"
    },
    {
        "new_title": "Model-Agnostic Interpretation Framework in Machine Learning: A\n  Comparative Study in NBA Sports",
        "new_link": "http://arxiv.org/abs/2401.02630v1",
        "new_summary": "  The field of machine learning has seen tremendous progress in recent years,\nwith deep learning models delivering exceptional performance across a range of\ntasks. However, these models often come at the cost of interpretability, as\nthey operate as opaque \"black boxes\" that obscure the rationale behind their\ndecisions. This lack of transparency can limit understanding of the models'\nunderlying principles and impede their deployment in sensitive domains, such as\nhealthcare or finance. To address this challenge, our research team has\nproposed an innovative framework designed to reconcile the trade-off between\nmodel performance and interpretability. Our approach is centered around modular\noperations on high-dimensional data, which enable end-to-end processing while\npreserving interpretability. By fusing diverse interpretability techniques and\nmodularized data processing, our framework sheds light on the decision-making\nprocesses of complex models without compromising their performance. We have\nextensively tested our framework and validated its superior efficacy in\nachieving a harmonious balance between computational efficiency and\ninterpretability. Our approach addresses a critical need in contemporary\nmachine learning applications by providing unprecedented insights into the\ninner workings of complex models, fostering trust, transparency, and\naccountability in their deployment across diverse domains.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.02630v1.pdf",
        "similarity": 0.4527239313616776,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-05"
    },
    {
        "new_title": "ED-SAM: An Efficient Diffusion Sampling Approach to Domain\n  Generalization in Vision-Language Foundation Models",
        "new_link": "http://arxiv.org/abs/2406.01432v1",
        "new_summary": "  The Vision-Language Foundation Model has recently shown outstanding\nperformance in various perception learning tasks. The outstanding performance\nof the vision-language model mainly relies on large-scale pre-training datasets\nand different data augmentation techniques. However, the domain generalization\nproblem of the vision-language foundation model needs to be addressed. This\nproblem has limited the generalizability of the vision-language foundation\nmodel to unknown data distributions. In this paper, we introduce a new simple\nbut efficient Diffusion Sampling approach to Domain Generalization (ED-SAM) to\nimprove the generalizability of the vision-language foundation model. Our\ntheoretical analysis in this work reveals the critical role and relation of the\ndiffusion model to domain generalization in the vision-language foundation\nmodel. Then, based on the insightful analysis, we introduce a new simple yet\neffective Transport Transformation to diffusion sampling method. It can\neffectively generate adversarial samples to improve the generalizability of the\nfoundation model against unknown data distributions. The experimental results\non different scales of vision-language pre-training datasets, including CC3M,\nCC12M, and LAION400M, have consistently shown State-of-the-Art performance and\nscalability of the proposed ED-SAM approach compared to the other recent\nmethods.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.01432v1.pdf",
        "similarity": 0.452186421117532,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-03"
    },
    {
        "new_title": "Weakly Supervised Deep Hyperspherical Quantization for Image Retrieval",
        "new_link": "http://arxiv.org/abs/2404.04998v1",
        "new_summary": "  Deep quantization methods have shown high efficiency on large-scale image\nretrieval. However, current models heavily rely on ground-truth information,\nhindering the application of quantization in label-hungry scenarios. A more\nrealistic demand is to learn from inexhaustible uploaded images that are\nassociated with informal tags provided by amateur users. Though such sketchy\ntags do not obviously reveal the labels, they actually contain useful semantic\ninformation for supervising deep quantization. To this end, we propose\nWeakly-Supervised Deep Hyperspherical Quantization (WSDHQ), which is the first\nwork to learn deep quantization from weakly tagged images. Specifically, 1) we\nuse word embeddings to represent the tags and enhance their semantic\ninformation based on a tag correlation graph. 2) To better preserve semantic\ninformation in quantization codes and reduce quantization error, we jointly\nlearn semantics-preserving embeddings and supervised quantizer on hypersphere\nby employing a well-designed fusion layer and tailor-made loss functions.\nExtensive experiments show that WSDHQ can achieve state-of-art performance on\nweakly-supervised compact coding. Code is available at\nhttps://github.com/gimpong/AAAI21-WSDHQ.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.04998v1.pdf",
        "similarity": 0.4515974360882808,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-07"
    },
    {
        "new_title": "Zero-shot Active Learning Using Self Supervised Learning",
        "new_link": "http://arxiv.org/abs/2401.01690v1",
        "new_summary": "  Deep learning algorithms are often said to be data hungry. The performance of\nsuch algorithms generally improve as more and more annotated data is fed into\nthe model. While collecting unlabelled data is easier (as they can be scraped\neasily from the internet), annotating them is a tedious and expensive task.\nGiven a fixed budget available for data annotation, Active Learning helps\nselecting the best subset of data for annotation, such that the deep learning\nmodel when trained over that subset will have maximum generalization\nperformance under this budget. In this work, we aim to propose a new Active\nLearning approach which is model agnostic as well as one doesn't require an\niterative process. We aim to leverage self-supervised learnt features for the\ntask of Active Learning. The benefit of self-supervised learning, is that one\ncan get useful feature representation of the input data, without having any\nannotation.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.01690v1.pdf",
        "similarity": 0.4512661349338395,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-03"
    },
    {
        "new_title": "GatedLexiconNet: A Comprehensive End-to-End Handwritten Paragraph Text\n  Recognition System",
        "new_link": "http://arxiv.org/abs/2404.14062v1",
        "new_summary": "  The Handwritten Text Recognition problem has been a challenge for researchers\nfor the last few decades, especially in the domain of computer vision, a\nsubdomain of pattern recognition. Variability of texts amongst writers,\ncursiveness, and different font styles of handwritten texts with degradation of\nhistorical text images make it a challenging problem. Recognizing scanned\ndocument images in neural network-based systems typically involves a two-step\napproach: segmentation and recognition. However, this method has several\ndrawbacks. These shortcomings encompass challenges in identifying text regions,\nanalyzing layout diversity within pages, and establishing accurate ground truth\nsegmentation. Consequently, these processes are prone to errors, leading to\nbottlenecks in achieving high recognition accuracies. Thus, in this study, we\npresent an end-to-end paragraph recognition system that incorporates internal\nline segmentation and gated convolutional layers based encoder. The gating is a\nmechanism that controls the flow of information and allows to adaptively\nselection of the more relevant features in handwritten text recognition models.\nThe attention module plays an important role in performing internal line\nsegmentation, allowing the page to be processed line-by-line. During the\ndecoding step, we have integrated a connectionist temporal classification-based\nword beam search decoder as a post-processing step. In this work, we have\nextended existing LexiconNet by carefully applying and utilizing gated\nconvolutional layers in the existing deep neural network. Our results at line\nand page levels also favour our new GatedLexiconNet. This study reported\ncharacter error rates of 2.27% on IAM, 0.9% on RIMES, and 2.13% on READ-16, and\nword error rates of 5.73% on IAM, 2.76% on RIMES, and 6.52% on READ-2016\ndatasets.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.14062v1.pdf",
        "similarity": 0.4500596342400452,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-22"
    },
    {
        "new_title": "An Interactive Human-Machine Learning Interface for Collecting and\n  Learning from Complex Annotations",
        "new_link": "http://arxiv.org/abs/2403.19339v1",
        "new_summary": "  Human-Computer Interaction has been shown to lead to improvements in machine\nlearning systems by boosting model performance, accelerating learning and\nbuilding user confidence. In this work, we aim to alleviate the expectation\nthat human annotators adapt to the constraints imposed by traditional labels by\nallowing for extra flexibility in the form that supervision information is\ncollected. For this, we propose a human-machine learning interface for binary\nclassification tasks which enables human annotators to utilise counterfactual\nexamples to complement standard binary labels as annotations for a dataset.\nFinally we discuss the challenges in future extensions of this work.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.19339v1.pdf",
        "similarity": 0.44969761274719944,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-28"
    },
    {
        "new_title": "Deep Learning Method to Predict Wound Healing Progress Based on Collagen\n  Fibers in Wound Tissue",
        "new_link": "http://arxiv.org/abs/2405.05297v1",
        "new_summary": "  Wound healing is a complex process involving changes in collagen fibers.\nAccurate monitoring of these changes is crucial for assessing the progress of\nwound healing and has significant implications for guiding clinical treatment\nstrategies and drug screening. However, traditional quantitative analysis\nmethods focus on spatial characteristics such as collagen fiber alignment and\nvariance, lacking threshold standards to differentiate between different stages\nof wound healing. To address this issue, we propose an innovative approach\nbased on deep learning to predict the progression of wound healing by analyzing\ncollagen fiber features in histological images of wound tissue. Leveraging the\nunique learning capabilities of deep learning models, our approach captures the\nfeature variations of collagen fibers in histological images from different\ncategories and classifies them into various stages of wound healing. To\novercome the limited availability of histological image data, we employ a\ntransfer learning strategy. Specifically, we fine-tune a VGG16 model pretrained\non the ImageNet dataset to adapt it to the classification task of histological\nimages of wounds. Through this process, our model achieves 82% accuracy in\nclassifying six stages of wound healing. Furthermore, to enhance the\ninterpretability of the model, we employ a class activation mapping technique\ncalled LayerCAM. LayerCAM reveals the image regions on which the model relies\nwhen making predictions, providing transparency to the model's decision-making\nprocess. This visualization not only helps us understand how the model\nidentifies and evaluates collagen fiber features but also enhances trust in the\nmodel's prediction results. To the best of our knowledge, our proposed model is\nthe first deep learning-based classification model used for predicting wound\nhealing stages.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.05297v1.pdf",
        "similarity": 0.4495449093879841,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-08"
    },
    {
        "new_title": "Searching for internal symbols underlying deep learning",
        "new_link": "http://arxiv.org/abs/2405.20605v1",
        "new_summary": "  Deep learning (DL) enables deep neural networks (DNNs) to automatically learn\ncomplex tasks or rules from given examples without instructions or guiding\nprinciples. As we do not engineer DNNs' functions, it is extremely difficult to\ndiagnose their decisions, and multiple lines of studies proposed to explain\nprinciples of DNNs/DL operations. Notably, one line of studies suggests that\nDNNs may learn concepts, the high level features recognizable to humans. Thus,\nwe hypothesized that DNNs develop abstract codes, not necessarily recognizable\nto humans, which can be used to augment DNNs' decision-making. To address this\nhypothesis, we combined foundation segmentation models and unsupervised\nlearning to extract internal codes and identify potential use of abstract codes\nto make DL's decision-making more reliable and safer.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.20605v1.pdf",
        "similarity": 0.4472603775711077,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-31"
    },
    {
        "new_title": "Noisy Label Processing for Classification: A Survey",
        "new_link": "http://arxiv.org/abs/2404.04159v1",
        "new_summary": "  In recent years, deep neural networks (DNNs) have gained remarkable\nachievement in computer vision tasks, and the success of DNNs often depends\ngreatly on the richness of data. However, the acquisition process of data and\nhigh-quality ground truth requires a lot of manpower and money. In the long,\ntedious process of data annotation, annotators are prone to make mistakes,\nresulting in incorrect labels of images, i.e., noisy labels. The emergence of\nnoisy labels is inevitable. Moreover, since research shows that DNNs can easily\nfit noisy labels, the existence of noisy labels will cause significant damage\nto the model training process. Therefore, it is crucial to combat noisy labels\nfor computer vision tasks, especially for classification tasks. In this survey,\nwe first comprehensively review the evolution of different deep learning\napproaches for noisy label combating in the image classification task. In\naddition, we also review different noise patterns that have been proposed to\ndesign robust algorithms. Furthermore, we explore the inner pattern of\nreal-world label noise and propose an algorithm to generate a synthetic label\nnoise pattern guided by real-world data. We test the algorithm on the\nwell-known real-world dataset CIFAR-10N to form a new real-world data-guided\nsynthetic benchmark and evaluate some typical noise-robust methods on the\nbenchmark.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.04159v1.pdf",
        "similarity": 0.44720984021639015,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-05"
    },
    {
        "new_title": "Alignment Calibration: Machine Unlearning for Contrastive Learning under\n  Auditing",
        "new_link": "http://arxiv.org/abs/2406.03603v1",
        "new_summary": "  Machine unlearning provides viable solutions to revoke the effect of certain\ntraining data on pre-trained model parameters. Existing approaches provide\nunlearning recipes for classification and generative models. However, a\ncategory of important machine learning models, i.e., contrastive learning (CL)\nmethods, is overlooked. In this paper, we fill this gap by first proposing the\nframework of Machine Unlearning for Contrastive learning (MUC) and adapting\nexisting methods. Furthermore, we observe that several methods are mediocre\nunlearners and existing auditing tools may not be sufficient for data owners to\nvalidate the unlearning effects in contrastive learning. We thus propose a\nnovel method called Alignment Calibration (AC) by explicitly considering the\nproperties of contrastive learning and optimizing towards novel auditing\nmetrics to easily verify unlearning. We empirically compare AC with baseline\nmethods on SimCLR, MoCo and CLIP. We observe that AC addresses drawbacks of\nexisting methods: (1) achieving state-of-the-art performance and approximating\nexact unlearning (retraining); (2) allowing data owners to clearly visualize\nthe effect caused by unlearning through black-box auditing.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.03603v1.pdf",
        "similarity": 0.447169749724118,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-05"
    },
    {
        "new_title": "A New Method for Vehicle Logo Recognition Based on Swin Transformer",
        "new_link": "http://arxiv.org/abs/2401.15458v1",
        "new_summary": "  Intelligent Transportation Systems (ITS) utilize sensors, cameras, and big\ndata analysis to monitor real-time traffic conditions, aiming to improve\ntraffic efficiency and safety. Accurate vehicle recognition is crucial in this\nprocess, and Vehicle Logo Recognition (VLR) stands as a key method. VLR enables\neffective management and monitoring by distinguishing vehicles on the road.\nConvolutional Neural Networks (CNNs) have made impressive strides in VLR\nresearch. However, achieving higher performance demands significant time and\ncomputational resources for training. Recently, the rise of Transformer models\nhas brought new opportunities to VLR. Swin Transformer, with its efficient\ncomputation and global feature modeling capabilities, outperforms CNNs under\nchallenging conditions. In this paper, we implement real-time VLR using Swin\nTransformer and fine-tune it for optimal performance. Extensive experiments\nconducted on three public vehicle logo datasets (HFUT-VL1, XMU, CTGU-VLD)\ndemonstrate impressive top accuracy results of 99.28%, 100%, and 99.17%,\nrespectively. Additionally, the use of a transfer learning strategy enables our\nmethod to be on par with state-of-the-art VLR methods. These findings affirm\nthe superiority of our approach over existing methods. Future research can\nexplore and optimize the application of the Swin Transformer in other vehicle\nvision recognition tasks to drive advancements in ITS.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.15458v1.pdf",
        "similarity": 0.4471498441685917,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-27"
    },
    {
        "new_title": "Fair MP-BOOST: Fair and Interpretable Minipatch Boosting",
        "new_link": "http://arxiv.org/abs/2404.01521v1",
        "new_summary": "  Ensemble methods, particularly boosting, have established themselves as\nhighly effective and widely embraced machine learning techniques for tabular\ndata. In this paper, we aim to leverage the robust predictive power of\ntraditional boosting methods while enhancing fairness and interpretability. To\nachieve this, we develop Fair MP-Boost, a stochastic boosting scheme that\nbalances fairness and accuracy by adaptively learning features and observations\nduring training. Specifically, Fair MP-Boost sequentially samples small subsets\nof observations and features, termed minipatches (MP), according to adaptively\nlearned feature and observation sampling probabilities. We devise these\nprobabilities by combining loss functions, or by combining feature importance\nscores to address accuracy and fairness simultaneously. Hence, Fair MP-Boost\nprioritizes important and fair features along with challenging instances, to\nselect the most relevant minipatches for learning. The learned probability\ndistributions also yield intrinsic interpretations of feature importance and\nimportant observations in Fair MP-Boost. Through empirical evaluation of\nsimulated and benchmark datasets, we showcase the interpretability, accuracy,\nand fairness of Fair MP-Boost.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.01521v1.pdf",
        "similarity": 0.4467410922164359,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-01"
    },
    {
        "new_title": "Unlocking Telemetry Potential: Self-Supervised Learning for Continuous\n  Clinical Electrocardiogram Monitoring",
        "new_link": "http://arxiv.org/abs/2406.16915v1",
        "new_summary": "  Machine learning (ML) applied to routine patient monitoring within intensive\ncare units (ICUs) has the potential to improve care by providing clinicians\nwith novel insights into each patient's health and expected response to\ninterventions. This paper applies deep learning to a large volume of unlabeled\nelectrocardiogram (ECG) telemetry signals, which are commonly used for\ncontinuous patient monitoring in hospitals but have important differences from\nthe standard, single time-point 12-lead ECG used in many prior machine learning\nstudies. We applied self-supervised learning to pretrain a spectrum of deep\nnetworks on approximately 147,000 hours of ECG telemetry data. Our approach\nleverages this dataset to train models that significantly improve performance\non four distinct downstream tasks compared with direct supervised learning\nusing labeled data. These pretrained models enable medically useful predictions\nand estimates in smaller patient cohorts that are typically limited by the\nscarcity of labels. Notably, we demonstrate that our pretrained networks can\ncontinuously annotate ECG telemetry signals, thereby providing monitoring\ncapabilities that are often unavailable due to the requirement for specialized\nexpertise and time-consuming professional annotations.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.16915v1.pdf",
        "similarity": 0.4461957392510683,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-07"
    },
    {
        "new_title": "Hyperspectral Image Analysis in Single-Modal and Multimodal setting\n  using Deep Learning Techniques",
        "new_link": "http://arxiv.org/abs/2403.01546v1",
        "new_summary": "  Hyperspectral imaging provides precise classification for land use and cover\ndue to its exceptional spectral resolution. However, the challenges of high\ndimensionality and limited spatial resolution hinder its effectiveness. This\nstudy addresses these challenges by employing deep learning techniques to\nefficiently process, extract features, and classify data in an integrated\nmanner. To enhance spatial resolution, we integrate information from\ncomplementary modalities such as LiDAR and SAR data through multimodal\nlearning. Moreover, adversarial learning and knowledge distillation are\nutilized to overcome issues stemming from domain disparities and missing\nmodalities. We also tailor deep learning architectures to suit the unique\ncharacteristics of HSI data, utilizing 1D convolutional and recurrent neural\nnetworks to handle its continuous spectral dimension. Techniques like visual\nattention and feedback connections within the architecture bolster the\nrobustness of feature extraction. Additionally, we tackle the issue of limited\ntraining samples through self-supervised learning methods, employing\nautoencoders for dimensionality reduction and exploring semi-supervised\nlearning techniques that leverage unlabeled data. Our proposed approaches are\nevaluated across various HSI datasets, consistently outperforming existing\nstate-of-the-art techniques.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.01546v1.pdf",
        "similarity": 0.4455667852795388,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-03"
    },
    {
        "new_title": "Lift3D: Zero-Shot Lifting of Any 2D Vision Model to 3D",
        "new_link": "http://arxiv.org/abs/2403.18922v1",
        "new_summary": "  In recent years, there has been an explosion of 2D vision models for numerous\ntasks such as semantic segmentation, style transfer or scene editing, enabled\nby large-scale 2D image datasets. At the same time, there has been renewed\ninterest in 3D scene representations such as neural radiance fields from\nmulti-view images. However, the availability of 3D or multiview data is still\nsubstantially limited compared to 2D image datasets, making extending 2D vision\nmodels to 3D data highly desirable but also very challenging. Indeed, extending\na single 2D vision operator like scene editing to 3D typically requires a\nhighly creative method specialized to that task and often requires per-scene\noptimization. In this paper, we ask the question of whether any 2D vision model\ncan be lifted to make 3D consistent predictions. We answer this question in the\naffirmative; our new Lift3D method trains to predict unseen views on feature\nspaces generated by a few visual models (i.e. DINO and CLIP), but then\ngeneralizes to novel vision operators and tasks, such as style transfer,\nsuper-resolution, open vocabulary segmentation and image colorization; for some\nof these tasks, there is no comparable previous 3D method. In many cases, we\neven outperform state-of-the-art methods specialized for the task in question.\nMoreover, Lift3D is a zero-shot method, in the sense that it requires no\ntask-specific training, nor scene-specific optimization.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.18922v1.pdf",
        "similarity": 0.4455440452467521,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-27"
    },
    {
        "new_title": "HANet: A Hierarchical Attention Network for Change Detection With\n  Bitemporal Very-High-Resolution Remote Sensing Images",
        "new_link": "http://arxiv.org/abs/2404.09178v1",
        "new_summary": "  Benefiting from the developments in deep learning technology,\ndeep-learning-based algorithms employing automatic feature extraction have\nachieved remarkable performance on the change detection (CD) task. However, the\nperformance of existing deep-learning-based CD methods is hindered by the\nimbalance between changed and unchanged pixels. To tackle this problem, a\nprogressive foreground-balanced sampling strategy on the basis of not adding\nchange information is proposed in this article to help the model accurately\nlearn the features of the changed pixels during the early training process and\nthereby improve detection performance.Furthermore, we design a discriminative\nSiamese network, hierarchical attention network (HANet), which can integrate\nmultiscale features and refine detailed features. The main part of HANet is the\nHAN module, which is a lightweight and effective self-attention mechanism.\nExtensive experiments and ablation studies on two CDdatasets with extremely\nunbalanced labels validate the effectiveness and efficiency of the proposed\nmethod.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.09178v1.pdf",
        "similarity": 0.4453176243751746,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-14"
    },
    {
        "new_title": "Detection and Recovery Against Deep Neural Network Fault Injection\n  Attacks Based on Contrastive Learning",
        "new_link": "http://arxiv.org/abs/2401.16766v1",
        "new_summary": "  Deep Neural Network (DNN) models when implemented on executing devices as the\ninference engines are susceptible to Fault Injection Attacks (FIAs) that\nmanipulate model parameters to disrupt inference execution with disastrous\nperformance. This work introduces Contrastive Learning (CL) of visual\nrepresentations i.e., a self-supervised learning approach into the deep\nlearning training and inference pipeline to implement DNN inference engines\nwith self-resilience under FIAs. Our proposed CL based FIA Detection and\nRecovery (CFDR) framework features (i) real-time detection with only a single\nbatch of testing data and (ii) fast recovery effective even with only a small\namount of unlabeled testing data. Evaluated with the CIFAR-10 dataset on\nmultiple types of FIAs, our CFDR shows promising detection and recovery\neffectiveness.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.16766v1.pdf",
        "similarity": 0.4452138648004212,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-30"
    },
    {
        "new_title": "Opening the black box of language acquisition",
        "new_link": "http://arxiv.org/abs/2402.11681v1",
        "new_summary": "  Recent advances in large language models using deep learning techniques have\nrenewed interest on how languages can be learned from data. However, it is\nunclear whether or how these models represent grammatical information from the\nlearned languages. In addition, the models must be pre-trained on large corpora\nbefore they can be used. In this work, we propose an alternative, more\ntransparent and cognitively plausible architecture for learning language.\nInstead of using deep learning, our approach uses a minimal cognitive\narchitecture based on sequence memory and chunking. The learning mechanism is\nbased on the principles of reinforcement learning. We test our architecture on\na number of natural-like toy languages. Results show that the model can learn\nthese artificial languages from scratch and extract grammatical information\nthat supports learning. Our study demonstrates the power of this simple\narchitecture and stresses the importance of sequence memory as a key component\nof the language learning process. Since other animals do not seem to have a\nfaithful sequence memory, this may explain why only humans have developed\ncomplex languages.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.11681v1.pdf",
        "similarity": 0.44489052483386304,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-18"
    },
    {
        "new_title": "Efficient Surgical Tool Recognition via HMM-Stabilized Deep Learning",
        "new_link": "http://arxiv.org/abs/2404.04992v1",
        "new_summary": "  Recognizing various surgical tools, actions and phases from surgery videos is\nan important problem in computer vision with exciting clinical applications.\nExisting deep-learning-based methods for this problem either process each\nsurgical video as a series of independent images without considering their\ndependence, or rely on complicated deep learning models to count for dependence\nof video frames. In this study, we revealed from exploratory data analysis that\nsurgical videos enjoy relatively simple semantic structure, where the presence\nof surgical phases and tools can be well modeled by a compact hidden Markov\nmodel (HMM). Based on this observation, we propose an HMM-stabilized deep\nlearning method for tool presence detection. A wide range of experiments\nconfirm that the proposed approaches achieve better performance with lower\ntraining and running costs, and support more flexible ways to construct and\nutilize training data in scenarios where not all surgery videos of interest are\nextensively labelled. These results suggest that popular deep learning\napproaches with over-complicated model structures may suffer from inefficient\nutilization of data, and integrating ingredients of deep learning and\nstatistical learning wisely may lead to more powerful algorithms that enjoy\ncompetitive performance, transparent interpretation and convenient model\ntraining simultaneously.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.04992v1.pdf",
        "similarity": 0.44400365015000076,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-07"
    },
    {
        "new_title": "BEND: Bagging Deep Learning Training Based on Efficient Neural Network\n  Diffusion",
        "new_link": "http://arxiv.org/abs/2403.15766v1",
        "new_summary": "  Bagging has achieved great success in the field of machine learning by\nintegrating multiple base classifiers to build a single strong classifier to\nreduce model variance. The performance improvement of bagging mainly relies on\nthe number and diversity of base classifiers. However, traditional deep\nlearning model training methods are expensive to train individually and\ndifficult to train multiple models with low similarity in a restricted dataset.\nRecently, diffusion models, which have been tremendously successful in the\nfields of imaging and vision, have been found to be effective in generating\nneural network model weights and biases with diversity. We creatively propose a\nBagging deep learning training algorithm based on Efficient Neural network\nDiffusion (BEND). The originality of BEND comes from the first use of a neural\nnetwork diffusion model to efficiently build base classifiers for bagging. Our\napproach is simple but effective, first using multiple trained model weights\nand biases as inputs to train autoencoder and latent diffusion model to realize\na diffusion model from noise to valid neural network parameters. Subsequently,\nwe generate several base classifiers using the trained diffusion model.\nFinally, we integrate these ba se classifiers for various inference tasks using\nthe Bagging method. Resulting experiments on multiple models and datasets show\nthat our proposed BEND algorithm can consistently outperform the mean and\nmedian accuracies of both the original trained model and the diffused model. At\nthe same time, new models diffused using the diffusion model have higher\ndiversity and lower cost than multiple models trained using traditional\nmethods. The BEND approach successfully introduces diffusion models into the\nnew deep learning training domain and provides a new paradigm for future deep\nlearning training and inference.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.15766v1.pdf",
        "similarity": 0.4433514517494155,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-23"
    },
    {
        "new_title": "Development of Skip Connection in Deep Neural Networks for Computer\n  Vision and Medical Image Analysis: A Survey",
        "new_link": "http://arxiv.org/abs/2405.01725v1",
        "new_summary": "  Deep learning has made significant progress in computer vision, specifically\nin image classification, object detection, and semantic segmentation. The skip\nconnection has played an essential role in the architecture of deep neural\nnetworks,enabling easier optimization through residual learning during the\ntraining stage and improving accuracy during testing. Many neural networks have\ninherited the idea of residual learning with skip connections for various\ntasks, and it has been the standard choice for designing neural networks. This\nsurvey provides a comprehensive summary and outlook on the development of skip\nconnections in deep neural networks. The short history of skip connections is\noutlined, and the development of residual learning in deep neural networks is\nsurveyed. The effectiveness of skip connections in the training and testing\nstages is summarized, and future directions for using skip connections in\nresidual learning are discussed. Finally, we summarize seminal papers, source\ncode, models, and datasets that utilize skip connections in computer vision,\nincluding image classification, object detection, semantic segmentation, and\nimage reconstruction. We hope this survey could inspire peer researchers in the\ncommunity to develop further skip connections in various forms and tasks and\nthe theory of residual learning in deep neural networks. The project page can\nbe found at https://github.com/apple1986/Residual_Learning_For_Images\n",
        "pdf_link": "https://arxiv.org/pdf/2405.01725v1.pdf",
        "similarity": 0.44251402518937105,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-02"
    },
    {
        "new_title": "Deep Image Composition Meets Image Forgery",
        "new_link": "http://arxiv.org/abs/2404.02897v2",
        "new_summary": "  Image forgery is a topic that has been studied for many years. Before the\nbreakthrough of deep learning, forged images were detected using handcrafted\nfeatures that did not require training. These traditional methods failed to\nperform satisfactorily even on datasets much worse in quality than real-life\nimage manipulations. Advances in deep learning have impacted image forgery\ndetection as much as they have impacted other areas of computer vision and have\nimproved the state of the art. Deep learning models require large amounts of\nlabeled data for training. In the case of image forgery, labeled data at the\npixel level is a very important factor for the models to learn. None of the\nexisting datasets have sufficient size, realism and pixel-level labeling at the\nsame time. This is due to the high cost of producing and labeling quality\nimages. It can take hours for an image editing expert to manipulate just one\nimage. To bridge this gap, we automate data generation using image composition\ntechniques that are very related to image forgery. Unlike other automated data\ngeneration frameworks, we use state of the art image composition deep learning\nmodels to generate spliced images close to the quality of real-life\nmanipulations. Finally, we test the generated dataset on the SOTA image\nmanipulation detection model and show that its prediction performance is lower\ncompared to existing datasets, i.e. we produce realistic images that are more\ndifficult to detect. Dataset will be available at\nhttps://github.com/99eren99/DIS25k .\n",
        "pdf_link": "https://arxiv.org/pdf/2404.02897v2.pdf",
        "similarity": 0.44201886530373913,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-03"
    },
    {
        "new_title": "Efficient Deep Learning with Decorrelated Backpropagation",
        "new_link": "http://arxiv.org/abs/2405.02385v2",
        "new_summary": "  The backpropagation algorithm remains the dominant and most successful method\nfor training deep neural networks (DNNs). At the same time, training DNNs at\nscale comes at a significant computational cost and therefore a high carbon\nfootprint. Converging evidence suggests that input decorrelation may speed up\ndeep learning. However, to date, this has not yet translated into substantial\nimprovements in training efficiency in large-scale DNNs. This is mainly caused\nby the challenge of enforcing fast and stable network-wide decorrelation. Here,\nwe show for the first time that much more efficient training of very deep\nneural networks using decorrelated backpropagation is feasible. To achieve this\ngoal we made use of a novel algorithm which induces network-wide input\ndecorrelation using minimal computational overhead. By combining this algorithm\nwith careful optimizations, we obtain a more than two-fold speed-up and higher\ntest accuracy compared to backpropagation when training a 18-layer deep\nresidual network. This demonstrates that decorrelation provides exciting\nprospects for efficient deep learning at scale.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.02385v2.pdf",
        "similarity": 0.4414865880994504,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-03"
    },
    {
        "new_title": "MASSM: An End-to-End Deep Learning Framework for Multi-Anatomy\n  Statistical Shape Modeling Directly From Images",
        "new_link": "http://arxiv.org/abs/2403.11008v2",
        "new_summary": "  Statistical Shape Modeling (SSM) effectively analyzes anatomical variations\nwithin populations but is limited by the need for manual localization and\nsegmentation, which relies on scarce medical expertise. Recent advances in deep\nlearning have provided a promising approach that automatically generates\nstatistical representations (as point distribution models or PDMs) from\nunsegmented images. Once trained, these deep learning-based models eliminate\nthe need for manual segmentation for new subjects. Most deep learning methods\nstill require manual pre-alignment of image volumes and bounding box\nspecification around the target anatomy, leading to a partially manual\ninference process. Recent approaches facilitate anatomy localization but only\nestimate population-level statistical representations and cannot directly\ndelineate anatomy in images. Additionally, they are limited to modeling a\nsingle anatomy. We introduce MASSM, a novel end-to-end deep learning framework\nthat simultaneously localizes multiple anatomies, estimates population-level\nstatistical representations, and delineates shape representations directly in\nimage space. Our results show that MASSM, which delineates anatomy in image\nspace and handles multiple anatomies through a multitask network, provides\nsuperior shape information compared to segmentation networks for medical\nimaging tasks. Estimating Statistical Shape Models (SSM) is a stronger task\nthan segmentation, as it encodes a more robust statistical prior for the\nobjects to be detected and delineated. MASSM allows for more accurate and\ncomprehensive shape representations, surpassing the capabilities of traditional\npixel-wise segmentation.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.11008v2.pdf",
        "similarity": 0.4413017463174896,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-16"
    },
    {
        "new_title": "Multiply Robust Estimation for Local Distribution Shifts with Multiple\n  Domains",
        "new_link": "http://arxiv.org/abs/2402.14145v2",
        "new_summary": "  Distribution shifts are ubiquitous in real-world machine learning\napplications, posing a challenge to the generalization of models trained on one\ndata distribution to another. We focus on scenarios where data distributions\nvary across multiple segments of the entire population and only make local\nassumptions about the differences between training and test (deployment)\ndistributions within each segment. We propose a two-stage multiply robust\nestimation method to improve model performance on each individual segment for\ntabular data analysis. The method involves fitting a linear combination of the\nbased models, learned using clusters of training data from multiple segments,\nfollowed by a refinement step for each segment. Our method is designed to be\nimplemented with commonly used off-the-shelf machine learning models. We\nestablish theoretical guarantees on the generalization bound of the method on\nthe test risk. With extensive experiments on synthetic and real datasets, we\ndemonstrate that the proposed method substantially improves over existing\nalternatives in prediction accuracy and robustness on both regression and\nclassification tasks. We also assess its effectiveness on a user city\nprediction dataset from Meta.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.14145v2.pdf",
        "similarity": 0.44077114454296934,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-21"
    },
    {
        "new_title": "How Transformers Learn Causal Structure with Gradient Descent",
        "new_link": "http://arxiv.org/abs/2402.14735v1",
        "new_summary": "  The incredible success of transformers on sequence modeling tasks can be\nlargely attributed to the self-attention mechanism, which allows information to\nbe transferred between different parts of a sequence. Self-attention allows\ntransformers to encode causal structure which makes them particularly suitable\nfor sequence modeling. However, the process by which transformers learn such\ncausal structure via gradient-based training algorithms remains poorly\nunderstood. To better understand this process, we introduce an in-context\nlearning task that requires learning latent causal structure. We prove that\ngradient descent on a simplified two-layer transformer learns to solve this\ntask by encoding the latent causal graph in the first attention layer. The key\ninsight of our proof is that the gradient of the attention matrix encodes the\nmutual information between tokens. As a consequence of the data processing\ninequality, the largest entries of this gradient correspond to edges in the\nlatent causal graph. As a special case, when the sequences are generated from\nin-context Markov chains, we prove that transformers learn an induction head\n(Olsson et al., 2022). We confirm our theoretical findings by showing that\ntransformers trained on our in-context learning task are able to recover a wide\nvariety of causal structures.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.14735v1.pdf",
        "similarity": 0.43960682605161655,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-22"
    },
    {
        "new_title": "LM4LV: A Frozen Large Language Model for Low-level Vision Tasks",
        "new_link": "http://arxiv.org/abs/2405.15734v2",
        "new_summary": "  The success of large language models (LLMs) has fostered a new research trend\nof multi-modality large language models (MLLMs), which changes the paradigm of\nvarious fields in computer vision. Though MLLMs have shown promising results in\nnumerous high-level vision and vision-language tasks such as VQA and\ntext-to-image, no works have demonstrated how low-level vision tasks can\nbenefit from MLLMs. We find that most current MLLMs are blind to low-level\nfeatures due to their design of vision modules, thus are inherently incapable\nfor solving low-level vision tasks. In this work, we purpose $\\textbf{LM4LV}$,\na framework that enables a FROZEN LLM to solve a range of low-level vision\ntasks without any multi-modal data or prior. This showcases the LLM's strong\npotential in low-level vision and bridges the gap between MLLMs and low-level\nvision tasks. We hope this work can inspire new perspectives on LLMs and deeper\nunderstanding of their mechanisms. Code is available at\nhttps://github.com/bytetriper/LM4LV.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.15734v2.pdf",
        "similarity": 0.43955717520581655,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-24"
    },
    {
        "new_title": "Neural Optimizer Equation, Decay Function, and Learning Rate Schedule\n  Joint Evolution",
        "new_link": "http://arxiv.org/abs/2404.06679v1",
        "new_summary": "  A major contributor to the quality of a deep learning model is the selection\nof the optimizer. We propose a new dual-joint search space in the realm of\nneural optimizer search (NOS), along with an integrity check, to automate the\nprocess of finding deep learning optimizers. Our dual-joint search space\nsimultaneously allows for the optimization of not only the update equation, but\nalso internal decay functions and learning rate schedules for optimizers. We\nsearch the space using our proposed mutation-only, particle-based genetic\nalgorithm able to be massively parallelized for our domain-specific problem. We\nevaluate our candidate optimizers on the CIFAR-10 dataset using a small\nConvNet. To assess generalization, the final optimizers were then transferred\nto large-scale image classification on CIFAR- 100 and TinyImageNet, while also\nbeing fine-tuned on Flowers102, Cars196, and Caltech101 using\nEfficientNetV2Small. We found multiple optimizers, learning rate schedules, and\nAdam variants that outperformed Adam, as well as other standard deep learning\noptimizers, across the image classification tasks.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.06679v1.pdf",
        "similarity": 0.4389739731049766,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-10"
    },
    {
        "new_title": "MHLR: Moving Haar Learning Rate Scheduler for Large-scale Face\n  Recognition Training with One GPU",
        "new_link": "http://arxiv.org/abs/2404.11118v1",
        "new_summary": "  Face recognition (FR) has seen significant advancements due to the\nutilization of large-scale datasets. Training deep FR models on large-scale\ndatasets with multiple GPUs is now a common practice. In fact, computing power\nhas evolved into a foundational and indispensable resource in the area of deep\nlearning. It is nearly impossible to train a deep FR model without holding\nadequate hardware resources. Recognizing this challenge, some FR approaches\nhave started exploring ways to reduce the time complexity of the\nfully-connected layer in FR models. Unlike other approaches, this paper\nintroduces a simple yet highly effective approach, Moving Haar Learning Rate\n(MHLR) scheduler, for scheduling the learning rate promptly and accurately in\nthe training process. MHLR supports large-scale FR training with only one GPU,\nwhich is able to accelerate the model to 1/4 of its original training time\nwithout sacrificing more than 1% accuracy. More specifically, MHLR only needs\n$30$ hours to train the model ResNet100 on the dataset WebFace12M containing\nmore than 12M face images with 0.6M identities. Extensive experiments validate\nthe efficiency and effectiveness of MHLR.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.11118v1.pdf",
        "similarity": 0.4388003838018934,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-17"
    },
    {
        "new_title": "MODL: Multilearner Online Deep Learning",
        "new_link": "http://arxiv.org/abs/2405.18281v1",
        "new_summary": "  Online deep learning solves the problem of learning from streams of data,\nreconciling two opposing objectives: learn fast and learn deep. Existing work\nfocuses almost exclusively on exploring pure deep learning solutions, which are\nmuch better suited to handle the \"deep\" than the \"fast\" part of the online\nlearning equation. In our work, we propose a different paradigm, based on a\nhybrid multilearner approach. First, we develop a fast online logistic\nregression learner. This learner does not rely on backpropagation. Instead, it\nuses closed form recursive updates of model parameters, handling the fast\nlearning part of the online learning problem. We then analyze the existing\nonline deep learning theory and show that the widespread ODL approach,\ncurrently operating at complexity $O(L^2)$ in terms of the number of layers\n$L$, can be equivalently implemented in $O(L)$ complexity. This further leads\nus to the cascaded multilearner design, in which multiple shallow and deep\nlearners are co-trained to solve the online learning problem in a cooperative,\nsynergistic fashion. We show that this approach achieves state-of-the-art\nresults on common online learning datasets, while also being able to handle\nmissing features gracefully. Our code is publicly available at\nhttps://github.com/AntonValk/MODL.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.18281v1.pdf",
        "similarity": 0.43856482339371217,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-28"
    },
    {
        "new_title": "Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive\n  Survey",
        "new_link": "http://arxiv.org/abs/2401.11734v1",
        "new_summary": "  Colorectal polyp segmentation (CPS), an essential problem in medical image\nanalysis, has garnered growing research attention. Recently, the deep\nlearning-based model completely overwhelmed traditional methods in the field of\nCPS, and more and more deep CPS methods have emerged, bringing the CPS into the\ndeep learning era. To help the researchers quickly grasp the main techniques,\ndatasets, evaluation metrics, challenges, and trending of deep CPS, this paper\npresents a systematic and comprehensive review of deep-learning-based CPS\nmethods from 2014 to 2023, a total of 115 technical papers. In particular, we\nfirst provide a comprehensive review of the current deep CPS with a novel\ntaxonomy, including network architectures, level of supervision, and learning\nparadigm. More specifically, network architectures include eight subcategories,\nthe level of supervision comprises six subcategories, and the learning paradigm\nencompasses 12 subcategories, totaling 26 subcategories. Then, we provided a\ncomprehensive analysis the characteristics of each dataset, including the\nnumber of datasets, annotation types, image resolution, polyp size, contrast\nvalues, and polyp location. Following that, we summarized CPS's commonly used\nevaluation metrics and conducted a detailed analysis of 40 deep SOTA models,\nincluding out-of-distribution generalization and attribute-based performance\nanalysis. Finally, we discussed deep learning-based CPS methods' main\nchallenges and opportunities.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.11734v1.pdf",
        "similarity": 0.4383230142557746,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-22"
    },
    {
        "new_title": "Employing Layerwised Unsupervised Learning to Lessen Data and Loss\n  Requirements in Forward-Forward Algorithms",
        "new_link": "http://arxiv.org/abs/2404.14664v1",
        "new_summary": "  Recent deep learning models such as ChatGPT utilizing the back-propagation\nalgorithm have exhibited remarkable performance. However, the disparity between\nthe biological brain processes and the back-propagation algorithm has been\nnoted. The Forward-Forward algorithm, which trains deep learning models solely\nthrough the forward pass, has emerged to address this. Although the\nForward-Forward algorithm cannot replace back-propagation due to limitations\nsuch as having to use special input and loss functions, it has the potential to\nbe useful in special situations where back-propagation is difficult to use. To\nwork around this limitation and verify usability, we propose an Unsupervised\nForward-Forward algorithm. Using an unsupervised learning model enables\ntraining with usual loss functions and inputs without restriction. Through this\napproach, we lead to stable learning and enable versatile utilization across\nvarious datasets and tasks. From a usability perspective, given the\ncharacteristics of the Forward-Forward algorithm and the advantages of the\nproposed method, we anticipate its practical application even in scenarios such\nas federated learning, where deep learning layers need to be trained separately\nin physically distributed environments.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.14664v1.pdf",
        "similarity": 0.4383222456994882,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-23"
    },
    {
        "new_title": "Learned feature representations are biased by complexity, learning\n  order, position, and more",
        "new_link": "http://arxiv.org/abs/2405.05847v2",
        "new_summary": "  Representation learning, and interpreting learned representations, are key\nareas of focus in machine learning and neuroscience. Both fields generally use\nrepresentations as a means to understand or improve a system's computations. In\nthis work, however, we explore surprising dissociations between representation\nand computation that may pose challenges for such efforts. We create datasets\nin which we attempt to match the computational role that different features\nplay, while manipulating other properties of the features or the data. We train\nvarious deep learning architectures to compute these multiple abstract features\nabout their inputs. We find that their learned feature representations are\nsystematically biased towards representing some features more strongly than\nothers, depending upon extraneous properties such as feature complexity, the\norder in which features are learned, and the distribution of features over the\ninputs. For example, features that are simpler to compute or learned first tend\nto be represented more strongly and densely than features that are more complex\nor learned later, even if all features are learned equally well. We also\nexplore how these biases are affected by architectures, optimizers, and\ntraining regimes (e.g., in transformers, features decoded earlier in the output\nsequence also tend to be represented more strongly). Our results help to\ncharacterize the inductive biases of gradient-based representation learning.\nThese results also highlight a key challenge for interpretability $-$ or for\ncomparing the representations of models and brains $-$ disentangling extraneous\nbiases from the computationally important aspects of a system's internal\nrepresentations.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.05847v2.pdf",
        "similarity": 0.43823447547575717,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-09"
    },
    {
        "new_title": "Self-STORM: Deep Unrolled Self-Supervised Learning for Super-Resolution\n  Microscopy",
        "new_link": "http://arxiv.org/abs/2403.16974v1",
        "new_summary": "  The use of fluorescent molecules to create long sequences of low-density,\ndiffraction-limited images enables highly-precise molecule localization.\nHowever, this methodology requires lengthy imaging times, which limits the\nability to view dynamic interactions of live cells on short time scales. Many\ntechniques have been developed to reduce the number of frames needed for\nlocalization, from classic iterative optimization to deep neural networks.\nParticularly, deep algorithm unrolling utilizes both the structure of iterative\nsparse recovery algorithms and the performance gains of supervised deep\nlearning. However, the robustness of this approach is highly dependant on\nhaving sufficient training data. In this paper we introduce deep unrolled\nself-supervised learning, which alleviates the need for such data by training a\nsequence-specific, model-based autoencoder that learns only from given\nmeasurements. Our proposed method exceeds the performance of its supervised\ncounterparts, thus allowing for robust, dynamic imaging well below the\ndiffraction limit without any labeled training samples. Furthermore, the\nsuggested model-based autoencoder scheme can be utilized to enhance\ngeneralization in any sparse recovery framework, without the need for external\ntraining data.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.16974v1.pdf",
        "similarity": 0.43730871615776645,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-25"
    },
    {
        "new_title": "3D Instance Segmentation Using Deep Learning on RGB-D Indoor Data",
        "new_link": "http://arxiv.org/abs/2406.14581v1",
        "new_summary": "  3D object recognition is a challenging task for intelligent and robot systems\nin industrial and home indoor environments. It is critical for such systems to\nrecognize and segment the 3D object instances that they encounter on a frequent\nbasis. The computer vision, graphics, and machine learning fields have all\ngiven it a lot of attention. Traditionally, 3D segmentation was done with\nhand-crafted features and designed approaches that did not achieve acceptable\nperformance and could not be generalized to large-scale data. Deep learning\napproaches have lately become the preferred method for 3D segmentation\nchallenges by their great success in 2D computer vision. However, the task of\ninstance segmentation is currently less explored. In this paper, we propose a\nnovel approach for efficient 3D instance segmentation using red green blue and\ndepth (RGB-D) data based on deep learning. The 2D region based convolutional\nneural networks (Mask R-CNN) deep learning model with point based rending\nmodule is adapted to integrate with depth information to recognize and segment\n3D instances of objects. In order to generate 3D point cloud coordinates (x, y,\nz), segmented 2D pixels (u, v) of recognized object regions in the RGB image\nare merged into (u, v) points of the depth image. Moreover, we conducted an\nexperiment and analysis to compare our proposed method from various points of\nview and distances. The experimentation shows the proposed 3D object\nrecognition and instance segmentation are sufficiently beneficial to support\nobject handling in robotic and intelligent systems.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.14581v1.pdf",
        "similarity": 0.4363569929647895,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-19"
    },
    {
        "new_title": "Improving Image Coding for Machines through Optimizing Encoder via\n  Auxiliary Loss",
        "new_link": "http://arxiv.org/abs/2402.08267v1",
        "new_summary": "  Image coding for machines (ICM) aims to compress images for machine analysis\nusing recognition models rather than human vision. Hence, in ICM, it is\nimportant for the encoder to recognize and compress the information necessary\nfor the machine recognition task. There are two main approaches in learned ICM;\noptimization of the compression model based on task loss, and Region of\nInterest (ROI) based bit allocation. These approaches provide the encoder with\nthe recognition capability. However, optimization with task loss becomes\ndifficult when the recognition model is deep, and ROI-based methods often\ninvolve extra overhead during evaluation. In this study, we propose a novel\ntraining method for learned ICM models that applies auxiliary loss to the\nencoder to improve its recognition capability and rate-distortion performance.\nOur method achieves Bjontegaard Delta rate improvements of 27.7% and 20.3% in\nobject detection and semantic segmentation tasks, compared to the conventional\ntraining method.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.08267v1.pdf",
        "similarity": 0.43623829191537283,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-13"
    },
    {
        "new_title": "Piecewise-Linear Manifolds for Deep Metric Learning",
        "new_link": "http://arxiv.org/abs/2403.14977v1",
        "new_summary": "  Unsupervised deep metric learning (UDML) focuses on learning a semantic\nrepresentation space using only unlabeled data. This challenging problem\nrequires accurately estimating the similarity between data points, which is\nused to supervise a deep network. For this purpose, we propose to model the\nhigh-dimensional data manifold using a piecewise-linear approximation, with\neach low-dimensional linear piece approximating the data manifold in a small\nneighborhood of a point. These neighborhoods are used to estimate similarity\nbetween data points. We empirically show that this similarity estimate\ncorrelates better with the ground truth than the similarity estimates of\ncurrent state-of-the-art techniques. We also show that proxies, commonly used\nin supervised metric learning, can be used to model the piecewise-linear\nmanifold in an unsupervised setting, helping improve performance. Our method\noutperforms existing unsupervised metric learning approaches on standard\nzero-shot image retrieval benchmarks.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.14977v1.pdf",
        "similarity": 0.4352362925729788,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-22"
    },
    {
        "new_title": "Contextual Emotion Recognition using Large Vision Language Models",
        "new_link": "http://arxiv.org/abs/2405.08992v1",
        "new_summary": "  \"How does the person in the bounding box feel?\" Achieving human-level\nrecognition of the apparent emotion of a person in real world situations\nremains an unsolved task in computer vision. Facial expressions are not enough:\nbody pose, contextual knowledge, and commonsense reasoning all contribute to\nhow humans perform this emotional theory of mind task. In this paper, we\nexamine two major approaches enabled by recent large vision language models: 1)\nimage captioning followed by a language-only LLM, and 2) vision language\nmodels, under zero-shot and fine-tuned setups. We evaluate the methods on the\nEmotions in Context (EMOTIC) dataset and demonstrate that a vision language\nmodel, fine-tuned even on a small dataset, can significantly outperform\ntraditional baselines. The results of this work aim to help robots and agents\nperform emotionally sensitive decision-making and interaction in the future.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.08992v1.pdf",
        "similarity": 0.43517398272313595,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-14"
    },
    {
        "new_title": "A Rate-Distortion View of Uncertainty Quantification",
        "new_link": "http://arxiv.org/abs/2406.10775v2",
        "new_summary": "  In supervised learning, understanding an input's proximity to the training\ndata can help a model decide whether it has sufficient evidence for reaching a\nreliable prediction. While powerful probabilistic models such as Gaussian\nProcesses naturally have this property, deep neural networks often lack it. In\nthis paper, we introduce Distance Aware Bottleneck (DAB), i.e., a new method\nfor enriching deep neural networks with this property. Building on prior\ninformation bottleneck approaches, our method learns a codebook that stores a\ncompressed representation of all inputs seen during training. The distance of a\nnew example from this codebook can serve as an uncertainty estimate for the\nexample. The resulting model is simple to train and provides deterministic\nuncertainty estimates by a single forward pass. Finally, our method achieves\nbetter out-of-distribution (OOD) detection and misclassification prediction\nthan prior methods, including expensive ensemble methods, deep kernel Gaussian\nProcesses, and approaches based on the standard information bottleneck.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.10775v2.pdf",
        "similarity": 0.4343404770860713,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-16"
    },
    {
        "new_title": "Streamflow Prediction with Uncertainty Quantification for Water\n  Management: A Constrained Reasoning and Learning Approach",
        "new_link": "http://arxiv.org/abs/2406.00133v1",
        "new_summary": "  Predicting the spatiotemporal variation in streamflow along with uncertainty\nquantification enables decision-making for sustainable management of scarce\nwater resources. Process-based hydrological models (aka physics-based models)\nare based on physical laws, but using simplifying assumptions which can lead to\npoor accuracy. Data-driven approaches offer a powerful alternative, but they\nrequire large amount of training data and tend to produce predictions that are\ninconsistent with physical laws. This paper studies a constrained reasoning and\nlearning (CRL) approach where physical laws represented as logical constraints\nare integrated as a layer in the deep neural network. To address small data\nsetting, we develop a theoretically-grounded training approach to improve the\ngeneralization accuracy of deep models. For uncertainty quantification, we\ncombine the synergistic strengths of Gaussian processes (GPs) and deep temporal\nmodels (i.e., deep models for time-series forecasting) by passing the learned\nlatent representation as input to a standard distance-based kernel. Experiments\non multiple real-world datasets demonstrate the effectiveness of both CRL and\nGP with deep kernel approaches over strong baseline methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.00133v1.pdf",
        "similarity": 0.4339524925628272,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-31"
    },
    {
        "new_title": "GreedyViG: Dynamic Axial Graph Construction for Efficient Vision GNNs",
        "new_link": "http://arxiv.org/abs/2405.06849v1",
        "new_summary": "  Vision graph neural networks (ViG) offer a new avenue for exploration in\ncomputer vision. A major bottleneck in ViGs is the inefficient k-nearest\nneighbor (KNN) operation used for graph construction. To solve this issue, we\npropose a new method for designing ViGs, Dynamic Axial Graph Construction\n(DAGC), which is more efficient than KNN as it limits the number of considered\ngraph connections made within an image. Additionally, we propose a novel\nCNN-GNN architecture, GreedyViG, which uses DAGC. Extensive experiments show\nthat GreedyViG beats existing ViG, CNN, and ViT architectures in terms of\naccuracy, GMACs, and parameters on image classification, object detection,\ninstance segmentation, and semantic segmentation tasks. Our smallest model,\nGreedyViG-S, achieves 81.1% top-1 accuracy on ImageNet-1K, 2.9% higher than\nVision GNN and 2.2% higher than Vision HyperGraph Neural Network (ViHGNN), with\nless GMACs and a similar number of parameters. Our largest model, GreedyViG-B\nobtains 83.9% top-1 accuracy, 0.2% higher than Vision GNN, with a 66.6%\ndecrease in parameters and a 69% decrease in GMACs. GreedyViG-B also obtains\nthe same accuracy as ViHGNN with a 67.3% decrease in parameters and a 71.3%\ndecrease in GMACs. Our work shows that hybrid CNN-GNN architectures not only\nprovide a new avenue for designing efficient models, but that they can also\nexceed the performance of current state-of-the-art models.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.06849v1.pdf",
        "similarity": 0.4337553217182396,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-10"
    },
    {
        "new_title": "A Dataset for Crucial Object Recognition in Blind and Low-Vision\n  Individuals' Navigation",
        "new_link": "http://arxiv.org/abs/2407.16777v1",
        "new_summary": "  This paper introduces a dataset for improving real-time object recognition\nsystems to aid blind and low-vision (BLV) individuals in navigation tasks. The\ndataset comprises 21 videos of BLV individuals navigating outdoor spaces, and a\ntaxonomy of 90 objects crucial for BLV navigation, refined through a focus\ngroup study. We also provide object labeling for the 90 objects across 31 video\nsegments created from the 21 videos. A deeper analysis reveals that most\ncontemporary datasets used in training computer vision models contain only a\nsmall subset of the taxonomy in our dataset. Preliminary evaluation of\nstate-of-the-art computer vision models on our dataset highlights shortcomings\nin accurately detecting key objects relevant to BLV navigation, emphasizing the\nneed for specialized datasets. We make our dataset publicly available, offering\nvaluable resources for developing more inclusive navigation systems for BLV\nindividuals.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.16777v1.pdf",
        "similarity": 0.4334594545895878,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-23"
    },
    {
        "new_title": "Position: Leverage Foundational Models for Black-Box Optimization",
        "new_link": "http://arxiv.org/abs/2405.03547v2",
        "new_summary": "  Undeniably, Large Language Models (LLMs) have stirred an extraordinary wave\nof innovation in the machine learning research domain, resulting in substantial\nimpact across diverse fields such as reinforcement learning, robotics, and\ncomputer vision. Their incorporation has been rapid and transformative, marking\na significant paradigm shift in the field of machine learning research.\nHowever, the field of experimental design, grounded on black-box optimization,\nhas been much less affected by such a paradigm shift, even though integrating\nLLMs with optimization presents a unique landscape ripe for exploration. In\nthis position paper, we frame the field of black-box optimization around\nsequence-based foundation models and organize their relationship with previous\nliterature. We discuss the most promising ways foundational language models can\nrevolutionize optimization, which include harnessing the vast wealth of\ninformation encapsulated in free-form text to enrich task comprehension,\nutilizing highly flexible sequence models such as Transformers to engineer\nsuperior optimization strategies, and enhancing performance prediction over\npreviously unseen search spaces.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.03547v2.pdf",
        "similarity": 0.43328036092682,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-06"
    },
    {
        "new_title": "Meta-Learning and representation learner: A short theoretical note",
        "new_link": "http://arxiv.org/abs/2407.04189v2",
        "new_summary": "  Meta-learning, or \"learning to learn,\" is a subfield of machine learning\nwhere the goal is to develop models and algorithms that can learn from various\ntasks and improve their learning process over time. Unlike traditional machine\nlearning methods focusing on learning a specific task, meta-learning aims to\nleverage experience from previous tasks to enhance future learning. This\napproach is particularly beneficial in scenarios where the available data for a\nnew task is limited, but there exists abundant data from related tasks. By\nextracting and utilizing the underlying structure and patterns across these\ntasks, meta-learning algorithms can achieve faster convergence and better\nperformance with fewer data. The following notes are mainly inspired from\n\\cite{vanschoren2018meta}, \\cite{baxter2019learning}, and\n\\cite{maurer2005algorithmic}.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.04189v2.pdf",
        "similarity": 0.4326511906597586,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-04"
    },
    {
        "new_title": "Computer Vision for Clinical Gait Analysis: A Gait Abnormality Video\n  Dataset",
        "new_link": "http://arxiv.org/abs/2407.04190v1",
        "new_summary": "  Clinical gait analysis (CGA) using computer vision is an emerging field in\nartificial intelligence that faces barriers of accessible, real-world data, and\nclear task objectives. This paper lays the foundation for current developments\nin CGA as well as vision-based methods and datasets suitable for gait analysis.\nWe introduce The Gait Abnormality in Video Dataset (GAVD) in response to our\nreview of over 150 current gait-related computer vision datasets, which\nhighlighted the need for a large and accessible gait dataset clinically\nannotated for CGA. GAVD stands out as the largest video gait dataset,\ncomprising 1874 sequences of normal, abnormal and pathological gaits.\nAdditionally, GAVD includes clinically annotated RGB data sourced from publicly\navailable content on online platforms. It also encompasses over 400 subjects\nwho have undergone clinical grade visual screening to represent a diverse range\nof abnormal gait patterns, captured in various settings, including hospital\nclinics and urban uncontrolled outdoor environments. We demonstrate the\nvalidity of the dataset and utility of action recognition models for CGA using\npretrained models Temporal Segment Networks(TSN) and SlowFast network to\nachieve video abnormality detection of 94% and 92% respectively when tested on\nGAVD dataset. A GitHub repository https://github.com/Rahmyyy/GAVD consisting of\nconvenient URL links, and clinically relevant annotation for CGA is provided\nfor over 450 online videos, featuring diverse subjects performing a range of\nnormal, pathological, and abnormal gait patterns.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.04190v1.pdf",
        "similarity": 0.4325592882347575,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-05"
    },
    {
        "new_title": "Computer Vision in the Food Industry: Accurate, Real-time, and Automatic\n  Food Recognition with Pretrained MobileNetV2",
        "new_link": "http://arxiv.org/abs/2405.11621v1",
        "new_summary": "  In contemporary society, the application of artificial intelligence for\nautomatic food recognition offers substantial potential for nutrition tracking,\nreducing food waste, and enhancing productivity in food production and\nconsumption scenarios. Modern technologies such as Computer Vision and Deep\nLearning are highly beneficial, enabling machines to learn automatically,\nthereby facilitating automatic visual recognition. Despite some research in\nthis field, the challenge of achieving accurate automatic food recognition\nquickly remains a significant research gap. Some models have been developed and\nimplemented, but maintaining high performance swiftly, with low computational\ncost and low access to expensive hardware accelerators, still needs further\nexploration and research. This study employs the pretrained MobileNetV2 model,\nwhich is efficient and fast, for food recognition on the public Food11 dataset,\ncomprising 16643 images. It also utilizes various techniques such as dataset\nunderstanding, transfer learning, data augmentation, regularization, dynamic\nlearning rate, hyperparameter tuning, and consideration of images in different\nsizes to enhance performance and robustness. These techniques aid in choosing\nappropriate metrics, achieving better performance, avoiding overfitting and\naccuracy fluctuations, speeding up the model, and increasing the generalization\nof findings, making the study and its results applicable to practical\napplications. Despite employing a light model with a simpler structure and\nfewer trainable parameters compared to some deep and dense models in the deep\nlearning area, it achieved commendable accuracy in a short time. This\nunderscores the potential for practical implementation, which is the main\nintention of this study.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.11621v1.pdf",
        "similarity": 0.4325488606893485,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-19"
    },
    {
        "new_title": "A Retrospective of the Tutorial on Opportunities and Challenges of\n  Online Deep Learning",
        "new_link": "http://arxiv.org/abs/2405.17222v2",
        "new_summary": "  Machine learning algorithms have become indispensable in today's world. They\nsupport and accelerate the way we make decisions based on the data at hand.\nThis acceleration means that data structures that were valid at one moment\ncould no longer be valid in the future. With these changing data structures, it\nis necessary to adapt machine learning (ML) systems incrementally to the new\ndata. This is done with the use of online learning or continuous ML\ntechnologies. While deep learning technologies have shown exceptional\nperformance on predefined datasets, they have not been widely applied to\nonline, streaming, and continuous learning. In this retrospective of our\ntutorial titled Opportunities and Challenges of Online Deep Learning held at\nECML PKDD 2023, we provide a brief overview of the opportunities but also the\npotential pitfalls for the application of neural networks in online learning\nenvironments using the frameworks River and Deep-River.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.17222v2.pdf",
        "similarity": 0.4323959774327109,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-27"
    },
    {
        "new_title": "Learning Geometric Invariant Features for Classification of Vector\n  Polygons with Graph Message-passing Neural Network",
        "new_link": "http://arxiv.org/abs/2407.04334v1",
        "new_summary": "  Geometric shape classification of vector polygons remains a non-trivial\nlearning task in spatial analysis. Previous studies mainly focus on devising\ndeep learning approaches for representation learning of rasterized vector\npolygons, whereas the study of discrete representations of polygons and\nsubsequent deep learning approaches have not been fully investigated. In this\nstudy, we investigate a graph representation of vector polygons and propose a\nnovel graph message-passing neural network (PolyMP) to learn the\ngeometric-invariant features for shape classification of polygons. Through\nextensive experiments, we show that the graph representation of polygons\ncombined with a permutation-invariant graph message-passing neural network\nachieves highly robust performances on benchmark datasets (i.e., synthetic\nglyph and real-world building footprint datasets) as compared to baseline\nmethods. We demonstrate that the proposed graph-based PolyMP network enables\nthe learning of expressive geometric features invariant to geometric\ntransformations of polygons (i.e., translation, rotation, scaling and shearing)\nand is robust to trivial vertex removals of polygons. We further show the\nstrong generalizability of PolyMP, which enables generalizing the learned\ngeometric features from the synthetic glyph polygons to the real-world building\nfootprints.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.04334v1.pdf",
        "similarity": 0.4317039020419357,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-05"
    },
    {
        "new_title": "Optimizing Curvature Learning for Robust Hyperbolic Deep Learning in\n  Computer Vision",
        "new_link": "http://arxiv.org/abs/2405.13979v1",
        "new_summary": "  Hyperbolic deep learning has become a growing research direction in computer\nvision for the unique properties afforded by the alternate embedding space. The\nnegative curvature and exponentially growing distance metric provide a natural\nframework for capturing hierarchical relationships between datapoints and\nallowing for finer separability between their embeddings. However, these\nmethods are still computationally expensive and prone to instability,\nespecially when attempting to learn the negative curvature that best suits the\ntask and the data. Current Riemannian optimizers do not account for changes in\nthe manifold which greatly harms performance and forces lower learning rates to\nminimize projection errors. Our paper focuses on curvature learning by\nintroducing an improved schema for popular learning algorithms and providing a\nnovel normalization approach to constrain embeddings within the variable\nrepresentative radius of the manifold. Additionally, we introduce a novel\nformulation for Riemannian AdamW, and alternative hybrid encoder techniques and\nfoundational formulations for current convolutional hyperbolic operations,\ngreatly reducing the computational penalty of the hyperbolic embedding space.\nOur approach demonstrates consistent performance improvements across both\ndirect classification and hierarchical metric learning tasks while allowing for\nlarger hyperbolic models.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.13979v1.pdf",
        "similarity": 0.4314884504054229,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-22"
    },
    {
        "new_title": "JSTR: Judgment Improves Scene Text Recognition",
        "new_link": "http://arxiv.org/abs/2404.05967v1",
        "new_summary": "  In this paper, we present a method for enhancing the accuracy of scene text\nrecognition tasks by judging whether the image and text match each other. While\nprevious studies focused on generating the recognition results from input\nimages, our approach also considers the model's misrecognition results to\nunderstand its error tendencies, thus improving the text recognition pipeline.\nThis method boosts text recognition accuracy by providing explicit feedback on\nthe data that the model is likely to misrecognize by predicting correct or\nincorrect between the image and text. The experimental results on publicly\navailable datasets demonstrate that our proposed method outperforms the\nbaseline and state-of-the-art methods in scene text recognition.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.05967v1.pdf",
        "similarity": 0.43145035340275545,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-09"
    },
    {
        "new_title": "Understanding attention-based encoder-decoder networks: a case study\n  with chess scoresheet recognition",
        "new_link": "http://arxiv.org/abs/2406.06538v1",
        "new_summary": "  Deep neural networks are largely used for complex prediction tasks. There is\nplenty of empirical evidence of their successful end-to-end training for a\ndiversity of tasks. Success is often measured based solely on the final\nperformance of the trained network, and explanations on when, why and how they\nwork are less emphasized. In this paper we study encoder-decoder recurrent\nneural networks with attention mechanisms for the task of reading handwritten\nchess scoresheets. Rather than prediction performance, our concern is to better\nunderstand how learning occurs in these type of networks. We characterize the\ntask in terms of three subtasks, namely input-output alignment, sequential\npattern recognition, and handwriting recognition, and experimentally\ninvestigate which factors affect their learning. We identify competition,\ncollaboration and dependence relations between the subtasks, and argue that\nsuch knowledge might help one to better balance factors to properly train a\nnetwork.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.06538v1.pdf",
        "similarity": 0.4313719724526962,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-23"
    },
    {
        "new_title": "Get rich quick: exact solutions reveal how unbalanced initializations\n  promote rapid feature learning",
        "new_link": "http://arxiv.org/abs/2406.06158v1",
        "new_summary": "  While the impressive performance of modern neural networks is often\nattributed to their capacity to efficiently extract task-relevant features from\ndata, the mechanisms underlying this rich feature learning regime remain\nelusive, with much of our theoretical understanding stemming from the opposing\nlazy regime. In this work, we derive exact solutions to a minimal model that\ntransitions between lazy and rich learning, precisely elucidating how\nunbalanced layer-specific initialization variances and learning rates determine\nthe degree of feature learning. Our analysis reveals that they conspire to\ninfluence the learning regime through a set of conserved quantities that\nconstrain and modify the geometry of learning trajectories in parameter and\nfunction space. We extend our analysis to more complex linear models with\nmultiple neurons, outputs, and layers and to shallow nonlinear networks with\npiecewise linear activation functions. In linear networks, rapid feature\nlearning only occurs with balanced initializations, where all layers learn at\nsimilar speeds. While in nonlinear networks, unbalanced initializations that\npromote faster learning in earlier layers can accelerate rich learning. Through\na series of experiments, we provide evidence that this unbalanced rich regime\ndrives feature learning in deep finite-width networks, promotes\ninterpretability of early layers in CNNs, reduces the sample complexity of\nlearning hierarchical data, and decreases the time to grokking in modular\narithmetic. Our theory motivates further exploration of unbalanced\ninitializations to enhance efficient feature learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.06158v1.pdf",
        "similarity": 0.4312688502616592,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-10"
    },
    {
        "new_title": "Recurrent Inference Machine for Medical Image Registration",
        "new_link": "http://arxiv.org/abs/2406.13413v1",
        "new_summary": "  Image registration is essential for medical image applications where\nalignment of voxels across multiple images is needed for qualitative or\nquantitative analysis. With recent advancements in deep neural networks and\nparallel computing, deep learning-based medical image registration methods\nbecome competitive with their flexible modelling and fast inference\ncapabilities. However, compared to traditional optimization-based registration\nmethods, the speed advantage may come at the cost of registration performance\nat inference time. Besides, deep neural networks ideally demand large training\ndatasets while optimization-based methods are training-free. To improve\nregistration accuracy and data efficiency, we propose a novel image\nregistration method, termed Recurrent Inference Image Registration (RIIR)\nnetwork. RIIR is formulated as a meta-learning solver to the registration\nproblem in an iterative manner. RIIR addresses the accuracy and data efficiency\nissues, by learning the update rule of optimization, with implicit\nregularization combined with explicit gradient input.\n  We evaluated RIIR extensively on brain MRI and quantitative cardiac MRI\ndatasets, in terms of both registration accuracy and training data efficiency.\nOur experiments showed that RIIR outperformed a range of deep learning-based\nmethods, even with only $5\\%$ of the training data, demonstrating high data\nefficiency. Key findings from our ablation studies highlighted the important\nadded value of the hidden states introduced in the recurrent inference\nframework for meta-learning. Our proposed RIIR offers a highly data-efficient\nframework for deep learning-based medical image registration.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.13413v1.pdf",
        "similarity": 0.431231143105727,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-19"
    },
    {
        "new_title": "Advanced Multimodal Deep Learning Architecture for Image-Text Matching",
        "new_link": "http://arxiv.org/abs/2406.15306v1",
        "new_summary": "  Image-text matching is a key multimodal task that aims to model the semantic\nassociation between images and text as a matching relationship. With the advent\nof the multimedia information age, image, and text data show explosive growth,\nand how to accurately realize the efficient and accurate semantic\ncorrespondence between them has become the core issue of common concern in\nacademia and industry. In this study, we delve into the limitations of current\nmultimodal deep learning models in processing image-text pairing tasks.\nTherefore, we innovatively design an advanced multimodal deep learning\narchitecture, which combines the high-level abstract representation ability of\ndeep neural networks for visual information with the advantages of natural\nlanguage processing models for text semantic understanding. By introducing a\nnovel cross-modal attention mechanism and hierarchical feature fusion strategy,\nthe model achieves deep fusion and two-way interaction between image and text\nfeature space. In addition, we also optimize the training objectives and loss\nfunctions to ensure that the model can better map the potential association\nstructure between images and text during the learning process. Experiments show\nthat compared with existing image-text matching models, the optimized new model\nhas significantly improved performance on a series of benchmark data sets. In\naddition, the new model also shows excellent generalization and robustness on\nlarge and diverse open scenario datasets and can maintain high matching\nperformance even in the face of previously unseen complex situations.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.15306v1.pdf",
        "similarity": 0.430650421229011,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-13"
    },
    {
        "new_title": "Demonstration of MaskSearch: Efficiently Querying Image Masks for\n  Machine Learning Workflows",
        "new_link": "http://arxiv.org/abs/2404.06563v1",
        "new_summary": "  We demonstrate MaskSearch, a system designed to accelerate queries over\ndatabases of image masks generated by machine learning models. MaskSearch\nformalizes and accelerates a new category of queries for retrieving images and\ntheir corresponding masks based on mask properties, which support various\napplications, from identifying spurious correlations learned by models to\nexploring discrepancies between model saliency and human attention. This\ndemonstration makes the following contributions:(1) the introduction of\nMaskSearch's graphical user interface (GUI), which enables interactive\nexploration of image databases through mask properties, (2) hands-on\nopportunities for users to explore MaskSearch's capabilities and constraints\nwithin machine learning workflows, and (3) an opportunity for conference\nattendees to understand how MaskSearch accelerates queries over image masks.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.06563v1.pdf",
        "similarity": 0.429638600499937,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-09"
    },
    {
        "new_title": "Continual Learning of Numerous Tasks from Long-tail Distributions",
        "new_link": "http://arxiv.org/abs/2404.02754v1",
        "new_summary": "  Continual learning, an important aspect of artificial intelligence and\nmachine learning research, focuses on developing models that learn and adapt to\nnew tasks while retaining previously acquired knowledge. Existing continual\nlearning algorithms usually involve a small number of tasks with uniform sizes\nand may not accurately represent real-world learning scenarios. In this paper,\nwe investigate the performance of continual learning algorithms with a large\nnumber of tasks drawn from a task distribution that is long-tail in terms of\ntask sizes. We design one synthetic dataset and two real-world continual\nlearning datasets to evaluate the performance of existing algorithms in such a\nsetting. Moreover, we study an overlooked factor in continual learning, the\noptimizer states, e.g. first and second moments in the Adam optimizer, and\ninvestigate how it can be used to improve continual learning performance. We\npropose a method that reuses the optimizer states in Adam by maintaining a\nweighted average of the second moments from previous tasks. We demonstrate that\nour method, compatible with most existing continual learning algorithms,\neffectively reduces forgetting with only a small amount of additional\ncomputational or memory costs, and provides further improvements on existing\ncontinual learning algorithms, particularly in a long-tail task sequence.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.02754v1.pdf",
        "similarity": 0.42952490871963084,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-03"
    },
    {
        "new_title": "Flexible infinite-width graph convolutional networks and the importance\n  of representation learning",
        "new_link": "http://arxiv.org/abs/2402.06525v1",
        "new_summary": "  A common theoretical approach to understanding neural networks is to take an\ninfinite-width limit, at which point the outputs become Gaussian process (GP)\ndistributed. This is known as a neural network Gaussian process (NNGP).\nHowever, the NNGP kernel is fixed, and tunable only through a small number of\nhyperparameters, eliminating any possibility of representation learning. This\ncontrasts with finite-width NNs, which are often believed to perform well\nprecisely because they are able to learn representations. Thus in simplifying\nNNs to make them theoretically tractable, NNGPs may eliminate precisely what\nmakes them work well (representation learning). This motivated us to understand\nwhether representation learning is necessary in a range of graph classification\ntasks. We develop a precise tool for this task, the graph convolutional deep\nkernel machine. This is very similar to an NNGP, in that it is an infinite\nwidth limit and uses kernels, but comes with a `knob' to control the amount of\nrepresentation learning. We found that representation learning is necessary (in\nthe sense that it gives dramatic performance improvements) in graph\nclassification tasks and heterophilous node classification tasks, but not in\nhomophilous node classification tasks.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.06525v1.pdf",
        "similarity": 0.42916871610627905,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-09"
    },
    {
        "new_title": "Deep-Graph-Sprints: Accelerated Representation Learning in\n  Continuous-Time Dynamic Graphs",
        "new_link": "http://arxiv.org/abs/2407.07712v2",
        "new_summary": "  Continuous-time dynamic graphs (CTDGs) are essential for modeling\ninterconnected, evolving systems. Traditional methods for extracting knowledge\nfrom these graphs often depend on feature engineering or deep learning. Feature\nengineering is limited by the manual and time-intensive nature of crafting\nfeatures, while deep learning approaches suffer from high inference latency,\nmaking them impractical for real-time applications. This paper introduces\nDeep-Graph-Sprints (DGS), a novel deep learning architecture designed for\nefficient representation learning on CTDGs with low-latency inference\nrequirements. We benchmark DGS against state-of-the-art feature engineering and\ngraph neural network methods using five diverse datasets. The results indicate\nthat DGS achieves competitive performance while improving inference speed up to\n12x compared to other deep learning approaches on our tested benchmarks. Our\nmethod effectively bridges the gap between deep representation learning and\nlow-latency application requirements for CTDGs.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.07712v2.pdf",
        "similarity": 0.4291659953683586,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-10"
    },
    {
        "new_title": "Towards Natural Machine Unlearning",
        "new_link": "http://arxiv.org/abs/2405.15495v1",
        "new_summary": "  Machine unlearning (MU) aims to eliminate information that has been learned\nfrom specific training data, namely forgetting data, from a pre-trained model.\nCurrently, the mainstream of existing MU methods involves modifying the\nforgetting data with incorrect labels and subsequently fine-tuning the model.\nWhile learning such incorrect information can indeed remove knowledge, the\nprocess is quite unnatural as the unlearning process undesirably reinforces the\nincorrect information and leads to over-forgetting. Towards more\n\\textit{natural} machine unlearning, we inject correct information from the\nremaining data to the forgetting samples when changing their labels. Through\npairing these adjusted samples with their labels, the model will tend to use\nthe injected correct information and naturally suppress the information meant\nto be forgotten. Albeit straightforward, such a first step towards natural\nmachine unlearning can significantly outperform current state-of-the-art\napproaches. In particular, our method substantially reduces the over-forgetting\nand leads to strong robustness to hyperparameters, making it a promising\ncandidate for practical machine unlearning.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.15495v1.pdf",
        "similarity": 0.42902041336637053,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-24"
    },
    {
        "new_title": "Large Language Model Agent for Hyper-Parameter Optimization",
        "new_link": "http://arxiv.org/abs/2402.01881v2",
        "new_summary": "  Hyperparameter optimization is critical in modern machine learning, requiring\nexpert knowledge, numerous trials, and high computational and human resources.\nDespite the advancements in Automated Machine Learning (AutoML), challenges in\nterms of trial efficiency, setup complexity, and interoperability still\npersist. To address these issues, we introduce a novel paradigm leveraging\nLarge Language Models (LLMs) to automate hyperparameter optimization across\ndiverse machine learning tasks, which is named AgentHPO (short for LLM\nAgent-based Hyperparameter Optimization). Specifically, AgentHPO processes the\ntask information autonomously, conducts experiments with specific\nhyperparameters (HPs), and iteratively optimizes them based on historical\ntrials. This human-like optimization process largely reduces the number of\nrequired trials, simplifies the setup process, and enhances interpretability\nand user trust, compared to traditional AutoML methods. Extensive empirical\nexperiments conducted on 12 representative machine-learning tasks indicate that\nAgentHPO not only matches but also often surpasses the best human trials in\nterms of performance while simultaneously providing explainable results.\nFurther analysis sheds light on the strategies employed by the LLM in\noptimizing these tasks, highlighting its effectiveness and adaptability in\nvarious scenarios.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.01881v2.pdf",
        "similarity": 0.4288546417692429,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-02"
    },
    {
        "new_title": "Integrating Hyperparameter Search into Model-Free AutoML with\n  Context-Free Grammars",
        "new_link": "http://arxiv.org/abs/2404.03419v2",
        "new_summary": "  Automated Machine Learning (AutoML) has become increasingly popular in recent\nyears due to its ability to reduce the amount of time and expertise required to\ndesign and develop machine learning systems. This is very important for the\npractice of machine learning, as it allows building strong baselines quickly,\nimproving the efficiency of the data scientists, and reducing the time to\nproduction. However, despite the advantages of AutoML, it faces several\nchallenges, such as defining the solutions space and exploring it efficiently.\nRecently, some approaches have been shown to be able to do it using tree-based\nsearch algorithms and context-free grammars. In particular, GramML presents a\nmodel-free reinforcement learning approach that leverages pipeline\nconfiguration grammars and operates using Monte Carlo tree search. However, one\nof the limitations of GramML is that it uses default hyperparameters, limiting\nthe search problem to finding optimal pipeline structures for the available\ndata preprocessors and models. In this work, we propose an extension to GramML\nthat supports larger search spaces including hyperparameter search. We\nevaluated the approach using an OpenML benchmark and found significant\nimprovements compared to other state-of-the-art techniques.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.03419v2.pdf",
        "similarity": 0.42852919691500824,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-04"
    },
    {
        "new_title": "Uncertainty Quantification with Deep Ensembles for 6D Object Pose\n  Estimation",
        "new_link": "http://arxiv.org/abs/2403.07741v2",
        "new_summary": "  The estimation of 6D object poses is a fundamental task in many computer\nvision applications. Particularly, in high risk scenarios such as human-robot\ninteraction, industrial inspection, and automation, reliable pose estimates are\ncrucial. In the last years, increasingly accurate and robust\ndeep-learning-based approaches for 6D object pose estimation have been\nproposed. Many top-performing methods are not end-to-end trainable but consist\nof multiple stages. In the context of deep uncertainty quantification, deep\nensembles are considered as state of the art since they have been proven to\nproduce well-calibrated and robust uncertainty estimates. However, deep\nensembles can only be applied to methods that can be trained end-to-end. In\nthis work, we propose a method to quantify the uncertainty of multi-stage 6D\nobject pose estimation approaches with deep ensembles. For the implementation,\nwe choose SurfEmb as representative, since it is one of the top-performing 6D\nobject pose estimation approaches in the BOP Challenge 2022. We apply\nestablished metrics and concepts for deep uncertainty quantification to\nevaluate the results. Furthermore, we propose a novel uncertainty calibration\nscore for regression tasks to quantify the quality of the estimated\nuncertainty.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.07741v2.pdf",
        "similarity": 0.4282891900586235,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-12"
    },
    {
        "new_title": "A Fair Evaluation of Various Deep Learning-Based Document Image\n  Binarization Approaches",
        "new_link": "http://arxiv.org/abs/2401.11831v1",
        "new_summary": "  Binarization of document images is an important pre-processing step in the\nfield of document analysis. Traditional image binarization techniques usually\nrely on histograms or local statistics to identify a valid threshold to\ndifferentiate between different aspects of the image. Deep learning techniques\nare able to generate binarized versions of the images by learning\ncontext-dependent features that are less error-prone to degradation typically\noccurring in document images. In recent years, many deep learning-based methods\nhave been developed for document binarization. But which one to choose? There\nhave been no studies that compare these methods rigorously. Therefore, this\nwork focuses on the evaluation of different deep learning-based methods under\nthe same evaluation protocol. We evaluate them on different Document Image\nBinarization Contest (DIBCO) datasets and obtain very heterogeneous results. We\nshow that the DE-GAN model was able to perform better compared to other models\nwhen evaluated on the DIBCO2013 dataset while DP-LinkNet performed best on the\nDIBCO2017 dataset. The 2-StageGAN performed best on the DIBCO2018 dataset while\nSauvolaNet outperformed the others on the DIBCO2019 challenge. Finally, we make\nthe code, all models and evaluation publicly available\n(https://github.com/RichSu95/Document_Binarization_Collection) to ensure\nreproducibility and simplify future binarization evaluations.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.11831v1.pdf",
        "similarity": 0.4281388885452682,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-22"
    },
    {
        "new_title": "Variational Stochastic Gradient Descent for Deep Neural Networks",
        "new_link": "http://arxiv.org/abs/2404.06549v1",
        "new_summary": "  Optimizing deep neural networks is one of the main tasks in successful deep\nlearning. Current state-of-the-art optimizers are adaptive gradient-based\noptimization methods such as Adam. Recently, there has been an increasing\ninterest in formulating gradient-based optimizers in a probabilistic framework\nfor better estimation of gradients and modeling uncertainties. Here, we propose\nto combine both approaches, resulting in the Variational Stochastic Gradient\nDescent (VSGD) optimizer. We model gradient updates as a probabilistic model\nand utilize stochastic variational inference (SVI) to derive an efficient and\neffective update rule. Further, we show how our VSGD method relates to other\nadaptive gradient-based optimizers like Adam. Lastly, we carry out experiments\non two image classification datasets and four deep neural network\narchitectures, where we show that VSGD outperforms Adam and SGD.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.06549v1.pdf",
        "similarity": 0.42809699108402915,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-09"
    },
    {
        "new_title": "Deep Implicit Optimization for Robust and Flexible Image Registration",
        "new_link": "http://arxiv.org/abs/2406.07361v1",
        "new_summary": "  Deep Learning in Image Registration (DLIR) methods have been tremendously\nsuccessful in image registration due to their speed and ability to incorporate\nweak label supervision at training time. However, DLIR methods forego many of\nthe benefits of classical optimization-based methods. The functional nature of\ndeep networks do not guarantee that the predicted transformation is a local\nminima of the registration objective, the representation of the transformation\n(displacement/velocity field/affine) is fixed, and the networks are not robust\nto domain shift. Our method aims to bridge this gap between classical and\nlearning methods by incorporating optimization as a layer in a deep network. A\ndeep network is trained to predict multi-scale dense feature images that are\nregistered using a black box iterative optimization solver. This optimal warp\nis then used to minimize image and label alignment errors. By implicitly\ndifferentiating end-to-end through an iterative optimization solver, our\nlearned features are registration and label-aware, and the warp functions are\nguaranteed to be local minima of the registration objective in the feature\nspace. Our framework shows excellent performance on in-domain datasets, and is\nagnostic to domain shift such as anisotropy and varying intensity profiles. For\nthe first time, our method allows switching between arbitrary transformation\nrepresentations (free-form to diffeomorphic) at test time with zero retraining.\nEnd-to-end feature learning also facilitates interpretability of features, and\nout-of-the-box promptability using additional label-fidelity terms at\ninference.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.07361v1.pdf",
        "similarity": 0.4277966623472296,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-11"
    },
    {
        "new_title": "Integrating Medical Imaging and Clinical Reports Using Multimodal Deep\n  Learning for Advanced Disease Analysis",
        "new_link": "http://arxiv.org/abs/2405.17459v1",
        "new_summary": "  In this paper, an innovative multi-modal deep learning model is proposed to\ndeeply integrate heterogeneous information from medical images and clinical\nreports. First, for medical images, convolutional neural networks were used to\nextract high-dimensional features and capture key visual information such as\nfocal details, texture and spatial distribution. Secondly, for clinical report\ntext, a two-way long and short-term memory network combined with an attention\nmechanism is used for deep semantic understanding, and key statements related\nto the disease are accurately captured. The two features interact and integrate\neffectively through the designed multi-modal fusion layer to realize the joint\nrepresentation learning of image and text. In the empirical study, we selected\na large medical image database covering a variety of diseases, combined with\ncorresponding clinical reports for model training and validation. The proposed\nmultimodal deep learning model demonstrated substantial superiority in the\nrealms of disease classification, lesion localization, and clinical description\ngeneration, as evidenced by the experimental results.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.17459v1.pdf",
        "similarity": 0.4276235140041028,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-23"
    },
    {
        "new_title": "Real-Time Detection and Analysis of Vehicles and Pedestrians using Deep\n  Learning",
        "new_link": "http://arxiv.org/abs/2404.08081v1",
        "new_summary": "  Computer vision, particularly vehicle and pedestrian identification is\ncritical to the evolution of autonomous driving, artificial intelligence, and\nvideo surveillance. Current traffic monitoring systems confront major\ndifficulty in recognizing small objects and pedestrians effectively in\nreal-time, posing a serious risk to public safety and contributing to traffic\ninefficiency. Recognizing these difficulties, our project focuses on the\ncreation and validation of an advanced deep-learning framework capable of\nprocessing complex visual input for precise, real-time recognition of cars and\npeople in a variety of environmental situations. On a dataset representing\ncomplicated urban settings, we trained and evaluated different versions of the\nYOLOv8 and RT-DETR models. The YOLOv8 Large version proved to be the most\neffective, especially in pedestrian recognition, with great precision and\nrobustness. The results, which include Mean Average Precision and recall rates,\ndemonstrate the model's ability to dramatically improve traffic monitoring and\nsafety. This study makes an important addition to real-time, reliable detection\nin computer vision, establishing new benchmarks for traffic management systems.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.08081v1.pdf",
        "similarity": 0.4275591985070746,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-11"
    },
    {
        "new_title": "A Multitask Deep Learning Model for Classification and Regression of\n  Hyperspectral Images: Application to the large-scale dataset",
        "new_link": "http://arxiv.org/abs/2407.16384v1",
        "new_summary": "  Multitask learning is a widely recognized technique in the field of computer\nvision and deep learning domain. However, it is still a research question in\nremote sensing, particularly for hyperspectral imaging. Moreover, most of the\nresearch in the remote sensing domain focuses on small and single-task-based\nannotated datasets, which limits the generalizability and scalability of the\ndeveloped models to more diverse and complex real-world scenarios. Thus, in\nthis study, we propose a multitask deep learning model designed to perform\nmultiple classification and regression tasks simultaneously on hyperspectral\nimages. We validated our approach on a large hyperspectral dataset called\nTAIGA, which contains 13 forest variables, including three categorical\nvariables and ten continuous variables with different biophysical parameters.\nWe design a sharing encoder and task-specific decoder network to streamline\nfeature learning while allowing each task-specific decoder to focus on the\nunique aspects of its respective task.\n  Additionally, a dense atrous pyramid pooling layer and attention network were\nintegrated to extract multi-scale contextual information and enable selective\ninformation processing by prioritizing task-specific features. Further, we\ncomputed multitask loss and optimized its parameters for the proposed framework\nto improve the model performance and efficiency across diverse tasks. A\ncomprehensive qualitative and quantitative analysis of the results shows that\nthe proposed method significantly outperforms other state-of-the-art methods.\nWe trained our model across 10 seeds/trials to ensure robustness. Our proposed\nmodel demonstrates higher mean performance while maintaining lower or\nequivalent variability. To make the work reproducible, the codes will be\navailable at\nhttps://github.com/Koushikey4596/Multitask-Deep-Learning-Model-for-Taiga-datatset.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.16384v1.pdf",
        "similarity": 0.4271779105959964,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-23"
    },
    {
        "new_title": "Predicting Learning Performance with Large Language Models: A Study in\n  Adult Literacy",
        "new_link": "http://arxiv.org/abs/2403.14668v1",
        "new_summary": "  Intelligent Tutoring Systems (ITSs) have significantly enhanced adult\nliteracy training, a key factor for societal participation, employment\nopportunities, and lifelong learning. Our study investigates the application of\nadvanced AI models, including Large Language Models (LLMs) like GPT-4, for\npredicting learning performance in adult literacy programs in ITSs. This\nresearch is motivated by the potential of LLMs to predict learning performance\nbased on its inherent reasoning and computational capabilities. By using\nreading comprehension datasets from the ITS, AutoTutor, we evaluate the\npredictive capabilities of GPT-4 versus traditional machine learning methods in\npredicting learning performance through five-fold cross-validation techniques.\nOur findings show that the GPT-4 presents the competitive predictive abilities\nwith traditional machine learning methods such as Bayesian Knowledge Tracing,\nPerformance Factor Analysis, Sparse Factor Analysis Lite (SPARFA-Lite), tensor\nfactorization and eXtreme Gradient Boosting (XGBoost). While XGBoost (trained\non local machine) outperforms GPT-4 in predictive accuracy, GPT-4-selected\nXGBoost and its subsequent tuning on the GPT-4 platform demonstrates superior\nperformance compared to local machine execution. Moreover, our investigation\ninto hyper-parameter tuning by GPT-4 versus grid-search suggests comparable\nperformance, albeit with less stability in the automated approach, using\nXGBoost as the case study. Our study contributes to the field by highlighting\nthe potential of integrating LLMs with traditional machine learning models to\nenhance predictive accuracy and personalize adult literacy education, setting a\nfoundation for future research in applying LLMs within ITSs.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.14668v1.pdf",
        "similarity": 0.42692146781492857,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-04"
    },
    {
        "new_title": "Schur's Positive-Definite Network: Deep Learning in the SPD cone with\n  structure",
        "new_link": "http://arxiv.org/abs/2406.09023v1",
        "new_summary": "  Estimating matrices in the symmetric positive-definite (SPD) cone is of\ninterest for many applications ranging from computer vision to graph learning.\nWhile there exist various convex optimization-based estimators, they remain\nlimited in expressivity due to their model-based approach. The success of deep\nlearning has thus led many to use neural networks to learn to estimate SPD\nmatrices in a data-driven fashion. For learning structured outputs, one\npromising strategy involves architectures designed by unrolling iterative\nalgorithms, which potentially benefit from inductive bias properties. However,\ndesigning correct unrolled architectures for SPD learning is difficult: they\neither do not guarantee that their output has all the desired properties, rely\non heavy computations, or are overly restrained to specific matrices which\nhinders their expressivity. In this paper, we propose a novel and generic\nlearning module with guaranteed SPD outputs called SpodNet, that also enables\nlearning a larger class of functions than existing approaches. Notably, it\nsolves the challenging task of learning jointly SPD and sparse matrices. Our\nexperiments demonstrate the versatility of SpodNet layers.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.09023v1.pdf",
        "similarity": 0.4266801487115715,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-13"
    },
    {
        "new_title": "Agriculture-Vision Challenge 2024 -- The Runner-Up Solution for\n  Agricultural Pattern Recognition via Class Balancing and Model Ensemble",
        "new_link": "http://arxiv.org/abs/2406.12271v1",
        "new_summary": "  The Agriculture-Vision Challenge at CVPR 2024 aims at leveraging semantic\nsegmentation models to produce pixel level semantic segmentation labels within\nregions of interest for multi-modality satellite images. It is one of the most\nfamous and competitive challenges for global researchers to break the boundary\nbetween computer vision and agriculture sectors. However, there is a serious\nclass imbalance problem in the agriculture-vision dataset, which hinders the\nsemantic segmentation performance. To solve this problem, firstly, we propose a\nmosaic data augmentation with a rare class sampling strategy to enrich\nlong-tail class samples. Secondly, we employ an adaptive class weight scheme to\nsuppress the contribution of the common classes while increasing the ones of\nrare classes. Thirdly, we propose a probability post-process to increase the\npredicted value of the rare classes. Our methodology achieved a mean\nIntersection over Union (mIoU) score of 0.547 on the test set, securing second\nplace in this challenge.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.12271v1.pdf",
        "similarity": 0.4266729456054349,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-18"
    },
    {
        "new_title": "GV-Rep: A Large-Scale Dataset for Genetic Variant Representation\n  Learning",
        "new_link": "http://arxiv.org/abs/2407.16940v1",
        "new_summary": "  Genetic variants (GVs) are defined as differences in the DNA sequences among\nindividuals and play a crucial role in diagnosing and treating genetic\ndiseases. The rapid decrease in next generation sequencing cost has led to an\nexponential increase in patient-level GV data. This growth poses a challenge\nfor clinicians who must efficiently prioritize patient-specific GVs and\nintegrate them with existing genomic databases to inform patient management. To\naddressing the interpretation of GVs, genomic foundation models (GFMs) have\nemerged. However, these models lack standardized performance assessments,\nleading to considerable variability in model evaluations. This poses the\nquestion: How effectively do deep learning methods classify unknown GVs and\nalign them with clinically-verified GVs? We argue that representation learning,\nwhich transforms raw data into meaningful feature spaces, is an effective\napproach for addressing both indexing and classification challenges. We\nintroduce a large-scale Genetic Variant dataset, named GV-Rep, featuring\nvariable-length contexts and detailed annotations, designed for deep learning\nmodels to learn GV representations across various traits, diseases, tissue\ntypes, and experimental contexts. Our contributions are three-fold: (i)\nConstruction of a comprehensive dataset with 7 million records, each labeled\nwith characteristics of the corresponding variants, alongside additional data\nfrom 17,548 gene knockout tests across 1,107 cell types, 1,808 variant\ncombinations, and 156 unique clinically verified GVs from real-world patients.\n(ii) Analysis of the structure and properties of the dataset. (iii)\nExperimentation of the dataset with pre-trained GFMs. The results show a\nsignificant gap between GFMs current capabilities and accurate GV\nrepresentation. We hope this dataset will help advance genomic deep learning to\nbridge this gap.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.16940v1.pdf",
        "similarity": 0.4263510300967984,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-24"
    },
    {
        "new_title": "On the hardness of learning under symmetries",
        "new_link": "http://arxiv.org/abs/2401.01869v1",
        "new_summary": "  We study the problem of learning equivariant neural networks via gradient\ndescent. The incorporation of known symmetries (\"equivariance\") into neural\nnets has empirically improved the performance of learning pipelines, in domains\nranging from biology to computer vision. However, a rich yet separate line of\nlearning theoretic research has demonstrated that actually learning shallow,\nfully-connected (i.e. non-symmetric) networks has exponential complexity in the\ncorrelational statistical query (CSQ) model, a framework encompassing gradient\ndescent. In this work, we ask: are known problem symmetries sufficient to\nalleviate the fundamental hardness of learning neural nets with gradient\ndescent? We answer this question in the negative. In particular, we give lower\nbounds for shallow graph neural networks, convolutional networks, invariant\npolynomials, and frame-averaged networks for permutation subgroups, which all\nscale either superpolynomially or exponentially in the relevant input\ndimension. Therefore, in spite of the significant inductive bias imparted via\nsymmetry, actually learning the complete classes of functions represented by\nequivariant neural networks via gradient descent remains hard.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.01869v1.pdf",
        "similarity": 0.4262309702796988,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-03"
    },
    {
        "new_title": "Dark Transformer: A Video Transformer for Action Recognition in the Dark",
        "new_link": "http://arxiv.org/abs/2407.12805v1",
        "new_summary": "  Recognizing human actions in adverse lighting conditions presents significant\nchallenges in computer vision, with wide-ranging applications in visual\nsurveillance and nighttime driving. Existing methods tackle action recognition\nand dark enhancement separately, limiting the potential for end-to-end learning\nof spatiotemporal representations for video action classification. This paper\nintroduces Dark Transformer, a novel video transformer-based approach for\naction recognition in low-light environments. Dark Transformer leverages\nspatiotemporal self-attention mechanisms in cross-domain settings to enhance\ncross-domain action recognition. By extending video transformers to learn\ncross-domain knowledge, Dark Transformer achieves state-of-the-art performance\non benchmark action recognition datasets, including InFAR, XD145, and ARID. The\nproposed approach demonstrates significant promise in addressing the challenges\nof action recognition in adverse lighting conditions, offering practical\nimplications for real-world applications.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.12805v1.pdf",
        "similarity": 0.4261346582374544,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-25"
    },
    {
        "new_title": "Deep-PE: A Learning-Based Pose Evaluator for Point Cloud Registration",
        "new_link": "http://arxiv.org/abs/2405.16085v1",
        "new_summary": "  In the realm of point cloud registration, the most prevalent pose evaluation\napproaches are statistics-based, identifying the optimal transformation by\nmaximizing the number of consistent correspondences. However, registration\nrecall decreases significantly when point clouds exhibit a low overlap rate,\ndespite efforts in designing feature descriptors and establishing\ncorrespondences. In this paper, we introduce Deep-PE, a lightweight,\nlearning-based pose evaluator designed to enhance the accuracy of pose\nselection, especially in challenging point cloud scenarios with low overlap.\nOur network incorporates a Pose-Aware Attention (PAA) module to simulate and\nlearn the alignment status of point clouds under various candidate poses,\nalongside a Pose Confidence Prediction (PCP) module that predicts the\nlikelihood of successful registration. These two modules facilitate the\nlearning of both local and global alignment priors. Extensive tests across\nmultiple benchmarks confirm the effectiveness of Deep-PE. Notably, on 3DLoMatch\nwith a low overlap rate, Deep-PE significantly outperforms state-of-the-art\nmethods by at least 8% and 11% in registration recall under handcrafted FPFH\nand learning-based FCGF descriptors, respectively. To the best of our\nknowledge, this is the first study to utilize deep learning to select the\noptimal pose without the explicit need for input correspondences.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.16085v1.pdf",
        "similarity": 0.42581903844877567,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-25"
    },
    {
        "new_title": "Improving deep learning with prior knowledge and cognitive models: A\n  survey on enhancing explainability, adversarial robustness and zero-shot\n  learning",
        "new_link": "http://arxiv.org/abs/2403.07078v1",
        "new_summary": "  We review current and emerging knowledge-informed and brain-inspired\ncognitive systems for realizing adversarial defenses, eXplainable Artificial\nIntelligence (XAI), and zero-shot or few-short learning. Data-driven deep\nlearning models have achieved remarkable performance and demonstrated\ncapabilities surpassing human experts in many applications. Yet, their\ninability to exploit domain knowledge leads to serious performance limitations\nin practical applications. In particular, deep learning systems are exposed to\nadversarial attacks, which can trick them into making glaringly incorrect\ndecisions. Moreover, complex data-driven models typically lack interpretability\nor explainability, i.e., their decisions cannot be understood by human\nsubjects. Furthermore, models are usually trained on standard datasets with a\nclosed-world assumption. Hence, they struggle to generalize to unseen cases\nduring inference in practical open-world environments, thus, raising the zero-\nor few-shot generalization problem. Although many conventional solutions exist,\nexplicit domain knowledge, brain-inspired neural network and cognitive\narchitectures offer powerful new dimensions towards alleviating these problems.\nPrior knowledge is represented in appropriate forms and incorporated in deep\nlearning frameworks to improve performance. Brain-inspired cognition methods\nuse computational models that mimic the human mind to enhance intelligent\nbehavior in artificial agents and autonomous robots. Ultimately, these models\nachieve better explainability, higher adversarial robustness and data-efficient\nlearning, and can, in turn, provide insights for cognitive science and\nneuroscience-that is, to deepen human understanding on how the brain works in\ngeneral, and how it handles these problems.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.07078v1.pdf",
        "similarity": 0.4258057466002269,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-11"
    },
    {
        "new_title": "Multi-threshold Deep Metric Learning for Facial Expression Recognition",
        "new_link": "http://arxiv.org/abs/2406.16434v1",
        "new_summary": "  Effective expression feature representations generated by a triplet-based\ndeep metric learning are highly advantageous for facial expression recognition\n(FER). The performance of triplet-based deep metric learning is contingent upon\nidentifying the best threshold for triplet loss. Threshold validation, however,\nis tough and challenging, as the ideal threshold changes among datasets and\neven across classes within the same dataset. In this paper, we present the\nmulti-threshold deep metric learning technique, which not only avoids the\ndifficult threshold validation but also vastly increases the capacity of\ntriplet loss learning to construct expression feature representations. We find\nthat each threshold of the triplet loss intrinsically determines a distinctive\ndistribution of inter-class variations and corresponds, thus, to a unique\nexpression feature representation. Therefore, rather than selecting a single\noptimal threshold from a valid threshold range, we thoroughly sample thresholds\nacross the range, allowing the representation characteristics manifested by\nthresholds within the range to be fully extracted and leveraged for FER. To\nrealize this approach, we partition the embedding layer of the deep metric\nlearning network into a collection of slices and model training these embedding\nslices as an end-to-end multi-threshold deep metric learning problem. Each\nembedding slice corresponds to a sample threshold and is learned by enforcing\nthe corresponding triplet loss, yielding a set of distinct expression features,\none for each embedding slice. It makes the embedding layer, which is composed\nof a set of slices, a more informative and discriminative feature, hence\nenhancing the FER accuracy. Extensive evaluations demonstrate the superior\nperformance of the proposed approach on both posed and spontaneous facial\nexpression datasets.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.16434v1.pdf",
        "similarity": 0.4257939894259801,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-24"
    },
    {
        "new_title": "Deep Learning-based Image and Video Inpainting: A Survey",
        "new_link": "http://arxiv.org/abs/2401.03395v1",
        "new_summary": "  Image and video inpainting is a classic problem in computer vision and\ncomputer graphics, aiming to fill in the plausible and realistic content in the\nmissing areas of images and videos. With the advance of deep learning, this\nproblem has achieved significant progress recently. The goal of this paper is\nto comprehensively review the deep learning-based methods for image and video\ninpainting. Specifically, we sort existing methods into different categories\nfrom the perspective of their high-level inpainting pipeline, present different\ndeep learning architectures, including CNN, VAE, GAN, diffusion models, etc.,\nand summarize techniques for module design. We review the training objectives\nand the common benchmark datasets. We present evaluation metrics for low-level\npixel and high-level perceptional similarity, conduct a performance evaluation,\nand discuss the strengths and weaknesses of representative inpainting methods.\nWe also discuss related real-world applications. Finally, we discuss open\nchallenges and suggest potential future research directions.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.03395v1.pdf",
        "similarity": 0.42522150117661506,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-07"
    },
    {
        "new_title": "Efficient Learning of Fuzzy Logic Systems for Large-Scale Data Using\n  Deep Learning",
        "new_link": "http://arxiv.org/abs/2404.12792v1",
        "new_summary": "  Type-1 and Interval Type-2 (IT2) Fuzzy Logic Systems (FLS) excel in handling\nuncertainty alongside their parsimonious rule-based structure. Yet, in learning\nlarge-scale data challenges arise, such as the curse of dimensionality and\ntraining complexity of FLSs. The complexity is due mainly to the constraints to\nbe satisfied as the learnable parameters define FSs and the complexity of the\ncenter of the sets calculation method, especially of IT2-FLSs. This paper\nexplicitly focuses on the learning problem of FLSs and presents a\ncomputationally efficient learning method embedded within the realm of Deep\nLearning (DL). The proposed method tackles the learning challenges of FLSs by\npresenting computationally efficient implementations of FLSs, thereby\nminimizing training time while leveraging mini-batched DL optimizers and\nautomatic differentiation provided within the DL frameworks. We illustrate the\nefficiency of the DL framework for FLSs on benchmark datasets.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.12792v1.pdf",
        "similarity": 0.42480885247792066,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-19"
    },
    {
        "new_title": "A Deep Learning Architectures for Kidney Disease Classification",
        "new_link": "http://arxiv.org/abs/2403.15895v1",
        "new_summary": "  Deep learning has become an extremely powerful tool for complex tasks such as\nimage classification and segmentation. The medical industry often lacks\nhigh-quality, balanced datasets, which can be a challenge for deep learning\nalgorithms that need sufficiently large amounts of data to train and increase\ntheir performance. This is especially important in the context of kidney issues\nsuch as for stones, cysts and tumors. We used deep learning models for this\nstudy to classify or detect several types of kidney diseases. We use different\nclassification models, such as VGG-19, (CNNs) Convolutional Neural Networks,\nResNet-101, VGG-16, ResNet-50, and DenseNet-169, which can be enhanced through\ntechniques such as classification, segmentation, and transfer learning. These\nalgorithms can help improve model accuracy by allowing them to learn from\nmultiple datasets. This technique has the potential to revolutionize the\ndiagnosis and treatment of kidney problems as it enables more accurate and\neffective classification of CT-scan images. This may ultimately lead to better\npatient outcomes and improved overall health outcomes.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.15895v1.pdf",
        "similarity": 0.42443595597804756,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-23"
    },
    {
        "new_title": "Informed Meta-Learning",
        "new_link": "http://arxiv.org/abs/2402.16105v3",
        "new_summary": "  In noisy and low-data regimes prevalent in real-world applications, a key\nchallenge of machine learning lies in effectively incorporating inductive\nbiases that promote data efficiency and robustness. Meta-learning and informed\nML stand out as two approaches for incorporating prior knowledge into ML\npipelines. While the former relies on a purely data-driven source of priors,\nthe latter is guided by prior domain knowledge. In this paper, we formalise a\nhybrid paradigm, informed meta-learning, facilitating the incorporation of\npriors from unstructured knowledge representations, such as natural language;\nthus, unlocking complementarity in cross-task knowledge sharing of humans and\nmachines. We establish the foundational components of informed meta-learning\nand present a concrete instantiation of this framework--the Informed Neural\nProcess. Through a series of experiments, we demonstrate the potential benefits\nof informed meta-learning in improving data efficiency, robustness to\nobservational noise and task distribution shifts.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.16105v3.pdf",
        "similarity": 0.42440810324164613,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-25"
    },
    {
        "new_title": "Interpretable Modeling of Deep Reinforcement Learning Driven Scheduling",
        "new_link": "http://arxiv.org/abs/2403.16293v1",
        "new_summary": "  In the field of high-performance computing (HPC), there has been recent\nexploration into the use of deep reinforcement learning for cluster scheduling\n(DRL scheduling), which has demonstrated promising outcomes. However, a\nsignificant challenge arises from the lack of interpretability in deep neural\nnetworks (DNN), rendering them as black-box models to system managers. This\nlack of model interpretability hinders the practical deployment of DRL\nscheduling. In this work, we present a framework called IRL (Interpretable\nReinforcement Learning) to address the issue of interpretability of DRL\nscheduling. The core idea is to interpret DNN (i.e., the DRL policy) as a\ndecision tree by utilizing imitation learning. Unlike DNN, decision tree models\nare non-parametric and easily comprehensible to humans. To extract an effective\nand efficient decision tree, IRL incorporates the Dataset Aggregation (DAgger)\nalgorithm and introduces the notion of critical state to prune the derived\ndecision tree. Through trace-based experiments, we demonstrate that IRL is\ncapable of converting a black-box DNN policy into an interpretable rulebased\ndecision tree while maintaining comparable scheduling performance.\nAdditionally, IRL can contribute to the setting of rewards in DRL scheduling.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.16293v1.pdf",
        "similarity": 0.4242927922195424,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-24"
    },
    {
        "new_title": "Nomic Embed Vision: Expanding the Latent Space",
        "new_link": "http://arxiv.org/abs/2406.18587v1",
        "new_summary": "  This technical report describes the training of nomic-embed-vision, a highly\nperformant, open-code, open-weights image embedding model that shares the same\nlatent space as nomic-embed-text. Together, nomic-embed-vision and\nnomic-embed-text form the first unified latent space to achieve high\nperformance across vision, language, and multimodal tasks.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.18587v1.pdf",
        "similarity": 0.42404446956367814,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-06"
    },
    {
        "new_title": "Video-Based Autism Detection with Deep Learning",
        "new_link": "http://arxiv.org/abs/2402.16774v2",
        "new_summary": "  Individuals with Autism Spectrum Disorder (ASD) often experience challenges\nin health, communication, and sensory processing; therefore, early diagnosis is\nnecessary for proper treatment and care. In this work, we consider the problem\nof detecting or classifying ASD children to aid medical professionals in early\ndiagnosis. We develop a deep learning model that analyzes video clips of\nchildren reacting to sensory stimuli, with the intent of capturing key\ndifferences in reactions and behavior between ASD and non-ASD participants.\nUnlike many recent studies in ASD classification with MRI data, which require\nexpensive specialized equipment, our method utilizes a powerful but relatively\naffordable GPU, a standard computer setup, and a video camera for inference.\nResults show that our model effectively generalizes and understands key\ndifferences in the distinct movements of the children. It is noteworthy that\nour model exhibits successful classification performance despite the limited\namount of data for a deep learning problem and limited temporal information\navailable for learning, even with the motion artifacts.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.16774v2.pdf",
        "similarity": 0.42360801498165457,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-26"
    },
    {
        "new_title": "A data-centric approach to class-specific bias in image data\n  augmentation",
        "new_link": "http://arxiv.org/abs/2403.04120v1",
        "new_summary": "  Data augmentation (DA) enhances model generalization in computer vision but\nmay introduce biases, impacting class accuracy unevenly. Our study extends this\ninquiry, examining DA's class-specific bias across various datasets, including\nthose distinct from ImageNet, through random cropping. We evaluated this\nphenomenon with ResNet50, EfficientNetV2S, and SWIN ViT, discovering that while\nresidual models showed similar bias effects, Vision Transformers exhibited\ngreater robustness or altered dynamics. This suggests a nuanced approach to\nmodel selection, emphasizing bias mitigation. We also refined a \"data\naugmentation robustness scouting\" method to manage DA-induced biases more\nefficiently, reducing computational demands significantly (training 112 models\ninstead of 1860; a reduction of factor 16.2) while still capturing essential\nbias trends.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.04120v1.pdf",
        "similarity": 0.4229861174751515,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-07"
    },
    {
        "new_title": "Deep ContourFlow: Advancing Active Contours with Deep Learning",
        "new_link": "http://arxiv.org/abs/2407.10696v1",
        "new_summary": "  This paper introduces a novel approach that combines unsupervised active\ncontour models with deep learning for robust and adaptive image segmentation.\nIndeed, traditional active contours, provide a flexible framework for contour\nevolution and learning offers the capacity to learn intricate features and\npatterns directly from raw data. Our proposed methodology leverages the\nstrengths of both paradigms, presenting a framework for both unsupervised and\none-shot approaches for image segmentation. It is capable of capturing complex\nobject boundaries without the need for extensive labeled training data. This is\nparticularly required in histology, a field facing a significant shortage of\nannotations due to the challenging and time-consuming nature of the annotation\nprocess. We illustrate and compare our results to state of the art methods on a\nhistology dataset and show significant improvements.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.10696v1.pdf",
        "similarity": 0.42228468520820145,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-15"
    },
    {
        "new_title": "Deep Learning for Multi-Label Learning: A Comprehensive Survey",
        "new_link": "http://arxiv.org/abs/2401.16549v3",
        "new_summary": "  Multi-label learning is a rapidly growing research area that aims to predict\nmultiple labels from a single input data point. In the era of big data, tasks\ninvolving multi-label classification (MLC) or ranking present significant and\nintricate challenges, capturing considerable attention in diverse domains.\nInherent difficulties in MLC include dealing with high-dimensional data,\naddressing label correlations, and handling partial labels, for which\nconventional methods prove ineffective. Recent years have witnessed a notable\nincrease in adopting deep learning (DL) techniques to address these challenges\nmore effectively in MLC. Notably, there is a burgeoning effort to harness the\nrobust learning capabilities of DL for improved modelling of label dependencies\nand other challenges in MLC. However, it is noteworthy that comprehensive\nstudies specifically dedicated to DL for multi-label learning are limited.\nThus, this survey aims to thoroughly review recent progress in DL for\nmulti-label learning, along with a summary of open research problems in MLC.\nThe review consolidates existing research efforts in DL for MLC,including deep\nneural networks, transformers, autoencoders, and convolutional and recurrent\narchitectures. Finally, the study presents a comparative analysis of the\nexisting methods to provide insightful observations and stimulate future\nresearch directions in this domain.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.16549v3.pdf",
        "similarity": 0.4209111340046627,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-29"
    },
    {
        "new_title": "Towards Point Cloud Compression for Machine Perception: A Simple and\n  Strong Baseline by Learning the Octree Depth Level Predictor",
        "new_link": "http://arxiv.org/abs/2406.00791v1",
        "new_summary": "  Point cloud compression has garnered significant interest in computer vision.\nHowever, existing algorithms primarily cater to human vision, while most point\ncloud data is utilized for machine vision tasks. To address this, we propose a\npoint cloud compression framework that simultaneously handles both human and\nmachine vision tasks. Our framework learns a scalable bit-stream, using only\nsubsets for different machine vision tasks to save bit-rate, while employing\nthe entire bit-stream for human vision tasks. Building on mainstream\noctree-based frameworks like VoxelContext-Net, OctAttention, and G-PCC, we\nintroduce a new octree depth-level predictor. This predictor adaptively\ndetermines the optimal depth level for each octree constructed from a point\ncloud, controlling the bit-rate for machine vision tasks. For simpler tasks\n(\\textit{e.g.}, classification) or objects/scenarios, we use fewer depth levels\nwith fewer bits, saving bit-rate. Conversely, for more complex tasks\n(\\textit{e.g}., segmentation) or objects/scenarios, we use deeper depth levels\nwith more bits to enhance performance. Experimental results on various datasets\n(\\textit{e.g}., ModelNet10, ModelNet40, ShapeNet, ScanNet, and KITTI) show that\nour point cloud compression approach improves performance for machine vision\ntasks without compromising human vision quality.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.00791v1.pdf",
        "similarity": 0.4204721665835557,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-02"
    },
    {
        "new_title": "Machine Learning vs Deep Learning: The Generalization Problem",
        "new_link": "http://arxiv.org/abs/2403.01621v1",
        "new_summary": "  The capacity to generalize beyond the range of training data is a pivotal\nchallenge, often synonymous with a model's utility and robustness. This study\ninvestigates the comparative abilities of traditional machine learning (ML)\nmodels and deep learning (DL) algorithms in terms of extrapolation -- a more\nchallenging aspect of generalization because it requires the model to make\ninferences about data points that lie outside the domain it has been trained\non. We present an empirical analysis where both ML and DL models are trained on\nan exponentially growing function and then tested on values outside the\ntraining domain. The choice of this function allows us to distinctly showcase\nthe divergence in performance when models are required to predict beyond the\nscope of their training data. Our findings suggest that deep learning models\npossess inherent capabilities to generalize beyond the training scope, an\nessential feature for real-world applications where data is often incomplete or\nextends beyond the observed range. This paper argues for a nuanced\nunderstanding of the structural differences between ML and DL models, with an\nemphasis on the implications for both theoretical research and practical\ndeployment.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.01621v1.pdf",
        "similarity": 0.4203215227333178,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-03"
    },
    {
        "new_title": "Vision Mamba for Classification of Breast Ultrasound Images",
        "new_link": "http://arxiv.org/abs/2407.03552v1",
        "new_summary": "  Mamba-based models, VMamba and Vim, are a recent family of vision encoders\nthat offer promising performance improvements in many computer vision tasks.\nThis paper compares Mamba-based models with traditional Convolutional Neural\nNetworks (CNNs) and Vision Transformers (ViTs) using the breast ultrasound BUSI\nand B datasets. Our evaluation, which includes multiple runs of experiments and\nstatistical significance analysis, demonstrates that Mamba-based architectures\nfrequently outperform CNN and ViT models with statistically significant\nresults. These Mamba-based models effectively capture long-range dependencies\nwhile maintaining inductive biases, making them suitable for applications with\nlimited data.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.03552v1.pdf",
        "similarity": 0.42023578983354193,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-04"
    },
    {
        "new_title": "MaskFi: Unsupervised Learning of WiFi and Vision Representations for\n  Multimodal Human Activity Recognition",
        "new_link": "http://arxiv.org/abs/2402.19258v1",
        "new_summary": "  Human activity recognition (HAR) has been playing an increasingly important\nrole in various domains such as healthcare, security monitoring, and metaverse\ngaming. Though numerous HAR methods based on computer vision have been\ndeveloped to show prominent performance, they still suffer from poor robustness\nin adverse visual conditions in particular low illumination, which motivates\nWiFi-based HAR to serve as a good complementary modality. Existing solutions\nusing WiFi and vision modalities rely on massive labeled data that are very\ncumbersome to collect. In this paper, we propose a novel unsupervised\nmultimodal HAR solution, MaskFi, that leverages only unlabeled video and WiFi\nactivity data for model training. We propose a new algorithm, masked\nWiFi-vision modeling (MI2M), that enables the model to learn cross-modal and\nsingle-modal features by predicting the masked sections in representation\nlearning. Benefiting from our unsupervised learning procedure, the network\nrequires only a small amount of annotated data for finetuning and can adapt to\nthe new environment with better performance. We conduct extensive experiments\non two WiFi-vision datasets collected in-house, and our method achieves human\nactivity recognition and human identification in terms of both robustness and\naccuracy.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.19258v1.pdf",
        "similarity": 0.41998691946065186,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-29"
    },
    {
        "new_title": "Open-Source Web Service with Morphological Dictionary-Supplemented Deep\n  Learning for Morphosyntactic Analysis of Czech",
        "new_link": "http://arxiv.org/abs/2406.12422v1",
        "new_summary": "  We present an open-source web service for Czech morphosyntactic analysis. The\nsystem combines a deep learning model with rescoring by a high-precision\nmorphological dictionary at inference time. We show that our hybrid method\nsurpasses two competitive baselines: While the deep learning model ensures\ngeneralization for out-of-vocabulary words and better disambiguation, an\nimprovement over an existing morphological analyser MorphoDiTa, at the same\ntime, the deep learning model benefits from inference-time guidance of a\nmanually curated morphological dictionary. We achieve 50% error reduction in\nlemmatization and 58% error reduction in POS tagging over MorphoDiTa, while\nalso offering dependency parsing. The model is trained on one of the currently\nlargest Czech morphosyntactic corpora, the PDT-C 1.0, with the trained models\navailable at https://hdl.handle.net/11234/1-5293. We provide the tool as a web\nservice deployed at https://lindat.mff.cuni.cz/services/udpipe/. The source\ncode is available at GitHub (https://github.com/ufal/udpipe/tree/udpipe-2),\nalong with a Python client for a simple use. The documentation for the models\ncan be found at https://ufal.mff.cuni.cz/udpipe/2/models#czech_pdtc1.0_model.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.12422v1.pdf",
        "similarity": 0.41994444512700574,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-18"
    },
    {
        "new_title": "Balancing Continual Learning and Fine-tuning for Human Activity\n  Recognition",
        "new_link": "http://arxiv.org/abs/2401.02255v1",
        "new_summary": "  Wearable-based Human Activity Recognition (HAR) is a key task in\nhuman-centric machine learning due to its fundamental understanding of human\nbehaviours. Due to the dynamic nature of human behaviours, continual learning\npromises HAR systems that are tailored to users' needs. However, because of the\ndifficulty in collecting labelled data with wearable sensors, existing\napproaches that focus on supervised continual learning have limited\napplicability, while unsupervised continual learning methods only handle\nrepresentation learning while delaying classifier training to a later stage.\nThis work explores the adoption and adaptation of CaSSLe, a continual\nself-supervised learning model, and Kaizen, a semi-supervised continual\nlearning model that balances representation learning and down-stream\nclassification, for the task of wearable-based HAR. These schemes re-purpose\ncontrastive learning for knowledge retention and, Kaizen combines that with\nself-training in a unified scheme that can leverage unlabelled and labelled\ndata for continual learning. In addition to comparing state-of-the-art\nself-supervised continual learning schemes, we further investigated the\nimportance of different loss terms and explored the trade-off between knowledge\nretention and learning from new tasks. In particular, our extensive evaluation\ndemonstrated that the use of a weighting factor that reflects the ratio between\nlearned and new classes achieves the best overall trade-off in continual\nlearning.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.02255v1.pdf",
        "similarity": 0.41993542142591395,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-04"
    },
    {
        "new_title": "Steerable Transformers",
        "new_link": "http://arxiv.org/abs/2405.15932v1",
        "new_summary": "  In this work we introduce Steerable Transformers, an extension of the Vision\nTransformer mechanism that maintains equivariance to the special Euclidean\ngroup $\\mathrm{SE}(d)$. We propose an equivariant attention mechanism that\noperates on features extracted by steerable convolutions. Operating in Fourier\nspace, our network utilizes Fourier space non-linearities. Our experiments in\nboth two and three dimensions show that adding a steerable transformer encoder\nlayer to a steerable convolution network enhances performance.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.15932v1.pdf",
        "similarity": 0.419849190018964,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-24"
    },
    {
        "new_title": "MMIS: Multimodal Dataset for Interior Scene Visual Generation and\n  Recognition",
        "new_link": "http://arxiv.org/abs/2407.05980v1",
        "new_summary": "  We introduce MMIS, a novel dataset designed to advance MultiModal Interior\nScene generation and recognition. MMIS consists of nearly 160,000 images. Each\nimage within the dataset is accompanied by its corresponding textual\ndescription and an audio recording of that description, providing rich and\ndiverse sources of information for scene generation and recognition. MMIS\nencompasses a wide range of interior spaces, capturing various styles, layouts,\nand furnishings. To construct this dataset, we employed careful processes\ninvolving the collection of images, the generation of textual descriptions, and\ncorresponding speech annotations. The presented dataset contributes to research\nin multi-modal representation learning tasks such as image generation,\nretrieval, captioning, and classification.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.05980v1.pdf",
        "similarity": 0.4198088344048786,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-08"
    },
    {
        "new_title": "Automatic Discovery of Visual Circuits",
        "new_link": "http://arxiv.org/abs/2404.14349v1",
        "new_summary": "  To date, most discoveries of network subcomponents that implement\nhuman-interpretable computations in deep vision models have involved close\nstudy of single units and large amounts of human labor. We explore scalable\nmethods for extracting the subgraph of a vision model's computational graph\nthat underlies recognition of a specific visual concept. We introduce a new\nmethod for identifying these subgraphs: specifying a visual concept using a few\nexamples, and then tracing the interdependence of neuron activations across\nlayers, or their functional connectivity. We find that our approach extracts\ncircuits that causally affect model output, and that editing these circuits can\ndefend large pretrained models from adversarial attacks.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.14349v1.pdf",
        "similarity": 0.4192095038917931,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-22"
    },
    {
        "new_title": "Convolutional Initialization for Data-Efficient Vision Transformers",
        "new_link": "http://arxiv.org/abs/2401.12511v1",
        "new_summary": "  Training vision transformer networks on small datasets poses challenges. In\ncontrast, convolutional neural networks (CNNs) can achieve state-of-the-art\nperformance by leveraging their architectural inductive bias. In this paper, we\ninvestigate whether this inductive bias can be reinterpreted as an\ninitialization bias within a vision transformer network. Our approach is\nmotivated by the finding that random impulse filters can achieve almost\ncomparable performance to learned filters in CNNs. We introduce a novel\ninitialization strategy for transformer networks that can achieve comparable\nperformance to CNNs on small datasets while preserving its architectural\nflexibility.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.12511v1.pdf",
        "similarity": 0.4183658141223002,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-23"
    },
    {
        "new_title": "On-device Online Learning and Semantic Management of TinyML Systems",
        "new_link": "http://arxiv.org/abs/2405.07601v2",
        "new_summary": "  Recent advances in Tiny Machine Learning (TinyML) empower low-footprint\nembedded devices for real-time on-device Machine Learning. While many\nacknowledge the potential benefits of TinyML, its practical implementation\npresents unique challenges. This study aims to bridge the gap between\nprototyping single TinyML models and developing reliable TinyML systems in\nproduction: (1) Embedded devices operate in dynamically changing conditions.\nExisting TinyML solutions primarily focus on inference, with models trained\noffline on powerful machines and deployed as static objects. However, static\nmodels may underperform in the real world due to evolving input data\ndistributions. We propose online learning to enable training on constrained\ndevices, adapting local models towards the latest field conditions. (2)\nNevertheless, current on-device learning methods struggle with heterogeneous\ndeployment conditions and the scarcity of labeled data when applied across\nnumerous devices. We introduce federated meta-learning incorporating online\nlearning to enhance model generalization, facilitating rapid learning. This\napproach ensures optimal performance among distributed devices by knowledge\nsharing. (3) Moreover, TinyML's pivotal advantage is widespread adoption.\nEmbedded devices and TinyML models prioritize extreme efficiency, leading to\ndiverse characteristics ranging from memory and sensors to model architectures.\nGiven their diversity and non-standardized representations, managing these\nresources becomes challenging as TinyML systems scale up. We present semantic\nmanagement for the joint management of models and devices at scale. We\ndemonstrate our methods through a basic regression example and then assess them\nin three real-world TinyML applications: handwritten character image\nclassification, keyword audio classification, and smart building presence\ndetection, confirming our approaches' effectiveness.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.07601v2.pdf",
        "similarity": 0.4182773256054299,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-13"
    },
    {
        "new_title": "Pick-or-Mix: Dynamic Channel Sampling for ConvNets",
        "new_link": "http://arxiv.org/abs/2406.10935v1",
        "new_summary": "  Channel pruning approaches for convolutional neural networks (ConvNets)\ndeactivate the channels, statically or dynamically, and require special\nimplementation. In addition, channel squeezing in representative ConvNets is\ncarried out via 1x1 convolutions which dominates a large portion of\ncomputations and network parameters. Given these challenges, we propose an\neffective multi-purpose module for dynamic channel sampling, namely Pick-or-Mix\n(PiX), which does not require special implementation. PiX divides a set of\nchannels into subsets and then picks from them, where the picking decision is\ndynamically made per each pixel based on the input activations. We plug PiX\ninto prominent ConvNet architectures and verify its multi-purpose utilities.\nAfter replacing 1x1 channel squeezing layers in ResNet with PiX, the network\nbecomes 25% faster without losing accuracy. We show that PiX allows ConvNets to\nlearn better data representation than widely adopted approaches to enhance\nnetworks' representation power (e.g., SE, CBAM, AFF, SKNet, and DWP). We also\nshow that PiX achieves state-of-the-art performance on network downscaling and\ndynamic channel pruning applications.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.10935v1.pdf",
        "similarity": 0.41798640355663463,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-16"
    },
    {
        "new_title": "From Structured to Unstructured:A Comparative Analysis of Computer\n  Vision and Graph Models in solving Mesh-based PDEs",
        "new_link": "http://arxiv.org/abs/2406.00081v1",
        "new_summary": "  This article investigates the application of computer vision and graph-based\nmodels in solving mesh-based partial differential equations within\nhigh-performance computing environments. Focusing on structured, graded\nstructured, and unstructured meshes, the study compares the performance and\ncomputational efficiency of three computer vision-based models against three\ngraph-based models across three data\\-sets. The research aims to identify the\nmost suitable models for different mesh topographies, particularly highlighting\nthe exploration of graded meshes, a less studied area. Results demonstrate that\ncomputer vision-based models, notably U-Net, outperform the graph models in\nprediction performance and efficiency in two (structured and graded) out of\nthree mesh topographies. The study also reveals the unexpected effectiveness of\ncomputer vision-based models in handling unstructured meshes, suggesting a\npotential shift in methodological approaches for data-driven partial\ndifferential equation learning. The article underscores deep learning as a\nviable and potentially sustainable way to enhance traditional high-performance\ncomputing methods, advocating for informed model selection based on the\ntopography of the mesh.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.00081v1.pdf",
        "similarity": 0.41792368211612435,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-31"
    },
    {
        "new_title": "LF-ViT: Reducing Spatial Redundancy in Vision Transformer for Efficient\n  Image Recognition",
        "new_link": "http://arxiv.org/abs/2402.00033v1",
        "new_summary": "  The Vision Transformer (ViT) excels in accuracy when handling high-resolution\nimages, yet it confronts the challenge of significant spatial redundancy,\nleading to increased computational and memory requirements. To address this, we\npresent the Localization and Focus Vision Transformer (LF-ViT). This model\noperates by strategically curtailing computational demands without impinging on\nperformance. In the Localization phase, a reduced-resolution image is\nprocessed; if a definitive prediction remains elusive, our pioneering\nNeighborhood Global Class Attention (NGCA) mechanism is triggered, effectively\nidentifying and spotlighting class-discriminative regions based on initial\nfindings. Subsequently, in the Focus phase, this designated region is used from\nthe original image to enhance recognition. Uniquely, LF-ViT employs consistent\nparameters across both phases, ensuring seamless end-to-end optimization. Our\nempirical tests affirm LF-ViT's prowess: it remarkably decreases Deit-S's FLOPs\nby 63\\% and concurrently amplifies throughput twofold. Code of this project is\nat https://github.com/edgeai1/LF-ViT.git.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.00033v1.pdf",
        "similarity": 0.4178574942017464,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-08"
    },
    {
        "new_title": "Scorch: A Library for Sparse Deep Learning",
        "new_link": "http://arxiv.org/abs/2405.16883v2",
        "new_summary": "  The rapid growth in the size of deep learning models strains the capabilities\nof traditional dense computation paradigms. Leveraging sparse computation has\nbecome increasingly popular for training and deploying large-scale models, but\nexisting deep learning frameworks lack extensive support for sparse operations.\nTo bridge this gap, we introduce Scorch, a library that seamlessly integrates\nefficient sparse tensor computation into the PyTorch ecosystem, with an initial\nfocus on inference workloads on CPUs. Scorch provides a flexible and intuitive\ninterface for sparse tensors, supporting diverse sparse data structures. Scorch\nintroduces a compiler stack that automates key optimizations, including\nautomatic loop ordering, tiling, and format inference. Combined with a runtime\nthat adapts its execution to both dense and sparse data, Scorch delivers\nsubstantial speedups over hand-written PyTorch Sparse (torch.sparse) operations\nwithout sacrificing usability. More importantly, Scorch enables efficient\ncomputation of complex sparse operations that lack hand-optimized PyTorch\nimplementations. This flexibility is crucial for exploring novel sparse\narchitectures. We demonstrate Scorch's ease of use and performance gains on\ndiverse deep learning models across multiple domains. With only minimal code\nchanges, Scorch achieves 1.05-5.78x speedups over PyTorch Sparse on end-to-end\ntasks. Scorch's seamless integration and performance gains make it a valuable\naddition to the PyTorch ecosystem. We believe Scorch will enable wider\nexploration of sparsity as a tool for scaling deep learning and inform the\ndevelopment of other sparse libraries.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.16883v2.pdf",
        "similarity": 0.4176239769231519,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-27"
    },
    {
        "new_title": "Multimodal Deep Learning for Low-Resource Settings: A Vector Embedding\n  Alignment Approach for Healthcare Applications",
        "new_link": "http://arxiv.org/abs/2406.02601v1",
        "new_summary": "  Large-scale multi-modal deep learning models have revolutionized domains such\nas healthcare, highlighting the importance of computational power. However, in\nresource-constrained regions like Low and Middle-Income Countries (LMICs),\nlimited access to GPUs and data poses significant challenges, often leaving\nCPUs as the sole resource. To address this, we advocate for leveraging vector\nembeddings to enable flexible and efficient computational methodologies,\ndemocratizing multimodal deep learning across diverse contexts.\n  Our paper investigates the efficiency and effectiveness of using vector\nembeddings from single-modal foundation models and multi-modal Vision-Language\nModels (VLMs) for multimodal deep learning in low-resource environments,\nparticularly in healthcare. Additionally, we propose a simple yet effective\ninference-time method to enhance performance by aligning image-text embeddings.\nComparing these approaches with traditional methods, we assess their impact on\ncomputational efficiency and model performance using metrics like accuracy,\nF1-score, inference time, training time, and memory usage across three medical\nmodalities: BRSET (ophthalmology), HAM10000 (dermatology), and SatelliteBench\n(public health).\n  Our findings show that embeddings reduce computational demands without\ncompromising model performance. Furthermore, our alignment method improves\nperformance in medical tasks. This research promotes sustainable AI practices\nby optimizing resources in constrained environments, highlighting the potential\nof embedding-based approaches for efficient multimodal learning. Vector\nembeddings democratize multimodal deep learning in LMICs, particularly in\nhealthcare, enhancing AI adaptability in varied use cases.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.02601v1.pdf",
        "similarity": 0.4176133856529155,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-02"
    },
    {
        "new_title": "On The Potential of The Fractal Geometry and The CNNs Ability to Encode\n  it",
        "new_link": "http://arxiv.org/abs/2401.04141v1",
        "new_summary": "  The fractal dimension provides a statistical index of object complexity by\nstudying how the pattern changes with the measuring scale. Although useful in\nseveral classification tasks, the fractal dimension is under-explored in deep\nlearning applications. In this work, we investigate the features that are\nlearned by deep models and we study whether these deep networks are able to\nencode features as complex and high-level as the fractal dimensions.\nSpecifically, we conduct a correlation analysis experiment to show that deep\nnetworks are not able to extract such a feature in none of their layers. We\ncombine our analytical study with a human evaluation to investigate the\ndifferences between deep learning networks and models that operate on the\nfractal feature solely. Moreover, we show the effectiveness of fractal features\nin applications where the object structure is crucial for the classification\ntask. We empirically show that training a shallow network on fractal features\nachieves performance comparable, even superior in specific cases, to that of\ndeep networks trained on raw data while requiring less computational resources.\nFractals improved the accuracy of the classification by 30% on average while\nrequiring up to 84% less time to train. We couple our empirical study with a\ncomplexity analysis of the computational cost of extracting the proposed\nfractal features, and we study its limitation.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.04141v1.pdf",
        "similarity": 0.4173999967274365,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-07"
    },
    {
        "new_title": "Large Language Model Enhanced Machine Learning Estimators for\n  Classification",
        "new_link": "http://arxiv.org/abs/2405.05445v1",
        "new_summary": "  Pre-trained large language models (LLM) have emerged as a powerful tool for\nsimulating various scenarios and generating output given specific instructions\nand multimodal input. In this work, we analyze the specific use of LLM to\nenhance a classical supervised machine learning method for classification\nproblems. We propose a few approaches to integrate LLM into a classical machine\nlearning estimator to further enhance the prediction performance. We examine\nthe performance of the proposed approaches through both standard supervised\nlearning binary classification tasks, and a transfer learning task where the\ntest data observe distribution changes compared to the training data. Numerical\nexperiments using four publicly available datasets are conducted and suggest\nthat using LLM to enhance classical machine learning estimators can provide\nsignificant improvement on prediction performance.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.05445v1.pdf",
        "similarity": 0.4171098782831926,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-08"
    },
    {
        "new_title": "Unsupervised Skin Feature Tracking with Deep Neural Networks",
        "new_link": "http://arxiv.org/abs/2405.04943v1",
        "new_summary": "  Facial feature tracking is essential in imaging ballistocardiography for\naccurate heart rate estimation and enables motor degradation quantification in\nParkinson's disease through skin feature tracking. While deep convolutional\nneural networks have shown remarkable accuracy in tracking tasks, they\ntypically require extensive labeled data for supervised training. Our proposed\npipeline employs a convolutional stacked autoencoder to match image crops with\na reference crop containing the target feature, learning deep feature encodings\nspecific to the object category in an unsupervised manner, thus reducing data\nrequirements. To overcome edge effects making the performance dependent on crop\nsize, we introduced a Gaussian weight on the residual errors of the pixels when\ncalculating the loss function. Training the autoencoder on facial images and\nvalidating its performance on manually labeled face and hand videos, our Deep\nFeature Encodings (DFE) method demonstrated superior tracking accuracy with a\nmean error ranging from 0.6 to 3.3 pixels, outperforming traditional methods\nlike SIFT, SURF, Lucas Kanade, and the latest transformers like PIPs++ and\nCoTracker. Overall, our unsupervised learning approach excels in tracking\nvarious skin features under significant motion conditions, providing superior\nfeature descriptors for tracking, matching, and image registration compared to\nboth traditional and state-of-the-art supervised learning methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.04943v1.pdf",
        "similarity": 0.41710581883577164,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-08"
    },
    {
        "new_title": "Verifying the Generalization of Deep Learning to Out-of-Distribution\n  Domains",
        "new_link": "http://arxiv.org/abs/2406.02024v3",
        "new_summary": "  Deep neural networks (DNNs) play a crucial role in the field of machine\nlearning, demonstrating state-of-the-art performance across various application\ndomains. However, despite their success, DNN-based models may occasionally\nexhibit challenges with generalization, i.e., may fail to handle inputs that\nwere not encountered during training. This limitation is a significant\nchallenge when it comes to deploying deep learning for safety-critical tasks,\nas well as in real-world settings characterized by substantial variability. We\nintroduce a novel approach for harnessing DNN verification technology to\nidentify DNN-driven decision rules that exhibit robust generalization to\npreviously unencountered input domains. Our method assesses generalization\nwithin an input domain by measuring the level of agreement between\nindependently trained deep neural networks for inputs in this domain. We also\nefficiently realize our approach by using off-the-shelf DNN verification\nengines, and extensively evaluate it on both supervised and unsupervised DNN\nbenchmarks, including a deep reinforcement learning (DRL) system for Internet\ncongestion control -- demonstrating the applicability of our approach for\nreal-world settings. Moreover, our research introduces a fresh objective for\nformal verification, offering the prospect of mitigating the challenges linked\nto deploying DNN-driven systems in real-world scenarios.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.02024v3.pdf",
        "similarity": 0.41679692133472324,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-04"
    },
    {
        "new_title": "Human-AI Interaction in Industrial Robotics: Design and Empirical\n  Evaluation of a User Interface for Explainable AI-Based Robot Program\n  Optimization",
        "new_link": "http://arxiv.org/abs/2404.19349v1",
        "new_summary": "  While recent advances in deep learning have demonstrated its transformative\npotential, its adoption for real-world manufacturing applications remains\nlimited. We present an Explanation User Interface (XUI) for a state-of-the-art\ndeep learning-based robot program optimizer which provides both naive and\nexpert users with different user experiences depending on their skill level, as\nwell as Explainable AI (XAI) features to facilitate the application of deep\nlearning methods in real-world applications. To evaluate the impact of the XUI\non task performance, user satisfaction and cognitive load, we present the\nresults of a preliminary user survey and propose a study design for a\nlarge-scale follow-up study.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.19349v1.pdf",
        "similarity": 0.4163974054419298,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-30"
    },
    {
        "new_title": "Arabic Handwritten Text for Person Biometric Identification: A Deep\n  Learning Approach",
        "new_link": "http://arxiv.org/abs/2406.00409v1",
        "new_summary": "  This study thoroughly investigates how well deep learning models can\nrecognize Arabic handwritten text for person biometric identification. It\ncompares three advanced architectures -- ResNet50, MobileNetV2, and\nEfficientNetB7 -- using three widely recognized datasets: AHAWP, Khatt, and\nLAMIS-MSHD. Results show that EfficientNetB7 outperforms the others, achieving\ntest accuracies of 98.57\\%, 99.15\\%, and 99.79\\% on AHAWP, Khatt, and\nLAMIS-MSHD datasets, respectively. EfficientNetB7's exceptional performance is\ncredited to its innovative techniques, including compound scaling, depth-wise\nseparable convolutions, and squeeze-and-excitation blocks. These features allow\nthe model to extract more abstract and distinctive features from handwritten\ntext images. The study's findings hold significant implications for enhancing\nidentity verification and authentication systems, highlighting the potential of\ndeep learning in Arabic handwritten text recognition for person biometric\nidentification.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.00409v1.pdf",
        "similarity": 0.41619686330683586,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-01"
    },
    {
        "new_title": "Deep Dependency Networks and Advanced Inference Schemes for Multi-Label\n  Classification",
        "new_link": "http://arxiv.org/abs/2404.11667v1",
        "new_summary": "  We present a unified framework called deep dependency networks (DDNs) that\ncombines dependency networks and deep learning architectures for multi-label\nclassification, with a particular emphasis on image and video data. The primary\nadvantage of dependency networks is their ease of training, in contrast to\nother probabilistic graphical models like Markov networks. In particular, when\ncombined with deep learning architectures, they provide an intuitive,\neasy-to-use loss function for multi-label classification. A drawback of DDNs\ncompared to Markov networks is their lack of advanced inference schemes,\nnecessitating the use of Gibbs sampling. To address this challenge, we propose\nnovel inference schemes based on local search and integer linear programming\nfor computing the most likely assignment to the labels given observations. We\nevaluate our novel methods on three video datasets (Charades, TACoS, Wetlab)\nand three image datasets (MS-COCO, PASCAL VOC, NUS-WIDE), comparing their\nperformance with (a) basic neural architectures and (b) neural architectures\ncombined with Markov networks equipped with advanced inference and learning\ntechniques. Our results demonstrate the superiority of our new DDN methods over\nthe two competing approaches.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.11667v1.pdf",
        "similarity": 0.41610897995202006,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-17"
    },
    {
        "new_title": "Deep Learning Based Amharic Chatbot for FAQs in Universities",
        "new_link": "http://arxiv.org/abs/2402.01720v1",
        "new_summary": "  University students often spend a considerable amount of time seeking answers\nto common questions from administrators or teachers. This can become tedious\nfor both parties, leading to a need for a solution. In response, this paper\nproposes a chatbot model that utilizes natural language processing and deep\nlearning techniques to answer frequently asked questions (FAQs) in the Amharic\nlanguage. Chatbots are computer programs that simulate human conversation\nthrough the use of artificial intelligence (AI), acting as a virtual assistant\nto handle questions and other tasks. The proposed chatbot program employs\ntokenization, normalization, stop word removal, and stemming to analyze and\ncategorize Amharic input sentences. Three machine learning model algorithms\nwere used to classify tokens and retrieve appropriate responses: Support Vector\nMachine (SVM), Multinomial Na\\\"ive Bayes, and deep neural networks implemented\nthrough TensorFlow, Keras, and NLTK. The deep learning model achieved the best\nresults with 91.55% accuracy and a validation loss of 0.3548 using an Adam\noptimizer and SoftMax activation function. The chatbot model was integrated\nwith Facebook Messenger and deployed on a Heroku server for 24-hour\naccessibility. The experimental results demonstrate that the chatbot framework\nachieved its objectives and effectively addressed challenges such as Amharic\nFidel variation, morphological variation, and lexical gaps. Future research\ncould explore the integration of Amharic WordNet to narrow the lexical gap and\nsupport more complex questions.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.01720v1.pdf",
        "similarity": 0.41586469729245956,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-26"
    },
    {
        "new_title": "Deep Learning-Based 3D Instance and Semantic Segmentation: A Review",
        "new_link": "http://arxiv.org/abs/2406.13308v1",
        "new_summary": "  The process of segmenting point cloud data into several homogeneous areas\nwith points in the same region having the same attributes is known as 3D\nsegmentation. Segmentation is challenging with point cloud data due to\nsubstantial redundancy, fluctuating sample density and lack of apparent\norganization. The research area has a wide range of robotics applications,\nincluding intelligent vehicles, autonomous mapping and navigation. A number of\nresearchers have introduced various methodologies and algorithms. Deep learning\nhas been successfully used to a spectrum of 2D vision domains as a prevailing\nA.I. methods. However, due to the specific problems of processing point clouds\nwith deep neural networks, deep learning on point clouds is still in its\ninitial stages. This study examines many strategies that have been presented to\n3D instance and semantic segmentation and gives a complete assessment of\ncurrent developments in deep learning-based 3D segmentation. In these\napproaches benefits, draw backs, and design mechanisms are studied and\naddressed. This study evaluates the impact of various segmentation algorithms\non competitiveness on various publicly accessible datasets, as well as the most\noften used pipelines, their advantages and limits, insightful findings and\nintriguing future research directions.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.13308v1.pdf",
        "similarity": 0.4156734525961517,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-19"
    },
    {
        "new_title": "Deep Submodular Peripteral Networks",
        "new_link": "http://arxiv.org/abs/2403.08199v2",
        "new_summary": "  Submodular functions, crucial for various applications, often lack practical\nlearning methods for their acquisition. Seemingly unrelated, learning a scaling\nfrom oracles offering graded pairwise preferences (GPC) is underexplored,\ndespite a rich history in psychometrics. In this paper, we introduce deep\nsubmodular peripteral networks (DSPNs), a novel parametric family of submodular\nfunctions, and methods for their training using a contrastive-learning inspired\nGPC-ready strategy to connect and then tackle both of the above challenges. We\nintroduce newly devised GPC-style \"peripteral\" loss which leverages numerically\ngraded relationships between pairs of objects (sets in our case). Unlike\ntraditional contrastive learning, our method utilizes graded comparisons,\nextracting more nuanced information than just binary-outcome comparisons, and\ncontrasts sets of any size (not just two). We also define a novel suite of\nautomatic sampling strategies for training, including active-learning inspired\nsubmodular feedback. We demonstrate DSPNs' efficacy in learning submodularity\nfrom a costly target submodular function showing superiority in downstream\ntasks such as experimental design and streaming applications.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.08199v2.pdf",
        "similarity": 0.4156049281756841,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-13"
    },
    {
        "new_title": "Vision-LSTM: xLSTM as Generic Vision Backbone",
        "new_link": "http://arxiv.org/abs/2406.04303v2",
        "new_summary": "  Transformers are widely used as generic backbones in computer vision, despite\ninitially introduced for natural language processing. Recently, the Long\nShort-Term Memory (LSTM) has been extended to a scalable and performant\narchitecture - the xLSTM - which overcomes long-standing LSTM limitations via\nexponential gating and parallelizable matrix memory structure. In this report,\nwe introduce Vision-LSTM (ViL), an adaption of the xLSTM building blocks to\ncomputer vision. ViL comprises a stack of xLSTM blocks where odd blocks process\nthe sequence of patch tokens from top to bottom while even blocks go from\nbottom to top. Experiments show that ViL holds promise to be further deployed\nas new generic backbone for computer vision architectures.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.04303v2.pdf",
        "similarity": 0.4154394406235167,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-06"
    },
    {
        "new_title": "Self-Supervised Learning for Covariance Estimation",
        "new_link": "http://arxiv.org/abs/2403.08662v1",
        "new_summary": "  We consider the use of deep learning for covariance estimation. We propose to\nglobally learn a neural network that will then be applied locally at inference\ntime. Leveraging recent advancements in self-supervised foundational models, we\ntrain the network without any labeling by simply masking different samples and\nlearning to predict their covariance given their surrounding neighbors. The\narchitecture is based on the popular attention mechanism. Its main advantage\nover classical methods is the automatic exploitation of global characteristics\nwithout any distributional assumptions or regularization. It can be pre-trained\nas a foundation model and then be repurposed for various downstream tasks,\ne.g., adaptive target detection in radar or hyperspectral imagery.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.08662v1.pdf",
        "similarity": 0.41527329646269,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-13"
    },
    {
        "new_title": "EmoCAM: Toward Understanding What Drives CNN-based Emotion Recognition",
        "new_link": "http://arxiv.org/abs/2407.14314v1",
        "new_summary": "  Convolutional Neural Networks are particularly suited for image analysis\ntasks, such as Image Classification, Object Recognition or Image Segmentation.\nLike all Artificial Neural Networks, however, they are \"black box\" models, and\nsuffer from poor explainability. This work is concerned with the specific\ndownstream task of Emotion Recognition from images, and proposes a framework\nthat combines CAM-based techniques with Object Detection on a corpus level to\nbetter understand on which image cues a particular model, in our case EmoNet,\nrelies to assign a specific emotion to an image. We demonstrate that the model\nmostly focuses on human characteristics, but also explore the pronounced effect\nof specific image modifications.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.14314v1.pdf",
        "similarity": 0.4146688338300764,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-19"
    },
    {
        "new_title": "NeuralSI: Neural Design of Semantic Interaction for Interactive Deep\n  Learning",
        "new_link": "http://arxiv.org/abs/2402.17178v1",
        "new_summary": "  An increasing number of studies have utilized interactive deep learning as\nthe analytic model of visual analytics systems for complex sensemaking tasks.\nIn these systems, traditional interactive dimensionality reduction (DR) models\nare commonly utilized to build a bi-directional bridge between high-dimensional\ndeep learning representations and low-dimensional visualizations. While these\nsystems better capture analysts' intents in the context of human-in-the-loop\ninteractive deep learning, traditional DR cannot support several desired\nproperties for visual analytics, including out-of-sample extensions, stability,\nand real-time inference. To avoid this issue, we propose the neural design\nframework of semantic interaction for interactive deep learning. In our\nframework, we replace the traditional DR with a neural projection network and\nappend it to the deep learning model as the task-specific output layer.\nTherefore, the analytic model (deep learning) and visualization method\n(interactive DR) form one integrated end-to-end trainable deep neural network.\nIn order to understand the performance of the neural design in comparison to\nthe state-of-the-art, we systematically performed two complementary studies, a\nhuman-centered qualitative case study and an algorithm-centered\nsimulation-based quantitative experiment. The results of these studies indicate\nthat the neural design can give semantic interaction systems substantial\nadvantages while still keeping comparable inference ability compared to the\nstate-of-the-art model.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.17178v1.pdf",
        "similarity": 0.41456763571906724,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-27"
    },
    {
        "new_title": "Deep Symbolic Optimization for Combinatorial Optimization: Accelerating\n  Node Selection by Discovering Potential Heuristics",
        "new_link": "http://arxiv.org/abs/2406.09740v2",
        "new_summary": "  Combinatorial optimization (CO) is one of the most fundamental mathematical\nmodels in real-world applications. Traditional CO solvers, such as\nBranch-and-Bound (B&B) solvers, heavily rely on expert-designed heuristics,\nwhich are reliable but require substantial manual tuning. Recent studies have\nleveraged deep learning (DL) models as an alternative to capture rich feature\npatterns for improved performance on GPU machines. Nonetheless, the drawbacks\nof high training and inference costs, as well as limited interpretability,\nseverely hinder the adoption of DL methods in real-world applications. To\naddress these challenges, we propose a novel deep symbolic optimization\nlearning framework that combines their advantages. Specifically, we focus on\nthe node selection module within B&B solvers -- namely, deep symbolic\noptimization for node selection (Dso4NS). With data-driven approaches, Dso4NS\nguides the search for mathematical expressions within the high-dimensional\ndiscrete symbolic space and then incorporates the highest-performing\nmathematical expressions into a solver. The data-driven model captures the rich\nfeature information in the input data and generates symbolic expressions, while\nthe expressions deployed in solvers enable fast inference with high\ninterpretability. Experiments demonstrate the effectiveness of Dso4NS in\nlearning high-quality expressions, outperforming existing approaches on a CPU\nmachine. Encouragingly, the learned CPU-based policies consistently achieve\nperformance comparable to state-of-the-art GPU-based approaches.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.09740v2.pdf",
        "similarity": 0.4144951060019146,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-14"
    },
    {
        "new_title": "LOTUS: Improving Transformer Efficiency with Sparsity Pruning and Data\n  Lottery Tickets",
        "new_link": "http://arxiv.org/abs/2405.00906v1",
        "new_summary": "  Vision transformers have revolutionized computer vision, but their\ncomputational demands present challenges for training and deployment. This\npaper introduces LOTUS (LOttery Transformers with Ultra Sparsity), a novel\nmethod that leverages data lottery ticket selection and sparsity pruning to\naccelerate vision transformer training while maintaining accuracy. Our approach\nfocuses on identifying and utilizing the most informative data subsets and\neliminating redundant model parameters to optimize the training process.\nThrough extensive experiments, we demonstrate the effectiveness of LOTUS in\nachieving rapid convergence and high accuracy with significantly reduced\ncomputational requirements. This work highlights the potential of combining\ndata selection and sparsity techniques for efficient vision transformer\ntraining, opening doors for further research and development in this area.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.00906v1.pdf",
        "similarity": 0.4132141961541207,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-01"
    },
    {
        "new_title": "Multi-modal Deep Learning",
        "new_link": "http://arxiv.org/abs/2403.03385v1",
        "new_summary": "  This article investigates deep learning methodologies for single-modality\nclinical data analysis, as a crucial precursor to multi-modal medical research.\nBuilding on Guo JingYuan's work, the study refines clinical data processing\nthrough Compact Convolutional Transformer (CCT), Patch Up, and the innovative\nCamCenterLoss technique, establishing a foundation for future multimodal\ninvestigations. The proposed methodology demonstrates improved prediction\naccuracy and at tentiveness to critically ill patients compared to Guo\nJingYuan's ResNet and StageNet approaches. Novelty that using image-pretrained\nvision transformer backbone to perform transfer learning time-series clinical\ndata.The study highlights the potential of CCT, Patch Up, and novel\nCamCenterLoss in processing single modality clinical data within deep learning\nframeworks, paving the way for future multimodal medical research and promoting\nprecision and personalized healthcare\n",
        "pdf_link": "https://arxiv.org/pdf/2403.03385v1.pdf",
        "similarity": 0.41313735752161723,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-06"
    },
    {
        "new_title": "Noise-Aware Differentially Private Regression via Meta-Learning",
        "new_link": "http://arxiv.org/abs/2406.08569v1",
        "new_summary": "  Many high-stakes applications require machine learning models that protect\nuser privacy and provide well-calibrated, accurate predictions. While\nDifferential Privacy (DP) is the gold standard for protecting user privacy,\nstandard DP mechanisms typically significantly impair performance. One approach\nto mitigating this issue is pre-training models on simulated data before DP\nlearning on the private data. In this work we go a step further, using\nsimulated data to train a meta-learning model that combines the Convolutional\nConditional Neural Process (ConvCNP) with an improved functional DP mechanism\nof Hall et al. [2013] yielding the DPConvCNP. DPConvCNP learns from simulated\ndata how to map private data to a DP predictive model in one forward pass, and\nthen provides accurate, well-calibrated predictions. We compare DPConvCNP with\na DP Gaussian Process (GP) baseline with carefully tuned hyperparameters. The\nDPConvCNP outperforms the GP baseline, especially on non-Gaussian data, yet is\nmuch faster at test time and requires less tuning.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.08569v1.pdf",
        "similarity": 0.41312222667957715,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-12"
    },
    {
        "new_title": "State-Constrained Offline Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2405.14374v1",
        "new_summary": "  Traditional offline reinforcement learning methods predominantly operate in a\nbatch-constrained setting. This confines the algorithms to a specific\nstate-action distribution present in the dataset, reducing the effects of\ndistributional shift but restricting the algorithm greatly. In this paper, we\nalleviate this limitation by introducing a novel framework named\n\\emph{state-constrained} offline reinforcement learning. By exclusively\nfocusing on the dataset's state distribution, our framework significantly\nenhances learning potential and reduces previous limitations. The proposed\nsetting not only broadens the learning horizon but also improves the ability to\ncombine different trajectories from the dataset effectively, a desirable\nproperty inherent in offline reinforcement learning. Our research is\nunderpinned by solid theoretical findings that pave the way for subsequent\nadvancements in this domain. Additionally, we introduce StaCQ, a deep learning\nalgorithm that is both performance-driven on the D4RL benchmark datasets and\nclosely aligned with our theoretical propositions. StaCQ establishes a strong\nbaseline for forthcoming explorations in state-constrained offline\nreinforcement learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.14374v1.pdf",
        "similarity": 0.41280012737301996,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-23"
    },
    {
        "new_title": "P$^2$OT: Progressive Partial Optimal Transport for Deep Imbalanced\n  Clustering",
        "new_link": "http://arxiv.org/abs/2401.09266v1",
        "new_summary": "  Deep clustering, which learns representation and semantic clustering without\nlabels information, poses a great challenge for deep learning-based approaches.\nDespite significant progress in recent years, most existing methods focus on\nuniformly distributed datasets, significantly limiting the practical\napplicability of their methods. In this paper, we first introduce a more\npractical problem setting named deep imbalanced clustering, where the\nunderlying classes exhibit an imbalance distribution. To tackle this problem,\nwe propose a novel pseudo-labeling-based learning framework. Our framework\nformulates pseudo-label generation as a progressive partial optimal transport\nproblem, which progressively transports each sample to imbalanced clusters\nunder prior distribution constraints, thus generating imbalance-aware\npseudo-labels and learning from high-confident samples. In addition, we\ntransform the initial formulation into an unbalanced optimal transport problem\nwith augmented constraints, which can be solved efficiently by a fast matrix\nscaling algorithm. Experiments on various datasets, including a human-curated\nlong-tailed CIFAR100, challenging ImageNet-R, and large-scale subsets of\nfine-grained iNaturalist2018 datasets, demonstrate the superiority of our\nmethod.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.09266v1.pdf",
        "similarity": 0.41253717983607435,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-17"
    },
    {
        "new_title": "Class-Aware Mask-Guided Feature Refinement for Scene Text Recognition",
        "new_link": "http://arxiv.org/abs/2402.13643v1",
        "new_summary": "  Scene text recognition is a rapidly developing field that faces numerous\nchallenges due to the complexity and diversity of scene text, including complex\nbackgrounds, diverse fonts, flexible arrangements, and accidental occlusions.\nIn this paper, we propose a novel approach called Class-Aware Mask-guided\nfeature refinement (CAM) to address these challenges. Our approach introduces\ncanonical class-aware glyph masks generated from a standard font to effectively\nsuppress background and text style noise, thereby enhancing feature\ndiscrimination. Additionally, we design a feature alignment and fusion module\nto incorporate the canonical mask guidance for further feature refinement for\ntext recognition. By enhancing the alignment between the canonical mask feature\nand the text feature, the module ensures more effective fusion, ultimately\nleading to improved recognition performance. We first evaluate CAM on six\nstandard text recognition benchmarks to demonstrate its effectiveness.\nFurthermore, CAM exhibits superiority over the state-of-the-art method by an\naverage performance gain of 4.1% across six more challenging datasets, despite\nutilizing a smaller model size. Our study highlights the importance of\nincorporating canonical mask guidance and aligned feature refinement techniques\nfor robust scene text recognition. The code is available at\nhttps://github.com/MelosY/CAM.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.13643v1.pdf",
        "similarity": 0.41253358519776756,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-21"
    },
    {
        "new_title": "ECRTime: Ensemble Integration of Classification and Retrieval for Time\n  Series Classification",
        "new_link": "http://arxiv.org/abs/2407.14735v1",
        "new_summary": "  Deep learning-based methods for Time Series Classification (TSC) typically\nutilize deep networks to extract features, which are then processed through a\ncombination of a Fully Connected (FC) layer and a SoftMax function. However, we\nhave observed the phenomenon of inter-class similarity and intra-class\ninconsistency in the datasets from the UCR archive and further analyzed how\nthis phenomenon adversely affects the \"FC+SoftMax\" paradigm. To address the\nissue, we introduce ECR, which, for the first time to our knowledge, applies\ndeep learning-based retrieval algorithm to the TSC problem and integrates\nclassification and retrieval models. Experimental results on 112 UCR datasets\ndemonstrate that ECR is state-of-the-art(sota) compared to existing deep\nlearning-based methods. Furthermore, we have developed a more precise\nclassifier, ECRTime, which is an ensemble of ECR. ECRTime surpasses the\ncurrently most accurate deep learning classifier, InceptionTime, in terms of\naccuracy, achieving this with reduced training time and comparable scalability.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.14735v1.pdf",
        "similarity": 0.4121333845350424,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-20"
    },
    {
        "new_title": "Feature Network Methods in Machine Learning and Applications",
        "new_link": "http://arxiv.org/abs/2401.04874v1",
        "new_summary": "  A machine learning (ML) feature network is a graph that connects ML features\nin learning tasks based on their similarity. This network representation allows\nus to view feature vectors as functions on the network. By leveraging function\noperations from Fourier analysis and from functional analysis, one can easily\ngenerate new and novel features, making use of the graph structure imposed on\nthe feature vectors. Such network structures have previously been studied\nimplicitly in image processing and computational biology. We thus describe\nfeature networks as graph structures imposed on feature vectors, and provide\napplications in machine learning. One application involves graph-based\ngeneralizations of convolutional neural networks, involving structured deep\nlearning with hierarchical representations of features that have varying depth\nor complexity. This extends also to learning algorithms that are able to\ngenerate useful new multilevel features. Additionally, we discuss the use of\nfeature networks to engineer new features, which can enhance the expressiveness\nof the model. We give a specific example of a deep tree-structured feature\nnetwork, where hierarchical connections are formed through feature clustering\nand feed-forward learning. This results in low learning complexity and\ncomputational efficiency. Unlike \"standard\" neural features which are limited\nto modulated (thresholded) linear combinations of adjacent ones, feature\nnetworks offer more general feedforward dependencies among features. For\nexample, radial basis functions or graph structure-based dependencies between\nfeatures can be utilized.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.04874v1.pdf",
        "similarity": 0.41190756990328925,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-10"
    },
    {
        "new_title": "Learn it or Leave it: Module Composition and Pruning for Continual\n  Learning",
        "new_link": "http://arxiv.org/abs/2406.18708v1",
        "new_summary": "  In real-world environments, continual learning is essential for machine\nlearning models, as they need to acquire new knowledge incrementally without\nforgetting what they have already learned. While pretrained language models\nhave shown impressive capabilities on various static tasks, applying them to\ncontinual learning poses significant challenges, including avoiding\ncatastrophic forgetting, facilitating knowledge transfer, and maintaining\nparameter efficiency. In this paper, we introduce MoCL-P, a novel lightweight\ncontinual learning method that addresses these challenges simultaneously.\nUnlike traditional approaches that continuously expand parameters for newly\narriving tasks, MoCL-P integrates task representation-guided module composition\nwith adaptive pruning, effectively balancing knowledge integration and\ncomputational overhead. Our evaluation across three continual learning\nbenchmarks with up to 176 tasks shows that MoCL-P achieves state-of-the-art\nperformance and improves parameter efficiency by up to three times,\ndemonstrating its potential for practical applications where resource\nrequirements are constrained.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.18708v1.pdf",
        "similarity": 0.41165140113753385,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-26"
    },
    {
        "new_title": "Deep Learning to Predict Glaucoma Progression using Structural Changes\n  in the Eye",
        "new_link": "http://arxiv.org/abs/2406.05605v1",
        "new_summary": "  Glaucoma is a chronic eye disease characterized by optic neuropathy, leading\nto irreversible vision loss. It progresses gradually, often remaining\nundiagnosed until advanced stages. Early detection is crucial to monitor\natrophy and develop treatment strategies to prevent further vision impairment.\nData-centric methods have enabled computer-aided algorithms for precise\nglaucoma diagnosis.\n  In this study, we use deep learning models to identify complex disease traits\nand progression criteria, detecting subtle changes indicative of glaucoma. We\nexplore the structure-function relationship in glaucoma progression and predict\nfunctional impairment from structural eye deterioration. We analyze statistical\nand machine learning methods, including deep learning techniques with optical\ncoherence tomography (OCT) scans for accurate progression prediction.\n  Addressing challenges like age variability, data imbalances, and noisy\nlabels, we develop novel semi-supervised time-series algorithms:\n  1. Weakly-Supervised Time-Series Learning: We create a CNN-LSTM model to\nencode spatiotemporal features from OCT scans. This approach uses age-related\nprogression and positive-unlabeled data to establish robust pseudo-progression\ncriteria, bypassing gold-standard labels.\n  2. Semi-Supervised Time-Series Learning: Using labels from Guided Progression\nAnalysis (GPA) in a contrastive learning scheme, the CNN-LSTM architecture\nlearns from potentially mislabeled data to improve prediction accuracy.\n  Our methods outperform conventional and state-of-the-art techniques.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.05605v1.pdf",
        "similarity": 0.4113567179243707,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-09"
    },
    {
        "new_title": "Lightweight Deep Learning for Resource-Constrained Environments: A\n  Survey",
        "new_link": "http://arxiv.org/abs/2404.07236v2",
        "new_summary": "  Over the past decade, the dominance of deep learning has prevailed across\nvarious domains of artificial intelligence, including natural language\nprocessing, computer vision, and biomedical signal processing. While there have\nbeen remarkable improvements in model accuracy, deploying these models on\nlightweight devices, such as mobile phones and microcontrollers, is constrained\nby limited resources. In this survey, we provide comprehensive design guidance\ntailored for these devices, detailing the meticulous design of lightweight\nmodels, compression methods, and hardware acceleration strategies. The\nprincipal goal of this work is to explore methods and concepts for getting\naround hardware constraints without compromising the model's accuracy.\nAdditionally, we explore two notable paths for lightweight deep learning in the\nfuture: deployment techniques for TinyML and Large Language Models. Although\nthese paths undoubtedly have potential, they also present significant\nchallenges, encouraging research into unexplored areas.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.07236v2.pdf",
        "similarity": 0.41108901911133694,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-08"
    },
    {
        "new_title": "An inclusive review on deep learning techniques and their scope in\n  handwriting recognition",
        "new_link": "http://arxiv.org/abs/2404.08011v1",
        "new_summary": "  Deep learning expresses a category of machine learning algorithms that have\nthe capability to combine raw inputs into intermediate features layers. These\ndeep learning algorithms have demonstrated great results in different fields.\nDeep learning has particularly witnessed for a great achievement of human level\nperformance across a number of domains in computer vision and pattern\nrecognition. For the achievement of state-of-the-art performances in diverse\ndomains, the deep learning used different architectures and these architectures\nused activation functions to perform various computations between hidden and\noutput layers of any architecture. This paper presents a survey on the existing\nstudies of deep learning in handwriting recognition field. Even though the\nrecent progress indicates that the deep learning methods has provided valuable\nmeans for speeding up or proving accurate results in handwriting recognition,\nbut following from the extensive literature survey, the present study finds\nthat the deep learning has yet to revolutionize more and has to resolve many of\nthe most pressing challenges in this field, but promising advances have been\nmade on the prior state of the art. Additionally, an inadequate availability of\nlabelled data to train presents problems in this domain. Nevertheless, the\npresent handwriting recognition survey foresees deep learning enabling changes\nat both bench and bedside with the potential to transform several domains as\nimage processing, speech recognition, computer vision, machine translation,\nrobotics and control, medical imaging, medical information processing,\nbio-informatics, natural language processing, cyber security, and many others.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.08011v1.pdf",
        "similarity": 0.4108161085557653,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-10"
    },
    {
        "new_title": "Fundamental Components of Deep Learning: A category-theoretic approach",
        "new_link": "http://arxiv.org/abs/2403.13001v1",
        "new_summary": "  Deep learning, despite its remarkable achievements, is still a young field.\nLike the early stages of many scientific disciplines, it is marked by the\ndiscovery of new phenomena, ad-hoc design decisions, and the lack of a uniform\nand compositional mathematical foundation. From the intricacies of the\nimplementation of backpropagation, through a growing zoo of neural network\narchitectures, to the new and poorly understood phenomena such as double\ndescent, scaling laws or in-context learning, there are few unifying principles\nin deep learning. This thesis develops a novel mathematical foundation for deep\nlearning based on the language of category theory. We develop a new framework\nthat is a) end-to-end, b) unform, and c) not merely descriptive, but\nprescriptive, meaning it is amenable to direct implementation in programming\nlanguages with sufficient features. We also systematise many existing\napproaches, placing many existing constructions and concepts from the\nliterature under the same umbrella. In Part I we identify and model two main\nproperties of deep learning systems parametricity and bidirectionality by we\nexpand on the previously defined construction of actegories and Para to study\nthe former, and define weighted optics to study the latter. Combining them\nyields parametric weighted optics, a categorical model of artificial neural\nnetworks, and more. Part II justifies the abstractions from Part I, applying\nthem to model backpropagation, architectures, and supervised learning. We\nprovide a lens-theoretic axiomatisation of differentiation, covering not just\nsmooth spaces, but discrete settings of boolean circuits as well. We survey\nexisting, and develop new categorical models of neural network architectures.\nWe formalise the notion of optimisers and lastly, combine all the existing\nconcepts together, providing a uniform and compositional framework for\nsupervised learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.13001v1.pdf",
        "similarity": 0.4106985621053234,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-13"
    },
    {
        "new_title": "HyperbolicLR: Epoch insensitive learning rate scheduler",
        "new_link": "http://arxiv.org/abs/2407.15200v1",
        "new_summary": "  This study proposes two novel learning rate schedulers: the Hyperbolic\nLearning Rate Scheduler (HyperbolicLR) and the Exponential Hyperbolic Learning\nRate Scheduler (ExpHyperbolicLR). These schedulers attempt to address the\ninconsistent learning curves often observed in conventional schedulers when\nadjusting the number of epochs. By leveraging the asymptotic behavior of\nhyperbolic curves, the proposed schedulers maintain more consistent learning\ncurves across varying epoch settings. The HyperbolicLR algorithm directly\napplies this property to the epoch-learning rate space, while the\nExpHyperbolicLR maps this concept onto the exponential space of epochs and\nlearning rates. To evaluate the performance of these schedulers, first we found\nthe optimal hyperparameters for each scheduler on a small number of epochs,\nfixed these values, and compared their performance as the number of epochs\nincreased. Our experimental results on various deep learning tasks and\narchitectures demonstrate that both HyperbolicLR and ExpHyperbolicLR maintain\nmore consistent performance improvements compared to conventional schedulers as\nthe number of epochs increases. These findings suggest that our\nhyperbolic-based learning rate schedulers offer a more robust and efficient\napproach to training deep neural networks, especially in scenarios where\ncomputational resources or time constraints limit extensive hyperparameter\nsearches.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.15200v1.pdf",
        "similarity": 0.4106588074232247,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-21"
    },
    {
        "new_title": "Towards Responsible and Reliable Traffic Flow Prediction with Large\n  Language Models",
        "new_link": "http://arxiv.org/abs/2404.02937v4",
        "new_summary": "  Traffic forecasting is crucial for intelligent transportation systems. It has\nexperienced significant advancements thanks to the power of deep learning in\ncapturing latent patterns of traffic data. However, recent deep-learning\narchitectures require intricate model designs and lack an intuitive\nunderstanding of the mapping from input data to predicted results. Achieving\nboth accuracy and responsibility in traffic prediction models remains a\nchallenge due to the complexity of traffic data and the inherent opacity of\ndeep learning models. To tackle these challenges, we propose a Responsible and\nReliable Traffic flow forecasting model with Large Language Models (R2T-LLM),\nwhich leverages large language models (LLMs) to generate responsible traffic\npredictions. By transferring multi-modal traffic data into natural language\ndescriptions, R2T-LLM captures complex spatial-temporal patterns and external\nfactors from comprehensive traffic data. The LLM framework is fine-tuned using\nlanguage-based instructions to align with spatial-temporal traffic flow data.\nEmpirically, R2T-LLM shows competitive accuracy compared with deep learning\nbaselines, while providing an intuitive and reliable explanation for\npredictions. We discuss the spatial-temporal and input dependencies for\nconditional future flow forecasting, showcasing R2T-LLM's potential for diverse\ncity prediction tasks. This paper contributes to advancing accountable traffic\nprediction models and lays a foundation for future exploration of LLM\napplications in transportation. To the best of our knowledge, this is the first\nstudy to use LLM for accountable and reliable prediction of traffic flows.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.02937v4.pdf",
        "similarity": 0.410424047022848,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-03"
    },
    {
        "new_title": "PureForest: A Large-Scale Aerial Lidar and Aerial Imagery Dataset for\n  Tree Species Classification in Monospecific Forests",
        "new_link": "http://arxiv.org/abs/2404.12064v2",
        "new_summary": "  Knowledge of tree species distribution is fundamental to managing forests.\nNew deep learning approaches promise significant accuracy gains for forest\nmapping, and are becoming a critical tool for mapping multiple tree species at\nscale. To advance the field, deep learning researchers need large benchmark\ndatasets with high-quality annotations. To this end, we present the PureForest\ndataset: a large-scale, open, multimodal dataset designed for tree species\nclassification from both Aerial Lidar Scanning (ALS) point clouds and Very High\nResolution (VHR) aerial images. Most current public Lidar datasets for tree\nspecies classification have low diversity as they only span a small area of a\nfew dozen annotated hectares at most. In contrast, PureForest has 18 tree\nspecies grouped into 13 semantic classes, and spans 339 km$^2$ across 449\ndistinct monospecific forests, and is to date the largest and most\ncomprehensive Lidar dataset for the identification of tree species. By making\nPureForest publicly available, we hope to provide a challenging benchmark\ndataset to support the development of deep learning approaches for tree species\nidentification from Lidar and/or aerial imagery. In this data paper, we\ndescribe the annotation workflow, the dataset, the recommended evaluation\nmethodology, and establish a baseline performance from both 3D and 2D\nmodalities.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.12064v2.pdf",
        "similarity": 0.41022592933214047,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-18"
    },
    {
        "new_title": "Uncertainty-aware Bridge based Mobile-Former Network for Event-based\n  Pattern Recognition",
        "new_link": "http://arxiv.org/abs/2401.11123v1",
        "new_summary": "  The mainstream human activity recognition (HAR) algorithms are developed\nbased on RGB cameras, which are easily influenced by low-quality images (e.g.,\nlow illumination, motion blur). Meanwhile, the privacy protection issue caused\nby ultra-high definition (HD) RGB cameras aroused more and more people's\nattention. Inspired by the success of event cameras which perform better on\nhigh dynamic range, no motion blur, and low energy consumption, we propose to\nrecognize human actions based on the event stream. We propose a lightweight\nuncertainty-aware information propagation based Mobile-Former network for\nefficient pattern recognition, which aggregates the MobileNet and Transformer\nnetwork effectively. Specifically, we first embed the event images using a stem\nnetwork into feature representations, then, feed them into uncertainty-aware\nMobile-Former blocks for local and global feature learning and fusion. Finally,\nthe features from MobileNet and Transformer branches are concatenated for\npattern recognition. Extensive experiments on multiple event-based recognition\ndatasets fully validated the effectiveness of our model. The source code of\nthis work will be released at\nhttps://github.com/Event-AHU/Uncertainty_aware_MobileFormer.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.11123v1.pdf",
        "similarity": 0.4101580950027899,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-20"
    },
    {
        "new_title": "DuEDL: Dual-Branch Evidential Deep Learning for Scribble-Supervised\n  Medical Image Segmentation",
        "new_link": "http://arxiv.org/abs/2405.14444v1",
        "new_summary": "  Despite the recent progress in medical image segmentation with scribble-based\nannotations, the segmentation results of most models are still not ro-bust and\ngeneralizable enough in open environments. Evidential deep learn-ing (EDL) has\nrecently been proposed as a promising solution to model predictive uncertainty\nand improve the reliability of medical image segmen-tation. However directly\napplying EDL to scribble-supervised medical im-age segmentation faces a\ntradeoff between accuracy and reliability. To ad-dress the challenge, we\npropose a novel framework called Dual-Branch Evi-dential Deep Learning (DuEDL).\nFirstly, the decoder of the segmentation network is changed to two different\nbranches, and the evidence of the two branches is fused to generate\nhigh-quality pseudo-labels. Then the frame-work applies partial evidence loss\nand two-branch consistent loss for joint training of the model to adapt to the\nscribble supervision learning. The pro-posed method was tested on two cardiac\ndatasets: ACDC and MSCMRseg. The results show that our method significantly\nenhances the reliability and generalization ability of the model without\nsacrificing accuracy, outper-forming state-of-the-art baselines. The code is\navailable at https://github.com/Gardnery/DuEDL.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.14444v1.pdf",
        "similarity": 0.4101450664056616,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-23"
    },
    {
        "new_title": "Deep Supervision by Gaussian Pseudo-label-based Morphological Attention\n  for Abdominal Aorta Segmentation in Non-Contrast CTs",
        "new_link": "http://arxiv.org/abs/2402.02514v1",
        "new_summary": "  The segmentation of the abdominal aorta in non-contrast CT images is a\nnon-trivial task for computer-assisted endovascular navigation, particularly in\nscenarios where contrast agents are unsuitable. While state-of-the-art deep\nlearning segmentation models have been proposed recently for this task, they\nare trained on manually annotated strong labels. However, the inherent\nambiguity in the boundary of the aorta in non-contrast CT may undermine the\nreliability of strong labels, leading to potential overfitting risks. This\npaper introduces a Gaussian-based pseudo label, integrated into conventional\ndeep learning models through deep supervision, to achieve Morphological\nAttention (MA) enhancement. As the Gaussian pseudo label retains the\nmorphological features of the aorta without explicitly representing its\nboundary distribution, we suggest that it preserves aortic morphology during\ntraining while mitigating the negative impact of ambiguous boundaries, reducing\nthe risk of overfitting. It is introduced in various 2D/3D deep learning models\nand validated on our local data set of 30 non-contrast CT volumes comprising\n5749 CT slices. The results underscore the effectiveness of MA in preserving\nthe morphological characteristics of the aorta and addressing overfitting\nconcerns, thereby enhancing the performance of the models.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.02514v1.pdf",
        "similarity": 0.40998257905590124,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-04"
    },
    {
        "new_title": "Training point-based deep learning networks for forest segmentation with\n  synthetic data",
        "new_link": "http://arxiv.org/abs/2403.14115v2",
        "new_summary": "  Remote sensing through unmanned aerial systems (UAS) has been increasing in\nforestry in recent years, along with using machine learning for data\nprocessing. Deep learning architectures, extensively applied in natural\nlanguage and image processing, have recently been extended to the point cloud\ndomain. However, the availability of point cloud datasets for training and\ntesting remains limited. Creating forested environment point cloud datasets is\nexpensive, requires high-precision sensors, and is time-consuming as manual\npoint classification is required. Moreover, forest areas could be inaccessible\nor dangerous for humans, further complicating data collection. Then, a question\narises whether it is possible to use synthetic data to train deep learning\nnetworks without the need to rely on large volumes of real forest data. To\nanswer this question, we developed a realistic simulator that procedurally\ngenerates synthetic forest scenes. Thanks to this, we have conducted a\ncomparative study of different state-of-the-art point-based deep learning\nnetworks for forest segmentation. Using created datasets, we determined the\nfeasibility of using synthetic data to train deep learning networks to classify\npoint clouds from real forest datasets. Both the simulator and the datasets are\nreleased as part of this work.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.14115v2.pdf",
        "similarity": 0.40971589329604974,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-21"
    },
    {
        "new_title": "Position: Bayesian Deep Learning is Needed in the Age of Large-Scale AI",
        "new_link": "http://arxiv.org/abs/2402.00809v4",
        "new_summary": "  In the current landscape of deep learning research, there is a predominant\nemphasis on achieving high predictive accuracy in supervised tasks involving\nlarge image and language datasets. However, a broader perspective reveals a\nmultitude of overlooked metrics, tasks, and data types, such as uncertainty,\nactive and continual learning, and scientific data, that demand attention.\nBayesian deep learning (BDL) constitutes a promising avenue, offering\nadvantages across these diverse settings. This paper posits that BDL can\nelevate the capabilities of deep learning. It revisits the strengths of BDL,\nacknowledges existing challenges, and highlights some exciting research avenues\naimed at addressing these obstacles. Looking ahead, the discussion focuses on\npossible ways to combine large-scale foundation models with BDL to unlock their\nfull potential.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.00809v4.pdf",
        "similarity": 0.4096147347724493,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-01"
    },
    {
        "new_title": "Semantic Augmentation in Images using Language",
        "new_link": "http://arxiv.org/abs/2404.02353v1",
        "new_summary": "  Deep Learning models are incredibly data-hungry and require very large\nlabeled datasets for supervised learning. As a consequence, these models often\nsuffer from overfitting, limiting their ability to generalize to real-world\nexamples. Recent advancements in diffusion models have enabled the generation\nof photorealistic images based on textual inputs. Leveraging the substantial\ndatasets used to train these diffusion models, we propose a technique to\nutilize generated images to augment existing datasets. This paper explores\nvarious strategies for effective data augmentation to improve the out-of-domain\ngeneralization capabilities of deep learning models.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.02353v1.pdf",
        "similarity": 0.4090387925966656,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-02"
    },
    {
        "new_title": "Learning Dynamics of LLM Finetuning",
        "new_link": "http://arxiv.org/abs/2407.10490v1",
        "new_summary": "  Learning dynamics, which describes how the learning of specific training\nexamples influences the model's prediction of other examples, give us a\npowerful tool for understanding the behavior of deep learning systems. We study\nthe learning dynamics of large language models during finetuning, by analyzing\nthe step-wise decomposition and accumulated influence among different\nresponses. Our framework allows a uniform interpretation of many interesting\nobservations about the training of popular algorithms for both instruction\ntuning and preference tuning. The analysis not only explains where the benefits\nof these methods come from but also inspires a simple, effective method to\nfurther improve the alignment performance. Code for experiments is available at\nhttps://github.com/Joshua-Ren/Learning_dynamics_LLM.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.10490v1.pdf",
        "similarity": 0.4087249761897702,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-15"
    },
    {
        "new_title": "DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT\n  Sensor Data",
        "new_link": "http://arxiv.org/abs/2403.19996v1",
        "new_summary": "  Internet of Things (IoT) sensor data or readings evince variations in\ntimestamp range, sampling frequency, geographical location, unit of\nmeasurement, etc. Such presented sequence data heterogeneity makes it difficult\nfor traditional time series classification algorithms to perform well.\nTherefore, addressing the heterogeneity challenge demands learning not only the\nsub-patterns (local features) but also the overall pattern (global feature). To\naddress the challenge of classifying heterogeneous IoT sensor data (e.g.,\ncategorizing sensor data types like temperature and humidity), we propose a\nnovel deep learning model that incorporates both Convolutional Neural Network\nand Bi-directional Gated Recurrent Unit to learn local and global features\nrespectively, in an end-to-end manner. Through rigorous experimentation on\nheterogeneous IoT sensor datasets, we validate the effectiveness of our\nproposed model, which outperforms recent state-of-the-art classification\nmethods as well as several machine learning and deep learning baselines. In\nparticular, the model achieves an average absolute improvement of 3.37% in\nAccuracy and 2.85% in F1-Score across datasets\n",
        "pdf_link": "https://arxiv.org/pdf/2403.19996v1.pdf",
        "similarity": 0.4085793273261736,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-29"
    },
    {
        "new_title": "Interpretable deep learning in single-cell omics",
        "new_link": "http://arxiv.org/abs/2401.06823v1",
        "new_summary": "  Recent developments in single-cell omics technologies have enabled the\nquantification of molecular profiles in individual cells at an unparalleled\nresolution. Deep learning, a rapidly evolving sub-field of machine learning,\nhas instilled a significant interest in single-cell omics research due to its\nremarkable success in analysing heterogeneous high-dimensional single-cell\nomics data. Nevertheless, the inherent multi-layer nonlinear architecture of\ndeep learning models often makes them `black boxes' as the reasoning behind\npredictions is often unknown and not transparent to the user. This has\nstimulated an increasing body of research for addressing the lack of\ninterpretability in deep learning models, especially in single-cell omics data\nanalyses, where the identification and understanding of molecular regulators\nare crucial for interpreting model predictions and directing downstream\nexperimental validations. In this work, we introduce the basics of single-cell\nomics technologies and the concept of interpretable deep learning. This is\nfollowed by a review of the recent interpretable deep learning models applied\nto various single-cell omics research. Lastly, we highlight the current\nlimitations and discuss potential future directions. We anticipate this review\nto bring together the single-cell and machine learning research communities to\nfoster future development and application of interpretable deep learning in\nsingle-cell omics research.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.06823v1.pdf",
        "similarity": 0.40837557278805636,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-11"
    },
    {
        "new_title": "ViT-MUL: A Baseline Study on Recent Machine Unlearning Methods Applied\n  to Vision Transformers",
        "new_link": "http://arxiv.org/abs/2403.09681v1",
        "new_summary": "  Machine unlearning (MUL) is an arising field in machine learning that seeks\nto erase the learned information of specific training data points from a\ntrained model. Despite the recent active research in MUL within computer\nvision, the majority of work has focused on ResNet-based models. Given that\nVision Transformers (ViT) have become the predominant model architecture, a\ndetailed study of MUL specifically tailored to ViT is essential. In this paper,\nwe present comprehensive experiments on ViTs using recent MUL algorithms and\ndatasets. We anticipate that our experiments, ablation studies, and findings\ncould provide valuable insights and inspire further research in this field.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.09681v1.pdf",
        "similarity": 0.4081793217487728,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-07"
    },
    {
        "new_title": "On Time-Indexing as Inductive Bias in Deep RL for Sequential\n  Manipulation Tasks",
        "new_link": "http://arxiv.org/abs/2401.01993v1",
        "new_summary": "  While solving complex manipulation tasks, manipulation policies often need to\nlearn a set of diverse skills to accomplish these tasks. The set of skills is\noften quite multimodal - each one may have a quite distinct distribution of\nactions and states. Standard deep policy-learning algorithms often model\npolicies as deep neural networks with a single output head (deterministic or\nstochastic). This structure requires the network to learn to switch between\nmodes internally, which can lead to lower sample efficiency and poor\nperformance. In this paper we explore a simple structure which is conducive to\nskill learning required for so many of the manipulation tasks. Specifically, we\npropose a policy architecture that sequentially executes different action heads\nfor fixed durations, enabling the learning of primitive skills such as reaching\nand grasping. Our empirical evaluation on the Metaworld tasks reveals that this\nsimple structure outperforms standard policy learning methods, highlighting its\npotential for improved skill acquisition.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.01993v1.pdf",
        "similarity": 0.4078590950306828,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-03"
    },
    {
        "new_title": "Feature Selection as Deep Sequential Generative Learning",
        "new_link": "http://arxiv.org/abs/2403.03838v1",
        "new_summary": "  Feature selection aims to identify the most pattern-discriminative feature\nsubset. In prior literature, filter (e.g., backward elimination) and embedded\n(e.g., Lasso) methods have hyperparameters (e.g., top-K, score thresholding)\nand tie to specific models, thus, hard to generalize; wrapper methods search a\nfeature subset in a huge discrete space and is computationally costly. To\ntransform the way of feature selection, we regard a selected feature subset as\na selection decision token sequence and reformulate feature selection as a deep\nsequential generative learning task that distills feature knowledge and\ngenerates decision sequences. Our method includes three steps: (1) We develop a\ndeep variational transformer model over a joint of sequential reconstruction,\nvariational, and performance evaluator losses. Our model can distill feature\nselection knowledge and learn a continuous embedding space to map feature\nselection decision sequences into embedding vectors associated with utility\nscores. (2) We leverage the trained feature subset utility evaluator as a\ngradient provider to guide the identification of the optimal feature subset\nembedding;(3) We decode the optimal feature subset embedding to\nautoregressively generate the best feature selection decision sequence with\nautostop. Extensive experimental results show this generative perspective is\neffective and generic, without large discrete search space and expert-specific\nhyperparameters.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.03838v1.pdf",
        "similarity": 0.40736440603930346,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-06"
    },
    {
        "new_title": "Onboard Processing of Hyperspectral Imagery: Deep Learning Advancements,\n  Methodologies, Challenges, and Emerging Trends",
        "new_link": "http://arxiv.org/abs/2404.06526v1",
        "new_summary": "  Recent advancements in deep learning techniques have spurred considerable\ninterest in their application to hyperspectral imagery processing. This paper\nprovides a comprehensive review of the latest developments in this field,\nfocusing on methodologies, challenges, and emerging trends. Deep learning\narchitectures such as Convolutional Neural Networks (CNNs), Autoencoders, Deep\nBelief Networks (DBNs), Generative Adversarial Networks (GANs), and Recurrent\nNeural Networks (RNNs) are examined for their suitability in processing\nhyperspectral data. Key challenges, including limited training data and\ncomputational constraints, are identified, along with strategies such as data\naugmentation and noise reduction using GANs. The paper discusses the efficacy\nof different network architectures, highlighting the advantages of lightweight\nCNN models and 1D CNNs for onboard processing. Moreover, the potential of\nhardware accelerators, particularly Field Programmable Gate Arrays (FPGAs), for\nenhancing processing efficiency is explored. The review concludes with insights\ninto ongoing research trends, including the integration of deep learning\ntechniques into Earth observation missions such as the CHIME mission, and\nemphasizes the need for further exploration and refinement of deep learning\nmethodologies to address the evolving demands of hyperspectral image\nprocessing.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.06526v1.pdf",
        "similarity": 0.40666524261511233,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-09"
    },
    {
        "new_title": "Value Prediction for Spatiotemporal Gait Data Using Deep Learning",
        "new_link": "http://arxiv.org/abs/2403.07926v1",
        "new_summary": "  Human gait has been commonly used for the diagnosis and evaluation of medical\nconditions and for monitoring the progress during treatment and rehabilitation.\nThe use of wearable sensors that capture pressure or motion has yielded\ntechniques that analyze the gait data to aid recovery, identify activity\nperformed, or identify individuals. Deep learning, usually employing\nclassification, has been successfully utilized in a variety of applications\nsuch as computer vision, biomedical imaging analysis, and natural language\nprocessing. We expand the application of deep learning to value prediction of\ntime-series of spatiotemporal gait data. Moreover, we explore several deep\nlearning architectures (Recurrent Neural Networks (RNN) and RNN combined with\nConvolutional Neural Networks (CNN)) to make short- and long-distance\npredictions using two different experimental setups. Our results show that\nshort-distance prediction has an RMSE as low as 0.060675, and long-distance\nprediction RMSE as low as 0.106365. Additionally, the results show that the\nproposed deep learning models are capable of predicting the entire trial when\ntrained and validated using the trials from the same participant. The proposed,\ncustomized models, used with value prediction open possibilities for additional\napplications, such as fall prediction, in-home progress monitoring, aiding of\nexoskeleton movement, and authentication.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.07926v1.pdf",
        "similarity": 0.40627311406776556,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-29"
    },
    {
        "new_title": "ProtoAL: Interpretable Deep Active Learning with prototypes for medical\n  imaging",
        "new_link": "http://arxiv.org/abs/2404.04736v1",
        "new_summary": "  The adoption of Deep Learning algorithms in the medical imaging field is a\nprominent area of research, with high potential for advancing AI-based\nComputer-aided diagnosis (AI-CAD) solutions. However, current solutions face\nchallenges due to a lack of interpretability features and high data demands,\nprompting recent efforts to address these issues. In this study, we propose the\nProtoAL method, where we integrate an interpretable DL model into the Deep\nActive Learning (DAL) framework. This approach aims to address both challenges\nby focusing on the medical imaging context and utilizing an inherently\ninterpretable model based on prototypes. We evaluated ProtoAL on the Messidor\ndataset, achieving an area under the precision-recall curve of 0.79 while\nutilizing only 76.54\\% of the available labeled data. These capabilities can\nenhances the practical usability of a DL model in the medical field, providing\na means of trust calibration in domain experts and a suitable solution for\nlearning in the data scarcity context often found.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.04736v1.pdf",
        "similarity": 0.4060997753419794,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-06"
    },
    {
        "new_title": "Leveraging Internal Representations of Model for Magnetic Image\n  Classification",
        "new_link": "http://arxiv.org/abs/2403.06797v1",
        "new_summary": "  Data generated by edge devices has the potential to train intelligent\nautonomous systems across various domains. Despite the emergence of diverse\nmachine learning approaches addressing privacy concerns and utilizing\ndistributed data, security issues persist due to the sensitive storage of data\nshards in disparate locations. This paper introduces a potentially\ngroundbreaking paradigm for machine learning model training, specifically\ndesigned for scenarios with only a single magnetic image and its corresponding\nlabel image available. We harness the capabilities of Deep Learning to generate\nconcise yet informative samples, aiming to overcome data scarcity. Through the\nutilization of deep learning's internal representations, our objective is to\nefficiently address data scarcity issues and produce meaningful results. This\nmethodology presents a promising avenue for training machine learning models\nwith minimal data.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.06797v1.pdf",
        "similarity": 0.4060883201122435,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-11"
    },
    {
        "new_title": "EcoMLS: A Self-Adaptation Approach for Architecting Green ML-Enabled\n  Systems",
        "new_link": "http://arxiv.org/abs/2404.11411v1",
        "new_summary": "  The sustainability of Machine Learning-Enabled Systems (MLS), particularly\nwith regard to energy efficiency, is an important challenge in their\ndevelopment and deployment. Self-adaptation techniques, recognized for their\npotential in energy savings within software systems, have yet to be extensively\nexplored in Machine Learning-Enabled Systems (MLS), where runtime uncertainties\ncan significantly impact model performance and energy consumption. This\nvariability, alongside the fluctuating energy demands of ML models during\noperation, necessitates a dynamic approach. Addressing these challenges, we\nintroduce EcoMLS approach, which leverages the Machine Learning Model Balancer\nconcept to enhance the sustainability of MLS through runtime ML model\nswitching. By adapting to monitored runtime conditions, EcoMLS optimally\nbalances energy consumption with model confidence, demonstrating a significant\nadvancement towards sustainable, energy-efficient machine learning solutions.\nThrough an object detection exemplar, we illustrate the application of EcoMLS,\nshowcasing its ability to reduce energy consumption while maintaining high\nmodel accuracy throughout its use. This research underscores the feasibility of\nenhancing MLS sustainability through intelligent runtime adaptations,\ncontributing a valuable perspective to the ongoing discourse on\nenergy-efficient machine learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.11411v1.pdf",
        "similarity": 0.40603148243319703,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-17"
    },
    {
        "new_title": "Apply Distributed CNN on Genomics to accelerate Transcription-Factor\n  TAL1 Motif Prediction",
        "new_link": "http://arxiv.org/abs/2405.16097v1",
        "new_summary": "  Big Data works perfectly along with Deep learning to extract knowledge from a\nhuge amount of data. However, this processing could take a lot of training\ntime. Genomics is a Big Data science with high dimensionality. It relies on\ndeep learning to solve complicated problems in certain diseases like cancer by\nusing different DNA information such as the transcription factor. TAL1 is a\ntranscription factor that is essential for the development of hematopoiesis and\nof the vascular system. In this paper, we highlight the potential of deep\nlearning in the field of genomics and its challenges such as the training time\nthat takes hours, weeks, and in some cases months. Therefore, we propose to\napply a distributed deep learning implementation based on Convolutional Neural\nNetworks (CNN) that showed good results in decreasing the training time and\nenhancing the accuracy performance with 95% by using multiple GPU and TPU as\naccelerators. We proved the efficiency of using a distributed strategy based on\ndata-parallelism in predicting the transcription-factor TAL1 motif faster.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.16097v1.pdf",
        "similarity": 0.40595097070855257,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-25"
    },
    {
        "new_title": "Unlearning during Learning: An Efficient Federated Machine Unlearning\n  Method",
        "new_link": "http://arxiv.org/abs/2405.15474v1",
        "new_summary": "  In recent years, Federated Learning (FL) has garnered significant attention\nas a distributed machine learning paradigm. To facilitate the implementation of\nthe right to be forgotten, the concept of federated machine unlearning (FMU)\nhas also emerged. However, current FMU approaches often involve additional\ntime-consuming steps and may not offer comprehensive unlearning capabilities,\nwhich renders them less practical in real FL scenarios. In this paper, we\nintroduce FedAU, an innovative and efficient FMU framework aimed at overcoming\nthese limitations. Specifically, FedAU incorporates a lightweight auxiliary\nunlearning module into the learning process and employs a straightforward\nlinear operation to facilitate unlearning. This approach eliminates the\nrequirement for extra time-consuming steps, rendering it well-suited for FL.\nFurthermore, FedAU exhibits remarkable versatility. It not only enables\nmultiple clients to carry out unlearning tasks concurrently but also supports\nunlearning at various levels of granularity, including individual data samples,\nspecific classes, and even at the client level. We conducted extensive\nexperiments on MNIST, CIFAR10, and CIFAR100 datasets to evaluate the\nperformance of FedAU. The results demonstrate that FedAU effectively achieves\nthe desired unlearning effect while maintaining model accuracy.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.15474v1.pdf",
        "similarity": 0.4059155765098933,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-24"
    },
    {
        "new_title": "Harmonic Machine Learning Models are Robust",
        "new_link": "http://arxiv.org/abs/2404.18825v1",
        "new_summary": "  We introduce Harmonic Robustness, a powerful and intuitive method to test the\nrobustness of any machine-learning model either during training or in black-box\nreal-time inference monitoring without ground-truth labels. It is based on\nfunctional deviation from the harmonic mean value property, indicating\ninstability and lack of explainability. We show implementation examples in\nlow-dimensional trees and feedforward NNs, where the method reliably identifies\noverfitting, as well as in more complex high-dimensional models such as\nResNet-50 and Vision Transformer where it efficiently measures adversarial\nvulnerability across image classes.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.18825v1.pdf",
        "similarity": 0.40579304732671123,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-29"
    },
    {
        "new_title": "Training all-mechanical neural networks for task learning through in\n  situ backpropagation",
        "new_link": "http://arxiv.org/abs/2404.15471v1",
        "new_summary": "  Recent advances unveiled physical neural networks as promising machine\nlearning platforms, offering faster and more energy-efficient information\nprocessing. Compared with extensively-studied optical neural networks, the\ndevelopment of mechanical neural networks (MNNs) remains nascent and faces\nsignificant challenges, including heavy computational demands and learning with\napproximate gradients. Here, we introduce the mechanical analogue of in situ\nbackpropagation to enable highly efficient training of MNNs. We demonstrate\nthat the exact gradient can be obtained locally in MNNs, enabling learning\nthrough their immediate vicinity. With the gradient information, we showcase\nthe successful training of MNNs for behavior learning and machine learning\ntasks, achieving high accuracy in regression and classification. Furthermore,\nwe present the retrainability of MNNs involving task-switching and damage,\ndemonstrating the resilience. Our findings, which integrate the theory for\ntraining MNNs and experimental and numerical validations, pave the way for\nmechanical machine learning hardware and autonomous self-learning material\nsystems.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.15471v1.pdf",
        "similarity": 0.40561764302909503,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-23"
    },
    {
        "new_title": "Federated Learning for Estimating Heterogeneous Treatment Effects",
        "new_link": "http://arxiv.org/abs/2402.17705v2",
        "new_summary": "  Machine learning methods for estimating heterogeneous treatment effects (HTE)\nfacilitate large-scale personalized decision-making across various domains such\nas healthcare, policy making, education, and more. Current machine learning\napproaches for HTE require access to substantial amounts of data per treatment,\nand the high costs associated with interventions makes centrally collecting so\nmuch data for each intervention a formidable challenge. To overcome this\nobstacle, in this work, we propose a novel framework for collaborative learning\nof HTE estimators across institutions via Federated Learning. We show that even\nunder a diversity of interventions and subject populations across clients, one\ncan jointly learn a common feature representation, while concurrently and\nprivately learning the specific predictive functions for outcomes under\ndistinct interventions across institutions. Our framework and the associated\nalgorithm are based on this insight, and leverage tabular transformers to map\nmultiple input data to feature representations which are then used for outcome\nprediction via multi-task learning. We also propose a novel way of federated\ntraining of personalised transformers that can work with heterogeneous input\nfeature spaces. Experimental results on real-world clinical trial data\ndemonstrate the effectiveness of our method.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.17705v2.pdf",
        "similarity": 0.40531925986208056,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-27"
    },
    {
        "new_title": "Designing High-Performing Networks for Multi-Scale Computer Vision",
        "new_link": "http://arxiv.org/abs/2402.12536v1",
        "new_summary": "  Since the emergence of deep learning, the computer vision field has\nflourished with models improving at a rapid pace on more and more complex\ntasks. We distinguish three main ways to improve a computer vision model: (1)\nimproving the data aspect by for example training on a large, more diverse\ndataset, (2) improving the training aspect by for example designing a better\noptimizer, and (3) improving the network architecture (or network for short).\nIn this thesis, we chose to improve the latter, i.e. improving the network\ndesigns of computer vision models. More specifically, we investigate new\nnetwork designs for multi-scale computer vision tasks, which are tasks\nrequiring to make predictions about concepts at different scales. The goal of\nthese new network designs is to outperform existing baseline designs from the\nliterature. Specific care is taken to make sure the comparisons are fair, by\nguaranteeing that the different network designs were trained and evaluated with\nthe same settings. Code is publicly available at\nhttps://github.com/CedricPicron/DetSeg.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.12536v1.pdf",
        "similarity": 0.40380624372088997,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-19"
    },
    {
        "new_title": "Deep Online Probability Aggregation Clustering",
        "new_link": "http://arxiv.org/abs/2407.05246v2",
        "new_summary": "  Combining machine clustering with deep models has shown remarkable\nsuperiority in deep clustering. It modifies the data processing pipeline into\ntwo alternating phases: feature clustering and model training. However, such\nalternating schedule may lead to instability and computational burden issues.\nWe propose a centerless clustering algorithm called Probability Aggregation\nClustering (PAC) to proactively adapt deep learning technologies, enabling easy\ndeployment in online deep clustering. PAC circumvents the cluster center and\naligns the probability space and distribution space by formulating clustering\nas an optimization problem with a novel objective function. Based on the\ncomputation mechanism of the PAC, we propose a general online probability\naggregation module to perform stable and flexible feature clustering over\nmini-batch data and further construct a deep visual clustering framework deep\nPAC (DPAC). Extensive experiments demonstrate that PAC has superior clustering\nrobustness and performance and DPAC remarkably outperforms the state-of-the-art\ndeep clustering methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.05246v2.pdf",
        "similarity": 0.4037835144919094,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-07"
    },
    {
        "new_title": "Light-SLAM: A Robust Deep-Learning Visual SLAM System Based on LightGlue\n  under Challenging Lighting Conditions",
        "new_link": "http://arxiv.org/abs/2407.02382v1",
        "new_summary": "  Simultaneous Localization and Mapping (SLAM) has become a critical technology\nfor intelligent transportation systems and autonomous robots and is widely used\nin autonomous driving. However, traditional manual feature-based methods in\nchallenging lighting environments make it difficult to ensure robustness and\naccuracy. Some deep learning-based methods show potential but still have\nsignificant drawbacks. To address this problem, we propose a novel hybrid\nsystem for visual SLAM based on the LightGlue deep learning network. It uses\ndeep local feature descriptors to replace traditional hand-crafted features and\na more efficient and accurate deep network to achieve fast and precise feature\nmatching. Thus, we use the robustness of deep learning to improve the whole\nsystem. We have combined traditional geometry-based approaches to introduce a\ncomplete visual SLAM system for monocular, binocular, and RGB-D sensors. We\nthoroughly tested the proposed system on four public datasets: KITTI, EuRoC,\nTUM, and 4Season, as well as on actual campus scenes. The experimental results\nshow that the proposed method exhibits better accuracy and robustness in\nadapting to low-light and strongly light-varying environments than traditional\nmanual features and deep learning-based methods. It can also run on GPU in real\ntime.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.02382v1.pdf",
        "similarity": 0.40355563301685393,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-10"
    },
    {
        "new_title": "Adaptive Prediction Ensemble: Improving Out-of-Distribution\n  Generalization of Motion Forecasting",
        "new_link": "http://arxiv.org/abs/2407.09475v1",
        "new_summary": "  Deep learning-based trajectory prediction models for autonomous driving often\nstruggle with generalization to out-of-distribution (OOD) scenarios, sometimes\nperforming worse than simple rule-based models. To address this limitation, we\npropose a novel framework, Adaptive Prediction Ensemble (APE), which integrates\ndeep learning and rule-based prediction experts. A learned routing function,\ntrained concurrently with the deep learning model, dynamically selects the most\nreliable prediction based on the input scenario. Our experiments on large-scale\ndatasets, including Waymo Open Motion Dataset (WOMD) and Argoverse, demonstrate\nimprovement in zero-shot generalization across datasets. We show that our\nmethod outperforms individual prediction models and other variants,\nparticularly in long-horizon prediction and scenarios with a high proportion of\nOOD data. This work highlights the potential of hybrid approaches for robust\nand generalizable motion prediction in autonomous driving.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.09475v1.pdf",
        "similarity": 0.40326962068333994,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-12"
    },
    {
        "new_title": "A Declarative Query Language for Scientific Machine Learning",
        "new_link": "http://arxiv.org/abs/2405.16159v1",
        "new_summary": "  The popularity of data science as a discipline and its importance in the\nemerging economy and industrial progress dictate that machine learning be\ndemocratized for the masses. This also means that the current practice of\nworkforce training using machine learning tools, which requires low-level\nstatistical and algorithmic details, is a barrier that needs to be addressed.\nSimilar to data management languages such as SQL, machine learning needs to be\npracticed at a conceptual level to help make it a staple tool for general\nusers. In particular, the technical sophistication demanded by existing machine\nlearning frameworks is prohibitive for many scientists who are not\ncomputationally savvy or well versed in machine learning techniques. The\nlearning curve to use the needed machine learning tools is also too high for\nthem to take advantage of these powerful platforms to rapidly advance science.\nIn this paper, we introduce a new declarative machine learning query language,\ncalled {\\em MQL}, for naive users. We discuss its merit and possible ways of\nimplementing it over a traditional relational database system. We discuss two\nmaterials science experiments implemented using MQL on a materials science\nworkflow system called MatFlow.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.16159v1.pdf",
        "similarity": 0.40296222583565583,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-25"
    },
    {
        "new_title": "Calibration-then-Calculation: A Variance Reduced Metric Framework in\n  Deep Click-Through Rate Prediction Models",
        "new_link": "http://arxiv.org/abs/2401.16692v2",
        "new_summary": "  The adoption of deep learning across various fields has been extensive, yet\nthere is a lack of focus on evaluating the performance of deep learning\npipelines. Typically, with the increased use of large datasets and complex\nmodels, the training process is run only once and the result is compared to\nprevious benchmarks. This practice can lead to imprecise comparisons due to the\nvariance in neural network evaluation metrics, which stems from the inherent\nrandomness in the training process. Traditional solutions, such as running the\ntraining process multiple times, are often infeasible due to computational\nconstraints. In this paper, we introduce a novel metric framework, the\nCalibrated Loss Metric, designed to address this issue by reducing the variance\npresent in its conventional counterpart. Consequently, this new metric enhances\nthe accuracy in detecting effective modeling improvements. Our approach is\nsubstantiated by theoretical justifications and extensive experimental\nvalidations within the context of Deep Click-Through Rate Prediction Models.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.16692v2.pdf",
        "similarity": 0.4027877010972273,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-30"
    },
    {
        "new_title": "Resource Allocation and Workload Scheduling for Large-Scale Distributed\n  Deep Learning: A Survey",
        "new_link": "http://arxiv.org/abs/2406.08115v1",
        "new_summary": "  With rapidly increasing distributed deep learning workloads in large-scale\ndata centers, efficient distributed deep learning framework strategies for\nresource allocation and workload scheduling have become the key to\nhigh-performance deep learning. The large-scale environment with large volumes\nof datasets, models, and computational and communication resources raises\nvarious unique challenges for resource allocation and workload scheduling in\ndistributed deep learning, such as scheduling complexity, resource and workload\nheterogeneity, and fault tolerance. To uncover these challenges and\ncorresponding solutions, this survey reviews the literature, mainly from 2019\nto 2024, on efficient resource allocation and workload scheduling strategies\nfor large-scale distributed DL. We explore these strategies by focusing on\nvarious resource types, scheduling granularity levels, and performance goals\nduring distributed training and inference processes. We highlight critical\nchallenges for each topic and discuss key insights of existing technologies. To\nillustrate practical large-scale resource allocation and workload scheduling in\nreal distributed deep learning scenarios, we use a case study of training large\nlanguage models. This survey aims to encourage computer science, artificial\nintelligence, and communications researchers to understand recent advances and\nexplore future research directions for efficient framework strategies for\nlarge-scale distributed deep learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.08115v1.pdf",
        "similarity": 0.40258327732642385,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-12"
    },
    {
        "new_title": "Semi-Supervised Multimodal Multi-Instance Learning for Aortic Stenosis\n  Diagnosis",
        "new_link": "http://arxiv.org/abs/2403.06024v1",
        "new_summary": "  Automated interpretation of ultrasound imaging of the heart (echocardiograms)\ncould improve the detection and treatment of aortic stenosis (AS), a deadly\nheart disease. However, existing deep learning pipelines for assessing AS from\nechocardiograms have two key limitations. First, most methods rely on limited\n2D cineloops, thereby ignoring widely available Doppler imaging that contains\nimportant complementary information about pressure gradients and blood flow\nabnormalities associated with AS. Second, obtaining labeled data is difficult.\nThere are often far more unlabeled echocardiogram recordings available, but\nthese remain underutilized by existing methods. To overcome these limitations,\nwe introduce Semi-supervised Multimodal Multiple-Instance Learning (SMMIL), a\nnew deep learning framework for automatic interpretation for structural heart\ndiseases like AS. When deployed, SMMIL can combine information from two input\nmodalities, spectral Dopplers and 2D cineloops, to produce a study-level AS\ndiagnosis. During training, SMMIL can combine a smaller labeled set and an\nabundant unlabeled set of both modalities to improve its classifier.\nExperiments demonstrate that SMMIL outperforms recent alternatives at 3-level\nAS severity classification as well as several clinically relevant AS detection\ntasks.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.06024v1.pdf",
        "similarity": 0.4021116845623205,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-09"
    },
    {
        "new_title": "QClusformer: A Quantum Transformer-based Framework for Unsupervised\n  Visual Clustering",
        "new_link": "http://arxiv.org/abs/2405.19722v1",
        "new_summary": "  Unsupervised vision clustering, a cornerstone in computer vision, has been\nstudied for decades, yielding significant outcomes across numerous vision\ntasks. However, these algorithms involve substantial computational demands when\nconfronted with vast amounts of unlabeled data. Conversely, Quantum computing\nholds promise in expediting unsupervised algorithms when handling large-scale\ndatabases. In this study, we introduce QClusformer, a pioneering\nTransformer-based framework leveraging Quantum machines to tackle unsupervised\nvision clustering challenges. Specifically, we design the Transformer\narchitecture, including the self-attention module and transformer blocks, from\na Quantum perspective to enable execution on Quantum hardware. In addition, we\npresent QClusformer, a variant based on the Transformer architecture, tailored\nfor unsupervised vision clustering tasks. By integrating these elements into an\nend-to-end framework, QClusformer consistently outperforms previous methods\nrunning on classical computers. Empirical evaluations across diverse\nbenchmarks, including MS-Celeb-1M and DeepFashion, underscore the superior\nperformance of QClusformer compared to state-of-the-art methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.19722v1.pdf",
        "similarity": 0.40193582778845965,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-30"
    },
    {
        "new_title": "Knowledge-driven deep learning for fast MR imaging: undersampled MR\n  image reconstruction from supervised to un-supervised learning",
        "new_link": "http://arxiv.org/abs/2402.02704v1",
        "new_summary": "  Deep learning (DL) has emerged as a leading approach in accelerating MR\nimaging. It employs deep neural networks to extract knowledge from available\ndatasets and then applies the trained networks to reconstruct accurate images\nfrom limited measurements. Unlike natural image restoration problems, MR\nimaging involves physics-based imaging processes, unique data properties, and\ndiverse imaging tasks. This domain knowledge needs to be integrated with\ndata-driven approaches. Our review will introduce the significant challenges\nfaced by such knowledge-driven DL approaches in the context of fast MR imaging\nalong with several notable solutions, which include learning neural networks\nand addressing different imaging application scenarios. The traits and trends\nof these techniques have also been given which have shifted from supervised\nlearning to semi-supervised learning, and finally, to unsupervised learning\nmethods. In addition, MR vendors' choices of DL reconstruction have been\nprovided along with some discussions on open questions and future directions,\nwhich are critical for the reliable imaging systems.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.02704v1.pdf",
        "similarity": 0.4015895395319134,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-05"
    },
    {
        "new_title": "Learning Interpretable Concepts: Unifying Causal Representation Learning\n  and Foundation Models",
        "new_link": "http://arxiv.org/abs/2402.09236v1",
        "new_summary": "  To build intelligent machine learning systems, there are two broad\napproaches. One approach is to build inherently interpretable models, as\nendeavored by the growing field of causal representation learning. The other\napproach is to build highly-performant foundation models and then invest\nefforts into understanding how they work. In this work, we relate these two\napproaches and study how to learn human-interpretable concepts from data.\nWeaving together ideas from both fields, we formally define a notion of\nconcepts and show that they can be provably recovered from diverse data.\nExperiments on synthetic data and large language models show the utility of our\nunified approach.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.09236v1.pdf",
        "similarity": 0.4014770272136308,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-14"
    },
    {
        "new_title": "Deep video representation learning: a survey",
        "new_link": "http://arxiv.org/abs/2405.06574v1",
        "new_summary": "  This paper provides a review on representation learning for videos. We\nclassify recent spatiotemporal feature learning methods for sequential visual\ndata and compare their pros and cons for general video analysis. Building\neffective features for videos is a fundamental problem in computer vision tasks\ninvolving video analysis and understanding. Existing features can be generally\ncategorized into spatial and temporal features. Their effectiveness under\nvariations of illumination, occlusion, view and background are discussed.\nFinally, we discuss the remaining challenges in existing deep video\nrepresentation learning studies.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.06574v1.pdf",
        "similarity": 0.40130775610553826,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-10"
    },
    {
        "new_title": "Standardness Fogs Meaning: A Position Regarding the Informed Usage of\n  Standard Datasets",
        "new_link": "http://arxiv.org/abs/2406.13552v1",
        "new_summary": "  Standard datasets are frequently used to train and evaluate Machine Learning\nmodels. However, the assumed standardness of these datasets leads to a lack of\nin-depth discussion on how their labels match the derived categories for the\nrespective use case. In other words, the standardness of the datasets seems to\nfog coherency and applicability, thus impeding the trust in Machine Learning\nmodels. We propose to adopt Grounded Theory and Hypotheses Testing through\nVisualization as methods to evaluate the match between use case, derived\ncategories, and labels of standard datasets. To showcase the approach, we apply\nit to the 20 Newsgroups dataset and the MNIST dataset. For the 20 Newsgroups\ndataset, we demonstrate that the labels are imprecise. Therefore, we argue that\nneither a Machine Learning model can learn a meaningful abstraction of derived\ncategories nor one can draw conclusions from achieving high accuracy. For the\nMNIST dataset, we demonstrate how the labels can be confirmed to be defined\nwell. We conclude that a concept of standardness of a dataset implies that\nthere is a match between use case, derived categories, and class labels, as in\nthe case of the MNIST dataset. We argue that this is necessary to learn a\nmeaningful abstraction and, thus, improve trust in the Machine Learning model.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.13552v1.pdf",
        "similarity": 0.4012911290042161,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-19"
    },
    {
        "new_title": "Predictive, scalable and interpretable knowledge tracing on structured\n  domains",
        "new_link": "http://arxiv.org/abs/2403.13179v1",
        "new_summary": "  Intelligent tutoring systems optimize the selection and timing of learning\nmaterials to enhance understanding and long-term retention. This requires\nestimates of both the learner's progress (''knowledge tracing''; KT), and the\nprerequisite structure of the learning domain (''knowledge mapping''). While\nrecent deep learning models achieve high KT accuracy, they do so at the expense\nof the interpretability of psychologically-inspired models. In this work, we\npresent a solution to this trade-off. PSI-KT is a hierarchical generative\napproach that explicitly models how both individual cognitive traits and the\nprerequisite structure of knowledge influence learning dynamics, thus achieving\ninterpretability by design. Moreover, by using scalable Bayesian inference,\nPSI-KT targets the real-world need for efficient personalization even with a\ngrowing body of learners and learning histories. Evaluated on three datasets\nfrom online learning platforms, PSI-KT achieves superior multi-step predictive\naccuracy and scalable inference in continual-learning settings, all while\nproviding interpretable representations of learner-specific traits and the\nprerequisite structure of knowledge that causally supports learning. In sum,\npredictive, scalable and interpretable knowledge tracing with solid knowledge\nmapping lays a key foundation for effective personalized learning to make\neducation accessible to a broad, global audience.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.13179v1.pdf",
        "similarity": 0.40123131226224273,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-19"
    },
    {
        "new_title": "Enhancing Context Through Contrast",
        "new_link": "http://arxiv.org/abs/2401.03314v1",
        "new_summary": "  Neural machine translation benefits from semantically rich representations.\nConsiderable progress in learning such representations has been achieved by\nlanguage modelling and mutual information maximization objectives using\ncontrastive learning. The language-dependent nature of language modelling\nintroduces a trade-off between the universality of the learned representations\nand the model's performance on the language modelling tasks. Although\ncontrastive learning improves performance, its success cannot be attributed to\nmutual information alone. We propose a novel Context Enhancement step to\nimprove performance on neural machine translation by maximizing mutual\ninformation using the Barlow Twins loss. Unlike other approaches, we do not\nexplicitly augment the data but view languages as implicit augmentations,\neradicating the risk of disrupting semantic information. Further, our method\ndoes not learn embeddings from scratch and can be generalised to any set of\npre-trained embeddings. Finally, we evaluate the language-agnosticism of our\nembeddings through language classification and use them for neural machine\ntranslation to compare with state-of-the-art approaches.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.03314v1.pdf",
        "similarity": 0.4012156541947464,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-06"
    },
    {
        "new_title": "Can Machine Translation Bridge Multilingual Pretraining and\n  Cross-lingual Transfer Learning?",
        "new_link": "http://arxiv.org/abs/2403.16777v1",
        "new_summary": "  Multilingual pretraining and fine-tuning have remarkably succeeded in various\nnatural language processing tasks. Transferring representations from one\nlanguage to another is especially crucial for cross-lingual learning. One can\nexpect machine translation objectives to be well suited to fostering such\ncapabilities, as they involve the explicit alignment of semantically equivalent\nsentences from different languages. This paper investigates the potential\nbenefits of employing machine translation as a continued training objective to\nenhance language representation learning, bridging multilingual pretraining and\ncross-lingual applications. We study this question through two lenses: a\nquantitative evaluation of the performance of existing models and an analysis\nof their latent representations. Our results show that, contrary to\nexpectations, machine translation as the continued training fails to enhance\ncross-lingual representation learning in multiple cross-lingual natural\nlanguage understanding tasks. We conclude that explicit sentence-level\nalignment in the cross-lingual scenario is detrimental to cross-lingual\ntransfer pretraining, which has important implications for future cross-lingual\ntransfer studies. We furthermore provide evidence through similarity measures\nand investigation of parameters that this lack of positive influence is due to\noutput separability -- which we argue is of use for machine translation but\ndetrimental elsewhere.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.16777v1.pdf",
        "similarity": 0.4011984144157747,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-25"
    },
    {
        "new_title": "Early learning of the optimal constant solution in neural networks and\n  humans",
        "new_link": "http://arxiv.org/abs/2406.17467v1",
        "new_summary": "  Deep neural networks learn increasingly complex functions over the course of\ntraining. Here, we show both empirically and theoretically that learning of the\ntarget function is preceded by an early phase in which networks learn the\noptimal constant solution (OCS) - that is, initial model responses mirror the\ndistribution of target labels, while entirely ignoring information provided in\nthe input. Using a hierarchical category learning task, we derive exact\nsolutions for learning dynamics in deep linear networks trained with bias\nterms. Even when initialized to zero, this simple architectural feature induces\nsubstantial changes in early dynamics. We identify hallmarks of this early OCS\nphase and illustrate how these signatures are observed in deep linear networks\nand larger, more complex (and nonlinear) convolutional neural networks solving\na hierarchical learning task based on MNIST and CIFAR10. We explain these\nobservations by proving that deep linear networks necessarily learn the OCS\nduring early learning. To further probe the generality of our results, we train\nhuman learners over the course of three days on the category learning task. We\nthen identify qualitative signatures of this early OCS phase in terms of the\ndynamics of true negative (correct-rejection) rates. Surprisingly, we find the\nsame early reliance on the OCS in the behaviour of human learners. Finally, we\nshow that learning of the OCS can emerge even in the absence of bias terms and\nis equivalently driven by generic correlations in the input data. Overall, our\nwork suggests the OCS as a universal learning principle in supervised,\nerror-corrective learning, and the mechanistic reasons for its prevalence.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.17467v1.pdf",
        "similarity": 0.4001421992848793,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-25"
    },
    {
        "new_title": "Geometric Constraints in Deep Learning Frameworks: A Survey",
        "new_link": "http://arxiv.org/abs/2403.12431v1",
        "new_summary": "  Stereophotogrammetry is an emerging technique of scene understanding. Its\norigins go back to at least the 1800s when people first started to investigate\nusing photographs to measure the physical properties of the world. Since then,\nthousands of approaches have been explored. The classic geometric techniques of\nShape from Stereo is built on using geometry to define constraints on scene and\ncamera geometry and then solving the non-linear systems of equations. More\nrecent work has taken an entirely different approach, using end-to-end deep\nlearning without any attempt to explicitly model the geometry. In this survey,\nwe explore the overlap for geometric-based and deep learning-based frameworks.\nWe compare and contrast geometry enforcing constraints integrated into a deep\nlearning framework for depth estimation or other closely related problems. We\npresent a new taxonomy for prevalent geometry enforcing constraints used in\nmodern deep learning frameworks. We also present insightful observations and\npotential future research directions.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.12431v1.pdf",
        "similarity": 0.40004022348070756,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-19"
    },
    {
        "new_title": "Semi-adaptive Synergetic Two-way Pseudoinverse Learning System",
        "new_link": "http://arxiv.org/abs/2406.18931v2",
        "new_summary": "  Deep learning has become a crucial technology for making breakthroughs in\nmany fields. Nevertheless, it still faces two important challenges in\ntheoretical and applied aspects. The first lies in the shortcomings of gradient\ndescent based learning schemes which are time-consuming and difficult to\ndetermine the learning control hyperparameters. Next, the architectural design\nof the model is usually tricky. In this paper, we propose a semi-adaptive\nsynergetic two-way pseudoinverse learning system, wherein each subsystem\nencompasses forward learning, backward learning, and feature concatenation\nmodules. The whole system is trained using a non-gradient descent learning\nalgorithm. It simplifies the hyperparameter tuning while improving the training\nefficiency. The architecture of the subsystems is designed using a data-driven\napproach that enables automated determination of the depth of the subsystems.\nWe compare our method with the baselines of mainstream non-gradient descent\nbased methods and the results demonstrate the effectiveness of our proposed\nmethod. The source code for this paper is available at\nhttp://github.com/B-berrypie/Semi-adaptive-Synergetic-Two-way-Pseudoinverse-Learning-System}{http://github.com/B-berrypie/Semi-adaptive-Synergetic-Two-way-Pseudoinverse-Learning-System.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.18931v2.pdf",
        "similarity": 0.39998990313518656,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-27"
    },
    {
        "new_title": "COVIDHealth: A Benchmark Twitter Dataset and Machine Learning based Web\n  Application for Classifying COVID-19 Discussions",
        "new_link": "http://arxiv.org/abs/2402.09897v1",
        "new_summary": "  The COVID-19 pandemic has had adverse effects on both physical and mental\nhealth. During this pandemic, numerous studies have focused on gaining insights\ninto health-related perspectives from social media. In this study, our primary\nobjective is to develop a machine learning-based web application for\nautomatically classifying COVID-19-related discussions on social media. To\nachieve this, we label COVID-19-related Twitter data, provide benchmark\nclassification results, and develop a web application. We collected data using\nthe Twitter API and labeled a total of 6,667 tweets into five different\nclasses: health risks, prevention, symptoms, transmission, and treatment. We\nextracted features using various feature extraction methods and applied them to\nseven different traditional machine learning algorithms, including Decision\nTree, Random Forest, Stochastic Gradient Descent, Adaboost, K-Nearest\nNeighbour, Logistic Regression, and Linear SVC. Additionally, we used four deep\nlearning algorithms: LSTM, CNN, RNN, and BERT, for classification. Overall, we\nachieved a maximum F1 score of 90.43% with the CNN algorithm in deep learning.\nThe Linear SVC algorithm exhibited the highest F1 score at 86.13%, surpassing\nother traditional machine learning approaches. Our study not only contributes\nto the field of health-related data analysis but also provides a valuable\nresource in the form of a web-based tool for efficient data classification,\nwhich can aid in addressing public health challenges and increasing awareness\nduring pandemics. We made the dataset and application publicly available, which\ncan be downloaded from this link\nhttps://github.com/Bishal16/COVID19-Health-Related-Data-Classification-Website.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.09897v1.pdf",
        "similarity": 0.39963961227699296,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-15"
    },
    {
        "new_title": "Addressing Loss of Plasticity and Catastrophic Forgetting in Continual\n  Learning",
        "new_link": "http://arxiv.org/abs/2404.00781v2",
        "new_summary": "  Deep representation learning methods struggle with continual learning,\nsuffering from both catastrophic forgetting of useful units and loss of\nplasticity, often due to rigid and unuseful units. While many methods address\nthese two issues separately, only a few currently deal with both\nsimultaneously. In this paper, we introduce Utility-based Perturbed Gradient\nDescent (UPGD) as a novel approach for the continual learning of\nrepresentations. UPGD combines gradient updates with perturbations, where it\napplies smaller modifications to more useful units, protecting them from\nforgetting, and larger modifications to less useful units, rejuvenating their\nplasticity. We use a challenging streaming learning setup where continual\nlearning problems have hundreds of non-stationarities and unknown task\nboundaries. We show that many existing methods suffer from at least one of the\nissues, predominantly manifested by their decreasing accuracy over tasks. On\nthe other hand, UPGD continues to improve performance and surpasses or is\ncompetitive with all methods in all problems. Finally, in extended\nreinforcement learning experiments with PPO, we show that while Adam exhibits a\nperformance drop after initial learning, UPGD avoids it by addressing both\ncontinual learning issues.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.00781v2.pdf",
        "similarity": 0.39949931200061684,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-31"
    },
    {
        "new_title": "DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement\n  and Imitation Learning",
        "new_link": "http://arxiv.org/abs/2402.05421v2",
        "new_summary": "  This paper introduces DiffTOP, which utilizes Differentiable Trajectory\nOPtimization as the policy representation to generate actions for deep\nreinforcement and imitation learning. Trajectory optimization is a powerful and\nwidely used algorithm in control, parameterized by a cost and a dynamics\nfunction. The key to our approach is to leverage the recent progress in\ndifferentiable trajectory optimization, which enables computing the gradients\nof the loss with respect to the parameters of trajectory optimization. As a\nresult, the cost and dynamics functions of trajectory optimization can be\nlearned end-to-end. DiffTOP addresses the ``objective mismatch'' issue of prior\nmodel-based RL algorithms, as the dynamics model in DiffTOP is learned to\ndirectly maximize task performance by differentiating the policy gradient loss\nthrough the trajectory optimization process. We further benchmark DiffTOP for\nimitation learning on standard robotic manipulation task suites with\nhigh-dimensional sensory observations and compare our method to feed-forward\npolicy classes as well as Energy-Based Models (EBM) and Diffusion. Across 15\nmodel-based RL tasks and 35imitation learning tasks with high-dimensional image\nand point cloud inputs, DiffTOP outperforms prior state-of-the-art methods in\nboth domains.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.05421v2.pdf",
        "similarity": 0.3994388094564018,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-08"
    },
    {
        "new_title": "Learning deep illumination-robust features from multispectral filter\n  array images",
        "new_link": "http://arxiv.org/abs/2407.15472v2",
        "new_summary": "  Multispectral (MS) snapshot cameras equipped with a MS filter array (MSFA),\ncapture multiple spectral bands in a single shot, resulting in a raw mosaic\nimage where each pixel holds only one channel value. The fully-defined MS image\nis estimated from the raw one through $\\textit{demosaicing}$, which inevitably\nintroduces spatio-spectral artifacts. Moreover, training on fully-defined MS\nimages can be computationally intensive, particularly with deep neural networks\n(DNNs), and may result in features lacking discrimination power due to\nsuboptimal learning of spatio-spectral interactions. Furthermore, outdoor MS\nimage acquisition occurs under varying lighting conditions, leading to\nillumination-dependent features. This paper presents an original approach to\nlearn discriminant and illumination-robust features directly from raw images.\nIt involves: $\\textit{raw spectral constancy}$ to mitigate the impact of\nillumination, $\\textit{MSFA-preserving}$ transformations suited for raw image\naugmentation to train DNNs on diverse raw textures, and $\\textit{raw-mixing}$\nto capture discriminant spatio-spectral interactions in raw images. Experiments\non MS image classification show that our approach outperforms both handcrafted\nand recent deep learning-based methods, while also requiring significantly less\ncomputational effort.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.15472v2.pdf",
        "similarity": 0.3994130759864005,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-22"
    },
    {
        "new_title": "Tilting the Odds at the Lottery: the Interplay of Overparameterisation\n  and Curricula in Neural Networks",
        "new_link": "http://arxiv.org/abs/2406.01589v1",
        "new_summary": "  A wide range of empirical and theoretical works have shown that\noverparameterisation can amplify the performance of neural networks. According\nto the lottery ticket hypothesis, overparameterised networks have an increased\nchance of containing a sub-network that is well-initialised to solve the task\nat hand. A more parsimonious approach, inspired by animal learning, consists in\nguiding the learner towards solving the task by curating the order of the\nexamples, i.e. providing a curriculum. However, this learning strategy seems to\nbe hardly beneficial in deep learning applications. In this work, we undertake\nan analytical study that connects curriculum learning and overparameterisation.\nIn particular, we investigate their interplay in the online learning setting\nfor a 2-layer network in the XOR-like Gaussian Mixture problem. Our results\nshow that a high degree of overparameterisation -- while simplifying the\nproblem -- can limit the benefit from curricula, providing a theoretical\naccount of the ineffectiveness of curricula in deep learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.01589v1.pdf",
        "similarity": 0.39919209334118866,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-03"
    },
    {
        "new_title": "Lacunarity Pooling Layers for Plant Image Classification using Texture\n  Analysis",
        "new_link": "http://arxiv.org/abs/2404.16268v2",
        "new_summary": "  Pooling layers (e.g., max and average) may overlook important information\nencoded in the spatial arrangement of pixel intensity and/or feature values. We\npropose a novel lacunarity pooling layer that aims to capture the spatial\nheterogeneity of the feature maps by evaluating the variability within local\nwindows. The layer operates at multiple scales, allowing the network to\nadaptively learn hierarchical features. The lacunarity pooling layer can be\nseamlessly integrated into any artificial neural network architecture.\nExperimental results demonstrate the layer's effectiveness in capturing\nintricate spatial patterns, leading to improved feature extraction\ncapabilities. The proposed approach holds promise in various domains,\nespecially in agricultural image analysis tasks. This work contributes to the\nevolving landscape of artificial neural network architectures by introducing a\nnovel pooling layer that enriches the representation of spatial features. Our\ncode is publicly available.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.16268v2.pdf",
        "similarity": 0.3989976573196742,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-25"
    },
    {
        "new_title": "Robust and Explainable Framework to Address Data Scarcity in Diagnostic\n  Imaging",
        "new_link": "http://arxiv.org/abs/2407.06566v1",
        "new_summary": "  Deep learning has significantly advanced automatic medical diagnostics and\nreleased the occupation of human resources to reduce clinical pressure, yet the\npersistent challenge of data scarcity in this area hampers its further\nimprovements and applications. To address this gap, we introduce a novel\nensemble framework called `Efficient Transfer and Self-supervised Learning\nbased Ensemble Framework' (ETSEF). ETSEF leverages features from multiple\npre-trained deep learning models to efficiently learn powerful representations\nfrom a limited number of data samples. To the best of our knowledge, ETSEF is\nthe first strategy that combines two pre-training methodologies (Transfer\nLearning and Self-supervised Learning) with ensemble learning approaches.\nVarious data enhancement techniques, including data augmentation, feature\nfusion, feature selection, and decision fusion, have also been deployed to\nmaximise the efficiency and robustness of the ETSEF model. Five independent\nmedical imaging tasks, including endoscopy, breast cancer, monkeypox, brain\ntumour, and glaucoma detection, were tested to demonstrate ETSEF's\neffectiveness and robustness. Facing limited sample numbers and challenging\nmedical tasks, ETSEF has proved its effectiveness by improving diagnostics\naccuracies from 10\\% to 13.3\\% when compared to strong ensemble baseline models\nand up to 14.4\\% improvements compared with published state-of-the-art methods.\nMoreover, we emphasise the robustness and trustworthiness of the ETSEF method\nthrough various vision-explainable artificial intelligence techniques,\nincluding Grad-CAM, SHAP, and t-SNE. Compared to those large-scale deep\nlearning models, ETSEF can be deployed flexibly and maintain superior\nperformance for challenging medical imaging tasks, showing the potential to be\napplied to more areas that lack training data\n",
        "pdf_link": "https://arxiv.org/pdf/2407.06566v1.pdf",
        "similarity": 0.3989003147593241,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-09"
    },
    {
        "new_title": "Boosting Cardiac Color Doppler Frame Rates with Deep Learning",
        "new_link": "http://arxiv.org/abs/2404.00067v1",
        "new_summary": "  Color Doppler echocardiography enables visualization of blood flow within the\nheart. However, the limited frame rate impedes the quantitative assessment of\nblood velocity throughout the cardiac cycle, thereby compromising a\ncomprehensive analysis of ventricular filling. Concurrently, deep learning is\ndemonstrating promising outcomes in post-processing of echocardiographic data\nfor various applications. This work explores the use of deep learning models\nfor intracardiac Doppler velocity estimation from a reduced number of filtered\nI/Q signals. We used a supervised learning approach by simulating patient-based\ncardiac color Doppler acquisitions and proposed data augmentation strategies to\nenlarge the training dataset. We implemented architectures based on\nconvolutional neural networks. In particular, we focused on comparing the U-Net\nmodel and the recent ConvNeXt models, alongside assessing real-valued versus\ncomplex-valued representations. We found that both models outperformed the\nstate-of-the-art autocorrelator method, effectively mitigating aliasing and\nnoise. We did not observe significant differences between the use of real and\ncomplex data. Finally, we validated the models on in vitro and in vivo\nexperiments. All models produced quantitatively comparable results to the\nbaseline and were more robust to noise. ConvNeXt emerged as the sole model to\nachieve high-quality results on in vivo aliased samples. These results\ndemonstrate the interest of supervised deep learning methods for Doppler\nvelocity estimation from a reduced number of acquisitions.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.00067v1.pdf",
        "similarity": 0.39840766905435826,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-28"
    },
    {
        "new_title": "YAMLE: Yet Another Machine Learning Environment",
        "new_link": "http://arxiv.org/abs/2402.06268v1",
        "new_summary": "  YAMLE: Yet Another Machine Learning Environment is an open-source framework\nthat facilitates rapid prototyping and experimentation with machine learning\n(ML) models and methods. The key motivation is to reduce repetitive work when\nimplementing new approaches and improve reproducibility in ML research. YAMLE\nincludes a command-line interface and integrations with popular and\nwell-maintained PyTorch-based libraries to streamline training, hyperparameter\noptimisation, and logging. The ambition for YAMLE is to grow into a shared\necosystem where researchers and practitioners can quickly build on and compare\nexisting implementations. Find it at: https://github.com/martinferianc/yamle.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.06268v1.pdf",
        "similarity": 0.39839403448086996,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-09"
    },
    {
        "new_title": "Deep Clustering Using the Soft Silhouette Score: Towards Compact and\n  Well-Separated Clusters",
        "new_link": "http://arxiv.org/abs/2402.00608v1",
        "new_summary": "  Unsupervised learning has gained prominence in the big data era, offering a\nmeans to extract valuable insights from unlabeled datasets. Deep clustering has\nemerged as an important unsupervised category, aiming to exploit the non-linear\nmapping capabilities of neural networks in order to enhance clustering\nperformance. The majority of deep clustering literature focuses on minimizing\nthe inner-cluster variability in some embedded space while keeping the learned\nrepresentation consistent with the original high-dimensional dataset. In this\nwork, we propose soft silhoutte, a probabilistic formulation of the silhouette\ncoefficient. Soft silhouette rewards compact and distinctly separated\nclustering solutions like the conventional silhouette coefficient. When\noptimized within a deep clustering framework, soft silhouette guides the\nlearned representations towards forming compact and well-separated clusters. In\naddition, we introduce an autoencoder-based deep learning architecture that is\nsuitable for optimizing the soft silhouette objective function. The proposed\ndeep clustering method has been tested and compared with several well-studied\ndeep clustering methods on various benchmark datasets, yielding very\nsatisfactory clustering results.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.00608v1.pdf",
        "similarity": 0.3983751480636347,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-01"
    },
    {
        "new_title": "Symmetry Discovery Beyond Affine Transformations",
        "new_link": "http://arxiv.org/abs/2406.03619v1",
        "new_summary": "  Symmetry detection has been shown to improve various machine learning tasks.\nIn the context of continuous symmetry detection, current state of the art\nexperiments are limited to the detection of affine transformations. Under the\nmanifold assumption, we outline a framework for discovering continuous symmetry\nin data beyond the affine transformation group. We also provide a similar\nframework for discovering discrete symmetry. We experimentally compare our\nmethod to an existing method known as LieGAN and show that our method is\ncompetitive at detecting affine symmetries for large sample sizes and superior\nthan LieGAN for small sample sizes. We also show our method is able to detect\ncontinuous symmetries beyond the affine group and is generally more\ncomputationally efficient than LieGAN.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.03619v1.pdf",
        "similarity": 0.39787920694161366,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-05"
    },
    {
        "new_title": "In-Context Freeze-Thaw Bayesian Optimization for Hyperparameter\n  Optimization",
        "new_link": "http://arxiv.org/abs/2404.16795v2",
        "new_summary": "  With the increasing computational costs associated with deep learning,\nautomated hyperparameter optimization methods, strongly relying on black-box\nBayesian optimization (BO), face limitations. Freeze-thaw BO offers a promising\ngrey-box alternative, strategically allocating scarce resources incrementally\nto different configurations. However, the frequent surrogate model updates\ninherent to this approach pose challenges for existing methods, requiring\nretraining or fine-tuning their neural network surrogates online, introducing\noverhead, instability, and hyper-hyperparameters. In this work, we propose\nFT-PFN, a novel surrogate for Freeze-thaw style BO. FT-PFN is a prior-data\nfitted network (PFN) that leverages the transformers' in-context learning\nability to efficiently and reliably do Bayesian learning curve extrapolation in\na single forward pass. Our empirical analysis across three benchmark suites\nshows that the predictions made by FT-PFN are more accurate and 10-100 times\nfaster than those of the deep Gaussian process and deep ensemble surrogates\nused in previous work. Furthermore, we show that, when combined with our novel\nacquisition mechanism (MFPI-random), the resulting in-context freeze-thaw BO\nmethod (ifBO), yields new state-of-the-art performance in the same three\nfamilies of deep learning HPO benchmarks considered in prior work.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.16795v2.pdf",
        "similarity": 0.3978387976133788,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-25"
    },
    {
        "new_title": "Classification of Tennis Actions Using Deep Learning",
        "new_link": "http://arxiv.org/abs/2402.02545v1",
        "new_summary": "  Recent advances of deep learning makes it possible to identify specific\nevents in videos with greater precision. This has great relevance in sports\nlike tennis in order to e.g., automatically collect game statistics, or replay\nactions of specific interest for game strategy or player improvements. In this\npaper, we investigate the potential and the challenges of using deep learning\nto classify tennis actions. Three models of different size, all based on the\ndeep learning architecture SlowFast were trained and evaluated on the academic\ntennis dataset THETIS. The best models achieve a generalization accuracy of 74\n%, demonstrating a good performance for tennis action classification. We\nprovide an error analysis for the best model and pinpoint directions for\nimprovement of tennis datasets in general. We discuss the limitations of the\ndata set, general limitations of current publicly available tennis data-sets,\nand future steps needed to make progress.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.02545v1.pdf",
        "similarity": 0.3973358107694158,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-04"
    },
    {
        "new_title": "Towards Calibrated Deep Clustering Network",
        "new_link": "http://arxiv.org/abs/2403.02998v2",
        "new_summary": "  Deep clustering has exhibited remarkable performance; however, the\nover-confidence problem, i.e., the estimated confidence for a sample belonging\nto a particular cluster greatly exceeds its actual prediction accuracy, has\nbeen overlooked in prior research. To tackle this critical issue, we pioneer\nthe development of a calibrated deep clustering framework. Specifically, we\npropose a novel dual-head (calibration head and clustering head) deep\nclustering model that can effectively calibrate the estimated confidence and\nthe actual accuracy. The calibration head adjusts the overconfident predictions\nof the clustering head, generating prediction confidence that match the model\nlearning status. Then, the clustering head dynamically select reliable\nhigh-confidence samples estimated by the calibration head for pseudo-label\nself-training. Additionally, we introduce an effective network initialization\nstrategy that enhances both training speed and network robustness. The\neffectiveness of the proposed calibration approach and initialization strategy\nare both endorsed with solid theoretical guarantees. Extensive experiments\ndemonstrate the proposed calibrated deep clustering model not only surpasses\nstate-of-the-art deep clustering methods by 10 times in terms of expected\ncalibration error but also significantly outperforms them in terms of\nclustering accuracy.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.02998v2.pdf",
        "similarity": 0.396486337300826,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-04"
    },
    {
        "new_title": "Equivariant Machine Learning on Graphs with Nonlinear Spectral Filters",
        "new_link": "http://arxiv.org/abs/2406.01249v1",
        "new_summary": "  Equivariant machine learning is an approach for designing deep learning\nmodels that respect the symmetries of the problem, with the aim of reducing\nmodel complexity and improving generalization. In this paper, we focus on an\nextension of shift equivariance, which is the basis of convolution networks on\nimages, to general graphs. Unlike images, graphs do not have a natural notion\nof domain translation. Therefore, we consider the graph functional shifts as\nthe symmetry group: the unitary operators that commute with the graph shift\noperator. Notably, such symmetries operate in the signal space rather than\ndirectly in the spatial space. We remark that each linear filter layer of a\nstandard spectral graph neural network (GNN) commutes with graph functional\nshifts, but the activation function breaks this symmetry. Instead, we propose\nnonlinear spectral filters (NLSFs) that are fully equivariant to graph\nfunctional shifts and show that they have universal approximation properties.\nThe proposed NLSFs are based on a new form of spectral domain that is\ntransferable between graphs. We demonstrate the superior performance of NLSFs\nover existing spectral GNNs in node and graph classification benchmarks.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.01249v1.pdf",
        "similarity": 0.3960333742818276,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-03"
    },
    {
        "new_title": "Emotion Recognition from the perspective of Activity Recognition",
        "new_link": "http://arxiv.org/abs/2403.16263v1",
        "new_summary": "  Applications of an efficient emotion recognition system can be found in\nseveral domains such as medicine, driver fatigue surveillance, social robotics,\nand human-computer interaction. Appraising human emotional states, behaviors,\nand reactions displayed in real-world settings can be accomplished using latent\ncontinuous dimensions. Continuous dimensional models of human affect, such as\nthose based on valence and arousal are more accurate in describing a broad\nrange of spontaneous everyday emotions than more traditional models of discrete\nstereotypical emotion categories (e.g. happiness, surprise). Most of the prior\nwork on estimating valence and arousal considers laboratory settings and acted\ndata. But, for emotion recognition systems to be deployed and integrated into\nreal-world mobile and computing devices, we need to consider data collected in\nthe world. Action recognition is a domain of Computer Vision that involves\ncapturing complementary information on appearance from still frames and motion\nbetween frames. In this paper, we treat emotion recognition from the\nperspective of action recognition by exploring the application of deep learning\narchitectures specifically designed for action recognition, for continuous\naffect recognition. We propose a novel three-stream end-to-end deep learning\nregression pipeline with an attention mechanism, which is an ensemble design\nbased on sub-modules of multiple state-of-the-art action recognition systems.\nThe pipeline constitutes a novel data pre-processing approach with a spatial\nself-attention mechanism to extract keyframes. The optical flow of\nhigh-attention regions of the face is extracted to capture temporal context.\nAFEW-VA in-the-wild dataset has been used to conduct comparative experiments.\nQuantitative analysis shows that the proposed model outperforms multiple\nstandard baselines of both emotion recognition and action recognition models.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.16263v1.pdf",
        "similarity": 0.3960082023823995,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-24"
    },
    {
        "new_title": "Data-driven Forecasting of Deep Learning Performance on GPUs",
        "new_link": "http://arxiv.org/abs/2407.13853v1",
        "new_summary": "  Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.13853v1.pdf",
        "similarity": 0.3952681016869158,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-18"
    },
    {
        "new_title": "Deep Spherical Superpixels",
        "new_link": "http://arxiv.org/abs/2407.17354v1",
        "new_summary": "  Over the years, the use of superpixel segmentation has become very popular in\nvarious applications, serving as a preprocessing step to reduce data size by\nadapting to the content of the image, regardless of its semantic content. While\nthe superpixel segmentation of standard planar images, captured with a 90{\\deg}\nfield of view, has been extensively studied, there has been limited focus on\ndedicated methods to omnidirectional or spherical images, captured with a\n360{\\deg} field of view. In this study, we introduce the first deep\nlearning-based superpixel segmentation approach tailored for omnidirectional\nimages called DSS (for Deep Spherical Superpixels). Our methodology leverages\non spherical CNN architectures and the differentiable K-means clustering\nparadigm for superpixels, to generate superpixels that follow the spherical\ngeometry. Additionally, we propose to use data augmentation techniques\nspecifically designed for 360{\\deg} images, enabling our model to efficiently\nlearn from a limited set of annotated omnidirectional data. Our extensive\nvalidation across two datasets demonstrates that taking into account the\ninherent circular geometry of such images into our framework improves the\nsegmentation performance over traditional and deep learning-based superpixel\nmethods. Our code is available online.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.17354v1.pdf",
        "similarity": 0.39518740902542465,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-24"
    },
    {
        "new_title": "A Survey on Deep Clustering: From the Prior Perspective",
        "new_link": "http://arxiv.org/abs/2406.19602v2",
        "new_summary": "  Facilitated by the powerful feature extraction ability of neural networks,\ndeep clustering has achieved great success in analyzing high-dimensional and\ncomplex real-world data. The performance of deep clustering methods is affected\nby various factors such as network structures and learning objectives. However,\nas pointed out in this survey, the essence of deep clustering lies in the\nincorporation and utilization of prior knowledge, which is largely ignored by\nexisting works. From pioneering deep clustering methods based on data structure\nassumptions to recent contrastive clustering methods based on data augmentation\ninvariances, the development of deep clustering intrinsically corresponds to\nthe evolution of prior knowledge. In this survey, we provide a comprehensive\nreview of deep clustering methods by categorizing them into six types of prior\nknowledge. We find that in general the prior innovation follows two trends,\nnamely, i) from mining to constructing, and ii) from internal to external.\nBesides, we provide a benchmark on five widely-used datasets and analyze the\nperformance of methods with diverse priors. By providing a novel prior\nknowledge perspective, we hope this survey could provide some novel insights\nand inspire future research in the deep clustering community.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.19602v2.pdf",
        "similarity": 0.39514089476182057,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-28"
    },
    {
        "new_title": "Token Space: A Category Theory Framework for AI Computations",
        "new_link": "http://arxiv.org/abs/2404.11624v1",
        "new_summary": "  This paper introduces the Token Space framework, a novel mathematical\nconstruct designed to enhance the interpretability and effectiveness of deep\nlearning models through the application of category theory. By establishing a\ncategorical structure at the Token level, we provide a new lens through which\nAI computations can be understood, emphasizing the relationships between\ntokens, such as grouping, order, and parameter types. We explore the\nfoundational methodologies of the Token Space, detailing its construction, the\nrole of construction operators and initial categories, and its application in\nanalyzing deep learning models, specifically focusing on attention mechanisms\nand Transformer architectures. The integration of category theory into AI\nresearch offers a unified framework to describe and analyze computational\nstructures, enabling new research paths and development possibilities. Our\ninvestigation reveals that the Token Space framework not only facilitates a\ndeeper theoretical understanding of deep learning models but also opens avenues\nfor the design of more efficient, interpretable, and innovative models,\nillustrating the significant role of category theory in advancing computational\nmodels.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.11624v1.pdf",
        "similarity": 0.3951308543820428,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-11"
    },
    {
        "new_title": "Average gradient outer product as a mechanism for deep neural collapse",
        "new_link": "http://arxiv.org/abs/2402.13728v2",
        "new_summary": "  Deep Neural Collapse (DNC) refers to the surprisingly rigid structure of the\ndata representations in the final layers of Deep Neural Networks (DNNs). Though\nthe phenomenon has been measured in a variety of settings, its emergence is\ntypically explained via data-agnostic approaches, such as the unconstrained\nfeatures model. In this work, we introduce a data-dependent setting where DNC\nforms due to feature learning through the average gradient outer product\n(AGOP). The AGOP is defined with respect to a learned predictor and is equal to\nthe uncentered covariance matrix of its input-output gradients averaged over\nthe training dataset. Deep Recursive Feature Machines are a method that\nconstructs a neural network by iteratively mapping the data with the AGOP and\napplying an untrained random feature map. We demonstrate theoretically and\nempirically that DNC occurs in Deep Recursive Feature Machines as a consequence\nof the projection with the AGOP matrix computed at each layer. We then provide\nevidence that this mechanism holds for neural networks more generally. We show\nthat the right singular vectors and values of the weights can be responsible\nfor the majority of within-class variability collapse for DNNs trained in the\nfeature learning regime. As observed in recent work, this singular structure is\nhighly correlated with that of the AGOP.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.13728v2.pdf",
        "similarity": 0.3951007797822183,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-21"
    },
    {
        "new_title": "Position: Categorical Deep Learning is an Algebraic Theory of All\n  Architectures",
        "new_link": "http://arxiv.org/abs/2402.15332v2",
        "new_summary": "  We present our position on the elusive quest for a general-purpose framework\nfor specifying and studying deep learning architectures. Our opinion is that\nthe key attempts made so far lack a coherent bridge between specifying\nconstraints which models must satisfy and specifying their implementations.\nFocusing on building a such a bridge, we propose to apply category theory --\nprecisely, the universal algebra of monads valued in a 2-category of parametric\nmaps -- as a single theory elegantly subsuming both of these flavours of neural\nnetwork design. To defend our position, we show how this theory recovers\nconstraints induced by geometric deep learning, as well as implementations of\nmany architectures drawn from the diverse landscape of neural networks, such as\nRNNs. We also illustrate how the theory naturally encodes many standard\nconstructs in computer science and automata theory.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.15332v2.pdf",
        "similarity": 0.39467158390739493,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-23"
    },
    {
        "new_title": "A Study of Acquisition Functions for Medical Imaging Deep Active\n  Learning",
        "new_link": "http://arxiv.org/abs/2401.15721v2",
        "new_summary": "  The Deep Learning revolution has enabled groundbreaking achievements in\nrecent years. From breast cancer detection to protein folding, deep learning\nalgorithms have been at the core of very important advancements. However, these\nmodern advancements are becoming more and more data-hungry, especially on\nlabeled data whose availability is scarce: this is even more prevalent in the\nmedical context. In this work, we show how active learning could be very\neffective in data scarcity situations, where obtaining labeled data (or\nannotation budget is very limited). We compare several selection criteria\n(BALD, MeanSTD, and MaxEntropy) on the ISIC 2016 dataset. We also explored the\neffect of acquired pool size on the model's performance. Our results suggest\nthat uncertainty is useful to the Melanoma detection task, and confirms the\nhypotheses of the author of the paper of interest, that \\textit{bald} performs\non average better than other acquisition functions. Our extended analyses\nhowever revealed that all acquisition functions perform badly on the positive\n(cancerous) samples, suggesting exploitation of class unbalance, which could be\ncrucial in real-world settings. We finish by suggesting future work directions\nthat would be useful to improve this current work. The code of our\nimplementation is open-sourced at\n\\url{https://github.com/bonaventuredossou/ece526_course_project}\n",
        "pdf_link": "https://arxiv.org/pdf/2401.15721v2.pdf",
        "similarity": 0.3946416799854989,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-28"
    },
    {
        "new_title": "How Does Gradient Descent Learn Features -- A Local Analysis for\n  Regularized Two-Layer Neural Networks",
        "new_link": "http://arxiv.org/abs/2406.01766v1",
        "new_summary": "  The ability of learning useful features is one of the major advantages of\nneural networks. Although recent works show that neural network can operate in\na neural tangent kernel (NTK) regime that does not allow feature learning, many\nworks also demonstrate the potential for neural networks to go beyond NTK\nregime and perform feature learning. Recently, a line of work highlighted the\nfeature learning capabilities of the early stages of gradient-based training.\nIn this paper we consider another mechanism for feature learning via gradient\ndescent through a local convergence analysis. We show that once the loss is\nbelow a certain threshold, gradient descent with a carefully regularized\nobjective will capture ground-truth directions. Our results demonstrate that\nfeature learning not only happens at the initial gradient steps, but can also\noccur towards the end of training.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.01766v1.pdf",
        "similarity": 0.3940279587842273,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-03"
    },
    {
        "new_title": "Deep learning for precipitation nowcasting: A survey from the\n  perspective of time series forecasting",
        "new_link": "http://arxiv.org/abs/2406.04867v2",
        "new_summary": "  Deep learning-based time series forecasting has dominated the short-term\nprecipitation forecasting field with the help of its ability to estimate motion\nflow in high-resolution datasets. The growing interest in precipitation\nnowcasting offers substantial opportunities for the advancement of current\nforecasting technologies. Nevertheless, there has been a scarcity of in-depth\nsurveys of time series precipitation forecasting using deep learning. Thus,\nthis paper systemically reviews recent progress in time series precipitation\nforecasting models. Specifically, we investigate the following key points\nwithin background components, covering: i) preprocessing, ii) objective\nfunctions, and iii) evaluation metrics. We then categorize forecasting models\ninto \\textit{recursive} and \\textit{multiple} strategies based on their\napproaches to predict future frames, investigate the impacts of models using\nthe strategies, and performance assessments. Finally, we evaluate current deep\nlearning-based models for precipitation forecasting on a public benchmark,\ndiscuss their limitations and challenges, and present some promising research\ndirections. Our contribution lies in providing insights for a better\nunderstanding of time series precipitation forecasting and in aiding the\ndevelopment of robust AI solutions for the future.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.04867v2.pdf",
        "similarity": 0.3937811148114443,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-07"
    },
    {
        "new_title": "Transfer Learning in ECG Diagnosis: Is It Effective?",
        "new_link": "http://arxiv.org/abs/2402.02021v2",
        "new_summary": "  The adoption of deep learning in ECG diagnosis is often hindered by the\nscarcity of large, well-labeled datasets in real-world scenarios, leading to\nthe use of transfer learning to leverage features learned from larger datasets.\nYet the prevailing assumption that transfer learning consistently outperforms\ntraining from scratch has never been systematically validated. In this study,\nwe conduct the first extensive empirical study on the effectiveness of transfer\nlearning in multi-label ECG classification, by investigating comparing the\nfine-tuning performance with that of training from scratch, covering a variety\nof ECG datasets and deep neural networks. We confirm that fine-tuning is the\npreferable choice for small downstream datasets; however, when the dataset is\nsufficiently large, training from scratch can achieve comparable performance,\nalbeit requiring a longer training time to catch up. Furthermore, we find that\ntransfer learning exhibits better compatibility with convolutional neural\nnetworks than with recurrent neural networks, which are the two most prevalent\narchitectures for time-series ECG applications. Our results underscore the\nimportance of transfer learning in ECG diagnosis, yet depending on the amount\nof available data, researchers may opt not to use it, considering the\nnon-negligible cost associated with pre-training.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.02021v2.pdf",
        "similarity": 0.3936422781920845,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-03"
    },
    {
        "new_title": "Be Persistent: Towards a Unified Solution for Mitigating Shortcuts in\n  Deep Learning",
        "new_link": "http://arxiv.org/abs/2402.11237v1",
        "new_summary": "  Deep neural networks (DNNs) are vulnerable to shortcut learning: rather than\nlearning the intended task, they tend to draw inconclusive relationships\nbetween their inputs and outputs. Shortcut learning is ubiquitous among many\nfailure cases of neural networks, and traces of this phenomenon can be seen in\ntheir generalizability issues, domain shift, adversarial vulnerability, and\neven bias towards majority groups. In this paper, we argue that this\ncommonality in the cause of various DNN issues creates a significant\nopportunity that should be leveraged to find a unified solution for shortcut\nlearning. To this end, we outline the recent advances in topological data\nanalysis~(TDA), and persistent homology~(PH) in particular, to sketch a unified\nroadmap for detecting shortcuts in deep learning. We demonstrate our arguments\nby investigating the topological features of computational graphs in DNNs using\ntwo cases of unlearnable examples and bias in decision-making as our test\nstudies. Our analysis of these two failure cases of DNNs reveals that finding a\nunified solution for shortcut learning in DNNs is not out of reach, and TDA can\nplay a significant role in forming such a framework.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.11237v1.pdf",
        "similarity": 0.39320306231279256,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-17"
    },
    {
        "new_title": "Universal Time-Series Representation Learning: A Survey",
        "new_link": "http://arxiv.org/abs/2401.03717v1",
        "new_summary": "  Time-series data exists in every corner of real-world systems and services,\nranging from satellites in the sky to wearable devices on human bodies.\nLearning representations by extracting and inferring valuable information from\nthese time series is crucial for understanding the complex dynamics of\nparticular phenomena and enabling informed decisions. With the learned\nrepresentations, we can perform numerous downstream analyses more effectively.\nAmong several approaches, deep learning has demonstrated remarkable performance\nin extracting hidden patterns and features from time-series data without manual\nfeature engineering. This survey first presents a novel taxonomy based on three\nfundamental elements in designing state-of-the-art universal representation\nlearning methods for time series. According to the proposed taxonomy, we\ncomprehensively review existing studies and discuss their intuitions and\ninsights into how these methods enhance the quality of learned representations.\nFinally, as a guideline for future studies, we summarize commonly used\nexperimental setups and datasets and discuss several promising research\ndirections. An up-to-date corresponding resource is available at\nhttps://github.com/itouchz/awesome-deep-time-series-representations.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.03717v1.pdf",
        "similarity": 0.3925282392744166,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-08"
    },
    {
        "new_title": "LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal\n  Data",
        "new_link": "http://arxiv.org/abs/2406.09864v1",
        "new_summary": "  Multimodal Deep Learning enhances decision-making by integrating diverse\ninformation sources, such as texts, images, audio, and videos. To develop\ntrustworthy multimodal approaches, it is essential to understand how\nuncertainty impacts these models. We introduce LUMA, a unique benchmark\ndataset, featuring audio, image, and textual data from 50 classes, for learning\nfrom uncertain and multimodal data. It extends the well-known CIFAR 10/100\ndataset with audio samples extracted from three audio corpora, and text data\ngenerated using the Gemma-7B Large Language Model (LLM). The LUMA dataset\nenables the controlled injection of varying types and degrees of uncertainty to\nachieve and tailor specific experiments and benchmarking initiatives. LUMA is\nalso available as a Python package including the functions for generating\nmultiple variants of the dataset with controlling the diversity of the data,\nthe amount of noise for each modality, and adding out-of-distribution samples.\nA baseline pre-trained model is also provided alongside three uncertainty\nquantification methods: Monte-Carlo Dropout, Deep Ensemble, and Reliable\nConflictive Multi-View Learning. This comprehensive dataset and its tools are\nintended to promote and support the development and benchmarking of trustworthy\nand robust multimodal deep learning approaches.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.09864v1.pdf",
        "similarity": 0.3920549394826687,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-14"
    },
    {
        "new_title": "Lightweight Object Detection: A Study Based on YOLOv7 Integrated with\n  ShuffleNetv2 and Vision Transformer",
        "new_link": "http://arxiv.org/abs/2403.01736v1",
        "new_summary": "  As mobile computing technology rapidly evolves, deploying efficient object\ndetection algorithms on mobile devices emerges as a pivotal research area in\ncomputer vision. This study zeroes in on optimizing the YOLOv7 algorithm to\nboost its operational efficiency and speed on mobile platforms while ensuring\nhigh accuracy. Leveraging a synergy of advanced techniques such as Group\nConvolution, ShuffleNetV2, and Vision Transformer, this research has\neffectively minimized the model's parameter count and memory usage, streamlined\nthe network architecture, and fortified the real-time object detection\nproficiency on resource-constrained devices. The experimental outcomes reveal\nthat the refined YOLO model demonstrates exceptional performance, markedly\nenhancing processing velocity while sustaining superior detection accuracy.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.01736v1.pdf",
        "similarity": 0.3919139660966277,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-04"
    },
    {
        "new_title": "Hybrid Pooling and Convolutional Network for Improving Accuracy and\n  Training Convergence Speed in Object Detection",
        "new_link": "http://arxiv.org/abs/2401.01134v1",
        "new_summary": "  This paper introduces HPC-Net, a high-precision and rapidly convergent object\ndetection network.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.01134v1.pdf",
        "similarity": 0.3911112406855471,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-02"
    },
    {
        "new_title": "Deep Configuration Performance Learning: A Systematic Survey and\n  Taxonomy",
        "new_link": "http://arxiv.org/abs/2403.03322v2",
        "new_summary": "  Performance is arguably the most crucial attribute that reflects the quality\nof a configurable software system. However, given the increasing scale and\ncomplexity of modern software, modeling and predicting how various\nconfigurations can impact performance becomes one of the major challenges in\nsoftware maintenance. As such, performance is often modeled without having a\nthorough knowledge of the software system, but relying mainly on data, which\nfits precisely with the purpose of deep learning.\n  In this paper, we conduct a comprehensive review exclusively on the topic of\ndeep learning for performance learning of configurable software, covering 1,206\nsearched papers spanning six indexing services, based on which 99 primary\npapers were extracted and analyzed. Our results outline key statistics,\ntaxonomy, strengths, weaknesses, and optimal usage scenarios for techniques\nrelated to the preparation of configuration data, the construction of deep\nlearning performance models, the evaluation of these models, and their\nutilization in various software configuration-related tasks.We also identify\nthe good practices and potentially problematic phenomena from the studies\nsurveyed, together with a comprehensive summary of actionable suggestions and\ninsights into future opportunities within the field. To promote open science,\nall the raw results of this survey can be accessed at our repository:\nhttps://github.com/ideas-labo/DCPL-SLR.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.03322v2.pdf",
        "similarity": 0.3905209246503301,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-05"
    },
    {
        "new_title": "A Comprehensive Survey on Deep-Learning-based Vehicle Re-Identification:\n  Models, Data Sets and Challenges",
        "new_link": "http://arxiv.org/abs/2401.10643v1",
        "new_summary": "  Vehicle re-identification (ReID) endeavors to associate vehicle images\ncollected from a distributed network of cameras spanning diverse traffic\nenvironments. This task assumes paramount importance within the spectrum of\nvehicle-centric technologies, playing a pivotal role in deploying Intelligent\nTransportation Systems (ITS) and advancing smart city initiatives. Rapid\nadvancements in deep learning have significantly propelled the evolution of\nvehicle ReID technologies in recent years. Consequently, undertaking a\ncomprehensive survey of methodologies centered on deep learning for vehicle\nre-identification has become imperative and inescapable. This paper extensively\nexplores deep learning techniques applied to vehicle ReID. It outlines the\ncategorization of these methods, encompassing supervised and unsupervised\napproaches, delves into existing research within these categories, introduces\ndatasets and evaluation criteria, and delineates forthcoming challenges and\npotential research directions. This comprehensive assessment examines the\nlandscape of deep learning in vehicle ReID and establishes a foundation and\nstarting point for future works. It aims to serve as a complete reference by\nhighlighting challenges and emerging trends, fostering advancements and\napplications in vehicle ReID utilizing deep learning models.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.10643v1.pdf",
        "similarity": 0.39049783473442923,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-19"
    },
    {
        "new_title": "Protein Representation Learning with Sequence Information Embedding:\n  Does it Always Lead to a Better Performance?",
        "new_link": "http://arxiv.org/abs/2406.19755v1",
        "new_summary": "  Deep learning has become a crucial tool in studying proteins. While the\nsignificance of modeling protein structure has been discussed extensively in\nthe literature, amino acid types are typically included in the input as a\ndefault operation for many inference tasks. This study demonstrates with\nstructure alignment task that embedding amino acid types in some cases may not\nhelp a deep learning model learn better representation. To this end, we propose\nProtLOCA, a local geometry alignment method based solely on amino acid\nstructure representation. The effectiveness of ProtLOCA is examined by a global\nstructure-matching task on protein pairs with an independent test dataset based\non CATH labels. Our method outperforms existing sequence- and structure-based\nrepresentation learning methods by more quickly and accurately matching\nstructurally consistent protein domains. Furthermore, in local structure\npairing tasks, ProtLOCA for the first time provides a valid solution to\nhighlight common local structures among proteins with different overall\nstructures but the same function. This suggests a new possibility for using\ndeep learning methods to analyze protein structure to infer function.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.19755v1.pdf",
        "similarity": 0.3904721547444659,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-28"
    },
    {
        "new_title": "DeepMachining: Online Prediction of Machining Errors of Lathe Machines",
        "new_link": "http://arxiv.org/abs/2403.16451v4",
        "new_summary": "  We describe DeepMachining, a deep learning-based AI system for online\nprediction of machining errors of lathe machine operations. We have built and\nevaluated DeepMachining based on manufacturing data from factories.\nSpecifically, we first pretrain a deep learning model for a given lathe\nmachine's operations to learn the salient features of machining states. Then,\nwe fine-tune the pretrained model to adapt to specific machining tasks. We\ndemonstrate that DeepMachining achieves high prediction accuracy for multiple\ntasks that involve different workpieces and cutting tools. To the best of our\nknowledge, this work is one of the first factory experiments using pre-trained\ndeep-learning models to predict machining errors of lathe machines.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.16451v4.pdf",
        "similarity": 0.3902891805623787,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-25"
    },
    {
        "new_title": "Real-time and On-site Aerodynamics using Stereoscopic PIV and Deep\n  Optical Flow Learning",
        "new_link": "http://arxiv.org/abs/2401.09932v1",
        "new_summary": "  We introduce Recurrent All-Pairs Field Transforms for Stereoscopic Particle\nImage Velocimetry (RAFT-StereoPIV). Our approach leverages deep optical flow\nlearning to analyze time-resolved and double-frame particle images from on-site\nmeasurements, particularly from the 'Ring of Fire,' as well as from wind tunnel\nmeasurements for real-time aerodynamic analysis. A multi-fidelity dataset\ncomprising both Reynolds-Averaged Navier-Stokes (RANS) and Direct Numerical\nSimulation (DNS) was used to train our model. RAFT-StereoPIV outperforms all\nPIV state-of-the-art deep learning models on benchmark datasets, with a 68$\\%$\nerror reduction on the validation dataset, Problem Class 2, and a 47$\\%$ error\nreduction on the unseen test dataset, Problem Class 1, demonstrating its\nrobustness and generalizability. In comparison to the most recent works in the\nfield of deep learning for PIV, where the main focus was the methodology\ndevelopment and the application was limited to either 2D flow cases or simple\nexperimental data, we extend deep learning-based PIV for industrial\napplications and 3D flow field estimation. As we apply the trained network to\nthree-dimensional highly turbulent PIV data, we are able to obtain flow\nestimates that maintain spatial resolution of the input image sequence. In\ncontrast, the traditional methods produce the flow field of $\\sim$16$\\times$\nlower resolution. We believe that this study brings the field of experimental\nfluid dynamics one step closer to the long-term goal of having experimental\nmeasurement systems that can be used for real-time flow field estimation.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.09932v1.pdf",
        "similarity": 0.3901335787552881,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-18"
    },
    {
        "new_title": "Chaining thoughts and LLMs to learn DNA structural biophysics",
        "new_link": "http://arxiv.org/abs/2403.01332v1",
        "new_summary": "  The future development of an AI scientist, a tool that is capable of\nintegrating a variety of experimental data and generating testable hypotheses,\nholds immense potential. So far, bespoke machine learning models have been\ncreated to specialize in singular scientific tasks, but otherwise lack the\nflexibility of a general purpose model. Here, we show that a general purpose\nlarge language model, chatGPT 3.5-turbo, can be fine-tuned to learn the\nstructural biophysics of DNA. We find that both fine-tuning models to return\nchain-of-thought responses and chaining together models fine-tuned for subtasks\nhave an enhanced ability to analyze and design DNA sequences and their\nstructures.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.01332v1.pdf",
        "similarity": 0.39000850690098765,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-02"
    },
    {
        "new_title": "EchoSpike Predictive Plasticity: An Online Local Learning Rule for\n  Spiking Neural Networks",
        "new_link": "http://arxiv.org/abs/2405.13976v2",
        "new_summary": "  The drive to develop artificial neural networks that efficiently utilize\nresources has generated significant interest in bio-inspired Spiking Neural\nNetworks (SNNs). These networks are particularly attractive due to their\npotential in applications requiring low power and memory. This potential is\nfurther enhanced by the ability to perform online local learning, enabling them\nto adapt to dynamic environments. This requires the model to be adaptive in a\nself-supervised manner. While self-supervised learning has seen great success\nin many deep learning domains, its application for online local learning in\nmulti-layer SNNs remains underexplored. In this paper, we introduce the\n\"EchoSpike Predictive Plasticity\" (ESPP) learning rule, a pioneering online\nlocal learning rule designed to leverage hierarchical temporal dynamics in SNNs\nthrough predictive and contrastive coding. We validate the effectiveness of\nthis approach using benchmark datasets, demonstrating that it performs on par\nwith current state-of-the-art supervised learning rules. The temporal and\nspatial locality of ESPP makes it particularly well-suited for low-cost\nneuromorphic processors, representing a significant advancement in developing\nbiologically plausible self-supervised learning models for neuromorphic\ncomputing at the edge.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.13976v2.pdf",
        "similarity": 0.3899172889895827,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-22"
    },
    {
        "new_title": "A Linear Programming Enhanced Genetic Algorithm for Hyperparameter\n  Tuning in Machine Learning",
        "new_link": "http://arxiv.org/abs/2407.00613v1",
        "new_summary": "  In this paper, we formulate the hyperparameter tuning problem in machine\nlearning as a bilevel program. The bilevel program is solved using a micro\ngenetic algorithm that is enhanced with a linear program. While the genetic\nalgorithm searches over discrete hyperparameters, the linear program\nenhancement allows hyper local search over continuous hyperparameters. The\nmajor contribution in this paper is the formulation of a linear program that\nsupports fast search over continuous hyperparameters, and can be integrated\nwith any hyperparameter search technique. It can also be applied directly on\nany trained machine learning or deep learning model for the purpose of\nfine-tuning. We test the performance of the proposed approach on two datasets,\nMNIST and CIFAR-10. Our results clearly demonstrate that using the linear\nprogram enhancement offers significant promise when incorporated with any\npopulation-based approach for hyperparameter tuning.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.00613v1.pdf",
        "similarity": 0.38934507423349995,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-30"
    },
    {
        "new_title": "MTDT: A Multi-Task Deep Learning Digital Twin",
        "new_link": "http://arxiv.org/abs/2405.00922v1",
        "new_summary": "  Traffic congestion has significant impacts on both the economy and the\nenvironment. Measures of Effectiveness (MOEs) have long been the standard for\nevaluating the level of service and operational efficiency of traffic\nintersections. However, the scarcity of traditional high-resolution loop\ndetector data (ATSPM) presents challenges in accurately measuring MOEs or\ncapturing the intricate temporospatial characteristics inherent in urban\nintersection traffic. In response to this challenge, we have introduced the\nMulti-Task Deep Learning Digital Twin (MTDT) as a solution for multifaceted and\nprecise intersection traffic flow simulation. MTDT enables accurate,\nfine-grained estimation of loop detector waveform time series for each lane of\nmovement, alongside successful estimation of several MOEs for each lane group\nassociated with a traffic phase concurrently and for all approaches of an\narbitrary urban intersection. Unlike existing deep learning methodologies, MTDT\ndistinguishes itself through its adaptability to local temporal and spatial\nfeatures, such as signal timing plans, intersection topology, driving\nbehaviors, and turning movement counts. While maintaining a straightforward\ndesign, our model emphasizes the advantages of multi-task learning in traffic\nmodeling. By consolidating the learning process across multiple tasks, MTDT\ndemonstrates reduced overfitting, increased efficiency, and enhanced\neffectiveness by sharing representations learned by different tasks.\nFurthermore, our approach facilitates sequential computation and lends itself\nto complete parallelization through GPU implementation. This not only\nstreamlines the computational process but also enhances scalability and\nperformance.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.00922v1.pdf",
        "similarity": 0.3893158252549007,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-02"
    },
    {
        "new_title": "stEnTrans: Transformer-based deep learning for spatial transcriptomics\n  enhancement",
        "new_link": "http://arxiv.org/abs/2407.08224v1",
        "new_summary": "  The spatial location of cells within tissues and organs is crucial for the\nmanifestation of their specific functions.Spatial transcriptomics technology\nenables comprehensive measurement of the gene expression patterns in tissues\nwhile retaining spatial information. However, current popular spatial\ntranscriptomics techniques either have shallow sequencing depth or low\nresolution. We present stEnTrans, a deep learning method based on Transformer\narchitecture that provides comprehensive predictions for gene expression in\nunmeasured areas or unexpectedly lost areas and enhances gene expression in\noriginal and inputed spots. Utilizing a self-supervised learning approach,\nstEnTrans establishes proxy tasks on gene expression profile without requiring\nadditional data, mining intrinsic features of the tissues as supervisory\ninformation. We evaluate stEnTrans on six datasets and the results indicate\nsuperior performance in enhancing spots resolution and predicting gene\nexpression in unmeasured areas compared to other deep learning and traditional\ninterpolation methods. Additionally, Our method also can help the discovery of\nspatial patterns in Spatial Transcriptomics and enrich to more biologically\nsignificant pathways. Our source code is available at\nhttps://github.com/shuailinxue/stEnTrans.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.08224v1.pdf",
        "similarity": 0.38917033431685816,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-11"
    },
    {
        "new_title": "Unified Explanations in Machine Learning Models: A Perturbation Approach",
        "new_link": "http://arxiv.org/abs/2405.20200v1",
        "new_summary": "  A high-velocity paradigm shift towards Explainable Artificial Intelligence\n(XAI) has emerged in recent years. Highly complex Machine Learning (ML) models\nhave flourished in many tasks of intelligence, and the questions have started\nto shift away from traditional metrics of validity towards something deeper:\nWhat is this model telling me about my data, and how is it arriving at these\nconclusions? Inconsistencies between XAI and modeling techniques can have the\nundesirable effect of casting doubt upon the efficacy of these explainability\napproaches. To address these problems, we propose a systematic,\nperturbation-based analysis against a popular, model-agnostic method in XAI,\nSHapley Additive exPlanations (Shap). We devise algorithms to generate relative\nfeature importance in settings of dynamic inference amongst a suite of popular\nmachine learning and deep learning methods, and metrics that allow us to\nquantify how well explanations generated under the static case hold. We propose\na taxonomy for feature importance methodology, measure alignment, and observe\nquantifiable similarity amongst explanation models across several datasets.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.20200v1.pdf",
        "similarity": 0.38910630768862653,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-30"
    },
    {
        "new_title": "Advancing Extrapolative Predictions of Material Properties through\n  Learning to Learn",
        "new_link": "http://arxiv.org/abs/2404.08657v1",
        "new_summary": "  Recent advancements in machine learning have showcased its potential to\nsignificantly accelerate the discovery of new materials. Central to this\nprogress is the development of rapidly computable property predictors, enabling\nthe identification of novel materials with desired properties from vast\nmaterial spaces. However, the limited availability of data resources poses a\nsignificant challenge in data-driven materials research, particularly hindering\nthe exploration of innovative materials beyond the boundaries of existing data.\nWhile machine learning predictors are inherently interpolative, establishing a\ngeneral methodology to create an extrapolative predictor remains a fundamental\nchallenge, limiting the search for innovative materials beyond existing data\nboundaries. In this study, we leverage an attention-based architecture of\nneural networks and meta-learning algorithms to acquire extrapolative\ngeneralization capability. The meta-learners, experienced repeatedly with\narbitrarily generated extrapolative tasks, can acquire outstanding\ngeneralization capability in unexplored material spaces. Through the tasks of\npredicting the physical properties of polymeric materials and hybrid\norganic--inorganic perovskites, we highlight the potential of such\nextrapolatively trained models, particularly with their ability to rapidly\nadapt to unseen material domains in transfer learning scenarios.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.08657v1.pdf",
        "similarity": 0.388913109438183,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-25"
    },
    {
        "new_title": "Reproducibility, energy efficiency and performance of pseudorandom\n  number generators in machine learning: a comparative study of python, numpy,\n  tensorflow, and pytorch implementations",
        "new_link": "http://arxiv.org/abs/2401.17345v2",
        "new_summary": "  Pseudo-Random Number Generators (PRNGs) have become ubiquitous in machine\nlearning technologies because they are interesting for numerous methods. The\nfield of machine learning holds the potential for substantial advancements\nacross various domains, as exemplified by recent breakthroughs in Large\nLanguage Models (LLMs). However, despite the growing interest, persistent\nconcerns include issues related to reproducibility and energy consumption.\nReproducibility is crucial for robust scientific inquiry and explainability,\nwhile energy efficiency underscores the imperative to conserve finite global\nresources. This study delves into the investigation of whether the leading\nPseudo-Random Number Generators (PRNGs) employed in machine learning languages,\nlibraries, and frameworks uphold statistical quality and numerical\nreproducibility when compared to the original C implementation of the\nrespective PRNG algorithms. Additionally, we aim to evaluate the time\nefficiency and energy consumption of various implementations. Our experiments\nencompass Python, NumPy, TensorFlow, and PyTorch, utilizing the Mersenne\nTwister, PCG, and Philox algorithms. Remarkably, we verified that the temporal\nperformance of machine learning technologies closely aligns with that of\nC-based implementations, with instances of achieving even superior\nperformances. On the other hand, it is noteworthy that ML technologies consumed\nonly 10% more energy than their C-implementation counterparts. However, while\nstatistical quality was found to be comparable, achieving numerical\nreproducibility across different platforms for identical seeds and algorithms\nwas not achieved.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.17345v2.pdf",
        "similarity": 0.3887546489926486,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-30"
    },
    {
        "new_title": "Is ReLU Adversarially Robust?",
        "new_link": "http://arxiv.org/abs/2405.03777v1",
        "new_summary": "  The efficacy of deep learning models has been called into question by the\npresence of adversarial examples. Addressing the vulnerability of deep learning\nmodels to adversarial examples is crucial for ensuring their continued\ndevelopment and deployment. In this work, we focus on the role of rectified\nlinear unit (ReLU) activation functions in the generation of adversarial\nexamples. ReLU functions are commonly used in deep learning models because they\nfacilitate the training process. However, our empirical analysis demonstrates\nthat ReLU functions are not robust against adversarial examples. We propose a\nmodified version of the ReLU function, which improves robustness against\nadversarial examples. Our results are supported by an experiment, which\nconfirms the effectiveness of our proposed modification. Additionally, we\ndemonstrate that applying adversarial training to our customized model further\nenhances its robustness compared to a general model.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.03777v1.pdf",
        "similarity": 0.3885859159019766,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-06"
    },
    {
        "new_title": "Advancing Household Robotics: Deep Interactive Reinforcement Learning\n  for Efficient Training and Enhanced Performance",
        "new_link": "http://arxiv.org/abs/2405.18687v1",
        "new_summary": "  The market for domestic robots made to perform household chores is growing as\nthese robots relieve people of everyday responsibilities. Domestic robots are\ngenerally welcomed for their role in easing human labor, in contrast to\nindustrial robots, which are frequently criticized for displacing human\nworkers. But before these robots can carry out domestic chores, they need to\nbecome proficient in several minor activities, such as recognizing their\nsurroundings, making decisions, and picking up on human behaviors.\nReinforcement learning, or RL, has emerged as a key robotics technology that\nenables robots to interact with their environment and learn how to optimize\ntheir actions to maximize rewards. However, the goal of Deep Reinforcement\nLearning is to address more complicated, continuous action-state spaces in\nreal-world settings by combining RL with Neural Networks. The efficacy of\nDeepRL can be further augmented through interactive feedback, in which a\ntrainer offers real-time guidance to expedite the robot's learning process.\nNevertheless, the current methods have drawbacks, namely the transient\napplication of guidance that results in repeated learning under identical\nconditions. Therefore, we present a novel method to preserve and reuse\ninformation and advice via Deep Interactive Reinforcement Learning, which\nutilizes a persistent rule-based system. This method not only expedites the\ntraining process but also lessens the number of repetitions that instructors\nwill have to carry out. This study has the potential to advance the development\nof household robots and improve their effectiveness and efficiency as learners.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.18687v1.pdf",
        "similarity": 0.3884857031840818,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-29"
    },
    {
        "new_title": "Unlocking Unlabeled Data: Ensemble Learning with the Hui- Walter\n  Paradigm for Performance Estimation in Online and Static Settings",
        "new_link": "http://arxiv.org/abs/2401.09376v1",
        "new_summary": "  In the realm of machine learning and statistical modeling, practitioners\noften work under the assumption of accessible, static, labeled data for\nevaluation and training. However, this assumption often deviates from reality\nwhere data may be private, encrypted, difficult- to-measure, or unlabeled. In\nthis paper, we bridge this gap by adapting the Hui-Walter paradigm, a method\ntraditionally applied in epidemiology and medicine, to the field of machine\nlearning. This approach enables us to estimate key performance metrics such as\nfalse positive rate, false negative rate, and priors in scenarios where no\nground truth is available. We further extend this paradigm for handling online\ndata, opening up new possibilities for dynamic data environments. Our\nmethodology involves partitioning data into latent classes to simulate multiple\ndata populations (if natural populations are unavailable) and independently\ntraining models to replicate multiple tests. By cross-tabulating binary\noutcomes across ensemble categorizers and multiple populations, we are able to\nestimate unknown parameters through Gibbs sampling, eliminating the need for\nground-truth or labeled data. This paper showcases the potential of our\nmethodology to transform machine learning practices by allowing for accurate\nmodel assessment under dynamic and uncertain data conditions.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.09376v1.pdf",
        "similarity": 0.38831763890992227,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-17"
    },
    {
        "new_title": "DistML.js: Installation-free Distributed Deep Learning Framework for Web\n  Browsers",
        "new_link": "http://arxiv.org/abs/2407.01023v1",
        "new_summary": "  We present \"DistML.js\", a library designed for training and inference of\nmachine learning models within web browsers. Not only does DistML.js facilitate\nmodel training on local devices, but it also supports distributed learning\nthrough communication with servers. Its design and define-by-run API for deep\nlearning model construction resemble PyTorch, thereby reducing the learning\ncurve for prototyping. Matrix computations involved in model training and\ninference are executed on the backend utilizing WebGL, enabling high-speed\ncalculations. We provide a comprehensive explanation of DistML.js's design,\nAPI, and implementation, alongside practical applications including data\nparallelism in learning. The source code is publicly available at\nhttps://github.com/mil-tokyo/distmljs.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.01023v1.pdf",
        "similarity": 0.3882530023456972,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-01"
    },
    {
        "new_title": "Deep Active Learning: A Reality Check",
        "new_link": "http://arxiv.org/abs/2403.14800v1",
        "new_summary": "  We conduct a comprehensive evaluation of state-of-the-art deep active\nlearning methods. Surprisingly, under general settings, no single-model method\ndecisively outperforms entropy-based active learning, and some even fall short\nof random sampling. We delve into overlooked aspects like starting budget,\nbudget step, and pretraining's impact, revealing their significance in\nachieving superior results. Additionally, we extend our evaluation to other\ntasks, exploring the active learning effectiveness in combination with\nsemi-supervised learning, and object detection. Our experiments provide\nvaluable insights and concrete recommendations for future active learning\nstudies. By uncovering the limitations of current methods and understanding the\nimpact of different experimental settings, we aim to inspire more efficient\ntraining of deep learning models in real-world scenarios with limited\nannotation budgets. This work contributes to advancing active learning's\nefficacy in deep learning and empowers researchers to make informed decisions\nwhen applying active learning to their tasks.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.14800v1.pdf",
        "similarity": 0.38819700574318766,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-21"
    },
    {
        "new_title": "Curriculum for Crowd Counting -- Is it Worthy?",
        "new_link": "http://arxiv.org/abs/2401.07586v1",
        "new_summary": "  Recent advances in deep learning techniques have achieved remarkable\nperformance in several computer vision problems. A notably intuitive technique\ncalled Curriculum Learning (CL) has been introduced recently for training deep\nlearning models. Surprisingly, curriculum learning achieves significantly\nimproved results in some tasks but marginal or no improvement in others. Hence,\nthere is still a debate about its adoption as a standard method to train\nsupervised learning models. In this work, we investigate the impact of\ncurriculum learning in crowd counting using the density estimation method. We\nperformed detailed investigations by conducting 112 experiments using six\ndifferent CL settings using eight different crowd models. Our experiments show\nthat curriculum learning improves the model learning performance and shortens\nthe convergence time.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.07586v1.pdf",
        "similarity": 0.3878233742536013,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-15"
    },
    {
        "new_title": "Visualization for Trust in Machine Learning Revisited: The State of the\n  Field in 2023",
        "new_link": "http://arxiv.org/abs/2403.12005v2",
        "new_summary": "  Visualization for explainable and trustworthy machine learning remains one of\nthe most important and heavily researched fields within information\nvisualization and visual analytics with various application domains, such as\nmedicine, finance, and bioinformatics. After our 2020 state-of-the-art report\ncomprising 200 techniques, we have persistently collected peer-reviewed\narticles describing visualization techniques, categorized them based on the\npreviously established categorization schema consisting of 119 categories, and\nprovided the resulting collection of 542 techniques in an online survey\nbrowser. In this survey article, we present the updated findings of new\nanalyses of this dataset as of fall 2023 and discuss trends, insights, and\neight open challenges for using visualizations in machine learning. Our results\ncorroborate the rapidly growing trend of visualization techniques for\nincreasing trust in machine learning models in the past three years, with\nvisualization found to help improve popular model explainability methods and\ncheck new deep learning architectures, for instance.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.12005v2.pdf",
        "similarity": 0.38773647041251935,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-18"
    },
    {
        "new_title": "Half-Space Feature Learning in Neural Networks",
        "new_link": "http://arxiv.org/abs/2404.04312v1",
        "new_summary": "  There currently exist two extreme viewpoints for neural network feature\nlearning -- (i) Neural networks simply implement a kernel method (a la NTK) and\nhence no features are learned (ii) Neural networks can represent (and hence\nlearn) intricate hierarchical features suitable for the data. We argue in this\npaper neither interpretation is likely to be correct based on a novel\nviewpoint. Neural networks can be viewed as a mixture of experts, where each\nexpert corresponds to a (number of layers length) path through a sequence of\nhidden units. We use this alternate interpretation to motivate a model, called\nthe Deep Linearly Gated Network (DLGN), which sits midway between deep linear\nnetworks and ReLU networks. Unlike deep linear networks, the DLGN is capable of\nlearning non-linear features (which are then linearly combined), and unlike\nReLU networks these features are ultimately simple -- each feature is\neffectively an indicator function for a region compactly described as an\nintersection of (number of layers) half-spaces in the input space. This\nviewpoint allows for a comprehensive global visualization of features, unlike\nthe local visualizations for neurons based on saliency/activation/gradient\nmaps. Feature learning in DLGNs is shown to happen and the mechanism with which\nthis happens is through learning half-spaces in the input space that contain\nsmooth regions of the target function. Due to the structure of DLGNs, the\nneurons in later layers are fundamentally the same as those in earlier layers\n-- they all represent a half-space -- however, the dynamics of gradient descent\nimpart a distinct clustering to the later layer neurons. We hypothesize that\nReLU networks also have similar feature learning behaviour.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.04312v1.pdf",
        "similarity": 0.387487298493987,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-05"
    },
    {
        "new_title": "Reduced Jeffries-Matusita distance: A Novel Loss Function to Improve\n  Generalization Performance of Deep Classification Models",
        "new_link": "http://arxiv.org/abs/2403.08408v1",
        "new_summary": "  The generalization performance of deep neural networks in classification\ntasks is a major concern in machine learning research. Despite widespread\ntechniques used to diminish the over-fitting issue such as data augmentation,\npseudo-labeling, regularization, and ensemble learning, this performance still\nneeds to be enhanced with other approaches. In recent years, it has been\ntheoretically demonstrated that the loss function characteristics i.e. its\nLipschitzness and maximum value affect the generalization performance of deep\nneural networks which can be utilized as a guidance to propose novel distance\nmeasures. In this paper, by analyzing the aforementioned characteristics, we\nintroduce a distance called Reduced Jeffries-Matusita as a loss function for\ntraining deep classification models to reduce the over-fitting issue. In our\nexperiments, we evaluate the new loss function in two different problems: image\nclassification in computer vision and node classification in the context of\ngraph learning. The results show that the new distance measure stabilizes the\ntraining process significantly, enhances the generalization ability, and\nimproves the performance of the models in the Accuracy and F1-score metrics,\neven if the training set size is small.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.08408v1.pdf",
        "similarity": 0.387297703819355,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-13"
    },
    {
        "new_title": "In-Context Learning of Energy Functions",
        "new_link": "http://arxiv.org/abs/2406.12785v1",
        "new_summary": "  In-context learning is a powerful capability of certain machine learning\nmodels that arguably underpins the success of today's frontier AI models.\nHowever, in-context learning is critically limited to settings where the\nin-context distribution of interest $p_{\\theta}^{ICL}( x|\\mathcal{D})$ can be\nstraightforwardly expressed and/or parameterized by the model; for instance,\nlanguage modeling relies on expressing the next-token distribution as a\ncategorical distribution parameterized by the network's output logits. In this\nwork, we present a more general form of in-context learning without such a\nlimitation that we call \\textit{in-context learning of energy functions}. The\nidea is to instead learn the unconstrained and arbitrary in-context energy\nfunction $E_{\\theta}^{ICL}(x|\\mathcal{D})$ corresponding to the in-context\ndistribution $p_{\\theta}^{ICL}(x|\\mathcal{D})$. To do this, we use classic\nideas from energy-based modeling. We provide preliminary evidence that our\nmethod empirically works on synthetic data. Interestingly, our work contributes\n(to the best of our knowledge) the first example of in-context learning where\nthe input space and output space differ from one another, suggesting that\nin-context learning is a more-general capability than previously realized.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.12785v1.pdf",
        "similarity": 0.3871999370450712,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-18"
    },
    {
        "new_title": "Automated Discovery of Integral with Deep Learning",
        "new_link": "http://arxiv.org/abs/2402.18040v1",
        "new_summary": "  Recent advancements in the realm of deep learning, particularly in the\ndevelopment of large language models (LLMs), have demonstrated AI's ability to\ntackle complex mathematical problems or solving programming challenges.\nHowever, the capability to solve well-defined problems based on extensive\ntraining data differs significantly from the nuanced process of making\nscientific discoveries. Trained on almost all human knowledge available,\ntoday's sophisticated LLMs basically learn to predict sequences of tokens. They\ngenerate mathematical derivations and write code in a similar way as writing an\nessay, and do not have the ability to pioneer scientific discoveries in the\nmanner a human scientist would do.\n  In this study we delve into the potential of using deep learning to\nrediscover a fundamental mathematical concept: integrals. By defining integrals\nas area under the curve, we illustrate how AI can deduce the integral of a\ngiven function, exemplified by inferring $\\int_{0}^{x} t^2 dt = \\frac{x^3}{3}$\nand $\\int_{0}^{x} ae^{bt} dt = \\frac{a}{b} e^{bx} - \\frac{a}{b}$. Our\nexperiments show that deep learning models can approach the task of inferring\nintegrals either through a sequence-to-sequence model, akin to language\ntranslation, or by uncovering the rudimentary principles of integration, such\nas $\\int_{0}^{x} t^n dt = \\frac{x^{n+1}}{n+1}$.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.18040v1.pdf",
        "similarity": 0.38715555340280766,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-28"
    },
    {
        "new_title": "Post-hoc and manifold explanations analysis of facial expression data\n  based on deep learning",
        "new_link": "http://arxiv.org/abs/2404.18352v1",
        "new_summary": "  The complex information processing system of humans generates a lot of\nobjective and subjective evaluations, making the exploration of human cognitive\nproducts of great cutting-edge theoretical value. In recent years, deep\nlearning technologies, which are inspired by biological brain mechanisms, have\nmade significant strides in the application of psychological or cognitive\nscientific research, particularly in the memorization and recognition of facial\ndata. This paper investigates through experimental research how neural networks\nprocess and store facial expression data and associate these data with a range\nof psychological attributes produced by humans. Researchers utilized deep\nlearning model VGG16, demonstrating that neural networks can learn and\nreproduce key features of facial data, thereby storing image memories.\nMoreover, the experimental results reveal the potential of deep learning models\nin understanding human emotions and cognitive processes and establish a\nmanifold visualization interpretation of cognitive products or psychological\nattributes from a non-Euclidean space perspective, offering new insights into\nenhancing the explainability of AI. This study not only advances the\napplication of AI technology in the field of psychology but also provides a new\npsychological theoretical understanding the information processing of the AI.\nThe code is available in here: https://github.com/NKUShaw/Psychoinformatics.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.18352v1.pdf",
        "similarity": 0.38692271588278676,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-29"
    },
    {
        "new_title": "A Survey of Deep Learning and Foundation Models for Time Series\n  Forecasting",
        "new_link": "http://arxiv.org/abs/2401.13912v1",
        "new_summary": "  Deep Learning has been successfully applied to many application domains, yet\nits advantages have been slow to emerge for time series forecasting. For\nexample, in the well-known Makridakis (M) Competitions, hybrids of traditional\nstatistical or machine learning techniques have only recently become the top\nperformers. With the recent architectural advances in deep learning being\napplied to time series forecasting (e.g., encoder-decoders with attention,\ntransformers, and graph neural networks), deep learning has begun to show\nsignificant advantages. Still, in the area of pandemic prediction, there remain\nchallenges for deep learning models: the time series is not long enough for\neffective training, unawareness of accumulated scientific knowledge, and\ninterpretability of the model. To this end, the development of foundation\nmodels (large deep learning models with extensive pre-training) allows models\nto understand patterns and acquire knowledge that can be applied to new related\nproblems before extensive training data becomes available. Furthermore, there\nis a vast amount of knowledge available that deep learning models can tap into,\nincluding Knowledge Graphs and Large Language Models fine-tuned with scientific\ndomain knowledge. There is ongoing research examining how to utilize or inject\nsuch knowledge into deep learning models. In this survey, several\nstate-of-the-art modeling techniques are reviewed, and suggestions for further\nwork are provided.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.13912v1.pdf",
        "similarity": 0.38685937092678774,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-25"
    },
    {
        "new_title": "A Comprehensive Guide to Combining R and Python code for Data Science,\n  Machine Learning and Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2407.14695v1",
        "new_summary": "  Python has gained widespread popularity in the fields of machine learning,\nartificial intelligence, and data engineering due to its effectiveness and\nextensive libraries. R, on its side, remains a dominant language for\nstatistical analysis and visualization. However, certain libraries have become\noutdated, limiting their functionality and performance. Users can use Python's\nadvanced machine learning and AI capabilities alongside R's robust statistical\npackages by combining these two programming languages. This paper explores\nusing R's reticulate package to call Python from R, providing practical\nexamples and highlighting scenarios where this integration enhances\nproductivity and analytical capabilities. With a few hello-world code snippets,\nwe demonstrate how to run Python's scikit-learn, pytorch and OpenAI gym\nlibraries for building Machine Learning, Deep Learning, and Reinforcement\nLearning projects easily.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.14695v1.pdf",
        "similarity": 0.38667302997554004,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-19"
    },
    {
        "new_title": "Is Efficient PAC Learning Possible with an Oracle That Responds 'Yes' or\n  'No'?",
        "new_link": "http://arxiv.org/abs/2406.11667v2",
        "new_summary": "  The empirical risk minimization (ERM) principle has been highly impactful in\nmachine learning, leading both to near-optimal theoretical guarantees for\nERM-based learning algorithms as well as driving many of the recent empirical\nsuccesses in deep learning. In this paper, we investigate the question of\nwhether the ability to perform ERM, which computes a hypothesis minimizing\nempirical risk on a given dataset, is necessary for efficient learning: in\nparticular, is there a weaker oracle than ERM which can nevertheless enable\nlearnability? We answer this question affirmatively, showing that in the\nrealizable setting of PAC learning for binary classification, a concept class\ncan be learned using an oracle which only returns a single bit indicating\nwhether a given dataset is realizable by some concept in the class. The sample\ncomplexity and oracle complexity of our algorithm depend polynomially on the VC\ndimension of the hypothesis class, thus showing that there is only a polynomial\nprice to pay for use of our weaker oracle. Our results extend to the agnostic\nlearning setting with a slight strengthening of the oracle, as well as to the\npartial concept, multiclass and real-valued learning settings. In the setting\nof partial concept classes, prior to our work no oracle-efficient algorithms\nwere known, even with a standard ERM oracle. Thus, our results address a\nquestion of Alon et al. (2021) who asked whether there are algorithmic\nprinciples which enable efficient learnability in this setting.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.11667v2.pdf",
        "similarity": 0.38656780210243424,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-17"
    },
    {
        "new_title": "Enhancing Facial Expression Recognition through Dual-Direction Attention\n  Mixed Feature Networks: Application to 7th ABAW Challenge",
        "new_link": "http://arxiv.org/abs/2407.12390v2",
        "new_summary": "  We present our contribution to the 7th ABAW challenge at ECCV 2024, by\nutilizing a Dual-Direction Attention Mixed Feature Network for multitask facial\nexpression recognition we achieve results far beyond the proposed baseline for\nthe Multi-Task ABAW challenge. Our proposal uses the well-known DDAMFN\narchitecture as base to effectively predict valence-arousal, emotion\nrecognition, and action units. We demonstrate the architecture ability to\nhandle these tasks simultaneously, providing insights into its architecture and\nthe rationale behind its design. Additionally, we compare our results for a\nmultitask solution with independent single-task performance.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.12390v2.pdf",
        "similarity": 0.38649429415952996,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-17"
    },
    {
        "new_title": "Asymptotics of Learning with Deep Structured (Random) Features",
        "new_link": "http://arxiv.org/abs/2402.13999v2",
        "new_summary": "  For a large class of feature maps we provide a tight asymptotic\ncharacterisation of the test error associated with learning the readout layer,\nin the high-dimensional limit where the input dimension, hidden layer widths,\nand number of training samples are proportionally large. This characterization\nis formulated in terms of the population covariance of the features. Our work\nis partially motivated by the problem of learning with Gaussian rainbow neural\nnetworks, namely deep non-linear fully-connected networks with random but\nstructured weights, whose row-wise covariances are further allowed to depend on\nthe weights of previous layers. For such networks we also derive a closed-form\nformula for the feature covariance in terms of the weight matrices. We further\nfind that in some cases our results can capture feature maps learned by deep,\nfinite-width neural networks trained under gradient descent.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.13999v2.pdf",
        "similarity": 0.38628443716059246,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-21"
    },
    {
        "new_title": "Sharing Knowledge in Multi-Task Deep Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2401.09561v1",
        "new_summary": "  We study the benefit of sharing representations among tasks to enable the\neffective use of deep neural networks in Multi-Task Reinforcement Learning. We\nleverage the assumption that learning from different tasks, sharing common\nproperties, is helpful to generalize the knowledge of them resulting in a more\neffective feature extraction compared to learning a single task. Intuitively,\nthe resulting set of features offers performance benefits when used by\nReinforcement Learning algorithms. We prove this by providing theoretical\nguarantees that highlight the conditions for which is convenient to share\nrepresentations among tasks, extending the well-known finite-time bounds of\nApproximate Value-Iteration to the multi-task setting. In addition, we\ncomplement our analysis by proposing multi-task extensions of three\nReinforcement Learning algorithms that we empirically evaluate on widely used\nReinforcement Learning benchmarks showing significant improvements over the\nsingle-task counterparts in terms of sample efficiency and performance.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.09561v1.pdf",
        "similarity": 0.3862268547759803,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-17"
    },
    {
        "new_title": "Frost Prediction Using Machine Learning Methods in Fars Province",
        "new_link": "http://arxiv.org/abs/2401.11462v1",
        "new_summary": "  One of the common hazards and issues in meteorology and agriculture is the\nproblem of frost, chilling or freezing. This event occurs when the minimum\nambient temperature falls below a certain value. This phenomenon causes a lot\nof damage to the country, especially Fars province. Solving this problem\nrequires that, in addition to predicting the minimum temperature, we can\nprovide enough time to implement the necessary measures. Empirical methods have\nbeen provided by the Food and Agriculture Organization (FAO), which can predict\nthe minimum temperature, but not in time. In addition to this, we can use\nmachine learning methods to model the minimum temperature. In this study, we\nhave used three methods Gated Recurrent Unit (GRU), Temporal Convolutional\nNetwork (TCN) as deep learning methods, and Gradient Boosting (XGBoost). A\ncustomized loss function designed for methods based on deep learning, which can\nbe effective in reducing prediction errors. With methods based on deep learning\nmodels, not only do we observe a reduction in RMSE error compared to empirical\nmethods but also have more time to predict minimum temperature. Thus, we can\nmodel the minimum temperature for the next 24 hours by having the current 24\nhours. With the gradient boosting model (XGBoost) we can keep the prediction\ntime as deep learning and RMSE error reduced. Finally, we experimentally\nconcluded that machine learning methods work better than empirical methods and\nXGBoost model can have better performance in this problem among other\nimplemented.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.11462v1.pdf",
        "similarity": 0.38598818659117057,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-21"
    },
    {
        "new_title": "Contrastive-Based Deep Embeddings for Label Noise-Resilient\n  Histopathology Image Classification",
        "new_link": "http://arxiv.org/abs/2404.07605v1",
        "new_summary": "  Recent advancements in deep learning have proven highly effective in medical\nimage classification, notably within histopathology. However, noisy labels\nrepresent a critical challenge in histopathology image classification, where\naccurate annotations are vital for training robust deep learning models.\nIndeed, deep neural networks can easily overfit label noise, leading to severe\ndegradations in model performance. While numerous public pathology foundation\nmodels have emerged recently, none have evaluated their resilience to label\nnoise. Through thorough empirical analyses across multiple datasets, we exhibit\nthe label noise resilience property of embeddings extracted from foundation\nmodels trained in a self-supervised contrastive manner. We demonstrate that\ntraining with such embeddings substantially enhances label noise robustness\nwhen compared to non-contrastive-based ones as well as commonly used\nnoise-resilient methods. Our results unequivocally underline the superiority of\ncontrastive learning in effectively mitigating the label noise challenge. Code\nis publicly available at\nhttps://github.com/LucasDedieu/NoiseResilientHistopathology.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.07605v1.pdf",
        "similarity": 0.38594490375777113,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-11"
    },
    {
        "new_title": "Practical Dataset Distillation Based on Deep Support Vectors",
        "new_link": "http://arxiv.org/abs/2405.00348v1",
        "new_summary": "  Conventional dataset distillation requires significant computational\nresources and assumes access to the entire dataset, an assumption impractical\nas it presumes all data resides on a central server. In this paper, we focus on\ndataset distillation in practical scenarios with access to only a fraction of\nthe entire dataset. We introduce a novel distillation method that augments the\nconventional process by incorporating general model knowledge via the addition\nof Deep KKT (DKKT) loss. In practical settings, our approach showed improved\nperformance compared to the baseline distribution matching distillation method\non the CIFAR-10 dataset. Additionally, we present experimental evidence that\nDeep Support Vectors (DSVs) offer unique information to the original\ndistillation, and their integration results in enhanced performance.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.00348v1.pdf",
        "similarity": 0.38576737344014217,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-01"
    },
    {
        "new_title": "Beyond the Labels: Unveiling Text-Dependency in Paralinguistic Speech\n  Recognition Datasets",
        "new_link": "http://arxiv.org/abs/2403.07767v1",
        "new_summary": "  Paralinguistic traits like cognitive load and emotion are increasingly\nrecognized as pivotal areas in speech recognition research, often examined\nthrough specialized datasets like CLSE and IEMOCAP. However, the integrity of\nthese datasets is seldom scrutinized for text-dependency. This paper critically\nevaluates the prevalent assumption that machine learning models trained on such\ndatasets genuinely learn to identify paralinguistic traits, rather than merely\ncapturing lexical features. By examining the lexical overlap in these datasets\nand testing the performance of machine learning models, we expose significant\ntext-dependency in trait-labeling. Our results suggest that some machine\nlearning models, especially large pre-trained models like HuBERT, might\ninadvertently focus on lexical characteristics rather than the intended\nparalinguistic features. The study serves as a call to action for the research\ncommunity to reevaluate the reliability of existing datasets and methodologies,\nensuring that machine learning models genuinely learn what they are designed to\nrecognize.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.07767v1.pdf",
        "similarity": 0.38553553317740574,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-12"
    },
    {
        "new_title": "PlantTracing: Tracing Arabidopsis Thaliana Apex with CenterTrack",
        "new_link": "http://arxiv.org/abs/2405.11351v1",
        "new_summary": "  This work applies an encoder-decoder-based machine learning network to detect\nand track the motion and growth of the flowering stem apex of Arabidopsis\nThaliana. Based on the CenterTrack, a machine learning back-end network, we\ntrained a model based on ten time-lapsed labeled videos and tested against\nthree videos.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.11351v1.pdf",
        "similarity": 0.385482540333211,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-18"
    },
    {
        "new_title": "Robustness Evaluation of Machine Learning Models for Robot Arm Action\n  Recognition in Noisy Environments",
        "new_link": "http://arxiv.org/abs/2401.09606v1",
        "new_summary": "  In the realm of robot action recognition, identifying distinct but spatially\nproximate arm movements using vision systems in noisy environments poses a\nsignificant challenge. This paper studies robot arm action recognition in noisy\nenvironments using machine learning techniques. Specifically, a vision system\nis used to track the robot's movements followed by a deep learning model to\nextract the arm's key points. Through a comparative analysis of machine\nlearning methods, the effectiveness and robustness of this model are assessed\nin noisy environments. A case study was conducted using the Tic-Tac-Toe game in\na 3-by-3 grid environment, where the focus is to accurately identify the\nactions of the arms in selecting specific locations within this constrained\nenvironment. Experimental results show that our approach can achieve precise\nkey point detection and action classification despite the addition of noise and\nuncertainties to the dataset.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.09606v1.pdf",
        "similarity": 0.38527768306040466,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-17"
    },
    {
        "new_title": "Joint chest X-ray diagnosis and clinical visual attention prediction\n  with multi-stage cooperative learning: enhancing interpretability",
        "new_link": "http://arxiv.org/abs/2403.16970v2",
        "new_summary": "  As deep learning has become the state-of-the-art for computer-assisted\ndiagnosis, interpretability of the automatic decisions is crucial for clinical\ndeployment. While various methods were proposed in this domain, visual\nattention maps of clinicians during radiological screening offer a unique asset\nto provide important insights and can potentially enhance the quality of\ncomputer-assisted diagnosis. With this paper, we introduce a novel\ndeep-learning framework for joint disease diagnosis and prediction of\ncorresponding visual saliency maps for chest X-ray scans. Specifically, we\ndesigned a novel dual-encoder multi-task UNet, which leverages both a\nDenseNet201 backbone and a Residual and Squeeze-and-Excitation block-based\nencoder to extract diverse features for saliency map prediction, and a\nmulti-scale feature-fusion classifier to perform disease classification. To\ntackle the issue of asynchronous training schedules of individual tasks in\nmulti-task learning, we proposed a multi-stage cooperative learning strategy,\nwith contrastive learning for feature encoder pretraining to boost performance.\nExperiments show that our proposed method outperformed existing techniques for\nchest X-ray diagnosis and the quality of visual saliency map prediction.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.16970v2.pdf",
        "similarity": 0.38448373524928964,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-25"
    },
    {
        "new_title": "Towards Less Biased Data-driven Scoring with Deep Learning-Based\n  End-to-end Database Search in Tandem Mass Spectrometry",
        "new_link": "http://arxiv.org/abs/2405.06511v1",
        "new_summary": "  Peptide identification in mass spectrometry-based proteomics is crucial for\nunderstanding protein function and dynamics. Traditional database search\nmethods, though widely used, rely on heuristic scoring functions and\nstatistical estimations have to be introduced for a higher identification rate.\nHere, we introduce DeepSearch, the first deep learning-based end-to-end\ndatabase search method for tandem mass spectrometry. DeepSearch leverages a\nmodified transformer-based encoder-decoder architecture under the contrastive\nlearning framework. Unlike conventional methods that rely on ion-to-ion\nmatching, DeepSearch adopts a data-driven approach to score peptide spectrum\nmatches. DeepSearch is also the first deep learning-based method that can\nprofile variable post-translational modifications in a zero-shot manner. We\nshowed that DeepSearch's scoring scheme expressed less bias and did not require\nany statistical estimation. We validated DeepSearch's accuracy and robustness\nacross various datasets, including those from species with diverse protein\ncompositions and a modification-enriched dataset. DeepSearch sheds new light on\ndatabase search methods in tandem mass spectrometry.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.06511v1.pdf",
        "similarity": 0.38419191624432303,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-08"
    },
    {
        "new_title": "Numeric Reward Machines",
        "new_link": "http://arxiv.org/abs/2404.19370v1",
        "new_summary": "  Reward machines inform reinforcement learning agents about the reward\nstructure of the environment and often drastically speed up the learning\nprocess. However, reward machines only accept Boolean features such as\nrobot-reached-gold. Consequently, many inherently numeric tasks cannot profit\nfrom the guidance offered by reward machines. To address this gap, we aim to\nextend reward machines with numeric features such as distance-to-gold. For\nthis, we present two types of reward machines: numeric-Boolean and numeric. In\na numeric-Boolean reward machine, distance-to-gold is emulated by two Boolean\nfeatures distance-to-gold-decreased and robot-reached-gold. In a numeric reward\nmachine, distance-to-gold is used directly alongside the Boolean feature\nrobot-reached-gold. We compare our new approaches to a baseline reward machine\nin the Craft domain, where the numeric feature is the agent-to-target distance.\nWe use cross-product Q-learning, Q-learning with counter-factual experiences,\nand the options framework for learning. Our experimental results show that our\nnew approaches significantly outperform the baseline approach. Extending reward\nmachines with numeric features opens up new possibilities of using reward\nmachines in inherently numeric tasks.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.19370v1.pdf",
        "similarity": 0.3841483511357481,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-30"
    },
    {
        "new_title": "Deep Learning for Economists",
        "new_link": "http://arxiv.org/abs/2407.15339v1",
        "new_summary": "  Deep learning provides powerful methods to impute structured information from\nlarge-scale, unstructured text and image datasets. For example, economists\nmight wish to detect the presence of economic activity in satellite images, or\nto measure the topics or entities mentioned in social media, the congressional\nrecord, or firm filings. This review introduces deep neural networks, covering\nmethods such as classifiers, regression models, generative AI, and embedding\nmodels. Applications include classification, document digitization, record\nlinkage, and methods for data exploration in massive scale text and image\ncorpora. When suitable methods are used, deep learning models can be cheap to\ntune and can scale affordably to problems involving millions or billions of\ndata points.. The review is accompanied by a companion website, EconDL, with\nuser-friendly demo notebooks, software resources, and a knowledge base that\nprovides technical details and additional applications.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.15339v1.pdf",
        "similarity": 0.38405796806513876,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-22"
    },
    {
        "new_title": "MathWriting: A Dataset For Handwritten Mathematical Expression\n  Recognition",
        "new_link": "http://arxiv.org/abs/2404.10690v1",
        "new_summary": "  We introduce MathWriting, the largest online handwritten mathematical\nexpression dataset to date. It consists of 230k human-written samples and an\nadditional 400k synthetic ones. MathWriting can also be used for offline HME\nrecognition and is larger than all existing offline HME datasets like\nIM2LATEX-100K. We introduce a benchmark based on MathWriting data in order to\nadvance research on both online and offline HME recognition.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.10690v1.pdf",
        "similarity": 0.3838918136286988,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-16"
    },
    {
        "new_title": "Frontiers of Deep Learning: From Novel Application to Real-World\n  Deployment",
        "new_link": "http://arxiv.org/abs/2407.14386v1",
        "new_summary": "  Deep learning continues to re-shape numerous fields, from natural language\nprocessing and imaging to data analytics and recommendation systems. This\nreport studies two research papers that represent recent progress on deep\nlearning from two largely different aspects: The first paper applied the\ntransformer networks, which are typically used in language models, to improve\nthe quality of synthetic aperture radar image by effectively reducing the\nspeckle noise. The second paper presents an in-storage computing design\nsolution to enable cost-efficient and high-performance implementations of deep\nlearning recommendation systems. In addition to summarizing each paper in terms\nof motivation, key ideas and techniques, and evaluation results, this report\nalso presents thoughts and discussions about possible future research\ndirections. By carrying out in-depth study on these two representative papers\nand related references, this doctoral candidate has developed better\nunderstanding on the far-reaching impact and efficient implementation of deep\nlearning models.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.14386v1.pdf",
        "similarity": 0.3836615069210998,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-19"
    },
    {
        "new_title": "Dependable Distributed Training of Compressed Machine Learning Models",
        "new_link": "http://arxiv.org/abs/2402.14346v1",
        "new_summary": "  The existing work on the distributed training of machine learning (ML) models\nhas consistently overlooked the distribution of the achieved learning quality,\nfocusing instead on its average value. This leads to a poor dependability}of\nthe resulting ML models, whose performance may be much worse than expected. We\nfill this gap by proposing DepL, a framework for dependable learning\norchestration, able to make high-quality, efficient decisions on (i) the data\nto leverage for learning, (ii) the models to use and when to switch among them,\nand (iii) the clusters of nodes, and the resources thereof, to exploit. For\nconcreteness, we consider as possible available models a full DNN and its\ncompressed versions. Unlike previous studies, DepL guarantees that a target\nlearning quality is reached with a target probability, while keeping the\ntraining cost at a minimum. We prove that DepL has constant competitive ratio\nand polynomial complexity, and show that it outperforms the state-of-the-art by\nover 27% and closely matches the optimum.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.14346v1.pdf",
        "similarity": 0.38304776237988275,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-22"
    },
    {
        "new_title": "Developing, Analyzing, and Evaluating Vehicular Lane Keeping Algorithms\n  Under Dynamic Lighting and Weather Conditions Using Electric Vehicles",
        "new_link": "http://arxiv.org/abs/2406.06899v1",
        "new_summary": "  Self-driving vehicles have the potential to reduce accidents and fatalities\non the road. Many production vehicles already come equipped with basic\nself-driving capabilities, but have trouble following lanes in adverse lighting\nand weather conditions. Therefore, we develop, analyze, and evaluate two\nvehicular lane-keeping algorithms under dynamic weather conditions using a\ncombined deep learning- and hand-crafted approach and an end-to-end deep\nlearning approach. We use image segmentation- and linear-regression based deep\nlearning to drive the vehicle toward the center of the lane, measuring the\namount of laps completed, average speed, and average steering error per lap.\nOur hybrid model completes more laps than our end-to-end deep learning model.\nIn the future, we are interested in combining our algorithms to form one\ncohesive approach to lane-following.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.06899v1.pdf",
        "similarity": 0.38289330293095014,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-11"
    },
    {
        "new_title": "Data-Efficient Operator Learning via Unsupervised Pretraining and\n  In-Context Learning",
        "new_link": "http://arxiv.org/abs/2402.15734v2",
        "new_summary": "  Recent years have witnessed the promise of coupling machine learning methods\nand physical domainspecific insights for solving scientific problems based on\npartial differential equations (PDEs). However, being data-intensive, these\nmethods still require a large amount of PDE data. This reintroduces the need\nfor expensive numerical PDE solutions, partially undermining the original goal\nof avoiding these expensive simulations. In this work, seeking data efficiency,\nwe design unsupervised pretraining for PDE operator learning. To reduce the\nneed for training data with heavy simulation costs, we mine unlabeled PDE data\nwithout simulated solutions, and pretrain neural operators with\nphysics-inspired reconstruction-based proxy tasks. To improve\nout-of-distribution performance, we further assist neural operators in flexibly\nleveraging in-context learning methods, without incurring extra training costs\nor designs. Extensive empirical evaluations on a diverse set of PDEs\ndemonstrate that our method is highly data-efficient, more generalizable, and\neven outperforms conventional vision-pretrained models.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.15734v2.pdf",
        "similarity": 0.3828643702542332,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-24"
    },
    {
        "new_title": "Proximal Curriculum with Task Correlations for Deep Reinforcement\n  Learning",
        "new_link": "http://arxiv.org/abs/2405.02481v1",
        "new_summary": "  Curriculum design for reinforcement learning (RL) can speed up an agent's\nlearning process and help it learn to perform well on complex tasks. However,\nexisting techniques typically require domain-specific hyperparameter tuning,\ninvolve expensive optimization procedures for task selection, or are suitable\nonly for specific learning objectives. In this work, we consider curriculum\ndesign in contextual multi-task settings where the agent's final performance is\nmeasured w.r.t. a target distribution over complex tasks. We base our\ncurriculum design on the Zone of Proximal Development concept, which has proven\nto be effective in accelerating the learning process of RL agents for uniform\ndistribution over all tasks. We propose a novel curriculum, ProCuRL-Target,\nthat effectively balances the need for selecting tasks that are not too\ndifficult for the agent while progressing the agent's learning toward the\ntarget distribution via leveraging task correlations. We theoretically justify\nthe task selection strategy of ProCuRL-Target by analyzing a simple learning\nsetting with REINFORCE learner model. Our experimental results across various\ndomains with challenging target task distributions affirm the effectiveness of\nour curriculum strategy over state-of-the-art baselines in accelerating the\ntraining process of deep RL agents.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.02481v1.pdf",
        "similarity": 0.38274390642621714,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-03"
    },
    {
        "new_title": "Data Collection and Labeling Techniques for Machine Learning",
        "new_link": "http://arxiv.org/abs/2407.12793v1",
        "new_summary": "  Data collection and labeling are critical bottlenecks in the deployment of\nmachine learning applications. With the increasing complexity and diversity of\napplications, the need for efficient and scalable data collection and labeling\ntechniques has become paramount. This paper provides a review of the\nstate-of-the-art methods in data collection, data labeling, and the improvement\nof existing data and models. By integrating perspectives from both the machine\nlearning and data management communities, we aim to provide a holistic view of\nthe current landscape and identify future research directions.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.12793v1.pdf",
        "similarity": 0.3826803936454155,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-19"
    },
    {
        "new_title": "Automated Web-Based Malaria Detection System with Machine Learning and\n  Deep Learning Techniques",
        "new_link": "http://arxiv.org/abs/2407.00120v1",
        "new_summary": "  Malaria parasites pose a significant global health burden, causing widespread\nsuffering and mortality. Detecting malaria infection accurately is crucial for\neffective treatment and control. However, existing automated detection\ntechniques have shown limitations in terms of accuracy and generalizability.\nMany studies have focused on specific features without exploring more\ncomprehensive approaches. In our case, we formulate a deep learning technique\nfor malaria-infected cell classification using traditional CNNs and transfer\nlearning models notably VGG19, InceptionV3, and Xception. The models were\ntrained using NIH datasets and tested using different performance metrics such\nas accuracy, precision, recall, and F1-score. The test results showed that deep\nCNNs achieved the highest accuracy -- 97%, followed by Xception with an\naccuracy of 95%. A machine learning model SVM achieved an accuracy of 83%,\nwhile an Inception-V3 achieved an accuracy of 94%. Furthermore, the system can\nbe accessed through a web interface, where users can upload blood smear images\nfor malaria detection.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.00120v1.pdf",
        "similarity": 0.38243866922090225,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-27"
    },
    {
        "new_title": "Improving Interpretability of Deep Active Learning for Flood Inundation\n  Mapping Through Class Ambiguity Indices Using Multi-spectral Satellite\n  Imagery",
        "new_link": "http://arxiv.org/abs/2404.19043v1",
        "new_summary": "  Flood inundation mapping is a critical task for responding to the increasing\nrisk of flooding linked to global warming. Significant advancements of deep\nlearning in recent years have triggered its extensive applications, including\nflood inundation mapping. To cope with the time-consuming and labor-intensive\ndata labeling process in supervised learning, deep active learning strategies\nare one of the feasible approaches. However, there remains limited exploration\ninto the interpretability of how deep active learning strategies operate, with\na specific focus on flood inundation mapping in the field of remote sensing. In\nthis study, we introduce a novel framework of Interpretable Deep Active\nLearning for Flood inundation Mapping (IDAL-FIM), specifically in terms of\nclass ambiguity of multi-spectral satellite images. In the experiments, we\nutilize Sen1Floods11 dataset, and adopt U-Net with MC-dropout. In addition, we\nemploy five acquisition functions, which are the random, K-means, BALD,\nentropy, and margin acquisition functions. Based on the experimental results,\nwe demonstrate that two proposed class ambiguity indices are effective\nvariables to interpret the deep active learning by establishing statistically\nsignificant correlation with the predictive uncertainty of the deep learning\nmodel at the tile level. Then, we illustrate the behaviors of deep active\nlearning through visualizing two-dimensional density plots and providing\ninterpretations regarding the operation of deep active learning, in flood\ninundation mapping.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.19043v1.pdf",
        "similarity": 0.38201131286208495,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-29"
    },
    {
        "new_title": "CHG Shapley: Efficient Data Valuation and Selection towards Trustworthy\n  Machine Learning",
        "new_link": "http://arxiv.org/abs/2406.11730v2",
        "new_summary": "  Understanding the decision-making process of machine learning models is\ncrucial for ensuring trustworthy machine learning. Data Shapley, a landmark\nstudy on data valuation, advances this understanding by assessing the\ncontribution of each datum to model accuracy. However, the resource-intensive\nand time-consuming nature of multiple model retraining poses challenges for\napplying Data Shapley to large datasets. To address this, we propose the CHG\n(Conduct of Hardness and Gradient) score, which approximates the utility of\neach data subset on model accuracy during a single model training. By deriving\nthe closed-form expression of the Shapley value for each data point under the\nCHG score utility function, we reduce the computational complexity to the\nequivalent of a single model retraining, an exponential improvement over\nexisting methods. Additionally, we employ CHG Shapley for real-time data\nselection, demonstrating its effectiveness in identifying high-value and noisy\ndata. CHG Shapley facilitates trustworthy model training through efficient data\nvaluation, introducing a novel data-centric perspective on trustworthy machine\nlearning.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.11730v2.pdf",
        "similarity": 0.38194868984873565,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-17"
    },
    {
        "new_title": "Latent Object Characteristics Recognition with Visual to Haptic-Audio\n  Cross-modal Transfer Learning",
        "new_link": "http://arxiv.org/abs/2403.10689v1",
        "new_summary": "  Recognising the characteristics of objects while a robot handles them is\ncrucial for adjusting motions that ensure stable and efficient interactions\nwith containers. Ahead of realising stable and efficient robot motions for\nhandling/transferring the containers, this work aims to recognise the latent\nunobservable object characteristics. While vision is commonly used for object\nrecognition by robots, it is ineffective for detecting hidden objects. However,\nrecognising objects indirectly using other sensors is a challenging task. To\naddress this challenge, we propose a cross-modal transfer learning approach\nfrom vision to haptic-audio. We initially train the model with vision, directly\nobserving the target object. Subsequently, we transfer the latent space learned\nfrom vision to a second module, trained only with haptic-audio and motor data.\nThis transfer learning framework facilitates the representation of object\ncharacteristics using indirect sensor data, thereby improving recognition\naccuracy. For evaluating the recognition accuracy of our proposed learning\nframework we selected shape, position, and orientation as the object\ncharacteristics. Finally, we demonstrate online recognition of both trained and\nuntrained objects using the humanoid robot Nextage Open.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.10689v1.pdf",
        "similarity": 0.38187981845494995,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-15"
    },
    {
        "new_title": "Vlearn: Off-Policy Learning with Efficient State-Value Function\n  Estimation",
        "new_link": "http://arxiv.org/abs/2403.04453v2",
        "new_summary": "  Existing off-policy reinforcement learning algorithms often rely on an\nexplicit state-action-value function representation, which can be problematic\nin high-dimensional action spaces due to the curse of dimensionality. This\nreliance results in data inefficiency as maintaining a state-action-value\nfunction in such spaces is challenging. We present an efficient approach that\nutilizes only a state-value function as the critic for off-policy deep\nreinforcement learning. This approach, which we refer to as Vlearn, effectively\ncircumvents the limitations of existing methods by eliminating the necessity\nfor an explicit state-action-value function. To this end, we introduce a novel\nimportance sampling loss for learning deep value functions from off-policy\ndata. While this is common for linear methods, it has not been combined with\ndeep value function networks. This transfer to deep methods is not\nstraightforward and requires novel design choices such as robust policy\nupdates, twin value function networks to avoid an optimization bias, and\nimportance weight clipping. We also present a novel analysis of the variance of\nour estimate compared to commonly used importance sampling estimators such as\nV-trace. Our approach improves sample complexity as well as final performance\nand ensures consistent and robust performance across various benchmark tasks.\nEliminating the state-action-value function in Vlearn facilitates a streamlined\nlearning process, enabling more effective exploration and exploitation in\ncomplex environments.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.04453v2.pdf",
        "similarity": 0.3812239676393493,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-07"
    },
    {
        "new_title": "Few-shot point cloud reconstruction and denoising via learned Guassian\n  splats renderings and fine-tuned diffusion features",
        "new_link": "http://arxiv.org/abs/2404.01112v4",
        "new_summary": "  Existing deep learning methods for the reconstruction and denoising of point\nclouds rely on small datasets of 3D shapes. We circumvent the problem by\nleveraging deep learning methods trained on billions of images. We propose a\nmethod to reconstruct point clouds from few images and to denoise point clouds\nfrom their rendering by exploiting prior knowledge distilled from image-based\ndeep learning models. To improve reconstruction in constraint settings, we\nregularize the training of a differentiable renderer with hybrid surface and\nappearance by introducing semantic consistency supervision. In addition, we\npropose a pipeline to finetune Stable Diffusion to denoise renderings of noisy\npoint clouds and we demonstrate how these learned filters can be used to remove\npoint cloud noise coming without 3D supervision. We compare our method with DSS\nand PointRadiance and achieved higher quality 3D reconstruction on the\nSketchfab Testset and SCUT Dataset.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.01112v4.pdf",
        "similarity": 0.3811057765044221,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-01"
    },
    {
        "new_title": "Deep Learning for Trajectory Data Management and Mining: A Survey and\n  Beyond",
        "new_link": "http://arxiv.org/abs/2403.14151v1",
        "new_summary": "  Trajectory computing is a pivotal domain encompassing trajectory data\nmanagement and mining, garnering widespread attention due to its crucial role\nin various practical applications such as location services, urban traffic, and\npublic safety. Traditional methods, focusing on simplistic spatio-temporal\nfeatures, face challenges of complex calculations, limited scalability, and\ninadequate adaptability to real-world complexities. In this paper, we present a\ncomprehensive review of the development and recent advances in deep learning\nfor trajectory computing (DL4Traj). We first define trajectory data and provide\na brief overview of widely-used deep learning models. Systematically, we\nexplore deep learning applications in trajectory management (pre-processing,\nstorage, analysis, and visualization) and mining (trajectory-related\nforecasting, trajectory-related recommendation, trajectory classification,\ntravel time estimation, anomaly detection, and mobility generation). Notably,\nwe encapsulate recent advancements in Large Language Models (LLMs) that hold\nthe potential to augment trajectory computing. Additionally, we summarize\napplication scenarios, public datasets, and toolkits. Finally, we outline\ncurrent challenges in DL4Traj research and propose future directions. Relevant\npapers and open-source resources have been collated and are continuously\nupdated at:\n\\href{https://github.com/yoshall/Awesome-Trajectory-Computing}{DL4Traj Repo}.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.14151v1.pdf",
        "similarity": 0.38031761571924566,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-21"
    },
    {
        "new_title": "Alzheimer's Magnetic Resonance Imaging Classification Using Deep and\n  Meta-Learning Models",
        "new_link": "http://arxiv.org/abs/2405.12126v1",
        "new_summary": "  Deep learning, a cutting-edge machine learning approach, outperforms\ntraditional machine learning in identifying intricate structures in complex\nhigh-dimensional data, particularly in the domain of healthcare. This study\nfocuses on classifying Magnetic Resonance Imaging (MRI) data for Alzheimer's\ndisease (AD) by leveraging deep learning techniques characterized by\nstate-of-the-art CNNs. Brain imaging techniques such as MRI have enabled the\nmeasurement of pathophysiological brain changes related to Alzheimer's disease.\nAlzheimer's disease is the leading cause of dementia in the elderly, and it is\nan irreversible brain illness that causes gradual cognitive function disorder.\nIn this paper, we train some benchmark deep models individually for the\napproach of the solution and later use an ensembling approach to combine the\neffect of multiple CNNs towards the observation of higher recall and accuracy.\nHere, the model's effectiveness is evaluated using various methods, including\nstacking, majority voting, and the combination of models with high recall\nvalues. The majority voting performs better than the alternative modelling\napproach as the majority voting approach typically reduces the variance in the\npredictions. We report a test accuracy of 90% with a precision score of 0.90\nand a recall score of 0.89 in our proposed approach. In future, this study can\nbe extended to incorporate other types of medical data, including signals,\nimages, and other data. The same or alternative datasets can be used with\nadditional classifiers, neural networks, and AI techniques to enhance\nAlzheimer's detection.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.12126v1.pdf",
        "similarity": 0.3803038138444017,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-20"
    },
    {
        "new_title": "Data Pipeline Training: Integrating AutoML to Optimize the Data Flow of\n  Machine Learning Models",
        "new_link": "http://arxiv.org/abs/2402.12916v1",
        "new_summary": "  Data Pipeline plays an indispensable role in tasks such as modeling machine\nlearning and developing data products. With the increasing diversification and\ncomplexity of Data sources, as well as the rapid growth of data volumes,\nbuilding an efficient Data Pipeline has become crucial for improving work\nefficiency and solving complex problems. This paper focuses on exploring how to\noptimize data flow through automated machine learning methods by integrating\nAutoML with Data Pipeline. We will discuss how to leverage AutoML technology to\nenhance the intelligence of Data Pipeline, thereby achieving better results in\nmachine learning tasks. By delving into the automation and optimization of Data\nflows, we uncover key strategies for constructing efficient data pipelines that\ncan adapt to the ever-changing data landscape. This not only accelerates the\nmodeling process but also provides innovative solutions to complex problems,\nenabling more significant outcomes in increasingly intricate data domains.\nKeywords- Data Pipeline Training;AutoML; Data environment; Machine learning\n",
        "pdf_link": "https://arxiv.org/pdf/2402.12916v1.pdf",
        "similarity": 0.380287292018855,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-20"
    },
    {
        "new_title": "Can machine learning solve the challenge of adaptive learning and the\n  individualization of learning paths? A field experiment in an online learning\n  platform",
        "new_link": "http://arxiv.org/abs/2407.03118v3",
        "new_summary": "  The individualization of learning contents based on digital technologies\npromises large individual and social benefits. However, it remains an open\nquestion how this individualization can be implemented. To tackle this question\nwe conduct a randomized controlled trial on a large digital self-learning\nplatform. We develop an algorithm based on two convolutional neural networks\nthat assigns tasks to $4,365$ learners according to their learning paths.\nLearners are randomized into three groups: two treatment groups -- a\ngroup-based adaptive treatment group and an individual adaptive treatment group\n-- and one control group. We analyze the difference between the three groups\nwith respect to effort learners provide and their performance on the platform.\nOur null results shed light on the multiple challenges associated with the\nindividualization of learning paths.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.03118v3.pdf",
        "similarity": 0.3802300158068287,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-03"
    },
    {
        "new_title": "Tiny Machine Learning: Progress and Futures",
        "new_link": "http://arxiv.org/abs/2403.19076v2",
        "new_summary": "  Tiny Machine Learning (TinyML) is a new frontier of machine learning. By\nsqueezing deep learning models into billions of IoT devices and\nmicrocontrollers (MCUs), we expand the scope of AI applications and enable\nubiquitous intelligence. However, TinyML is challenging due to hardware\nconstraints: the tiny memory resource makes it difficult to hold deep learning\nmodels designed for cloud and mobile platforms. There is also limited compiler\nand inference engine support for bare-metal devices. Therefore, we need to\nco-design the algorithm and system stack to enable TinyML. In this review, we\nwill first discuss the definition, challenges, and applications of TinyML. We\nthen survey the recent progress in TinyML and deep learning on MCUs. Next, we\nwill introduce MCUNet, showing how we can achieve ImageNet-scale AI\napplications on IoT devices with system-algorithm co-design. We will further\nextend the solution from inference to training and introduce tiny on-device\ntraining techniques. Finally, we present future directions in this area.\nToday's large model might be tomorrow's tiny model. The scope of TinyML should\nevolve and adapt over time.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.19076v2.pdf",
        "similarity": 0.3798223222841599,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-28"
    },
    {
        "new_title": "From Discrete to Continuous: Deep Fair Clustering With Transferable\n  Representations",
        "new_link": "http://arxiv.org/abs/2403.16201v1",
        "new_summary": "  We consider the problem of deep fair clustering, which partitions data into\nclusters via the representations extracted by deep neural networks while hiding\nsensitive data attributes. To achieve fairness, existing methods present a\nvariety of fairness-related objective functions based on the group fairness\ncriterion. However, these works typically assume that the sensitive attributes\nare discrete and do not work for continuous sensitive variables, such as the\nproportion of the female population in an area. Besides, the potential of the\nrepresentations learned from clustering tasks to improve performance on other\ntasks is ignored by existing works. In light of these limitations, we propose a\nflexible deep fair clustering method that can handle discrete and continuous\nsensitive attributes simultaneously. Specifically, we design an information\nbottleneck style objective function to learn fair and clustering-friendly\nrepresentations. Furthermore, we explore for the first time the transferability\nof the extracted representations to other downstream tasks. Unlike existing\nworks, we impose fairness at the representation level, which could guarantee\nfairness for the transferred task regardless of clustering results. To verify\nthe effectiveness of the proposed method, we perform extensive experiments on\ndatasets with discrete and continuous sensitive attributes, demonstrating the\nadvantage of our method in comparison with state-of-the-art methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.16201v1.pdf",
        "similarity": 0.3791778740837132,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-24"
    },
    {
        "new_title": "AI-Enhanced Cognitive Behavioral Therapy: Deep Learning and Large\n  Language Models for Extracting Cognitive Pathways from Social Media Texts",
        "new_link": "http://arxiv.org/abs/2404.11449v1",
        "new_summary": "  Cognitive Behavioral Therapy (CBT) is an effective technique for addressing\nthe irrational thoughts stemming from mental illnesses, but it necessitates\nprecise identification of cognitive pathways to be successfully implemented in\npatient care. In current society, individuals frequently express negative\nemotions on social media on specific topics, often exhibiting cognitive\ndistortions, including suicidal behaviors in extreme cases. Yet, there is a\nnotable absence of methodologies for analyzing cognitive pathways that could\naid psychotherapists in conducting effective interventions online. In this\nstudy, we gathered data from social media and established the task of\nextracting cognitive pathways, annotating the data based on a cognitive\ntheoretical framework. We initially categorized the task of extracting\ncognitive pathways as a hierarchical text classification with four main\ncategories and nineteen subcategories. Following this, we structured a text\nsummarization task to help psychotherapists quickly grasp the essential\ninformation. Our experiments evaluate the performance of deep learning and\nlarge language models (LLMs) on these tasks. The results demonstrate that our\ndeep learning method achieved a micro-F1 score of 62.34% in the hierarchical\ntext classification task. Meanwhile, in the text summarization task, GPT-4\nattained a Rouge-1 score of 54.92 and a Rouge-2 score of 30.86, surpassing the\nexperimental deep learning model's performance. However, it may suffer from an\nissue of hallucination. We have made all models and codes publicly available to\nsupport further research in this field.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.11449v1.pdf",
        "similarity": 0.3790748041657529,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-17"
    },
    {
        "new_title": "The Implicit Bias of Adam on Separable Data",
        "new_link": "http://arxiv.org/abs/2406.10650v1",
        "new_summary": "  Adam has become one of the most favored optimizers in deep learning problems.\nDespite its success in practice, numerous mysteries persist regarding its\ntheoretical understanding. In this paper, we study the implicit bias of Adam in\nlinear logistic regression. Specifically, we show that when the training data\nare linearly separable, Adam converges towards a linear classifier that\nachieves the maximum $\\ell_\\infty$-margin. Notably, for a general class of\ndiminishing learning rates, this convergence occurs within polynomial time. Our\nresult shed light on the difference between Adam and (stochastic) gradient\ndescent from a theoretical perspective.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.10650v1.pdf",
        "similarity": 0.37853208127698973,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-15"
    },
    {
        "new_title": "ExioML: Eco-economic dataset for Machine Learning in Global Sectoral\n  Sustainability",
        "new_link": "http://arxiv.org/abs/2406.09046v2",
        "new_summary": "  The Environmental Extended Multi-Regional Input-Output analysis is the\npredominant framework in Ecological Economics for assessing the environmental\nimpact of economic activities. This paper introduces ExioML, the first Machine\nLearning benchmark dataset designed for sustainability analysis, aimed at\nlowering barriers and fostering collaboration between Machine Learning and\nEcological Economics research. A crucial greenhouse gas emission regression\ntask was conducted to evaluate sectoral sustainability and demonstrate the\nusability of the dataset. We compared the performance of traditional shallow\nmodels with deep learning models, utilizing a diverse Factor Accounting table\nand incorporating various categorical and numerical features. Our findings\nreveal that ExioML, with its high usability, enables deep and ensemble models\nto achieve low mean square errors, establishing a baseline for future Machine\nLearning research. Through ExioML, we aim to build a foundational dataset\nsupporting various Machine Learning applications and promote climate actions\nand sustainable investment decisions.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.09046v2.pdf",
        "similarity": 0.37842410237660196,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-11"
    },
    {
        "new_title": "Opening the Black Box: predicting the trainability of deep neural\n  networks with reconstruction entropy",
        "new_link": "http://arxiv.org/abs/2406.12916v1",
        "new_summary": "  An important challenge in machine learning is to predict the initial\nconditions under which a given neural network will be trainable. We present a\nmethod for predicting the trainable regime in parameter space for deep\nfeedforward neural networks, based on reconstructing the input from subsequent\nactivation layers via a cascade of single-layer auxiliary networks. For both\nMNIST and CIFAR10, we show that a single epoch of training of the shallow\ncascade networks is sufficient to predict the trainability of the deep\nfeedforward network, thereby providing a significant reduction in overall\ntraining time. We achieve this by computing the relative entropy between\nreconstructed images and the original inputs, and show that this probe of\ninformation loss is sensitive to the phase behaviour of the network. Our\nresults provide a concrete link between the flow of information and the\ntrainability of deep neural networks, further elucidating the role of\ncriticality in these systems.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.12916v1.pdf",
        "similarity": 0.37832127877745636,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-13"
    },
    {
        "new_title": "LatentExplainer: Explaining Latent Representations in Deep Generative\n  Models with Multi-modal Foundation Models",
        "new_link": "http://arxiv.org/abs/2406.14862v3",
        "new_summary": "  Deep generative models like VAEs and diffusion models have advanced various\ngeneration tasks by leveraging latent variables to learn data distributions and\ngenerate high-quality samples. Despite the field of explainable AI making\nstrides in interpreting machine learning models, understanding latent variables\nin generative models remains challenging. This paper introduces\nLatentExplainer, a framework for automatically generating semantically\nmeaningful explanations of latent variables in deep generative models.\nLatentExplainer tackles three main challenges: inferring the meaning of latent\nvariables, aligning explanations with inductive biases, and handling varying\ndegrees of explainability. By perturbing latent variables and interpreting\nchanges in generated data, the framework provides a systematic approach to\nunderstanding and controlling the data generation process, enhancing the\ntransparency and interpretability of deep generative models. We evaluate our\nproposed method on several real-world and synthetic datasets, and the results\ndemonstrate superior performance in generating high-quality explanations of\nlatent variables.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.14862v3.pdf",
        "similarity": 0.3782601204989311,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-21"
    },
    {
        "new_title": "Towards Improved Variational Inference for Deep Bayesian Models",
        "new_link": "http://arxiv.org/abs/2401.12418v1",
        "new_summary": "  Deep learning has revolutionized the last decade, being at the forefront of\nextraordinary advances in a wide range of tasks including computer vision,\nnatural language processing, and reinforcement learning, to name but a few.\nHowever, it is well-known that deep models trained via maximum likelihood\nestimation tend to be overconfident and give poorly-calibrated predictions.\nBayesian deep learning attempts to address this by placing priors on the model\nparameters, which are then combined with a likelihood to perform posterior\ninference. Unfortunately, for deep models, the true posterior is intractable,\nforcing the user to resort to approximations. In this thesis, we explore the\nuse of variational inference (VI) as an approximation, as it is unique in\nsimultaneously approximating the posterior and providing a lower bound to the\nmarginal likelihood. If tight enough, this lower bound can be used to optimize\nhyperparameters and to facilitate model selection. However, this capacity has\nrarely been used to its full extent for Bayesian neural networks, likely\nbecause the approximate posteriors typically used in practice can lack the\nflexibility to effectively bound the marginal likelihood. We therefore explore\nthree aspects of Bayesian learning for deep models: 1) we ask whether it is\nnecessary to perform inference over as many parameters as possible, or whether\nit is reasonable to treat many of them as optimizable hyperparameters; 2) we\npropose a variational posterior that provides a unified view of inference in\nBayesian neural networks and deep Gaussian processes; 3) we demonstrate how VI\ncan be improved in certain deep Gaussian process models by analytically\nremoving symmetries from the posterior, and performing inference on Gram\nmatrices instead of features. We hope that our contributions will provide a\nstepping stone to fully realize the promises of VI in the future.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.12418v1.pdf",
        "similarity": 0.3779930847560123,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-23"
    },
    {
        "new_title": "Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy,\n  Advances, and Outlook",
        "new_link": "http://arxiv.org/abs/2402.19348v2",
        "new_summary": "  As cities continue to burgeon, Urban Computing emerges as a pivotal\ndiscipline for sustainable development by harnessing the power of cross-domain\ndata fusion from diverse sources (e.g., geographical, traffic, social media,\nand environmental data) and modalities (e.g., spatio-temporal, visual, and\ntextual modalities). Recently, we are witnessing a rising trend that utilizes\nvarious deep-learning methods to facilitate cross-domain data fusion in smart\ncities. To this end, we propose the first survey that systematically reviews\nthe latest advancements in deep learning-based data fusion methods tailored for\nurban computing. Specifically, we first delve into data perspective to\ncomprehend the role of each modality and data source. Secondly, we classify the\nmethodology into four primary categories: feature-based, alignment-based,\ncontrast-based, and generation-based fusion methods. Thirdly, we further\ncategorize multi-modal urban applications into seven types: urban planning,\ntransportation, economy, public safety, society, environment, and energy.\nCompared with previous surveys, we focus more on the synergy of deep learning\nmethods with urban computing applications. Furthermore, we shed light on the\ninterplay between Large Language Models (LLMs) and urban computing, postulating\nfuture research directions that could revolutionize the field. We firmly\nbelieve that the taxonomy, progress, and prospects delineated in our survey\nstand poised to significantly enrich the research community. The summary of the\ncomprehensive and up-to-date paper list can be found at\nhttps://github.com/yoshall/Awesome-Multimodal-Urban-Computing.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.19348v2.pdf",
        "similarity": 0.3777938740392438,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-29"
    },
    {
        "new_title": "Hybrid deep learning and physics-based neural network for programmable\n  illumination computational microscopy",
        "new_link": "http://arxiv.org/abs/2403.12970v1",
        "new_summary": "  Relying on either deep models or physical models are two mainstream\napproaches for solving inverse sample reconstruction problems in programmable\nillumination computational microscopy. Solutions based on physical models\npossess strong generalization capabilities while struggling with global\noptimization of inverse problems due to a lack of insufficient physical\nconstraints. In contrast, deep learning methods have strong problem-solving\nabilities, but their generalization ability is often questioned because of the\nunclear physical principles. Besides, conventional deep models are difficult to\napply to some specific scenes because of the difficulty in acquiring\nhigh-quality training data and their limited capacity to generalize across\ndifferent scenarios. In this paper, to combine the advantages of deep models\nand physical models together, we propose a hybrid framework consisting of three\nsub-neural networks (two deep learning networks and one physics-based network).\nWe first obtain a result with rich semantic information through a light deep\nlearning neural network and then use it as the initial value of the physical\nnetwork to make its output comply with physical process constraints. These two\nresults are then used as the input of a fusion deep learning neural work which\nutilizes the paired features between the reconstruction results of two\ndifferent models to further enhance imaging quality. The final result\nintegrates the advantages of both deep models and physical models and can\nquickly solve the computational reconstruction inverse problem in programmable\nillumination computational microscopy and achieve better results. We verified\nthe feasibility and effectiveness of the proposed hybrid framework with\ntheoretical analysis and actual experiments on resolution targets and\nbiological samples.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.12970v1.pdf",
        "similarity": 0.3776966085848612,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-17"
    },
    {
        "new_title": "Heterogeneous Graph Neural Networks with Loss-decrease-aware Curriculum\n  Learning",
        "new_link": "http://arxiv.org/abs/2405.06522v1",
        "new_summary": "  In recent years, heterogeneous graph neural networks (HGNNs) have achieved\nexcellent performance in handling heterogeneous information networks (HINs).\nCurriculum learning is a machine learning strategy where training examples are\npresented to a model in a structured order, starting with easy examples and\ngradually increasing difficulty, aiming to improve learning efficiency and\ngeneralization. To better exploit the rich information in HINs, previous\nmethods have started to explore the use of curriculum learning strategy to\ntrain HGNNs. Specifically, these works utilize the absolute value of the loss\nat each training epoch to evaluate the learning difficulty of each training\nsample. However, the relative loss, rather than the absolute value of loss,\nreveals the learning difficulty. Therefore, we propose a novel\nloss-decrease-aware training schedule (LDTS). LDTS uses the trend of loss\ndecrease between each training epoch to better evaluating the difficulty of\ntraining samples, thereby enhancing the curriculum learning of HGNNs for\ndownstream tasks. Additionally, we propose a sampling strategy to alleviate\ntraining imbalance issues. Our method further demonstrate the efficacy of\ncurriculum learning in enhancing HGNNs capabilities. We call our method\nLoss-decrease-aware Heterogeneous Graph Neural Networks (LDHGNN). The code is\npublic at https://github.com/wangyili00/LDHGNN.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.06522v1.pdf",
        "similarity": 0.37746990503451316,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-10"
    },
    {
        "new_title": "Enhancing the Utility of Privacy-Preserving Cancer Classification using\n  Synthetic Data",
        "new_link": "http://arxiv.org/abs/2407.12669v1",
        "new_summary": "  Deep learning holds immense promise for aiding radiologists in breast cancer\ndetection. However, achieving optimal model performance is hampered by\nlimitations in availability and sharing of data commonly associated to patient\nprivacy concerns. Such concerns are further exacerbated, as traditional deep\nlearning models can inadvertently leak sensitive training information. This\nwork addresses these challenges exploring and quantifying the utility of\nprivacy-preserving deep learning techniques, concretely, (i) differentially\nprivate stochastic gradient descent (DP-SGD) and (ii) fully synthetic training\ndata generated by our proposed malignancy-conditioned generative adversarial\nnetwork. We assess these methods via downstream malignancy classification of\nmammography masses using a transformer model. Our experimental results depict\nthat synthetic data augmentation can improve privacy-utility tradeoffs in\ndifferentially private model training. Further, model pretraining on synthetic\ndata achieves remarkable performance, which can be further increased with\nDP-SGD fine-tuning across all privacy guarantees. With this first in-depth\nexploration of privacy-preserving deep learning in breast imaging, we address\ncurrent and emerging clinical privacy requirements and pave the way towards the\nadoption of private high-utility deep diagnostic models. Our reproducible\ncodebase is publicly available at https://github.com/RichardObi/mammo_dp.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.12669v1.pdf",
        "similarity": 0.37734125637459764,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-17"
    },
    {
        "new_title": "When Representations Align: Universality in Representation Learning\n  Dynamics",
        "new_link": "http://arxiv.org/abs/2402.09142v2",
        "new_summary": "  Deep neural networks come in many sizes and architectures. The choice of\narchitecture, in conjunction with the dataset and learning algorithm, is\ncommonly understood to affect the learned neural representations. Yet, recent\nresults have shown that different architectures learn representations with\nstriking qualitative similarities. Here we derive an effective theory of\nrepresentation learning under the assumption that the encoding map from input\nto hidden representation and the decoding map from representation to output are\narbitrary smooth functions. This theory schematizes representation learning\ndynamics in the regime of complex, large architectures, where hidden\nrepresentations are not strongly constrained by the parametrization. We show\nthrough experiments that the effective theory describes aspects of\nrepresentation learning dynamics across a range of deep networks with different\nactivation functions and architectures, and exhibits phenomena similar to the\n\"rich\" and \"lazy\" regime. While many network behaviors depend quantitatively on\narchitecture, our findings point to certain behaviors that are widely conserved\nonce models are sufficiently flexible.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.09142v2.pdf",
        "similarity": 0.3769184825480815,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-14"
    },
    {
        "new_title": "S-CycleGAN: Semantic Segmentation Enhanced CT-Ultrasound Image-to-Image\n  Translation for Robotic Ultrasonography",
        "new_link": "http://arxiv.org/abs/2406.01191v1",
        "new_summary": "  Ultrasound imaging is pivotal in various medical diagnoses due to its\nnon-invasive nature and safety. In clinical practice, the accuracy and\nprecision of ultrasound image analysis are critical. Recent advancements in\ndeep learning are showing great capacity of processing medical images. However,\nthe data hungry nature of deep learning and the shortage of high-quality\nultrasound image training data suppress the development of deep learning based\nultrasound analysis methods. To address these challenges, we introduce an\nadvanced deep learning model, dubbed S-CycleGAN, which generates high-quality\nsynthetic ultrasound images from computed tomography (CT) data. This model\nincorporates semantic discriminators within a CycleGAN framework to ensure that\ncritical anatomical details are preserved during the style transfer process.\nThe synthetic images produced are used to augment training datasets for\nsemantic segmentation models and robot-assisted ultrasound scanning system\ndevelopment, enhancing their ability to accurately parse real ultrasound\nimagery.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.01191v1.pdf",
        "similarity": 0.3765294746989305,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-03"
    },
    {
        "new_title": "A Question-centric Multi-experts Contrastive Learning Framework for\n  Improving the Accuracy and Interpretability of Deep Sequential Knowledge\n  Tracing Models",
        "new_link": "http://arxiv.org/abs/2403.07322v3",
        "new_summary": "  Knowledge tracing (KT) plays a crucial role in predicting students' future\nperformance by analyzing their historical learning processes. Deep neural\nnetworks (DNNs) have shown great potential in solving the KT problem. However,\nthere still exist some important challenges when applying deep learning\ntechniques to model the KT process. The first challenge lies in taking the\nindividual information of the question into modeling. This is crucial because,\ndespite questions sharing the same knowledge component (KC), students'\nknowledge acquisition on homogeneous questions can vary significantly. The\nsecond challenge lies in interpreting the prediction results from existing deep\nlearning-based KT models. In real-world applications, while it may not be\nnecessary to have complete transparency and interpretability of the model\nparameters, it is crucial to present the model's prediction results in a manner\nthat teachers find interpretable. This makes teachers accept the rationale\nbehind the prediction results and utilize them to design teaching activities\nand tailored learning strategies for students. However, the inherent black-box\nnature of deep learning techniques often poses a hurdle for teachers to fully\nembrace the model's prediction results. To address these challenges, we propose\na Question-centric Multi-experts Contrastive Learning framework for KT called\nQ-MCKT. We have provided all the datasets and code on our website at\nhttps://github.com/rattlesnakey/Q-MCKT.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.07322v3.pdf",
        "similarity": 0.37646080743063837,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-12"
    },
    {
        "new_title": "A look under the hood of the Interactive Deep Learning Enterprise\n  (No-IDLE)",
        "new_link": "http://arxiv.org/abs/2406.19054v1",
        "new_summary": "  This DFKI technical report presents the anatomy of the No-IDLE prototype\nsystem (funded by the German Federal Ministry of Education and Research) that\nprovides not only basic and fundamental research in interactive machine\nlearning, but also reveals deeper insights into users' behaviours, needs, and\ngoals. Machine learning and deep learning should become accessible to millions\nof end users. No-IDLE's goals and scienfific challenges centre around the\ndesire to increase the reach of interactive deep learning solutions for\nnon-experts in machine learning. One of the key innovations described in this\ntechnical report is a methodology for interactive machine learning combined\nwith multimodal interaction which will become central when we start interacting\nwith semi-intelligent machines in the upcoming area of neural networks and\nlarge language models.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.19054v1.pdf",
        "similarity": 0.3761379371716988,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-27"
    },
    {
        "new_title": "Federated Learning for Face Recognition via Intra-subject\n  Self-supervised Learning",
        "new_link": "http://arxiv.org/abs/2407.16289v1",
        "new_summary": "  Federated Learning (FL) for face recognition aggregates locally optimized\nmodels from individual clients to construct a generalized face recognition\nmodel. However, previous studies present two major challenges: insufficient\nincorporation of self-supervised learning and the necessity for clients to\naccommodate multiple subjects. To tackle these limitations, we propose FedFS\n(Federated Learning for personalized Face recognition via intra-subject\nSelf-supervised learning framework), a novel federated learning architecture\ntailored to train personalized face recognition models without imposing\nsubjects. Our proposed FedFS comprises two crucial components that leverage\naggregated features of the local and global models to cooperate with\nrepresentations of an off-the-shelf model. These components are (1) adaptive\nsoft label construction, utilizing dot product operations to reformat labels\nwithin intra-instances, and (2) intra-subject self-supervised learning,\nemploying cosine similarity operations to strengthen robust intra-subject\nrepresentations. Additionally, we introduce a regularization loss to prevent\noverfitting and ensure the stability of the optimized model. To assess the\neffectiveness of FedFS, we conduct comprehensive experiments on the DigiFace-1M\nand VGGFace datasets, demonstrating superior performance compared to previous\nmethods.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.16289v1.pdf",
        "similarity": 0.3759493556777176,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-23"
    },
    {
        "new_title": "The Paradox of Motion: Evidence for Spurious Correlations in\n  Skeleton-based Gait Recognition Models",
        "new_link": "http://arxiv.org/abs/2402.08320v1",
        "new_summary": "  Gait, an unobtrusive biometric, is valued for its capability to identify\nindividuals at a distance, across external outfits and environmental\nconditions. This study challenges the prevailing assumption that vision-based\ngait recognition, in particular skeleton-based gait recognition, relies\nprimarily on motion patterns, revealing a significant role of the implicit\nanthropometric information encoded in the walking sequence. We show through a\ncomparative analysis that removing height information leads to notable\nperformance degradation across three models and two benchmarks (CASIA-B and\nGREW). Furthermore, we propose a spatial transformer model processing\nindividual poses, disregarding any temporal information, which achieves\nunreasonably good accuracy, emphasizing the bias towards appearance information\nand indicating spurious correlations in existing benchmarks. These findings\nunderscore the need for a nuanced understanding of the interplay between motion\nand appearance in vision-based gait recognition, prompting a reevaluation of\nthe methodological assumptions in this field. Our experiments indicate that\n\"in-the-wild\" datasets are less prone to spurious correlations, prompting the\nneed for more diverse and large scale datasets for advancing the field.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.08320v1.pdf",
        "similarity": 0.3758315693709502,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-13"
    },
    {
        "new_title": "The Cram Method for Efficient Simultaneous Learning and Evaluation",
        "new_link": "http://arxiv.org/abs/2403.07031v1",
        "new_summary": "  We introduce the \"cram\" method, a general and efficient approach to\nsimultaneous learning and evaluation using a generic machine learning (ML)\nalgorithm. In a single pass of batched data, the proposed method repeatedly\ntrains an ML algorithm and tests its empirical performance. Because it utilizes\nthe entire sample for both learning and evaluation, cramming is significantly\nmore data-efficient than sample-splitting. The cram method also naturally\naccommodates online learning algorithms, making its implementation\ncomputationally efficient. To demonstrate the power of the cram method, we\nconsider the standard policy learning setting where cramming is applied to the\nsame data to both develop an individualized treatment rule (ITR) and estimate\nthe average outcome that would result if the learned ITR were to be deployed.\nWe show that under a minimal set of assumptions, the resulting crammed\nevaluation estimator is consistent and asymptotically normal. While our\nasymptotic results require a relatively weak stabilization condition of ML\nalgorithm, we develop a simple, generic method that can be used with any policy\nlearning algorithm to satisfy this condition. Our extensive simulation studies\nshow that, when compared to sample-splitting, cramming reduces the evaluation\nstandard error by more than 40% while improving the performance of learned\npolicy. We also apply the cram method to a randomized clinical trial to\ndemonstrate its applicability to real-world problems. Finally, we briefly\ndiscuss future extensions of the cram method to other learning and evaluation\nsettings.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.07031v1.pdf",
        "similarity": 0.37568069647778785,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-11"
    },
    {
        "new_title": "Deep-ELA: Deep Exploratory Landscape Analysis with Self-Supervised\n  Pretrained Transformers for Single- and Multi-Objective Continuous\n  Optimization Problems",
        "new_link": "http://arxiv.org/abs/2401.01192v1",
        "new_summary": "  In many recent works, the potential of Exploratory Landscape Analysis (ELA)\nfeatures to numerically characterize, in particular, single-objective\ncontinuous optimization problems has been demonstrated. These numerical\nfeatures provide the input for all kinds of machine learning tasks on\ncontinuous optimization problems, ranging, i.a., from High-level Property\nPrediction to Automated Algorithm Selection and Automated Algorithm\nConfiguration. Without ELA features, analyzing and understanding the\ncharacteristics of single-objective continuous optimization problems would be\nimpossible.\n  Yet, despite their undisputed usefulness, ELA features suffer from several\ndrawbacks. These include, in particular, (1.) a strong correlation between\nmultiple features, as well as (2.) its very limited applicability to\nmulti-objective continuous optimization problems. As a remedy, recent works\nproposed deep learning-based approaches as alternatives to ELA. In these works,\ne.g., point-cloud transformers were used to characterize an optimization\nproblem's fitness landscape. However, these approaches require a large amount\nof labeled training data.\n  Within this work, we propose a hybrid approach, Deep-ELA, which combines (the\nbenefits of) deep learning and ELA features. Specifically, we pre-trained four\ntransformers on millions of randomly generated optimization problems to learn\ndeep representations of the landscapes of continuous single- and\nmulti-objective optimization problems. Our proposed framework can either be\nused out-of-the-box for analyzing single- and multi-objective continuous\noptimization problems, or subsequently fine-tuned to various tasks focussing on\nalgorithm behavior and problem understanding.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.01192v1.pdf",
        "similarity": 0.3753562686861253,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-02"
    },
    {
        "new_title": "Graph Reasoning Networks",
        "new_link": "http://arxiv.org/abs/2407.05816v1",
        "new_summary": "  Graph neural networks (GNNs) are the predominant approach for graph-based\nmachine learning. While neural networks have shown great performance at\nlearning useful representations, they are often criticized for their limited\nhigh-level reasoning abilities. In this work, we present Graph Reasoning\nNetworks (GRNs), a novel approach to combine the strengths of fixed and learned\ngraph representations and a reasoning module based on a differentiable\nsatisfiability solver. While results on real-world datasets show comparable\nperformance to GNN, experiments on synthetic datasets demonstrate the potential\nof the newly proposed method.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.05816v1.pdf",
        "similarity": 0.37523081046434964,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-08"
    },
    {
        "new_title": "Deep Learning-based Point Cloud Registration for Augmented\n  Reality-guided Surgery",
        "new_link": "http://arxiv.org/abs/2405.03314v1",
        "new_summary": "  Point cloud registration aligns 3D point clouds using spatial\ntransformations. It is an important task in computer vision, with applications\nin areas such as augmented reality (AR) and medical imaging. This work explores\nthe intersection of two research trends: the integration of AR into\nimage-guided surgery and the use of deep learning for point cloud registration.\nThe main objective is to evaluate the feasibility of applying deep\nlearning-based point cloud registration methods for image-to-patient\nregistration in augmented reality-guided surgery. We created a dataset of point\nclouds from medical imaging and corresponding point clouds captured with a\npopular AR device, the HoloLens 2. We evaluate three well-established deep\nlearning models in registering these data pairs. While we find that some deep\nlearning methods show promise, we show that a conventional registration\npipeline still outperforms them on our challenging dataset.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.03314v1.pdf",
        "similarity": 0.3751883410289775,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-06"
    },
    {
        "new_title": "A Survey of Learned Indexes for the Multi-dimensional Space",
        "new_link": "http://arxiv.org/abs/2403.06456v1",
        "new_summary": "  A recent research trend involves treating database index structures as\nMachine Learning (ML) models. In this domain, single or multiple ML models are\ntrained to learn the mapping from keys to positions inside a data set. This\nclass of indexes is known as \"Learned Indexes.\" Learned indexes have\ndemonstrated improved search performance and reduced space requirements for\none-dimensional data. The concept of one-dimensional learned indexes has\nnaturally been extended to multi-dimensional (e.g., spatial) data, leading to\nthe development of \"Learned Multi-dimensional Indexes\". This survey focuses on\nlearned multi-dimensional index structures. Specifically, it reviews the\ncurrent state of this research area, explains the core concepts behind each\nproposed method, and classifies these methods based on several well-defined\ncriteria. We present a taxonomy that classifies and categorizes each learned\nmulti-dimensional index, and survey the existing literature on learned\nmulti-dimensional indexes according to this taxonomy. Additionally, we present\na timeline to illustrate the evolution of research on learned indexes. Finally,\nwe highlight several open challenges and future research directions in this\nemerging and highly active field.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.06456v1.pdf",
        "similarity": 0.37490151112072245,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-11"
    },
    {
        "new_title": "Comparative Analysis on Snowmelt-Driven Streamflow Forecasting Using\n  Machine Learning Techniques",
        "new_link": "http://arxiv.org/abs/2404.13327v2",
        "new_summary": "  The rapid advancement of machine learning techniques has led to their\nwidespread application in various domains including water resources. However,\nsnowmelt modeling remains an area that has not been extensively explored. In\nthis study, we propose a state-of-the-art (SOTA) deep learning sequential\nmodel, leveraging the Temporal Convolutional Network (TCN), for snowmelt-driven\ndischarge modeling in the Himalayan basin of the Hindu Kush Himalayan Region.\nTo evaluate the performance of our proposed model, we conducted a comparative\nanalysis with other popular models including Support Vector Regression (SVR),\nLong Short Term Memory (LSTM), and Transformer. Furthermore, Nested\ncross-validation (CV) is used with five outer folds and three inner folds, and\nhyper-parameter tuning is performed on the inner folds. To evaluate the\nperformance of the model mean absolute error (MAE), root mean square error\n(RMSE), R square ($R^{2}$), Kling-Gupta Efficiency (KGE), and Nash-Sutcliffe\nEfficiency (NSE) are computed for each outer fold. The average metrics revealed\nthat TCN outperformed the other models, with an average MAE of 0.011, RMSE of\n0.023, $R^{2}$ of 0.991, KGE of 0.992, and NSE of 0.991. The findings of this\nstudy demonstrate the effectiveness of the deep learning model as compared to\ntraditional machine learning approaches for snowmelt-driven streamflow\nforecasting. Moreover, the superior performance of TCN highlights its potential\nas a promising deep learning model for similar hydrological applications.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.13327v2.pdf",
        "similarity": 0.3743467492699061,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-20"
    },
    {
        "new_title": "The Physics of Learning: From Autoencoders to Truly Autonomous Learning\n  Machines",
        "new_link": "http://arxiv.org/abs/2407.04700v1",
        "new_summary": "  The fact that accurately predicted information can serve as an energy source\npaves the way for new approaches to autonomous learning. The energy derived\nfrom a sequence of successful predictions can be recycled as an immediate\nincentive and resource, driving the enhancement of predictive capabilities in\nAI agents. We propose that, through a series of straightforward\nmeta-architectural adjustments, any unsupervised learning apparatus could\nachieve complete independence from external energy sources, evolving into a\nself-sustaining physical system with a strong intrinsic 'drive' for continual\nlearning. This concept, while still purely theoretical, is exemplified through\nthe autoencoder, a quintessential model for unsupervised efficient coding. We\nuse this model to demonstrate how progressive paradigm shifts can profoundly\nalter our comprehension of learning and intelligence. By reconceptualizing\nlearning as an energy-seeking process, we highlight the potential for achieving\ntrue autonomy in learning systems, thereby bridging the gap between algorithmic\nconcepts and physical models of intelligence.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.04700v1.pdf",
        "similarity": 0.3737514036995989,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-12"
    },
    {
        "new_title": "A Deep Learning Approach to Diabetes Diagnosis",
        "new_link": "http://arxiv.org/abs/2403.07483v1",
        "new_summary": "  Diabetes, resulting from inadequate insulin production or utilization, causes\nextensive harm to the body. Existing diagnostic methods are often invasive and\ncome with drawbacks, such as cost constraints. Although there are machine\nlearning models like Classwise k Nearest Neighbor (CkNN) and General Regression\nNeural Network (GRNN), they struggle with imbalanced data and result in\nunder-performance. Leveraging advancements in sensor technology and machine\nlearning, we propose a non-invasive diabetes diagnosis using a Back Propagation\nNeural Network (BPNN) with batch normalization, incorporating data re-sampling\nand normalization for class balancing. Our method addresses existing challenges\nsuch as limited performance associated with traditional machine learning.\nExperimental results on three datasets show significant improvements in overall\naccuracy, sensitivity, and specificity compared to traditional methods.\nNotably, we achieve accuracies of 89.81% in Pima diabetes dataset, 75.49% in\nCDC BRFSS2015 dataset, and 95.28% in Mesra Diabetes dataset. This underscores\nthe potential of deep learning models for robust diabetes diagnosis. See\nproject website https://steve-zeyu-zhang.github.io/DiabetesDiagnosis/\n",
        "pdf_link": "https://arxiv.org/pdf/2403.07483v1.pdf",
        "similarity": 0.3735921737806861,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-12"
    },
    {
        "new_title": "Data-Centric Human Preference Optimization with Rationales",
        "new_link": "http://arxiv.org/abs/2407.14477v2",
        "new_summary": "  Reinforcement learning from human feedback plays a crucial role in aligning\nlanguage models towards human preferences, traditionally represented through\ncomparisons between pairs or sets of responses within a given context. While\nmany studies have enhanced algorithmic techniques to optimize learning from\nsuch data, this work shifts focus to improving preference learning through a\ndata-centric approach. Specifically, we propose enriching existing preference\ndatasets with machine-generated rationales that explain the reasons behind\nchoices. We develop a simple and principled framework to augment current\npreference learning methods with rationale information. Our comprehensive\nanalysis highlights how rationales enhance learning efficiency. Extensive\nexperiments reveal that rationale-enriched preference learning offers multiple\nadvantages: it improves data efficiency, accelerates convergence to\nhigher-performing models, and reduces verbosity bias and hallucination.\nFurthermore, this framework is versatile enough to integrate with various\npreference optimization algorithms. Overall, our findings highlight the\npotential of re-imagining data design for preference learning, demonstrating\nthat even freely available machine-generated rationales can significantly boost\nperformance across multiple dimensions. The code repository is available at\nhttps: //github.com/reds-lab/preference-learning-with-rationales\n",
        "pdf_link": "https://arxiv.org/pdf/2407.14477v2.pdf",
        "similarity": 0.3733726515635597,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-19"
    },
    {
        "new_title": "The lazy (NTK) and rich ($\u03bc$P) regimes: a gentle tutorial",
        "new_link": "http://arxiv.org/abs/2404.19719v1",
        "new_summary": "  A central theme of the modern machine learning paradigm is that larger neural\nnetworks achieve better performance on a variety of metrics. Theoretical\nanalyses of these overparameterized models have recently centered around\nstudying very wide neural networks. In this tutorial, we provide a nonrigorous\nbut illustrative derivation of the following fact: in order to train wide\nnetworks effectively, there is only one degree of freedom in choosing\nhyperparameters such as the learning rate and the size of the initial weights.\nThis degree of freedom controls the richness of training behavior: at minimum,\nthe wide network trains lazily like a kernel machine, and at maximum, it\nexhibits feature learning in the so-called $\\mu$P regime. In this paper, we\nexplain this richness scale, synthesize recent research results into a coherent\nwhole, offer new perspectives and intuitions, and provide empirical evidence\nsupporting our claims. In doing so, we hope to encourage further study of the\nrichness scale, as it may be key to developing a scientific theory of feature\nlearning in practical deep neural networks.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.19719v1.pdf",
        "similarity": 0.37332739100067114,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-30"
    },
    {
        "new_title": "Self-Adaptive Robust Motion Planning for High DoF Robot Manipulator\n  using Deep MPC",
        "new_link": "http://arxiv.org/abs/2407.12887v1",
        "new_summary": "  In contemporary control theory, self-adaptive methodologies are highly\nesteemed for their inherent flexibility and robustness in managing modeling\nuncertainties. Particularly, robust adaptive control stands out owing to its\npotent capability of leveraging robust optimization algorithms to approximate\ncost functions and relax the stringent constraints often associated with\nconventional self-adaptive control paradigms. Deep learning methods,\ncharacterized by their extensive layered architecture, offer significantly\nenhanced approximation prowess. Notwithstanding, the implementation of deep\nlearning is replete with challenges, particularly the phenomena of vanishing\nand exploding gradients encountered during the training process. This paper\nintroduces a self-adaptive control scheme integrating a deep MPC, governed by\nan innovative weight update law designed to mitigate the vanishing and\nexploding gradient predicament by employing the gradient sign exclusively. The\nproffered controller is a self-adaptive dynamic inversion mechanism,\nintegrating an augmented state observer within an auxiliary estimation circuit\nto enhance the training phase. This approach enables the deep MPC to learn the\nentire plant model in real-time and the efficacy of the controller is\ndemonstrated through simulations involving a high-DoF robot manipulator,\nwherein the controller adeptly learns the nonlinear plant dynamics\nexpeditiously and exhibits commendable performance in the motion planning task.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.12887v1.pdf",
        "similarity": 0.3732983576998851,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-17"
    },
    {
        "new_title": "Hallucination Benchmark in Medical Visual Question Answering",
        "new_link": "http://arxiv.org/abs/2401.05827v2",
        "new_summary": "  The recent success of large language and vision models (LLVMs) on vision\nquestion answering (VQA), particularly their applications in medicine\n(Med-VQA), has shown a great potential of realizing effective visual assistants\nfor healthcare. However, these models are not extensively tested on the\nhallucination phenomenon in clinical settings. Here, we created a hallucination\nbenchmark of medical images paired with question-answer sets and conducted a\ncomprehensive evaluation of the state-of-the-art models. The study provides an\nin-depth analysis of current models' limitations and reveals the effectiveness\nof various prompting strategies.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.05827v2.pdf",
        "similarity": 0.3732193275551977,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-11"
    },
    {
        "new_title": "What is Wrong with End-to-End Learning for Phase Retrieval?",
        "new_link": "http://arxiv.org/abs/2403.15448v1",
        "new_summary": "  For nonlinear inverse problems that are prevalent in imaging science,\nsymmetries in the forward model are common. When data-driven deep learning\napproaches are used to solve such problems, these intrinsic symmetries can\ncause substantial learning difficulties. In this paper, we explain how such\ndifficulties arise and, more importantly, how to overcome them by preprocessing\nthe training set before any learning, i.e., symmetry breaking. We take\nfar-field phase retrieval (FFPR), which is central to many areas of scientific\nimaging, as an example and show that symmetric breaking can substantially\nimprove data-driven learning. We also formulate the mathematical principle of\nsymmetry breaking.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.15448v1.pdf",
        "similarity": 0.3730438716657747,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-18"
    },
    {
        "new_title": "SpikingResformer: Bridging ResNet and Vision Transformer in Spiking\n  Neural Networks",
        "new_link": "http://arxiv.org/abs/2403.14302v2",
        "new_summary": "  The remarkable success of Vision Transformers in Artificial Neural Networks\n(ANNs) has led to a growing interest in incorporating the self-attention\nmechanism and transformer-based architecture into Spiking Neural Networks\n(SNNs). While existing methods propose spiking self-attention mechanisms that\nare compatible with SNNs, they lack reasonable scaling methods, and the overall\narchitectures proposed by these methods suffer from a bottleneck in effectively\nextracting local features. To address these challenges, we propose a novel\nspiking self-attention mechanism named Dual Spike Self-Attention (DSSA) with a\nreasonable scaling method. Based on DSSA, we propose a novel spiking Vision\nTransformer architecture called SpikingResformer, which combines the\nResNet-based multi-stage architecture with our proposed DSSA to improve both\nperformance and energy efficiency while reducing parameters. Experimental\nresults show that SpikingResformer achieves higher accuracy with fewer\nparameters and lower energy consumption than other spiking Vision Transformer\ncounterparts. Notably, our SpikingResformer-L achieves 79.40% top-1 accuracy on\nImageNet with 4 time-steps, which is the state-of-the-art result in the SNN\nfield.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.14302v2.pdf",
        "similarity": 0.3730032388746227,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-21"
    },
    {
        "new_title": "Improving Visual Perception of a Social Robot for Controlled and\n  In-the-wild Human-robot Interaction",
        "new_link": "http://arxiv.org/abs/2403.01766v2",
        "new_summary": "  Social robots often rely on visual perception to understand their users and\nthe environment. Recent advancements in data-driven approaches for computer\nvision have demonstrated great potentials for applying deep-learning models to\nenhance a social robot's visual perception. However, the high computational\ndemands of deep-learning methods, as opposed to the more resource-efficient\nshallow-learning models, bring up important questions regarding their effects\non real-world interaction and user experience. It is unclear how will the\nobjective interaction performance and subjective user experience be influenced\nwhen a social robot adopts a deep-learning based visual perception model. We\nemployed state-of-the-art human perception and tracking models to improve the\nvisual perception function of the Pepper robot and conducted a controlled lab\nstudy and an in-the-wild human-robot interaction study to evaluate this novel\nperception function for following a specific user with other people present in\nthe scene.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.01766v2.pdf",
        "similarity": 0.37259785317064914,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-04"
    },
    {
        "new_title": "DUCPS: Deep Unfolding the Cauchy Proximal Splitting Algorithm for\n  B-Lines Quantification in Lung Ultrasound Images",
        "new_link": "http://arxiv.org/abs/2407.10667v2",
        "new_summary": "  The identification of artefacts, particularly B-lines, in lung ultrasound\n(LUS), is crucial for assisting clinical diagnosis, prompting the development\nof innovative methodologies. While the Cauchy proximal splitting (CPS)\nalgorithm has demonstrated effective performance in B-line detection, the\nprocess is slow and has limited generalization. This paper addresses these\nissues with a novel unsupervised deep unfolding network structure (DUCPS). The\nframework utilizes deep unfolding procedures to merge traditional model-based\ntechniques with deep learning approaches. By unfolding the CPS algorithm into a\ndeep network, DUCPS enables the parameters in the optimization algorithm to be\nlearnable, thus enhancing generalization performance and facilitating rapid\nconvergence. We conducted entirely unsupervised training using the\nNeighbor2Neighbor (N2N) and the Structural Similarity Index Measure (SSIM)\nlosses. When combined with an improved line identification method proposed in\nthis paper, state-of-the-art performance is achieved, with the recall and F2\nscore reaching 0.70 and 0.64, respectively. Notably, DUCPS significantly\nimproves computational efficiency eliminating the need for extensive data\nlabeling, representing a notable advancement over both traditional algorithms\nand existing deep learning approaches.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.10667v2.pdf",
        "similarity": 0.37207302075270704,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-15"
    },
    {
        "new_title": "Application of Deep Learning Methods to Processing of Noisy Medical\n  Video Data",
        "new_link": "http://arxiv.org/abs/2404.10319v1",
        "new_summary": "  Cells count become a challenging problem when the cells move in a continuous\nstream, and their boundaries are difficult for visual detection. To resolve\nthis problem we modified the training and decision making processes using\ncurriculum learning and multi-view predictions techniques, respectively.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.10319v1.pdf",
        "similarity": 0.3717951619280974,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-16"
    },
    {
        "new_title": "Deep learning for 3D human pose estimation and mesh recovery: A survey",
        "new_link": "http://arxiv.org/abs/2402.18844v2",
        "new_summary": "  3D human pose estimation and mesh recovery have attracted widespread research\ninterest in many areas, such as computer vision, autonomous driving, and\nrobotics. Deep learning on 3D human pose estimation and mesh recovery has\nrecently thrived, with numerous methods proposed to address different problems\nin this area. In this paper, to stimulate future research, we present a\ncomprehensive review of recent progress over the past five years in deep\nlearning methods for this area by delving into over 200 references. To the best\nof our knowledge, this survey is arguably the first to comprehensively cover\ndeep learning methods for 3D human pose estimation, including both\nsingle-person and multi-person approaches, as well as human mesh recovery,\nencompassing methods based on explicit models and implicit representations. We\nalso present comparative results on several publicly available datasets,\ntogether with insightful observations and inspiring future research directions.\nA regularly updated project page can be found at\nhttps://github.com/liuyangme/SOTA-3DHPE-HMR.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.18844v2.pdf",
        "similarity": 0.3715376037669545,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-29"
    },
    {
        "new_title": "A Large Dimensional Analysis of Multi-task Semi-Supervised Learning",
        "new_link": "http://arxiv.org/abs/2402.13646v1",
        "new_summary": "  This article conducts a large dimensional study of a simple yet quite\nversatile classification model, encompassing at once multi-task and\nsemi-supervised learning, and taking into account uncertain labeling. Using\ntools from random matrix theory, we characterize the asymptotics of some key\nfunctionals, which allows us on the one hand to predict the performances of the\nalgorithm, and on the other hand to reveal some counter-intuitive guidance on\nhow to use it efficiently. The model, powerful enough to provide good\nperformance guarantees, is also straightforward enough to provide strong\ninsights into its behavior.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.13646v1.pdf",
        "similarity": 0.37146808452808633,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-21"
    },
    {
        "new_title": "Investigating Generalization Behaviours of Generative Flow Networks",
        "new_link": "http://arxiv.org/abs/2402.05309v1",
        "new_summary": "  Generative Flow Networks (GFlowNets, GFNs) are a generative framework for\nlearning unnormalized probability mass functions over discrete spaces. Since\ntheir inception, GFlowNets have proven to be useful for learning generative\nmodels in applications where the majority of the discrete space is unvisited\nduring training. This has inspired some to hypothesize that GFlowNets, when\npaired with deep neural networks (DNNs), have favourable generalization\nproperties. In this work, we empirically verify some of the hypothesized\nmechanisms of generalization of GFlowNets. In particular, we find that the\nfunctions that GFlowNets learn to approximate have an implicit underlying\nstructure which facilitate generalization. We also find that GFlowNets are\nsensitive to being trained offline and off-policy; however, the reward\nimplicitly learned by GFlowNets is robust to changes in the training\ndistribution.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.05309v1.pdf",
        "similarity": 0.37121316450844904,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-07"
    },
    {
        "new_title": "Can Tree Based Approaches Surpass Deep Learning in Anomaly Detection? A\n  Benchmarking Study",
        "new_link": "http://arxiv.org/abs/2402.07281v2",
        "new_summary": "  Detection of anomalous situations for complex mission-critical systems holds\nparamount importance when their service continuity needs to be ensured. A major\nchallenge in detecting anomalies from the operational data arises due to the\nimbalanced class distribution problem since the anomalies are supposed to be\nrare events. This paper evaluates a diverse array of machine learning-based\nanomaly detection algorithms through a comprehensive benchmark study. The paper\ncontributes significantly by conducting an unbiased comparison of various\nanomaly detection algorithms, spanning classical machine learning including\nvarious tree-based approaches to deep learning and outlier detection methods.\nThe inclusion of 104 publicly available and a few proprietary industrial\nsystems datasets enhances the diversity of the study, allowing for a more\nrealistic evaluation of algorithm performance and emphasizing the importance of\nadaptability to real-world scenarios. The paper dispels the deep learning myth,\ndemonstrating that though powerful, deep learning is not a universal solution\nin this case. We observed that recently proposed tree-based evolutionary\nalgorithms outperform in many scenarios. We noticed that tree-based approaches\ncatch a singleton anomaly in a dataset where deep learning methods fail. On the\nother hand, classical SVM performs the best on datasets with more than 10%\nanomalies, implying that such scenarios can be best modeled as a classification\nproblem rather than anomaly detection. To our knowledge, such a study on a\nlarge number of state-of-the-art algorithms using diverse data sets, with the\nobjective of guiding researchers and practitioners in making informed\nalgorithmic choices, has not been attempted earlier.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.07281v2.pdf",
        "similarity": 0.37117750335391086,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-11"
    },
    {
        "new_title": "EvGNN: An Event-driven Graph Neural Network Accelerator for Edge Vision",
        "new_link": "http://arxiv.org/abs/2404.19489v1",
        "new_summary": "  Edge vision systems combining sensing and embedded processing promise\nlow-latency, decentralized, and energy-efficient solutions that forgo reliance\non the cloud. As opposed to conventional frame-based vision sensors,\nevent-based cameras deliver a microsecond-scale temporal resolution with sparse\ninformation encoding, thereby outlining new opportunities for edge vision\nsystems. However, mainstream algorithms for frame-based vision, which mostly\nrely on convolutional neural networks (CNNs), can hardly exploit the advantages\nof event-based vision as they are typically optimized for dense matrix-vector\nmultiplications. While event-driven graph neural networks (GNNs) have recently\nemerged as a promising solution for sparse event-based vision, their irregular\nstructure is a challenge that currently hinders the design of efficient\nhardware accelerators. In this paper, we propose EvGNN, the first event-driven\nGNN accelerator for low-footprint, ultra-low-latency, and high-accuracy edge\nvision with event-based cameras. It relies on three central ideas: (i) directed\ndynamic graphs exploiting single-hop nodes with edge-free storage, (ii) event\nqueues for the efficient identification of local neighbors within a\nspatiotemporally decoupled search range, and (iii) a novel layer-parallel\nprocessing scheme enabling the low-latency execution of multi-layer GNNs. We\ndeployed EvGNN on a Xilinx KV260 Ultrascale+ MPSoC platform and benchmarked it\non the N-CARS dataset for car recognition, demonstrating a classification\naccuracy of 87.8% and an average latency per event of 16$\\mu$s, thereby\nenabling real-time, microsecond-resolution event-based vision at the edge.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.19489v1.pdf",
        "similarity": 0.3700808314846179,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-30"
    },
    {
        "new_title": "Transfer Learning from Whisper for Microscopic Intelligibility\n  Prediction",
        "new_link": "http://arxiv.org/abs/2404.01737v1",
        "new_summary": "  Macroscopic intelligibility models predict the expected human word-error-rate\nfor a given speech-in-noise stimulus. In contrast, microscopic intelligibility\nmodels aim to make fine-grained predictions about listeners' perception, e.g.\npredicting phonetic or lexical responses. State-of-the-art macroscopic models\nuse transfer learning from large scale deep learning models for speech\nprocessing, whereas such methods have rarely been used for microscopic\nmodeling. In this paper, we study the use of transfer learning from Whisper, a\nstate-of-the-art deep learning model for automatic speech recognition, for\nmicroscopic intelligibility prediction at the level of lexical responses. Our\nmethod outperforms the considered baselines, even in a zero-shot setup, and\nyields a relative improvement of up to 66\\% when fine-tuned to predict\nlisteners' responses. Our results showcase the promise of large scale deep\nlearning based methods for microscopic intelligibility prediction.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.01737v1.pdf",
        "similarity": 0.36950255962180084,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-02"
    },
    {
        "new_title": "Towards Leveraging AutoML for Sustainable Deep Learning: A\n  Multi-Objective HPO Approach on Deep Shift Neural Networks",
        "new_link": "http://arxiv.org/abs/2404.01965v2",
        "new_summary": "  Deep Learning (DL) has advanced various fields by extracting complex patterns\nfrom large datasets. However, the computational demands of DL models pose\nenvironmental and resource challenges. Deep shift neural networks (DSNNs) offer\na solution by leveraging shift operations to reduce computational complexity at\ninference. Following the insights from standard DNNs, we are interested in\nleveraging the full potential of DSNNs by means of AutoML techniques. We study\nthe impact of hyperparameter optimization (HPO) to maximize DSNN performance\nwhile minimizing resource consumption. Since this combines multi-objective (MO)\noptimization with accuracy and energy consumption as potentially complementary\nobjectives, we propose to combine state-of-the-art multi-fidelity (MF) HPO with\nmulti-objective optimization. Experimental results demonstrate the\neffectiveness of our approach, resulting in models with over 80\\% in accuracy\nand low computational cost. Overall, our method accelerates efficient model\ndevelopment while enabling sustainable AI applications.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.01965v2.pdf",
        "similarity": 0.3694773165518071,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-02"
    },
    {
        "new_title": "OAML: Outlier Aware Metric Learning for OOD Detection Enhancement",
        "new_link": "http://arxiv.org/abs/2406.16525v1",
        "new_summary": "  Out-of-distribution (OOD) detection methods have been developed to identify\nobjects that a model has not seen during training. The Outlier Exposure (OE)\nmethods use auxiliary datasets to train OOD detectors directly. However, the\ncollection and learning of representative OOD samples may pose challenges. To\ntackle these issues, we propose the Outlier Aware Metric Learning (OAML)\nframework. The main idea of our method is to use the k-NN algorithm and Stable\nDiffusion model to generate outliers for training at the feature level without\nmaking any distributional assumptions. To increase feature discrepancies in the\nsemantic space, we develop a mutual information-based contrastive learning\napproach for learning from OOD data effectively. Both theoretical and empirical\nresults confirm the effectiveness of this contrastive learning technique.\nFurthermore, we incorporate knowledge distillation into our learning framework\nto prevent degradation of in-distribution classification accuracy. The\ncombination of contrastive learning and knowledge distillation algorithms\nsignificantly enhances the performance of OOD detection. Experimental results\nacross various datasets show that our method significantly outperforms previous\nOE methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.16525v1.pdf",
        "similarity": 0.3694646062407827,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-24"
    },
    {
        "new_title": "Knowledge-Informed Machine Learning for Cancer Diagnosis and Prognosis:\n  A review",
        "new_link": "http://arxiv.org/abs/2401.06406v1",
        "new_summary": "  Cancer remains one of the most challenging diseases to treat in the medical\nfield. Machine learning has enabled in-depth analysis of rich multi-omics\nprofiles and medical imaging for cancer diagnosis and prognosis. Despite these\nadvancements, machine learning models face challenges stemming from limited\nlabeled sample sizes, the intricate interplay of high-dimensionality data\ntypes, the inherent heterogeneity observed among patients and within tumors,\nand concerns about interpretability and consistency with existing biomedical\nknowledge. One approach to surmount these challenges is to integrate biomedical\nknowledge into data-driven models, which has proven potential to improve the\naccuracy, robustness, and interpretability of model results. Here, we review\nthe state-of-the-art machine learning studies that adopted the fusion of\nbiomedical knowledge and data, termed knowledge-informed machine learning, for\ncancer diagnosis and prognosis. Emphasizing the properties inherent in four\nprimary data types including clinical, imaging, molecular, and treatment data,\nwe highlight modeling considerations relevant to these contexts. We provide an\noverview of diverse forms of knowledge representation and current strategies of\nknowledge integration into machine learning pipelines with concrete examples.\nWe conclude the review article by discussing future directions to advance\ncancer research through knowledge-informed machine learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.06406v1.pdf",
        "similarity": 0.3693662504759354,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-12"
    },
    {
        "new_title": "HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to\n  Detect Machine-Generated Text?",
        "new_link": "http://arxiv.org/abs/2402.11815v2",
        "new_summary": "  This paper describes our system developed for SemEval-2024 Task 8,\n``Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated\nText Detection'' Machine-generated texts have been one of the main concerns due\nto the use of large language models (LLM) in fake text generation, phishing,\ncheating in exams, or even plagiarizing copyright materials. A lot of systems\nhave been developed to detect machine-generated text. Nonetheless, the majority\nof these systems rely on the text-generating model. This limitation is\nimpractical in real-world scenarios, as it's often impossible to know which\nspecific model the user has used for text generation. In this work, we propose\na $\\textbf{single}$ model based on contrastive learning, which uses\n$\\textbf{$\\approx$40% of the baseline's parameters}$ (149M vs. 355M) but shows\na comparable performance on the test dataset $(\\textbf{21st out of 137\nparticipants})$. Our key finding is that even without an ensemble of multiple\nmodels, a single base model can have comparable performance with the help of\ndata augmentation and contrastive learning. Our code is publicly available at\nhttps://github.com/dipta007/SemEval24-Task8.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.11815v2.pdf",
        "similarity": 0.36916778600838235,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-19"
    },
    {
        "new_title": "Full-stack evaluation of Machine Learning inference workloads for RISC-V\n  systems",
        "new_link": "http://arxiv.org/abs/2405.15380v1",
        "new_summary": "  Architectural simulators hold a vital role in RISC-V research, providing a\ncrucial platform for workload evaluation without the need for costly physical\nprototypes. They serve as a dynamic environment for exploring innovative\narchitectural concepts, enabling swift iteration and thorough analysis of\nperformance metrics. As deep learning algorithms become increasingly pervasive,\nit is essential to benchmark new architectures with machine learning workloads.\nThe diverse computational kernels used in deep learning algorithms highlight\nthe necessity for a comprehensive compilation toolchain to map to target\nhardware platforms. This study evaluates the performance of a wide array of\nmachine learning workloads on RISC-V architectures using gem5, an open-source\narchitectural simulator. Leveraging an open-source compilation toolchain based\non Multi-Level Intermediate Representation (MLIR), the research presents\nbenchmarking results specifically focused on deep learning inference workloads.\nAdditionally, the study sheds light on current limitations of gem5 when\nsimulating RISC-V architectures, offering insights for future development and\nrefinement.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.15380v1.pdf",
        "similarity": 0.36841254966760273,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-24"
    },
    {
        "new_title": "Iterative Refinement Strategy for Automated Data Labeling: Facial\n  Landmark Diagnosis in Medical Imaging",
        "new_link": "http://arxiv.org/abs/2404.05348v1",
        "new_summary": "  Automated data labeling techniques are crucial for accelerating the\ndevelopment of deep learning models, particularly in complex medical imaging\napplications. However, ensuring accuracy and efficiency remains challenging.\nThis paper presents iterative refinement strategies for automated data labeling\nin facial landmark diagnosis to enhance accuracy and efficiency for deep\nlearning models in medical applications, including dermatology, plastic\nsurgery, and ophthalmology. Leveraging feedback mechanisms and advanced\nalgorithms, our approach iteratively refines initial labels, reducing reliance\non manual intervention while improving label quality. Through empirical\nevaluation and case studies, we demonstrate the effectiveness of our proposed\nstrategies in deep learning tasks across medical imaging domains. Our results\nhighlight the importance of iterative refinement in automated data labeling to\nenhance the capabilities of deep learning systems in medical imaging\napplications.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.05348v1.pdf",
        "similarity": 0.36833813012525435,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-08"
    },
    {
        "new_title": "A Deep Learning Approach for Brain Tumor Classification and Segmentation\n  Using a Multiscale Convolutional Neural Network",
        "new_link": "http://arxiv.org/abs/2402.05975v1",
        "new_summary": "  In this paper, we present a fully automatic brain tumor segmentation and\nclassification model using a Deep Convolutional Neural Network that includes a\nmultiscale approach. One of the differences of our proposal with respect to\nprevious works is that input images are processed in three spatial scales along\ndifferent processing pathways. This mechanism is inspired in the inherent\noperation of the Human Visual System. The proposed neural model can analyze MRI\nimages containing three types of tumors: meningioma, glioma, and pituitary\ntumor, over sagittal, coronal, and axial views and does not need preprocessing\nof input images to remove skull or vertebral column parts in advance. The\nperformance of our method on a publicly available MRI image dataset of 3064\nslices from 233 patients is compared with previously classical machine learning\nand deep learning published methods. In the comparison, our method remarkably\nobtained a tumor classification accuracy of 0.973, higher than the other\napproaches using the same database.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.05975v1.pdf",
        "similarity": 0.3678664741439771,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-04"
    },
    {
        "new_title": "Interpreting Adaptive Gradient Methods by Parameter Scaling for\n  Learning-Rate-Free Optimization",
        "new_link": "http://arxiv.org/abs/2401.03240v1",
        "new_summary": "  We address the challenge of estimating the learning rate for adaptive\ngradient methods used in training deep neural networks. While several\nlearning-rate-free approaches have been proposed, they are typically tailored\nfor steepest descent. However, although steepest descent methods offer an\nintuitive approach to finding minima, many deep learning applications require\nadaptive gradient methods to achieve faster convergence. In this paper, we\ninterpret adaptive gradient methods as steepest descent applied on\nparameter-scaled networks, proposing learning-rate-free adaptive gradient\nmethods. Experimental results verify the effectiveness of this approach,\ndemonstrating comparable performance to hand-tuned learning rates across\nvarious scenarios. This work extends the applicability of learning-rate-free\nmethods, enhancing training with adaptive gradient methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.03240v1.pdf",
        "similarity": 0.36778336175774784,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-06"
    },
    {
        "new_title": "Beyond Euclid: An Illustrated Guide to Modern Machine Learning with\n  Geometric, Topological, and Algebraic Structures",
        "new_link": "http://arxiv.org/abs/2407.09468v1",
        "new_summary": "  The enduring legacy of Euclidean geometry underpins classical machine\nlearning, which, for decades, has been primarily developed for data lying in\nEuclidean space. Yet, modern machine learning increasingly encounters richly\nstructured data that is inherently nonEuclidean. This data can exhibit\nintricate geometric, topological and algebraic structure: from the geometry of\nthe curvature of space-time, to topologically complex interactions between\nneurons in the brain, to the algebraic transformations describing symmetries of\nphysical systems. Extracting knowledge from such non-Euclidean data\nnecessitates a broader mathematical perspective. Echoing the 19th-century\nrevolutions that gave rise to non-Euclidean geometry, an emerging line of\nresearch is redefining modern machine learning with non-Euclidean structures.\nIts goal: generalizing classical methods to unconventional data types with\ngeometry, topology, and algebra. In this review, we provide an accessible\ngateway to this fast-growing field and propose a graphical taxonomy that\nintegrates recent advances into an intuitive unified framework. We subsequently\nextract insights into current challenges and highlight exciting opportunities\nfor future development in this field.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.09468v1.pdf",
        "similarity": 0.3673199795187482,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-12"
    },
    {
        "new_title": "Towards Open-World Gesture Recognition",
        "new_link": "http://arxiv.org/abs/2401.11144v1",
        "new_summary": "  Static machine learning methods in gesture recognition assume that training\nand test data come from the same underlying distribution. However, in\nreal-world applications involving gesture recognition on wrist-worn devices,\ndata distribution may change over time. We formulate this problem of adapting\nrecognition models to new tasks, where new data patterns emerge, as open-world\ngesture recognition (OWGR). We propose leveraging continual learning to make\nmachine learning models adaptive to new tasks without degrading performance on\npreviously learned tasks. However, the exploration of parameters for questions\naround when and how to train and deploy recognition models requires\ntime-consuming user studies and is sometimes impractical. To address this\nchallenge, we propose a design engineering approach that enables offline\nanalysis on a collected large-scale dataset with various parameters and\ncompares different continual learning methods. Finally, design guidelines are\nprovided to enhance the development of an open-world wrist-worn gesture\nrecognition process.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.11144v1.pdf",
        "similarity": 0.3672058039053975,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-20"
    },
    {
        "new_title": "Deep Clustering Evaluation: How to Validate Internal Clustering\n  Validation Measures",
        "new_link": "http://arxiv.org/abs/2403.14830v1",
        "new_summary": "  Deep clustering, a method for partitioning complex, high-dimensional data\nusing deep neural networks, presents unique evaluation challenges. Traditional\nclustering validation measures, designed for low-dimensional spaces, are\nproblematic for deep clustering, which involves projecting data into\nlower-dimensional embeddings before partitioning. Two key issues are\nidentified: 1) the curse of dimensionality when applying these measures to raw\ndata, and 2) the unreliable comparison of clustering results across different\nembedding spaces stemming from variations in training procedures and parameter\nsettings in different clustering models. This paper addresses these challenges\nin evaluating clustering quality in deep learning. We present a theoretical\nframework to highlight ineffectiveness arising from using internal validation\nmeasures on raw and embedded data and propose a systematic approach to applying\nclustering validity indices in deep clustering contexts. Experiments show that\nthis framework aligns better with external validation measures, effectively\nreducing the misguidance from the improper use of clustering validity indices\nin deep learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.14830v1.pdf",
        "similarity": 0.3670601039754886,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-21"
    },
    {
        "new_title": "Deep Learning for Educational Data Science",
        "new_link": "http://arxiv.org/abs/2404.19675v1",
        "new_summary": "  With the ever-growing presence of deep artificial neural networks in every\nfacet of modern life, a growing body of researchers in educational data science\n-- a field consisting of various interrelated research communities -- have\nturned their attention to leveraging these powerful algorithms within the\ndomain of education. Use cases range from advanced knowledge tracing models\nthat can leverage open-ended student essays or snippets of code to automatic\naffect and behavior detectors that can identify when a student is frustrated or\naimlessly trying to solve problems unproductively -- and much more. This\nchapter provides a brief introduction to deep learning, describes some of its\nadvantages and limitations, presents a survey of its many uses in education,\nand discusses how it may further come to shape the field of educational data\nscience.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.19675v1.pdf",
        "similarity": 0.3666991138820794,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-12"
    },
    {
        "new_title": "The Hidden Pitfalls of the Cosine Similarity Loss",
        "new_link": "http://arxiv.org/abs/2406.16468v1",
        "new_summary": "  We show that the gradient of the cosine similarity between two points goes to\nzero in two under-explored settings: (1) if a point has large magnitude or (2)\nif the points are on opposite ends of the latent space. Counterintuitively, we\nprove that optimizing the cosine similarity between points forces them to grow\nin magnitude. Thus, (1) is unavoidable in practice. We then observe that these\nderivations are extremely general -- they hold across deep learning\narchitectures and for many of the standard self-supervised learning (SSL) loss\nfunctions. This leads us to propose cut-initialization: a simple change to\nnetwork initialization that helps all studied SSL methods converge faster.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.16468v1.pdf",
        "similarity": 0.36631621576740997,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-24"
    },
    {
        "new_title": "SAFE-RL: Saliency-Aware Counterfactual Explainer for Deep Reinforcement\n  Learning Policies",
        "new_link": "http://arxiv.org/abs/2404.18326v1",
        "new_summary": "  While Deep Reinforcement Learning (DRL) has emerged as a promising solution\nfor intricate control tasks, the lack of explainability of the learned policies\nimpedes its uptake in safety-critical applications, such as automated driving\nsystems (ADS). Counterfactual (CF) explanations have recently gained prominence\nfor their ability to interpret black-box Deep Learning (DL) models. CF examples\nare associated with minimal changes in the input, resulting in a complementary\noutput by the DL model. Finding such alternations, particularly for\nhigh-dimensional visual inputs, poses significant challenges. Besides, the\ntemporal dependency introduced by the reliance of the DRL agent action on a\nhistory of past state observations further complicates the generation of CF\nexamples. To address these challenges, we propose using a saliency map to\nidentify the most influential input pixels across the sequence of past observed\nstates by the agent. Then, we feed this map to a deep generative model,\nenabling the generation of plausible CFs with constrained modifications centred\non the salient regions. We evaluate the effectiveness of our framework in\ndiverse domains, including ADS, Atari Pong, Pacman and space-invaders games,\nusing traditional performance metrics such as validity, proximity and sparsity.\nExperimental results demonstrate that this framework generates more informative\nand plausible CFs than the state-of-the-art for a wide range of environments\nand DRL agents. In order to foster research in this area, we have made our\ndatasets and codes publicly available at\nhttps://github.com/Amir-Samadi/SAFE-RL.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.18326v1.pdf",
        "similarity": 0.36608564006877803,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-28"
    },
    {
        "new_title": "A comparative analysis of deep learning models for lung segmentation on\n  X-ray images",
        "new_link": "http://arxiv.org/abs/2404.06455v1",
        "new_summary": "  Robust and highly accurate lung segmentation in X-rays is crucial in medical\nimaging. This study evaluates deep learning solutions for this task, ranking\nexisting methods and analyzing their performance under diverse image\nmodifications. Out of 61 analyzed papers, only nine offered implementation or\npre-trained models, enabling assessment of three prominent methods: Lung VAE,\nTransResUNet, and CE-Net. The analysis revealed that CE-Net performs best,\ndemonstrating the highest values in dice similarity coefficient and\nintersection over union metric.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.06455v1.pdf",
        "similarity": 0.36598338331295244,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-09"
    },
    {
        "new_title": "Mapping the Multiverse of Latent Representations",
        "new_link": "http://arxiv.org/abs/2402.01514v2",
        "new_summary": "  Echoing recent calls to counter reliability and robustness concerns in\nmachine learning via multiverse analysis, we present PRESTO, a principled\nframework for mapping the multiverse of machine-learning models that rely on\nlatent representations. Although such models enjoy widespread adoption, the\nvariability in their embeddings remains poorly understood, resulting in\nunnecessary complexity and untrustworthy representations. Our framework uses\npersistent homology to characterize the latent spaces arising from different\ncombinations of diverse machine-learning methods, (hyper)parameter\nconfigurations, and datasets, allowing us to measure their pairwise\n(dis)similarity and statistically reason about their distributions. As we\ndemonstrate both theoretically and empirically, our pipeline preserves\ndesirable properties of collections of latent representations, and it can be\nleveraged to perform sensitivity analysis, detect anomalous embeddings, or\nefficiently and effectively navigate hyperparameter search spaces.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.01514v2.pdf",
        "similarity": 0.36597080503285684,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-02"
    },
    {
        "new_title": "Understanding Encoder-Decoder Structures in Machine Learning Using\n  Information Measures",
        "new_link": "http://arxiv.org/abs/2405.20452v1",
        "new_summary": "  We present new results to model and understand the role of encoder-decoder\ndesign in machine learning (ML) from an information-theoretic angle. We use two\nmain information concepts, information sufficiency (IS) and mutual information\nloss (MIL), to represent predictive structures in machine learning. Our first\nmain result provides a functional expression that characterizes the class of\nprobabilistic models consistent with an IS encoder-decoder latent predictive\nstructure. This result formally justifies the encoder-decoder forward stages\nmany modern ML architectures adopt to learn latent (compressed) representations\nfor classification. To illustrate IS as a realistic and relevant model\nassumption, we revisit some known ML concepts and present some interesting new\nexamples: invariant, robust, sparse, and digital models. Furthermore, our IS\ncharacterization allows us to tackle the fundamental question of how much\nperformance (predictive expressiveness) could be lost, using the cross entropy\nrisk, when a given encoder-decoder architecture is adopted in a learning\nsetting. Here, our second main result shows that a mutual information loss\nquantifies the lack of expressiveness attributed to the choice of a (biased)\nencoder-decoder ML design. Finally, we address the problem of universal\ncross-entropy learning with an encoder-decoder design where necessary and\nsufficiency conditions are established to meet this requirement. In all these\nresults, Shannon's information measures offer new interpretations and\nexplanations for representation learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.20452v1.pdf",
        "similarity": 0.3658147396773908,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-30"
    },
    {
        "new_title": "Scheduled Curiosity-Deep Dyna-Q: Efficient Exploration for Dialog Policy\n  Learning",
        "new_link": "http://arxiv.org/abs/2402.00085v2",
        "new_summary": "  Training task-oriented dialog agents based on reinforcement learning is\ntime-consuming and requires a large number of interactions with real users. How\nto grasp dialog policy within limited dialog experiences remains an obstacle\nthat makes the agent training process less efficient. In addition, most\nprevious frameworks start training by randomly choosing training samples, which\ndiffers from the human learning method and hurts the efficiency and stability\nof training. Therefore, we propose Scheduled Curiosity-Deep Dyna-Q (SC-DDQ), a\ncuriosity-driven curriculum learning framework based on a state-of-the-art\nmodel-based reinforcement learning dialog model, Deep Dyna-Q (DDQ).\nFurthermore, we designed learning schedules for SC-DDQ and DDQ, respectively,\nfollowing two opposite training strategies: classic curriculum learning and its\nreverse version. Our results show that by introducing scheduled learning and\ncuriosity, the new framework leads to a significant improvement over the DDQ\nand Deep Q-learning(DQN). Surprisingly, we found that traditional curriculum\nlearning was not always effective. Specifically, according to the experimental\nresults, the easy-first and difficult-first strategies are more suitable for\nSC-DDQ and DDQ. To analyze our results, we adopted the entropy of sampled\nactions to depict action exploration and found that training strategies with\nhigh entropy in the first stage and low entropy in the last stage lead to\nbetter performance.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.00085v2.pdf",
        "similarity": 0.36557007877351905,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-31"
    },
    {
        "new_title": "A Review of Neuroscience-Inspired Machine Learning",
        "new_link": "http://arxiv.org/abs/2403.18929v1",
        "new_summary": "  One major criticism of deep learning centers around the biological\nimplausibility of the credit assignment schema used for learning --\nbackpropagation of errors. This implausibility translates into practical\nlimitations, spanning scientific fields, including incompatibility with\nhardware and non-differentiable implementations, thus leading to expensive\nenergy requirements. In contrast, biologically plausible credit assignment is\ncompatible with practically any learning condition and is energy-efficient. As\na result, it accommodates hardware and scientific modeling, e.g. learning with\nphysical systems and non-differentiable behavior. Furthermore, it can lead to\nthe development of real-time, adaptive neuromorphic processing systems. In\naddressing this problem, an interdisciplinary branch of artificial intelligence\nresearch that lies at the intersection of neuroscience, cognitive science, and\nmachine learning has emerged. In this paper, we survey several vital algorithms\nthat model bio-plausible rules of credit assignment in artificial neural\nnetworks, discussing the solutions they provide for different scientific fields\nas well as their advantages on CPUs, GPUs, and novel implementations of\nneuromorphic hardware. We conclude by discussing the future challenges that\nwill need to be addressed in order to make such algorithms more useful in\npractical applications.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.18929v1.pdf",
        "similarity": 0.36555096186935593,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-16"
    },
    {
        "new_title": "LIBR+: Improving Intraoperative Liver Registration by Learning the\n  Residual of Biomechanics-Based Deformable Registration",
        "new_link": "http://arxiv.org/abs/2403.06901v1",
        "new_summary": "  The surgical environment imposes unique challenges to the intraoperative\nregistration of organ shapes to their preoperatively-imaged geometry.\nBiomechanical model-based registration remains popular, while deep learning\nsolutions remain limited due to the sparsity and variability of intraoperative\nmeasurements and the limited ground-truth deformation of an organ that can be\nobtained during the surgery. In this paper, we propose a novel \\textit{hybrid}\nregistration approach that leverage a linearized iterative boundary\nreconstruction (LIBR) method based on linear elastic biomechanics, and use deep\nneural networks to learn its residual to the ground-truth deformation (LIBR+).\nWe further formulate a dual-branch spline-residual graph convolutional neural\nnetwork (SR-GCN) to assimilate information from sparse and variable\nintraoperative measurements and effectively propagate it through the geometry\nof the 3D organ. Experiments on a large intraoperative liver registration\ndataset demonstrated the consistent improvements achieved by LIBR+ in\ncomparison to existing rigid, biomechnical model-based non-rigid, and\ndeep-learning based non-rigid approaches to intraoperative liver registration.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.06901v1.pdf",
        "similarity": 0.36544500056390766,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-11"
    },
    {
        "new_title": "Addressing a fundamental limitation in deep vision models: lack of\n  spatial attention",
        "new_link": "http://arxiv.org/abs/2407.01782v1",
        "new_summary": "  The primary aim of this manuscript is to underscore a significant limitation\nin current deep learning models, particularly vision models. Unlike human\nvision, which efficiently selects only the essential visual areas for further\nprocessing, leading to high speed and low energy consumption, deep vision\nmodels process the entire image. In this work, we examine this issue from a\nbroader perspective and propose a solution that could pave the way for the next\ngeneration of more efficient vision models. Basically, convolution and pooling\noperations are selectively applied to altered regions, with a change map sent\nto subsequent layers. This map indicates which computations need to be\nrepeated. The code is available at\nhttps://github.com/aliborji/spatial_attention.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.01782v1.pdf",
        "similarity": 0.3652063755339154,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-01"
    },
    {
        "new_title": "Striking a Balance between Classical and Deep Learning Approaches in\n  Natural Language Processing Pedagogy",
        "new_link": "http://arxiv.org/abs/2405.09854v2",
        "new_summary": "  While deep learning approaches represent the state-of-the-art of natural\nlanguage processing (NLP) today, classical algorithms and approaches still find\na place in NLP textbooks and courses of recent years. This paper discusses the\nperspectives of conveners of two introductory NLP courses taught in Australia\nand India, and examines how classical and deep learning approaches can be\nbalanced within the lecture plan and assessments of the courses. We also draw\nparallels with the objects-first and objects-later debate in CS1 education. We\nobserve that teaching classical approaches adds value to student learning by\nbuilding an intuitive understanding of NLP problems, potential solutions, and\neven deep learning models themselves. Despite classical approaches not being\nstate-of-the-art, the paper makes a case for their inclusion in NLP courses\ntoday.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.09854v2.pdf",
        "similarity": 0.36484388693833864,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-16"
    },
    {
        "new_title": "Scalable Interactive Machine Learning for Future Command and Control",
        "new_link": "http://arxiv.org/abs/2402.06501v2",
        "new_summary": "  Future warfare will require Command and Control (C2) personnel to make\ndecisions at shrinking timescales in complex and potentially ill-defined\nsituations. Given the need for robust decision-making processes and\ndecision-support tools, integration of artificial and human intelligence holds\nthe potential to revolutionize the C2 operations process to ensure adaptability\nand efficiency in rapidly changing operational environments. We propose to\nleverage recent promising breakthroughs in interactive machine learning, in\nwhich humans can cooperate with machine learning algorithms to guide machine\nlearning algorithm behavior. This paper identifies several gaps in\nstate-of-the-art science and technology that future work should address to\nextend these approaches to function in complex C2 contexts. In particular, we\ndescribe three research focus areas that together, aim to enable scalable\ninteractive machine learning (SIML): 1) developing human-AI interaction\nalgorithms to enable planning in complex, dynamic situations; 2) fostering\nresilient human-AI teams through optimizing roles, configurations, and trust;\nand 3) scaling algorithms and human-AI teams for flexibility across a range of\npotential contexts and situations.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.06501v2.pdf",
        "similarity": 0.36476657523415223,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-09"
    },
    {
        "new_title": "Fermionic Machine Learning",
        "new_link": "http://arxiv.org/abs/2404.19032v1",
        "new_summary": "  We introduce fermionic machine learning (FermiML), a machine learning\nframework based on fermionic quantum computation. FermiML models are expressed\nin terms of parameterized matchgate circuits, a restricted class of quantum\ncircuits that map exactly to systems of free Majorana fermions. The FermiML\nframework allows for building fermionic counterparts of any quantum machine\nlearning (QML) model based on parameterized quantum circuits, including models\nthat produce highly entangled quantum states. Importantly, matchgate circuits\nare efficiently simulable classically, thus rendering FermiML a flexible\nframework for utility benchmarks of QML methods on large real-world datasets.\nWe initiate the exploration of FermiML by benchmarking it against unrestricted\nPQCs in the context of classification with random quantum kernels. Through\nexperiments on standard datasets (Digits and Wisconsin Breast Cancer), we\ndemonstrate that FermiML kernels are on-par with unrestricted PQC kernels in\nclassification tasks using support-vector machines. Furthermore, we find that\nFermiML kernels outperform their unrestricted candidates on multi-class\nclassification, including on datasets with several tens of relevant features.\nWe thus show how FermiML enables us to explore regimes previously inaccessible\nto QML methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.19032v1.pdf",
        "similarity": 0.36465587674897587,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-29"
    },
    {
        "new_title": "Pushing the Boundary: Specialising Deep Configuration Performance\n  Learning",
        "new_link": "http://arxiv.org/abs/2407.02706v1",
        "new_summary": "  Software systems often have numerous configuration options that can be\nadjusted to meet different performance requirements. However, understanding the\ncombined impact of these options on performance is often challenging,\nespecially with limited real-world data. To tackle this issue, deep learning\ntechniques have gained popularity due to their ability to capture complex\nrelationships even with limited samples. This thesis begins with a systematic\nliterature review of deep learning techniques in configuration performance\nmodeling, analyzing 85 primary papers out of 948 searched papers. It identifies\nknowledge gaps and sets three objectives for the thesis. The first knowledge\ngap is the lack of understanding about which encoding scheme is better and in\nwhat circumstances. To address this, the thesis conducts an empirical study\ncomparing three popular encoding schemes. Actionable suggestions are provided\nto support more reliable decisions. Another knowledge gap is the sparsity\ninherited from the configuration landscape. To handle this, the thesis proposes\na model-agnostic and sparsity-robust framework called DaL, which uses a\n\"divide-and-learn\" approach. DaL outperforms state-of-the-art approaches in\naccuracy improvement across various real-world systems. The thesis also\naddresses the limitation of predicting under static environments by proposing a\nsequential meta-learning framework called SeMPL. Unlike traditional\nmeta-learning frameworks, SeMPL trains meta-environments in a specialized\norder, resulting in significantly improved prediction accuracy in\nmulti-environment scenarios. Overall, the thesis identifies and addresses\ncritical knowledge gaps in deep performance learning, significantly advancing\nthe accuracy of performance prediction.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.02706v1.pdf",
        "similarity": 0.3646129554477024,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-02"
    },
    {
        "new_title": "Single-Image Shadow Removal Using Deep Learning: A Comprehensive Survey",
        "new_link": "http://arxiv.org/abs/2407.08865v1",
        "new_summary": "  Shadow removal aims at restoring the image content within shadow regions,\npursuing a uniform distribution of illumination that is consistent between\nshadow and non-shadow regions. {Comparing to other image restoration tasks,\nthere are two unique challenges in shadow removal:} 1) The patterns of shadows\nare arbitrary, varied, and often have highly complex trace structures, making\n``trace-less'' image recovery difficult. 2) The degradation caused by shadows\nis spatially non-uniform, resulting in inconsistencies in illumination and\ncolor between shadow and non-shadow areas. Recent developments in this field\nare primarily driven by deep learning-based solutions, employing a variety of\nlearning strategies, network architectures, loss functions, and training data.\nNevertheless, a thorough and insightful review of deep learning-based shadow\nremoval techniques is still lacking. In this paper, we are the first to provide\na comprehensive survey to cover various aspects ranging from technical details\nto applications. We highlight the major advancements in deep learning-based\nsingle-image shadow removal methods, thoroughly review previous research across\nvarious categories, and provide insights into the historical progression of\nthese developments. Additionally, we summarize performance comparisons both\nquantitatively and qualitatively. Beyond the technical aspects of shadow\nremoval methods, we also explore potential future directions for this field.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.08865v1.pdf",
        "similarity": 0.3644955187664232,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-11"
    },
    {
        "new_title": "Deep Learning-based Text-in-Image Watermarking",
        "new_link": "http://arxiv.org/abs/2404.13134v1",
        "new_summary": "  In this work, we introduce a novel deep learning-based approach to\ntext-in-image watermarking, a method that embeds and extracts textual\ninformation within images to enhance data security and integrity. Leveraging\nthe capabilities of deep learning, specifically through the use of\nTransformer-based architectures for text processing and Vision Transformers for\nimage feature extraction, our method sets new benchmarks in the domain. The\nproposed method represents the first application of deep learning in\ntext-in-image watermarking that improves adaptivity, allowing the model to\nintelligently adjust to specific image characteristics and emerging threats.\nThrough testing and evaluation, our method has demonstrated superior robustness\ncompared to traditional watermarking techniques, achieving enhanced\nimperceptibility that ensures the watermark remains undetectable across various\nimage contents.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.13134v1.pdf",
        "similarity": 0.36445416864947294,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-19"
    },
    {
        "new_title": "Forgetting Order of Continual Learning: Examples That are Learned First\n  are Forgotten Last",
        "new_link": "http://arxiv.org/abs/2406.09935v1",
        "new_summary": "  Catastrophic forgetting poses a significant challenge in continual learning,\nwhere models often forget previous tasks when trained on new data. Our\nempirical analysis reveals a strong correlation between catastrophic forgetting\nand the learning speed of examples: examples learned early are rarely\nforgotten, while those learned later are more susceptible to forgetting. We\ndemonstrate that replay-based continual learning methods can leverage this\nphenomenon by focusing on mid-learned examples for rehearsal. We introduce\nGoldilocks, a novel replay buffer sampling method that filters out examples\nlearned too quickly or too slowly, keeping those learned at an intermediate\nspeed. Goldilocks improves existing continual learning algorithms, leading to\nstate-of-the-art performance across several image classification tasks.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.09935v1.pdf",
        "similarity": 0.36419726547247605,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-14"
    },
    {
        "new_title": "Communication-Efficient Large-Scale Distributed Deep Learning: A\n  Comprehensive Survey",
        "new_link": "http://arxiv.org/abs/2404.06114v1",
        "new_summary": "  With the rapid growth in the volume of data sets, models, and devices in the\ndomain of deep learning, there is increasing attention on large-scale\ndistributed deep learning. In contrast to traditional distributed deep\nlearning, the large-scale scenario poses new challenges that include fault\ntolerance, scalability of algorithms and infrastructures, and heterogeneity in\ndata sets, models, and resources. Due to intensive synchronization of models\nand sharing of data across GPUs and computing nodes during distributed training\nand inference processes, communication efficiency becomes the bottleneck for\nachieving high performance at a large scale. This article surveys the\nliterature over the period of 2018-2023 on algorithms and technologies aimed at\nachieving efficient communication in large-scale distributed deep learning at\nvarious levels, including algorithms, frameworks, and infrastructures.\nSpecifically, we first introduce efficient algorithms for model synchronization\nand communication data compression in the context of large-scale distributed\ntraining. Next, we introduce efficient strategies related to resource\nallocation and task scheduling for use in distributed training and inference.\nAfter that, we present the latest technologies pertaining to modern\ncommunication infrastructures used in distributed deep learning with a focus on\nexamining the impact of the communication overhead in a large-scale and\nheterogeneous setting. Finally, we conduct a case study on the distributed\ntraining of large language models at a large scale to illustrate how to apply\nthese technologies in real cases. This article aims to offer researchers a\ncomprehensive understanding of the current landscape of large-scale distributed\ndeep learning and to reveal promising future research directions toward\ncommunication-efficient solutions in this scope.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.06114v1.pdf",
        "similarity": 0.3637718166287737,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-09"
    },
    {
        "new_title": "On the Robustness of Global Feature Effect Explanations",
        "new_link": "http://arxiv.org/abs/2406.09069v1",
        "new_summary": "  We study the robustness of global post-hoc explanations for predictive models\ntrained on tabular data. Effects of predictor features in black-box supervised\nlearning are an essential diagnostic tool for model debugging and scientific\ndiscovery in applied sciences. However, how vulnerable they are to data and\nmodel perturbations remains an open research question. We introduce several\ntheoretical bounds for evaluating the robustness of partial dependence plots\nand accumulated local effects. Our experimental results with synthetic and\nreal-world datasets quantify the gap between the best and worst-case scenarios\nof (mis)interpreting machine learning predictions globally.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.09069v1.pdf",
        "similarity": 0.3637263093898575,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-13"
    },
    {
        "new_title": "A Survey on Deep Learning for Theorem Proving",
        "new_link": "http://arxiv.org/abs/2404.09939v1",
        "new_summary": "  Theorem proving is a fundamental aspect of mathematics, spanning from\ninformal reasoning in mathematical language to rigorous derivations in formal\nsystems. In recent years, the advancement of deep learning, especially the\nemergence of large language models, has sparked a notable surge of research\nexploring these techniques to enhance the process of theorem proving. This\npaper presents a pioneering comprehensive survey of deep learning for theorem\nproving by offering i) a thorough review of existing approaches across various\ntasks such as autoformalization, premise selection, proofstep generation, and\nproof search; ii) a meticulous summary of available datasets and strategies for\ndata generation; iii) a detailed analysis of evaluation metrics and the\nperformance of state-of-the-art; and iv) a critical discussion on the\npersistent challenges and the promising avenues for future exploration. Our\nsurvey aims to serve as a foundational reference for deep learning approaches\nin theorem proving, seeking to catalyze further research endeavors in this\nrapidly growing field.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.09939v1.pdf",
        "similarity": 0.36371472835060714,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-15"
    },
    {
        "new_title": "Simplifying Hyperparameter Tuning in Online Machine Learning -- The\n  spotRiverGUI",
        "new_link": "http://arxiv.org/abs/2402.11594v1",
        "new_summary": "  Batch Machine Learning (BML) reaches its limits when dealing with very large\namounts of streaming data. This is especially true for available memory,\nhandling drift in data streams, and processing new, unknown data. Online\nMachine Learning (OML) is an alternative to BML that overcomes the limitations\nof BML. OML is able to process data in a sequential manner, which is especially\nuseful for data streams. The `river` package is a Python OML-library, which\nprovides a variety of online learning algorithms for classification,\nregression, clustering, anomaly detection, and more. The `spotRiver` package\nprovides a framework for hyperparameter tuning of OML models. The\n`spotRiverGUI` is a graphical user interface for the `spotRiver` package. The\n`spotRiverGUI` releases the user from the burden of manually searching for the\noptimal hyperparameter setting. After the data is provided, users can compare\ndifferent OML algorithms from the powerful `river` package in a convenient way\nand tune the selected algorithms very efficiently.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.11594v1.pdf",
        "similarity": 0.3635411289020331,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-18"
    },
    {
        "new_title": "Identification and Uses of Deep Learning Backbones via Pattern Mining",
        "new_link": "http://arxiv.org/abs/2403.18278v1",
        "new_summary": "  Deep learning is extensively used in many areas of data mining as a black-box\nmethod with impressive results. However, understanding the core mechanism of\nhow deep learning makes predictions is a relatively understudied problem. Here\nwe explore the notion of identifying a backbone of deep learning for a given\ngroup of instances. A group here can be instances of the same class or even\nmisclassified instances of the same class. We view each instance for a given\ngroup as activating a subset of neurons and attempt to find a subgraph of\nneurons associated with a given concept/group. We formulate this problem as a\nset cover style problem and show it is intractable and presents a highly\nconstrained integer linear programming (ILP) formulation. As an alternative, we\nexplore a coverage-based heuristic approach related to pattern mining, and show\nit converges to a Pareto equilibrium point of the ILP formulation.\nExperimentally we explore these backbones to identify mistakes and improve\nperformance, explanation, and visualization. We demonstrate application-based\nresults using several challenging data sets, including Bird Audio Detection\n(BAD) Challenge and Labeled Faces in the Wild (LFW), as well as the classic\nMNIST data.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.18278v1.pdf",
        "similarity": 0.363481760493236,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-27"
    },
    {
        "new_title": "Recent Advances in Predictive Modeling with Electronic Health Records",
        "new_link": "http://arxiv.org/abs/2402.01077v1",
        "new_summary": "  The development of electronic health records (EHR) systems has enabled the\ncollection of a vast amount of digitized patient data. However, utilizing EHR\ndata for predictive modeling presents several challenges due to its unique\ncharacteristics. With the advancements in machine learning techniques, deep\nlearning has demonstrated its superiority in various applications, including\nhealthcare. This survey systematically reviews recent advances in deep\nlearning-based predictive models using EHR data. Specifically, we begin by\nintroducing the background of EHR data and providing a mathematical definition\nof the predictive modeling task. We then categorize and summarize predictive\ndeep models from multiple perspectives. Furthermore, we present benchmarks and\ntoolkits relevant to predictive modeling in healthcare. Finally, we conclude\nthis survey by discussing open challenges and suggesting promising directions\nfor future research.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.01077v1.pdf",
        "similarity": 0.36346805385264075,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-02"
    },
    {
        "new_title": "Classifying point clouds at the facade-level using geometric features\n  and deep learning networks",
        "new_link": "http://arxiv.org/abs/2402.06506v1",
        "new_summary": "  3D building models with facade details are playing an important role in many\napplications now. Classifying point clouds at facade-level is key to create\nsuch digital replicas of the real world. However, few studies have focused on\nsuch detailed classification with deep neural networks. We propose a method\nfusing geometric features with deep learning networks for point cloud\nclassification at facade-level. Our experiments conclude that such early-fused\nfeatures improve deep learning methods' performance. This method can be applied\nfor compensating deep learning networks' ability in capturing local geometric\ninformation and promoting the advancement of semantic segmentation.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.06506v1.pdf",
        "similarity": 0.36333915465456484,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-09"
    },
    {
        "new_title": "How Does Unlabeled Data Provably Help Out-of-Distribution Detection?",
        "new_link": "http://arxiv.org/abs/2402.03502v1",
        "new_summary": "  Using unlabeled data to regularize the machine learning models has\ndemonstrated promise for improving safety and reliability in detecting\nout-of-distribution (OOD) data. Harnessing the power of unlabeled in-the-wild\ndata is non-trivial due to the heterogeneity of both in-distribution (ID) and\nOOD data. This lack of a clean set of OOD samples poses significant challenges\nin learning an optimal OOD classifier. Currently, there is a lack of research\non formally understanding how unlabeled data helps OOD detection. This paper\nbridges the gap by introducing a new learning framework SAL (Separate And\nLearn) that offers both strong theoretical guarantees and empirical\neffectiveness. The framework separates candidate outliers from the unlabeled\ndata and then trains an OOD classifier using the candidate outliers and the\nlabeled ID data. Theoretically, we provide rigorous error bounds from the lens\nof separability and learnability, formally justifying the two components in our\nalgorithm. Our theory shows that SAL can separate the candidate outliers with\nsmall error rates, which leads to a generalization guarantee for the learned\nOOD classifier. Empirically, SAL achieves state-of-the-art performance on\ncommon benchmarks, reinforcing our theoretical insights. Code is publicly\navailable at https://github.com/deeplearning-wisc/sal.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.03502v1.pdf",
        "similarity": 0.36329589503675297,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-05"
    },
    {
        "new_title": "Online Continual Learning For Interactive Instruction Following Agents",
        "new_link": "http://arxiv.org/abs/2403.07548v2",
        "new_summary": "  In learning an embodied agent executing daily tasks via language directives,\nthe literature largely assumes that the agent learns all training data at the\nbeginning. We argue that such a learning scenario is less realistic since a\nrobotic agent is supposed to learn the world continuously as it explores and\nperceives it. To take a step towards a more realistic embodied agent learning\nscenario, we propose two continual learning setups for embodied agents;\nlearning new behaviors (Behavior Incremental Learning, Behavior-IL) and new\nenvironments (Environment Incremental Learning, Environment-IL) For the tasks,\nprevious 'data prior' based continual learning methods maintain logits for the\npast tasks. However, the stored information is often insufficiently learned\ninformation and requires task boundary information, which might not always be\navailable. Here, we propose to update them based on confidence scores without\ntask boundary information during training (i.e., task-free) in a moving average\nfashion, named Confidence-Aware Moving Average (CAMA). In the proposed\nBehavior-IL and Environment-IL setups, our simple CAMA outperforms prior state\nof the art in our empirical validations by noticeable margins. The project page\nincluding codes is https://github.com/snumprlab/cl-alfred.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.07548v2.pdf",
        "similarity": 0.36324821171608174,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-12"
    },
    {
        "new_title": "Evaluating the transferability potential of deep learning models for\n  climate downscaling",
        "new_link": "http://arxiv.org/abs/2407.12517v1",
        "new_summary": "  Climate downscaling, the process of generating high-resolution climate data\nfrom low-resolution simulations, is essential for understanding and adapting to\nclimate change at regional and local scales. Deep learning approaches have\nproven useful in tackling this problem. However, existing studies usually focus\non training models for one specific task, location and variable, which are\ntherefore limited in their generalizability and transferability. In this paper,\nwe evaluate the efficacy of training deep learning downscaling models on\nmultiple diverse climate datasets to learn more robust and transferable\nrepresentations. We evaluate the effectiveness of architectures zero-shot\ntransferability using CNNs, Fourier Neural Operators (FNOs), and vision\nTransformers (ViTs). We assess the spatial, variable, and product\ntransferability of downscaling models experimentally, to understand the\ngeneralizability of these different architecture types.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.12517v1.pdf",
        "similarity": 0.36314309712688425,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-17"
    },
    {
        "new_title": "Demystifying the Hypercomplex: Inductive Biases in Hypercomplex Deep\n  Learning",
        "new_link": "http://arxiv.org/abs/2405.07024v1",
        "new_summary": "  Hypercomplex algebras have recently been gaining prominence in the field of\ndeep learning owing to the advantages of their division algebras over real\nvector spaces and their superior results when dealing with multidimensional\nsignals in real-world 3D and 4D paradigms. This paper provides a foundational\nframework that serves as a roadmap for understanding why hypercomplex deep\nlearning methods are so successful and how their potential can be exploited.\nSuch a theoretical framework is described in terms of inductive bias, i.e., a\ncollection of assumptions, properties, and constraints that are built into\ntraining algorithms to guide their learning process toward more efficient and\naccurate solutions. We show that it is possible to derive specific inductive\nbiases in the hypercomplex domains, which extend complex numbers to encompass\ndiverse numbers and data structures. These biases prove effective in managing\nthe distinctive properties of these domains, as well as the complex structures\nof multidimensional and multimodal signals. This novel perspective for\nhypercomplex deep learning promises to both demystify this class of methods and\nclarify their potential, under a unifying framework, and in this way promotes\nhypercomplex models as viable alternatives to traditional real-valued deep\nlearning for multidimensional signal processing.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.07024v1.pdf",
        "similarity": 0.3629589902063547,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-11"
    },
    {
        "new_title": "State of the art applications of deep learning within tracking and\n  detecting marine debris: A survey",
        "new_link": "http://arxiv.org/abs/2403.18067v1",
        "new_summary": "  Deep learning techniques have been explored within the marine litter problem\nfor approximately 20 years but the majority of the research has developed\nrapidly in the last five years. We provide an in-depth, up to date, summary and\nanalysis of 28 of the most recent and significant contributions of deep\nlearning in marine debris. From cross referencing the research paper results,\nthe YOLO family significantly outperforms all other methods of object detection\nbut there are many respected contributions to this field that have\ncategorically agreed that a comprehensive database of underwater debris is not\ncurrently available for machine learning. Using a small dataset curated and\nlabelled by us, we tested YOLOv5 on a binary classification task and found the\naccuracy was low and the rate of false positives was high; highlighting the\nimportance of a comprehensive database. We conclude this survey with over 40\nfuture research recommendations and open challenges.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.18067v1.pdf",
        "similarity": 0.3626708402275539,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-26"
    },
    {
        "new_title": "Impact of Data Bias on Machine Learning for Crystal Compound\n  Synthesizability Predictions",
        "new_link": "http://arxiv.org/abs/2406.17956v1",
        "new_summary": "  Machine learning models are susceptible to being misled by biases in training\ndata that emphasize incidental correlations over the intended learning task. In\nthis study, we demonstrate the impact of data bias on the performance of a\nmachine learning model designed to predict the synthesizability likelihood of\ncrystal compounds. The model performs a binary classification on labeled\ncrystal samples. Despite using the same architecture for the machine learning\nmodel, we showcase how the model's learning and prediction behavior differs\nonce trained on distinct data. We use two data sets for illustration: a\nmixed-source data set that integrates experimental and computational crystal\nsamples and a single-source data set consisting of data exclusively from one\ncomputational database. We present simple procedures to detect data bias and to\nevaluate its effect on the model's performance and generalization. This study\nreveals how inconsistent, unbalanced data can propagate bias, undermining\nreal-world applicability even for advanced machine learning techniques.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.17956v1.pdf",
        "similarity": 0.36252195170253204,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-25"
    },
    {
        "new_title": "Towards Independence Criterion in Machine Unlearning of Features and\n  Labels",
        "new_link": "http://arxiv.org/abs/2403.08124v1",
        "new_summary": "  This work delves into the complexities of machine unlearning in the face of\ndistributional shifts, particularly focusing on the challenges posed by\nnon-uniform feature and label removal. With the advent of regulations like the\nGDPR emphasizing data privacy and the right to be forgotten, machine learning\nmodels face the daunting task of unlearning sensitive information without\ncompromising their integrity or performance. Our research introduces a novel\napproach that leverages influence functions and principles of distributional\nindependence to address these challenges. By proposing a comprehensive\nframework for machine unlearning, we aim to ensure privacy protection while\nmaintaining model performance and adaptability across varying distributions.\nOur method not only facilitates efficient data removal but also dynamically\nadjusts the model to preserve its generalization capabilities. Through\nextensive experimentation, we demonstrate the efficacy of our approach in\nscenarios characterized by significant distributional shifts, making\nsubstantial contributions to the field of machine unlearning. This research\npaves the way for developing more resilient and adaptable unlearning\ntechniques, ensuring models remain robust and accurate in the dynamic landscape\nof data privacy and machine learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.08124v1.pdf",
        "similarity": 0.3623752402501635,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-12"
    },
    {
        "new_title": "TDML -- A Trustworthy Distributed Machine Learning Framework",
        "new_link": "http://arxiv.org/abs/2407.07339v1",
        "new_summary": "  Recent years have witnessed a surge in deep learning research, marked by the\nintroduction of expansive generative models like OpenAI's SORA and GPT, Meta\nAI's LLAMA series, and Google's FLAN, BART, and Gemini models. However, the\nrapid advancement of large models (LM) has intensified the demand for computing\nresources, particularly GPUs, which are crucial for their parallel processing\ncapabilities. This demand is exacerbated by limited GPU availability due to\nsupply chain delays and monopolistic acquisition by major tech firms.\nDistributed Machine Learning (DML) methods, such as Federated Learning (FL),\nmitigate these challenges by partitioning data and models across multiple\nservers, though implementing optimizations like tensor and pipeline parallelism\nremains complex. Blockchain technology emerges as a promising solution,\nensuring data integrity, scalability, and trust in distributed computing\nenvironments, but still lacks guidance on building practical DML systems. In\nthis paper, we propose a \\textit{trustworthy distributed machine learning}\n(TDML) framework that leverages blockchain to coordinate remote trainers and\nvalidate workloads, achieving privacy, transparency, and efficient model\ntraining across public remote computing resources. Experimental validation\ndemonstrates TDML's efficacy in overcoming performance limitations and\nmalicious node detection, positioning it as a robust solution for scalable and\nsecure distributed machine learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.07339v1.pdf",
        "similarity": 0.36184183155378097,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-10"
    },
    {
        "new_title": "Generalization analysis with deep ReLU networks for metric and\n  similarity learning",
        "new_link": "http://arxiv.org/abs/2405.06415v1",
        "new_summary": "  While considerable theoretical progress has been devoted to the study of\nmetric and similarity learning, the generalization mystery is still missing. In\nthis paper, we study the generalization performance of metric and similarity\nlearning by leveraging the specific structure of the true metric (the target\nfunction). Specifically, by deriving the explicit form of the true metric for\nmetric and similarity learning with the hinge loss, we construct a structured\ndeep ReLU neural network as an approximation of the true metric, whose\napproximation ability relies on the network complexity. Here, the network\ncomplexity corresponds to the depth, the number of nonzero weights and the\ncomputation units of the network. Consider the hypothesis space which consists\nof the structured deep ReLU networks, we develop the excess generalization\nerror bounds for a metric and similarity learning problem by estimating the\napproximation error and the estimation error carefully. An optimal excess risk\nrate is derived by choosing the proper capacity of the constructed hypothesis\nspace. To the best of our knowledge, this is the first-ever-known\ngeneralization analysis providing the excess generalization error for metric\nand similarity learning. In addition, we investigate the properties of the true\nmetric of metric and similarity learning with general losses.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.06415v1.pdf",
        "similarity": 0.3616090923615306,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-10"
    },
    {
        "new_title": "Learning of deep convolutional network image classifiers via stochastic\n  gradient descent and over-parametrization",
        "new_link": "http://arxiv.org/abs/2404.07128v1",
        "new_summary": "  Image classification from independent and identically distributed random\nvariables is considered. Image classifiers are defined which are based on a\nlinear combination of deep convolutional networks with max-pooling layer. Here\nall the weights are learned by stochastic gradient descent. A general result is\npresented which shows that the image classifiers are able to approximate the\nbest possible deep convolutional network. In case that the a posteriori\nprobability satisfies a suitable hierarchical composition model it is shown\nthat the corresponding deep convolutional neural network image classifier\nachieves a rate of convergence which is independent of the dimension of the\nimages.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.07128v1.pdf",
        "similarity": 0.36160317523963664,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-10"
    },
    {
        "new_title": "Wearable-based behaviour interpolation for semi-supervised human\n  activity recognition",
        "new_link": "http://arxiv.org/abs/2405.15962v1",
        "new_summary": "  While traditional feature engineering for Human Activity Recognition (HAR)\ninvolves a trial-anderror process, deep learning has emerged as a preferred\nmethod for high-level representations of sensor-based human activities.\nHowever, most deep learning-based HAR requires a large amount of labelled data\nand extracting HAR features from unlabelled data for effective deep learning\ntraining remains challenging. We, therefore, introduce a deep semi-supervised\nHAR approach, MixHAR, which concurrently uses labelled and unlabelled\nactivities. Our MixHAR employs a linear interpolation mechanism to blend\nlabelled and unlabelled activities while addressing both inter- and\nintra-activity variability. A unique challenge identified is the\nactivityintrusion problem during mixing, for which we propose a mixing\ncalibration mechanism to mitigate it in the feature embedding space.\nAdditionally, we rigorously explored and evaluated the five\nconventional/popular deep semi-supervised technologies on HAR, acting as the\nbenchmark of deep semi-supervised HAR. Our results demonstrate that MixHAR\nsignificantly improves performance, underscoring the potential of deep\nsemi-supervised techniques in HAR.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.15962v1.pdf",
        "similarity": 0.3615680162955655,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-24"
    },
    {
        "new_title": "Detecting Moving Objects With Machine Learning",
        "new_link": "http://arxiv.org/abs/2405.06148v1",
        "new_summary": "  The scientific study of the Solar System's minor bodies ultimately starts\nwith a search for those bodies. This chapter presents a review of the use of\nmachine learning techniques to find moving objects, both natural and\nartificial, in astronomical imagery. After a short review of the classical\nnon-machine learning techniques that are historically used, I review the\nrelatively nascent machine learning literature, which can broadly be summarized\ninto three categories: streak detection, detection of moving point sources in\nimage sequences, and detection of moving sources in shift and stack searches.\nIn most cases, convolutional neural networks are utilized, which is the obvious\nchoice given the imagery nature of the inputs. In this chapter I present two\nexample networks: a Residual Network I designed which is in use in various\nshift and stack searches, and a convolutional neural network that was designed\nfor prediction of source brightnesses and their uncertainties in those same\nshift-stacks. In discussion of the literature and example networks, I discuss\nvarious pitfalls with the use of machine learning techniques, including a\ndiscussion on the important issue of overfitting. I discuss various pitfall\nassociated with the use of machine learning techniques, and what I consider\nbest practices to follow in the application of machine learning to a new\nproblem, including methods for the creation of robust training sets,\nvalidation, and training to avoid overfitting.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.06148v1.pdf",
        "similarity": 0.36128316592189497,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-10"
    },
    {
        "new_title": "Distilling Conditional Diffusion Models for Offline Reinforcement\n  Learning through Trajectory Stitching",
        "new_link": "http://arxiv.org/abs/2402.00807v1",
        "new_summary": "  Deep generative models have recently emerged as an effective approach to\noffline reinforcement learning. However, their large model size poses\nchallenges in computation. We address this issue by proposing a knowledge\ndistillation method based on data augmentation. In particular, high-return\ntrajectories are generated from a conditional diffusion model, and they are\nblended with the original trajectories through a novel stitching algorithm that\nleverages a new reward generator. Applying the resulting dataset to behavioral\ncloning, the learned shallow policy whose size is much smaller outperforms or\nnearly matches deep generative planners on several D4RL benchmarks.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.00807v1.pdf",
        "similarity": 0.3610867690932062,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-01"
    },
    {
        "new_title": "Research on the Application of Computer Vision Based on Deep Learning in\n  Autonomous Driving Technology",
        "new_link": "http://arxiv.org/abs/2406.00490v2",
        "new_summary": "  This research aims to explore the application of deep learning in autonomous\ndriving computer vision technology and its impact on improving system\nperformance. By using advanced technologies such as convolutional neural\nnetworks (CNN), multi-task joint learning methods, and deep reinforcement\nlearning, this article analyzes in detail the application of deep learning in\nimage recognition, real-time target tracking and classification, environment\nperception and decision support, and path planning and navigation. Application\nprocess in key areas. Research results show that the proposed system has an\naccuracy of over 98% in image recognition, target tracking and classification,\nand also demonstrates efficient performance and practicality in environmental\nperception and decision support, path planning and navigation. The conclusion\npoints out that deep learning technology can significantly improve the accuracy\nand real-time response capabilities of autonomous driving systems. Although\nthere are still challenges in environmental perception and decision support,\nwith the advancement of technology, it is expected to achieve wider\napplications and greater capabilities in the future. potential.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.00490v2.pdf",
        "similarity": 0.36024352356454936,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-01"
    },
    {
        "new_title": "Leveraging Deep Learning and Xception Architecture for High-Accuracy MRI\n  Classification in Alzheimer Diagnosis",
        "new_link": "http://arxiv.org/abs/2403.16212v1",
        "new_summary": "  Exploring the application of deep learning technologies in the field of\nmedical diagnostics, Magnetic Resonance Imaging (MRI) provides a unique\nperspective for observing and diagnosing complex neurodegenerative diseases\nsuch as Alzheimer Disease (AD). With advancements in deep learning,\nparticularly in Convolutional Neural Networks (CNNs) and the Xception network\narchitecture, we are now able to analyze and classify vast amounts of MRI data\nwith unprecedented accuracy. The progress of this technology not only enhances\nour understanding of brain structural changes but also opens up new avenues for\nmonitoring disease progression through non-invasive means and potentially\nallows for precise diagnosis in the early stages of the disease.\n  This study aims to classify MRI images using deep learning models to identify\ndifferent stages of Alzheimer Disease through a series of innovative data\nprocessing and model construction steps. Our experimental results show that the\ndeep learning framework based on the Xception model achieved a 99.6% accuracy\nrate in the multi-class MRI image classification task, demonstrating its\npotential application value in assistive diagnosis. Future research will focus\non expanding the dataset, improving model interpretability, and clinical\nvalidation to further promote the application of deep learning technology in\nthe medical field, with the hope of bringing earlier diagnosis and more\npersonalized treatment plans to Alzheimer Disease patients.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.16212v1.pdf",
        "similarity": 0.3599792613454535,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-24"
    },
    {
        "new_title": "Fruit Classification System with Deep Learning and Neural Architecture\n  Search",
        "new_link": "http://arxiv.org/abs/2406.01869v1",
        "new_summary": "  The fruit identification process involves analyzing and categorizing\ndifferent types of fruits based on their visual characteristics. This activity\ncan be achieved using a range of methodologies, encompassing manual\nexamination, conventional computer vision methodologies, and more sophisticated\nmethodologies employing machine learning and deep learning. Our study\nidentified a total of 15 distinct categories of fruit, consisting of class\nAvocado, Banana, Cherry, Apple Braeburn, Apple golden 1, Apricot, Grape, Kiwi,\nMango, Orange, Papaya, Peach, Pineapple, Pomegranate and Strawberry. Neural\nArchitecture Search (NAS) is a technological advancement employed within the\nrealm of deep learning and artificial intelligence, to automate conceptualizing\nand refining neural network topologies. NAS aims to identify neural network\nstructures that are highly suitable for tasks, such as the detection of fruits.\nOur suggested model with 99.98% mAP increased the detection performance of the\npreceding research study that used Fruit datasets. In addition, after the\ncompletion of the study, a comparative analysis was carried out to assess the\nfindings in conjunction with those of another research that is connected to the\ntopic. When compared to the findings of earlier studies, the detector that was\nproposed exhibited higher performance in terms of both its accuracy and its\nprecision.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.01869v1.pdf",
        "similarity": 0.3596878519042162,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-04"
    },
    {
        "new_title": "Learning Decision Trees and Forests with Algorithmic Recourse",
        "new_link": "http://arxiv.org/abs/2406.01098v1",
        "new_summary": "  This paper proposes a new algorithm for learning accurate tree-based models\nwhile ensuring the existence of recourse actions. Algorithmic Recourse (AR)\naims to provide a recourse action for altering the undesired prediction result\ngiven by a model. Typical AR methods provide a reasonable action by solving an\noptimization task of minimizing the required effort among executable actions.\nIn practice, however, such actions do not always exist for models optimized\nonly for predictive performance. To alleviate this issue, we formulate the task\nof learning an accurate classification tree under the constraint of ensuring\nthe existence of reasonable actions for as many instances as possible. Then, we\npropose an efficient top-down greedy algorithm by leveraging the adversarial\ntraining techniques. We also show that our proposed algorithm can be applied to\nthe random forest, which is known as a popular framework for learning tree\nensembles. Experimental results demonstrated that our method successfully\nprovided reasonable actions to more instances than the baselines without\nsignificantly degrading accuracy and computational efficiency.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.01098v1.pdf",
        "similarity": 0.3592284216024835,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-03"
    },
    {
        "new_title": "Conjugate-Gradient-like Based Adaptive Moment Estimation Optimization\n  Algorithm for Deep Learning",
        "new_link": "http://arxiv.org/abs/2404.01714v3",
        "new_summary": "  Training deep neural networks is a challenging task. In order to speed up\ntraining and enhance the performance of deep neural networks, we rectify the\nvanilla conjugate gradient as conjugate-gradient-like and incorporate it into\nthe generic Adam, and thus propose a new optimization algorithm named\nCG-like-Adam for deep learning. Specifically, both the first-order and the\nsecond-order moment estimation of generic Adam are replaced by the\nconjugate-gradient-like. Convergence analysis handles the cases where the\nexponential moving average coefficient of the first-order moment estimation is\nconstant and the first-order moment estimation is unbiased. Numerical\nexperiments show the superiority of the proposed algorithm based on the\nCIFAR10/100 dataset.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.01714v3.pdf",
        "similarity": 0.3592075955018982,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-02"
    },
    {
        "new_title": "Optimized Detection and Classification on GTRSB: Advancing Traffic Sign\n  Recognition with Convolutional Neural Networks",
        "new_link": "http://arxiv.org/abs/2403.08283v1",
        "new_summary": "  In the rapidly evolving landscape of transportation, the proliferation of\nautomobiles has made road traffic more complex, necessitating advanced\nvision-assisted technologies for enhanced safety and navigation. These\ntechnologies are imperative for providing critical traffic sign information,\ninfluencing driver behavior, and supporting vehicle control, especially for\ndrivers with disabilities and in the burgeoning field of autonomous vehicles.\nTraffic sign detection and recognition have emerged as key areas of research\ndue to their essential roles in ensuring road safety and compliance with\ntraffic regulations. Traditional computer vision methods have faced challenges\nin achieving optimal accuracy and speed due to real-world variabilities.\nHowever, the advent of deep learning and Convolutional Neural Networks (CNNs)\nhas revolutionized this domain, offering solutions that significantly surpass\nprevious capabilities in terms of speed and reliability. This paper presents an\ninnovative approach leveraging CNNs that achieves an accuracy of nearly 96\\%,\nhighlighting the potential for even greater precision through advanced\nlocalization techniques. Our findings not only contribute to the ongoing\nadvancement of traffic sign recognition technology but also underscore the\ncritical impact of these developments on road safety and the future of\nautonomous driving.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.08283v1.pdf",
        "similarity": 0.35918888490053813,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-13"
    },
    {
        "new_title": "Tempo: Confidentiality Preservation in Cloud-Based Neural Network\n  Training",
        "new_link": "http://arxiv.org/abs/2401.11531v1",
        "new_summary": "  Cloud deep learning platforms provide cost-effective deep neural network\n(DNN) training for customers who lack computation resources. However, cloud\nsystems are often untrustworthy and vulnerable to attackers, leading to growing\nconcerns about model privacy. Recently, researchers have sought to protect data\nprivacy in deep learning by leveraging CPU trusted execution environments\n(TEEs), which minimize the use of cryptography, but existing works failed to\nsimultaneously utilize the computational resources of GPUs to assist in\ntraining and prevent model leakage. This paper presents Tempo, the first\ncloud-based deep learning system that cooperates with TEE and distributed GPUs\nfor efficient DNN training with model confidentiality preserved. To tackle the\nchallenge of preserving privacy while offloading linear algebraic operations\nfrom TEE to GPUs for efficient batch computation, we introduce a customized\npermutation-based obfuscation algorithm to blind both inputs and model\nparameters. An optimization mechanism that reduces encryption operations is\nproposed for faster weight updates during backpropagation to speed up training.\nWe implement Tempo and evaluate it with both training and inference for two\nprevalent DNNs. Empirical results indicate that Tempo outperforms baselines and\noffers sufficient privacy protection.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.11531v1.pdf",
        "similarity": 0.35916402117821433,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-21"
    },
    {
        "new_title": "Deep Generative Models for Offline Policy Learning: Tutorial, Survey,\n  and Perspectives on Future Directions",
        "new_link": "http://arxiv.org/abs/2402.13777v5",
        "new_summary": "  Deep generative models (DGMs) have demonstrated great success across various\ndomains, particularly in generating texts, images, and videos using models\ntrained from offline data. Similarly, data-driven decision-making and robotic\ncontrol also necessitate learning a generator function from the offline data to\nserve as the strategy or policy. In this case, applying deep generative models\nin offline policy learning exhibits great potential, and numerous studies have\nexplored in this direction. However, this field still lacks a comprehensive\nreview and so developments of different branches are relatively independent. In\nthis paper, we provide the first systematic review on the applications of deep\ngenerative models for offline policy learning. In particular, we cover five\nmainstream deep generative models, including Variational Auto-Encoders,\nGenerative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion\nModels, and their applications in both offline reinforcement learning (offline\nRL) and imitation learning (IL). Offline RL and IL are two main branches of\noffline policy learning and are widely-adopted techniques for sequential\ndecision-making. Notably, for each type of DGM-based offline policy learning,\nwe distill its fundamental scheme, categorize related works based on the usage\nof the DGM, and sort out the development process of algorithms in that field.\nSubsequent to the main content, we provide in-depth discussions on deep\ngenerative models and offline policy learning as a summary, based on which we\npresent our perspectives on future research directions. This work offers a\nhands-on reference for the research progress in deep generative models for\noffline policy learning, and aims to inspire improved DGM-based offline RL or\nIL algorithms. For convenience, we maintain a paper list on\nhttps://github.com/LucasCJYSDL/DGMs-for-Offline-Policy-Learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.13777v5.pdf",
        "similarity": 0.3591529602121843,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-21"
    },
    {
        "new_title": "Professional Insights into Benefits and Limitations of Implementing\n  MLOps Principles",
        "new_link": "http://arxiv.org/abs/2403.13115v1",
        "new_summary": "  Context: Machine Learning Operations (MLOps) has emerged as a set of\npractices that combines development, testing, and operations to deploy and\nmaintain machine learning applications. Objective: In this paper, we assess the\nbenefits and limitations of using the MLOps principles in online supervised\nlearning. Method: We conducted two focus group sessions on the benefits and\nlimitations of applying MLOps principles for online machine learning\napplications with six experienced machine learning developers. Results: The\nfocus group revealed that machine learning developers see many benefits of\nusing MLOps principles but also that these do not apply to all the projects\nthey worked on. According to experts, this investment tends to pay off for\nlarger applications with continuous deployment that require well-prepared\nautomated processes. However, for initial versions of machine learning\napplications, the effort taken to implement the principles could enlarge the\nproject's scope and increase the time needed to deploy a first version to\nproduction. The discussion brought up that most of the benefits are related to\navoiding error-prone manual steps, enabling to restore the application to a\nprevious state, and having a robust continuous automated deployment pipeline.\nConclusions: It is important to balance the trade-offs of investing time and\neffort in implementing the MLOps principles considering the scope and needs of\nthe project, favoring such investments for larger applications with continuous\nmodel deployment requirements.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.13115v1.pdf",
        "similarity": 0.3591060050693529,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-19"
    },
    {
        "new_title": "A Simple Attention-Based Mechanism for Bimodal Emotion Classification",
        "new_link": "http://arxiv.org/abs/2407.00134v1",
        "new_summary": "  Big data contain rich information for machine learning algorithms to utilize\nwhen learning important features during classification tasks. Human beings\nexpress their emotion using certain words, speech (tone, pitch, speed) or\nfacial expression. Artificial Intelligence approach to emotion classification\nare largely based on learning from textual information. However, public\ndatasets containing text and speech data provide sufficient resources to train\nmachine learning algorithms for the tack of emotion classification. In this\npaper, we present novel bimodal deep learning-based architectures enhanced with\nattention mechanism trained and tested on text and speech data for emotion\nclassification. We report details of different deep learning based\narchitectures and show the performance of each architecture including rigorous\nerror analyses. Our finding suggests that deep learning based architectures\ntrained on different types of data (text and speech) outperform architectures\ntrained only on text or speech. Our proposed attention-based bimodal\narchitecture outperforms several state-of-the-art systems in emotion\nclassification.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.00134v1.pdf",
        "similarity": 0.35909076438263193,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-28"
    },
    {
        "new_title": "AudioProtoPNet: An interpretable deep learning model for bird sound\n  classification",
        "new_link": "http://arxiv.org/abs/2404.10420v2",
        "new_summary": "  Recently, scientists have proposed several deep learning models to monitor\nthe diversity of bird species. These models can detect bird species with high\naccuracy by analyzing acoustic signals. However, traditional deep learning\nalgorithms are black-box models that provide no insight into their\ndecision-making process. For domain experts, such as ornithologists, it is\ncrucial that these models are not only efficient, but also interpretable in\norder to be used as assistive tools. In this study, we present an adaption of\nthe Prototypical Part Network (ProtoPNet) for audio classification that\nprovides inherent interpretability through its model architecture. Our approach\nis based on a ConvNeXt backbone architecture for feature extraction and learns\nprototypical patterns for each bird species using spectrograms of the training\ndata. Classification of new data is done by comparison with these prototypes in\nlatent space, which simultaneously serve as easily understandable explanations\nfor the model's decisions. We evaluated the performance of our model on seven\ndifferent datasets representing bird species from different geographical\nregions. In our experiments, the model showed excellent results, achieving an\naverage AUROC of 0.82 and an average cmAP of 0.37 across the seven datasets,\nmaking it comparable to state-of-the-art black-box models for bird sound\nclassification. Thus, this work demonstrates that even for the challenging task\nof bioacoustic bird classification, powerful yet interpretable deep learning\nmodels can be developed to provide valuable insights to domain experts.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.10420v2.pdf",
        "similarity": 0.3590685704867292,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-16"
    },
    {
        "new_title": "Interpretable Machine Learning for Weather and Climate Prediction: A\n  Survey",
        "new_link": "http://arxiv.org/abs/2403.18864v1",
        "new_summary": "  Advanced machine learning models have recently achieved high predictive\naccuracy for weather and climate prediction. However, these complex models\noften lack inherent transparency and interpretability, acting as \"black boxes\"\nthat impede user trust and hinder further model improvements. As such,\ninterpretable machine learning techniques have become crucial in enhancing the\ncredibility and utility of weather and climate modeling. In this survey, we\nreview current interpretable machine learning approaches applied to\nmeteorological predictions. We categorize methods into two major paradigms: 1)\nPost-hoc interpretability techniques that explain pre-trained models, such as\nperturbation-based, game theory based, and gradient-based attribution methods.\n2) Designing inherently interpretable models from scratch using architectures\nlike tree ensembles and explainable neural networks. We summarize how each\ntechnique provides insights into the predictions, uncovering novel\nmeteorological relationships captured by machine learning. Lastly, we discuss\nresearch challenges around achieving deeper mechanistic interpretations aligned\nwith physical principles, developing standardized evaluation benchmarks,\nintegrating interpretability into iterative model development workflows, and\nproviding explainability for large foundation models.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.18864v1.pdf",
        "similarity": 0.3589383170256817,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-24"
    },
    {
        "new_title": "Reducing Spatial Discretization Error on Coarse CFD Simulations Using an\n  OpenFOAM-Embedded Deep Learning Framework",
        "new_link": "http://arxiv.org/abs/2405.07441v2",
        "new_summary": "  We propose a method for reducing the spatial discretization error of coarse\ncomputational fluid dynamics (CFD) problems by enhancing the quality of\nlow-resolution simulations using a deep learning model fed with high-quality\ndata. We substitute the default differencing scheme for the convection term by\na feed-forward neural network that interpolates velocities from cell centers to\nface values to produce velocities that approximate the fine-mesh data well. The\ndeep learning framework incorporates the open-source CFD code OpenFOAM,\nresulting in an end-to-end differentiable model. We automatically differentiate\nthe CFD physics using a discrete adjoint code version. We present a fast\ncommunication method between TensorFlow (Python) and OpenFOAM (c++) that\naccelerates the training process. We applied the model to the flow past a\nsquare cylinder problem, reducing the error to about 50% for simulations\noutside the training distribution compared to the traditional solver in the x-\nand y-velocity components using an 8x coarser mesh. The training is affordable\nin terms of time and data samples since the architecture exploits the local\nfeatures of the physics while generating stable predictions for mid-term\nsimulations.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.07441v2.pdf",
        "similarity": 0.35890969122354655,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-13"
    },
    {
        "new_title": "Learning to better see the unseen: Broad-Deep Mixed Anti-Forgetting\n  Framework for Incremental Zero-Shot Fault Diagnosis",
        "new_link": "http://arxiv.org/abs/2403.13845v1",
        "new_summary": "  Zero-shot fault diagnosis (ZSFD) is capable of identifying unseen faults via\npredicting fault attributes labeled by human experts. We first recognize the\ndemand of ZSFD to deal with continuous changes in industrial processes, i.e.,\nthe model's ability to adapt to new fault categories and attributes while\navoiding forgetting the diagnosis ability learned previously. To overcome the\nissue that the existing ZSFD paradigm cannot learn from evolving streams of\ntraining data in industrial scenarios, the incremental ZSFD (IZSFD) paradigm is\nproposed for the first time, which incorporates category increment and\nattribute increment for both traditional ZSFD and generalized ZSFD paradigms.\nTo achieve IZSFD, we present a broad-deep mixed anti-forgetting framework\n(BDMAFF) that aims to learn from new fault categories and attributes. To tackle\nthe issue of forgetting, BDMAFF effectively accumulates previously acquired\nknowledge from two perspectives: features and attribute prototypes. The feature\nmemory is established through a deep generative model that employs\nanti-forgetting training strategies, ensuring the generation quality of\nhistorical categories is supervised and maintained. The diagnosis model SEEs\nthe UNSEEN faults with the help of generated samples from the generative model.\nThe attribute prototype memory is established through a diagnosis model\ninspired by the broad learning system. Unlike traditional incremental learning\nalgorithms, BDMAFF introduces a memory-driven iterative update strategy for the\ndiagnosis model, which allows the model to learn new faults and attributes\nwithout requiring the storage of all historical training samples. The\neffectiveness of the proposed method is verified by a real hydraulic system and\nthe Tennessee-Eastman benchmark process.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.13845v1.pdf",
        "similarity": 0.35848570636720556,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-18"
    },
    {
        "new_title": "Deep Learning Model for Detecting Abnormal Corn Kernels",
        "new_link": "http://arxiv.org/abs/2405.19628v1",
        "new_summary": "  This research aims to detect the physical characteristics of corn kernels and\nanalyze images using a deep learning model. The data analysis based on the\nCRISP-DM framework which consists of six steps, business understanding, data\nunderstanding, data preparation, modelling, evaluation, and deployment. The\nbusiness goal reduces the cost of the separation of abnormal corn kernels. The\ndataset comprises 1,800 images of corn kernels and divided equally between\nnormal and abnormal corn kernels. The dataset was divided into three subsets:\n1,000 images for training the deep learning model, 600 images for validation\nand 200 images for testing. The tools for analysis in this research are Jupyter\nLab, Python, TensorFlow Keras, and Convolutional Neural Networks. The results\nrevealed that the deep learning model achieved the accuracy rate of 99% in\ndifferentiating between normal and abnormal corn kernel images that is a highly\neffective model in this context.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.19628v1.pdf",
        "similarity": 0.35828916193315574,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-30"
    },
    {
        "new_title": "Why Do Animals Need Shaping? A Theory of Task Composition and Curriculum\n  Learning",
        "new_link": "http://arxiv.org/abs/2402.18361v2",
        "new_summary": "  Diverse studies in systems neuroscience begin with extended periods of\ncurriculum training known as `shaping' procedures. These involve progressively\nstudying component parts of more complex tasks, and can make the difference\nbetween learning a task quickly, slowly or not at all. Despite the importance\nof shaping to the acquisition of complex tasks, there is as yet no theory that\ncan help guide the design of shaping procedures, or more fundamentally, provide\ninsight into its key role in learning. Modern deep reinforcement learning\nsystems might implicitly learn compositional primitives within their multilayer\npolicy networks. Inspired by these models, we propose and analyse a model of\ndeep policy gradient learning of simple compositional reinforcement learning\ntasks. Using the tools of statistical physics, we solve for exact learning\ndynamics and characterise different learning strategies including primitives\npre-training, in which task primitives are studied individually before learning\ncompositional tasks. We find a complex interplay between task complexity and\nthe efficacy of shaping strategies. Overall, our theory provides an analytical\nunderstanding of the benefits of shaping in a class of compositional tasks and\na quantitative account of how training protocols can disclose useful task\nprimitives, ultimately yielding faster and more robust learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.18361v2.pdf",
        "similarity": 0.3582602713672563,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-28"
    },
    {
        "new_title": "A comprehensive overview of deep learning techniques for 3D point cloud\n  classification and semantic segmentation",
        "new_link": "http://arxiv.org/abs/2405.11903v1",
        "new_summary": "  Point cloud analysis has a wide range of applications in many areas such as\ncomputer vision, robotic manipulation, and autonomous driving. While deep\nlearning has achieved remarkable success on image-based tasks, there are many\nunique challenges faced by deep neural networks in processing massive,\nunordered, irregular and noisy 3D points. To stimulate future research, this\npaper analyzes recent progress in deep learning methods employed for point\ncloud processing and presents challenges and potential directions to advance\nthis field. It serves as a comprehensive review on two major tasks in 3D point\ncloud processing-- namely, 3D shape classification and semantic segmentation.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.11903v1.pdf",
        "similarity": 0.3582478078117791,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-20"
    },
    {
        "new_title": "Deep Learning-Based Detection for Marker Codes over Insertion and\n  Deletion Channels",
        "new_link": "http://arxiv.org/abs/2401.01155v1",
        "new_summary": "  Marker code is an effective coding scheme to protect data from insertions and\ndeletions. It has potential applications in future storage systems, such as DNA\nstorage and racetrack memory. When decoding marker codes, perfect channel state\ninformation (CSI), i.e., insertion and deletion probabilities, are required to\ndetect insertion and deletion errors. Sometimes, the perfect CSI is not easy to\nobtain or the accurate channel model is unknown. Therefore, it is deserved to\ndevelop detecting algorithms for marker code without the knowledge of perfect\nCSI. In this paper, we propose two CSI-agnostic detecting algorithms for marker\ncode based on deep learning. The first one is a model-driven deep learning\nmethod, which deep unfolds the original iterative detecting algorithm of marker\ncode. In this method, CSI become weights in neural networks and these weights\ncan be learned from training data. The second one is a data-driven method which\nis an end-to-end system based on the deep bidirectional gated recurrent unit\nnetwork. Simulation results show that error performances of the proposed\nmethods are significantly better than that of the original detection algorithm\nwith CSI uncertainty. Furthermore, the proposed data-driven method exhibits\nbetter error performances than other methods for unknown channel models.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.01155v1.pdf",
        "similarity": 0.35809144904601103,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-02"
    },
    {
        "new_title": "Reassessing How to Compare and Improve the Calibration of Machine\n  Learning Models",
        "new_link": "http://arxiv.org/abs/2406.04068v1",
        "new_summary": "  A machine learning model is calibrated if its predicted probability for an\noutcome matches the observed frequency for that outcome conditional on the\nmodel prediction. This property has become increasingly important as the impact\nof machine learning models has continued to spread to various domains. As a\nresult, there are now a dizzying number of recent papers on measuring and\nimproving the calibration of (specifically deep learning) models. In this work,\nwe reassess the reporting of calibration metrics in the recent literature. We\nshow that there exist trivial recalibration approaches that can appear\nseemingly state-of-the-art unless calibration and prediction metrics (i.e. test\naccuracy) are accompanied by additional generalization metrics such as negative\nlog-likelihood. We then derive a calibration-based decomposition of Bregman\ndivergences that can be used to both motivate a choice of calibration metric\nbased on a generalization metric, and to detect trivial calibration. Finally,\nwe apply these ideas to develop a new extension to reliability diagrams that\ncan be used to jointly visualize calibration as well as the estimated\ngeneralization error of a model.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.04068v1.pdf",
        "similarity": 0.3579946451920373,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-06"
    },
    {
        "new_title": "Interpretable Data Fusion for Distributed Learning: A Representative\n  Approach via Gradient Matching",
        "new_link": "http://arxiv.org/abs/2405.03782v1",
        "new_summary": "  This paper introduces a representative-based approach for distributed\nlearning that transforms multiple raw data points into a virtual\nrepresentation. Unlike traditional distributed learning methods such as\nFederated Learning, which do not offer human interpretability, our method makes\ncomplex machine learning processes accessible and comprehensible. It achieves\nthis by condensing extensive datasets into digestible formats, thus fostering\nintuitive human-machine interactions. Additionally, this approach maintains\nprivacy and communication efficiency, and it matches the training performance\nof models using raw data. Simulation results show that our approach is\ncompetitive with or outperforms traditional Federated Learning in accuracy and\nconvergence, especially in scenarios with complex models and a higher number of\nclients. This framework marks a step forward in integrating human intuition\nwith machine intelligence, which potentially enhances human-machine learning\ninterfaces and collaborative efforts.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.03782v1.pdf",
        "similarity": 0.357412279886629,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-06"
    },
    {
        "new_title": "SurvReLU: Inherently Interpretable Survival Analysis via Deep ReLU\n  Networks",
        "new_link": "http://arxiv.org/abs/2407.14463v1",
        "new_summary": "  Survival analysis models time-to-event distributions with censorship.\nRecently, deep survival models using neural networks have dominated due to\ntheir representational power and state-of-the-art performance. However, their\n\"black-box\" nature hinders interpretability, which is crucial in real-world\napplications. In contrast, \"white-box\" tree-based survival models offer better\ninterpretability but struggle to converge to global optima due to greedy\nexpansion. In this paper, we bridge the gap between previous deep survival\nmodels and traditional tree-based survival models through deep rectified linear\nunit (ReLU) networks. We show that a deliberately constructed deep ReLU network\n(SurvReLU) can harness the interpretability of tree-based structures with the\nrepresentational power of deep survival models. Empirical studies on both\nsimulated and real survival benchmark datasets show the effectiveness of the\nproposed SurvReLU in terms of performance and interoperability. The code is\navailable at \\href{https://github.com/xs018/SurvReLU}{\\color{magenta}{\nhttps://github.com/xs018/SurvReLU}}.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.14463v1.pdf",
        "similarity": 0.35722404641756733,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-19"
    },
    {
        "new_title": "NSD-DIL: Null-Shot Deblurring Using Deep Identity Learning",
        "new_link": "http://arxiv.org/abs/2407.04815v1",
        "new_summary": "  In this paper, we propose to reformulate the blind image deblurring task to\ndirectly learn an inverse of the degradation model using a deep linear network.\nWe introduce Deep Identity Learning (DIL), a novel learning strategy that\nincludes a dedicated regularization term based on the properties of linear\nsystems, to exploit the identity relation between the degradation and inverse\ndegradation models. The salient aspect of our proposed framework is it neither\nrelies on a deblurring dataset nor a single input blurred image (like Polyblur,\na self-supervised method). Since it is purely image-data-independent, we term\nour model as Null-Shot deblurring Using Deep Identity Learning (NSD-DIL). We\nalso provide an explicit representation of the learned deep linear network in a\nmatrix form, called Deep Restoration Kernel (DRK) for deblurring task. The\nproposed framework detours the typical degradation kernel estimation step\ninvolved in most of the existing blind deblurring solutions by the proposition\nof our Random Kernel Gallery (RKG) dataset. In this work, we focus on the\nrestoration of mild blur images, generated by small out-of-focus, lens blur, or\nslight camera motion, which often occurs in real images. Our experiments show\nthat the proposed method outperforms both traditional and deep learning based\ndeblurring methods, with at least an order of 100 lesser computational\nresources. The proposed NSD-DIL method can be effortlessly extended to the\nImage Super-Resolution (ISR) task as well to restore the low-resolution images\nwith fine details. The NSD-DIL model and its kernel form representation (DRK)\nare lightweight yet robust and restore the mild blur input in a fraction of a\nsecond. Hence, more suitable for wide real-time applications.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.04815v1.pdf",
        "similarity": 0.35677976499436403,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-05"
    },
    {
        "new_title": "Infusing Self-Consistency into Density Functional Theory Hamiltonian\n  Prediction via Deep Equilibrium Models",
        "new_link": "http://arxiv.org/abs/2406.03794v1",
        "new_summary": "  In this study, we introduce a unified neural network architecture, the Deep\nEquilibrium Density Functional Theory Hamiltonian (DEQH) model, which\nincorporates Deep Equilibrium Models (DEQs) for predicting Density Functional\nTheory (DFT) Hamiltonians. The DEQH model inherently captures the\nself-consistency nature of Hamiltonian, a critical aspect often overlooked by\ntraditional machine learning approaches for Hamiltonian prediction. By\nemploying DEQ within our model architecture, we circumvent the need for DFT\ncalculations during the training phase to introduce the Hamiltonian's\nself-consistency, thus addressing computational bottlenecks associated with\nlarge or complex systems. We propose a versatile framework that combines DEQ\nwith off-the-shelf machine learning models for predicting Hamiltonians. When\nbenchmarked on the MD17 and QH9 datasets, DEQHNet, an instantiation of the DEQH\nframework, has demonstrated a significant improvement in prediction accuracy.\nBeyond a predictor, the DEQH model is a Hamiltonian solver, in the sense that\nit uses the fixed-point solving capability of the deep equilibrium model to\niteratively solve for the Hamiltonian. Ablation studies of DEQHNet further\nelucidate the network's effectiveness, offering insights into the potential of\nDEQ-integrated networks for Hamiltonian learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.03794v1.pdf",
        "similarity": 0.3567559652373484,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-06"
    },
    {
        "new_title": "Diffusion-Reward Adversarial Imitation Learning",
        "new_link": "http://arxiv.org/abs/2405.16194v1",
        "new_summary": "  Imitation learning aims to learn a policy from observing expert\ndemonstrations without access to reward signals from environments. Generative\nadversarial imitation learning (GAIL) formulates imitation learning as\nadversarial learning, employing a generator policy learning to imitate expert\nbehaviors and discriminator learning to distinguish the expert demonstrations\nfrom agent trajectories. Despite its encouraging results, GAIL training is\noften brittle and unstable. Inspired by the recent dominance of diffusion\nmodels in generative modeling, this work proposes Diffusion-Reward Adversarial\nImitation Learning (DRAIL), which integrates a diffusion model into GAIL,\naiming to yield more precise and smoother rewards for policy learning.\nSpecifically, we propose a diffusion discriminative classifier to construct an\nenhanced discriminator; then, we design diffusion rewards based on the\nclassifier's output for policy learning. We conduct extensive experiments in\nnavigation, manipulation, and locomotion, verifying DRAIL's effectiveness\ncompared to prior imitation learning methods. Moreover, additional experimental\nresults demonstrate the generalizability and data efficiency of DRAIL.\nVisualized learned reward functions of GAIL and DRAIL suggest that DRAIL can\nproduce more precise and smoother rewards.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.16194v1.pdf",
        "similarity": 0.35626226032165814,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-25"
    },
    {
        "new_title": "On the Convergence of Continual Learning with Adaptive Methods",
        "new_link": "http://arxiv.org/abs/2404.05555v2",
        "new_summary": "  One of the objectives of continual learning is to prevent catastrophic\nforgetting in learning multiple tasks sequentially, and the existing solutions\nhave been driven by the conceptualization of the plasticity-stability dilemma.\nHowever, the convergence of continual learning for each sequential task is less\nstudied so far. In this paper, we provide a convergence analysis of\nmemory-based continual learning with stochastic gradient descent and empirical\nevidence that training current tasks causes the cumulative degradation of\nprevious tasks. We propose an adaptive method for nonconvex continual learning\n(NCCL), which adjusts step sizes of both previous and current tasks with the\ngradients. The proposed method can achieve the same convergence rate as the SGD\nmethod when the catastrophic forgetting term which we define in the paper is\nsuppressed at each iteration. Further, we demonstrate that the proposed\nalgorithm improves the performance of continual learning over existing methods\nfor several image classification tasks.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.05555v2.pdf",
        "similarity": 0.35530342652981645,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-08"
    },
    {
        "new_title": "End-to-End Mesh Optimization of a Hybrid Deep Learning Black-Box PDE\n  Solver",
        "new_link": "http://arxiv.org/abs/2404.11766v2",
        "new_summary": "  Deep learning has been widely applied to solve partial differential equations\n(PDEs) in computational fluid dynamics. Recent research proposed a PDE\ncorrection framework that leverages deep learning to correct the solution\nobtained by a PDE solver on a coarse mesh. However, end-to-end training of such\na PDE correction model over both solver-dependent parameters such as mesh\nparameters and neural network parameters requires the PDE solver to support\nautomatic differentiation through the iterative numerical process. Such a\nfeature is not readily available in many existing solvers. In this study, we\nexplore the feasibility of end-to-end training of a hybrid model with a\nblack-box PDE solver and a deep learning model for fluid flow prediction.\nSpecifically, we investigate a hybrid model that integrates a black-box PDE\nsolver into a differentiable deep graph neural network. To train this model, we\nuse a zeroth-order gradient estimator to differentiate the PDE solver via\nforward propagation. Although experiments show that the proposed approach based\non zeroth-order gradient estimation underperforms the baseline that computes\nexact derivatives using automatic differentiation, our proposed method\noutperforms the baseline trained with a frozen input mesh to the solver.\nMoreover, with a simple warm-start on the neural network parameters, we show\nthat models trained by these zeroth-order algorithms achieve an accelerated\nconvergence and improved generalization performance.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.11766v2.pdf",
        "similarity": 0.35519324036029126,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-17"
    },
    {
        "new_title": "Generating Galaxy Clusters Mass Density Maps from Mock Multiview Images\n  via Deep Learning",
        "new_link": "http://arxiv.org/abs/2404.05400v2",
        "new_summary": "  Galaxy clusters are composed of dark matter, gas and stars. Their dark matter\ncomponent, which amounts to around 80\\% of the total mass, cannot be directly\nobserved but traced by the distribution of diffused gas and galaxy members. In\nthis work, we aim to infer the cluster's projected total mass distribution from\nmock observational data, i.e. stars, Sunyaev-Zeldovich, and X-ray, by training\ndeep learning models. To this end, we have created a multiview images dataset\nfrom {\\sc{The Three Hundred}} simulation that is optimal for training Machine\nLearning models. We further study deep learning architectures based on the\nU-Net to account for single-input and multi-input models. We show that the\npredicted mass distribution agrees well with the true one.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.05400v2.pdf",
        "similarity": 0.3549346651565266,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-08"
    },
    {
        "new_title": "Machine Learning a Universal Harmonic Interatomic Potential for\n  Predicting Phonons in Crystalline Solids",
        "new_link": "http://arxiv.org/abs/2402.11383v1",
        "new_summary": "  Phonons, as quantized vibrational modes in crystalline materials, play a\ncrucial role in determining a wide range of physical properties, such as\nthermal and electrical conductivity, making their study a cornerstone in\nmaterials science. In this study, we present a simple yet effective strategy\nfor deep learning harmonic phonons in crystalline solids by leveraging existing\nphonon databases and state-of-the-art machine learning techniques. The key of\nour method lies in transforming existing phonon datasets, primarily represented\nin interatomic force constants, into a force-displacement representation\nsuitable for training machine learning universal interatomic potentials. By\napplying our approach to one of the largest phonon databases publicly\navailable, we demonstrate that the resultant machine learning universal\nharmonic interatomic potential not only accurately predicts full harmonic\nphonon spectra but also calculates key thermodynamic properties with remarkable\nprecision. Furthermore, the restriction to a harmonic potential energy surface\nin our model provides a way of assessing uncertainty in machine learning\npredictions of vibrational properties, essential for guiding further\nimprovements and applications in materials science.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.11383v1.pdf",
        "similarity": 0.3549062953628965,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-17"
    },
    {
        "new_title": "The Dimension of Self-Directed Learning",
        "new_link": "http://arxiv.org/abs/2402.13400v1",
        "new_summary": "  Understanding the self-directed learning complexity has been an important\nproblem that has captured the attention of the online learning theory community\nsince the early 1990s. Within this framework, the learner is allowed to\nadaptively choose its next data point in making predictions unlike the setting\nin adversarial online learning.\n  In this paper, we study the self-directed learning complexity in both the\nbinary and multi-class settings, and we develop a dimension, namely $SDdim$,\nthat exactly characterizes the self-directed learning mistake-bound for any\nconcept class. The intuition behind $SDdim$ can be understood as a two-player\ngame called the \"labelling game\". Armed with this two-player game, we calculate\n$SDdim$ on a whole host of examples with notable results on axis-aligned\nrectangles, VC dimension $1$ classes, and linear separators. We demonstrate\nseveral learnability gaps with a central focus on self-directed learning and\noffline sequence learning models that include either the best or worst\nordering. Finally, we extend our analysis to the self-directed binary agnostic\nsetting where we derive upper and lower bounds.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.13400v1.pdf",
        "similarity": 0.35487643202609465,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-20"
    },
    {
        "new_title": "Reproducibility and Geometric Intrinsic Dimensionality: An Investigation\n  on Graph Neural Network Research",
        "new_link": "http://arxiv.org/abs/2403.08438v2",
        "new_summary": "  Difficulties in replication and reproducibility of empirical evidences in\nmachine learning research have become a prominent topic in recent years.\nEnsuring that machine learning research results are sound and reliable requires\nreproducibility, which verifies the reliability of research findings using the\nsame code and data. This promotes open and accessible research, robust\nexperimental workflows, and the rapid integration of new findings. Evaluating\nthe degree to which research publications support these different aspects of\nreproducibility is one goal of the present work. For this we introduce an\nontology of reproducibility in machine learning and apply it to methods for\ngraph neural networks. Building on these efforts we turn towards another\ncritical challenge in machine learning, namely the curse of dimensionality,\nwhich poses challenges in data collection, representation, and analysis, making\nit harder to find representative data and impeding the training and inference\nprocesses. Using the closely linked concept of geometric intrinsic dimension we\ninvestigate to which extend the used machine learning models are influenced by\nthe intrinsic dimension of the data sets they are trained on.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.08438v2.pdf",
        "similarity": 0.35451439980819616,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-13"
    },
    {
        "new_title": "Review of Deep Representation Learning Techniques for Brain-Computer\n  Interfaces and Recommendations",
        "new_link": "http://arxiv.org/abs/2405.19345v1",
        "new_summary": "  In the field of brain-computer interfaces (BCIs), the potential for\nleveraging deep learning techniques for representing electroencephalogram (EEG)\nsignals has gained substantial interest. This review synthesizes empirical\nfindings from a collection of articles using deep representation learning\ntechniques for BCI decoding, to provide a comprehensive analysis of the current\nstate-of-the-art. Each article was scrutinized based on three criteria: (1) the\ndeep representation learning technique employed, (2) the underlying motivation\nfor its utilization, and (3) the approaches adopted for characterizing the\nlearned representations. Among the 81 articles finally reviewed in depth, our\nanalysis reveals a predominance of 31 articles using autoencoders. We\nidentified 13 studies employing self-supervised learning (SSL) techniques,\namong which ten were published in 2022 or later, attesting to the relative\nyouth of the field. However, at the time being, none of these have led to\nstandard foundation models that are picked up by the BCI community. Likewise,\nonly a few studies have introspected their learned representations. We observed\nthat the motivation in most studies for using representation learning\ntechniques is for solving transfer learning tasks, but we also found more\nspecific motivations such as to learn robustness or invariances, as an\nalgorithmic bridge, or finally to uncover the structure of the data. Given the\npotential of foundation models to effectively tackle these challenges, we\nadvocate for a continued dedication to the advancement of foundation models\nspecifically designed for EEG signal decoding by using SSL techniques. We also\nunderline the imperative of establishing specialized benchmarks and datasets to\nfacilitate the development and continuous improvement of such foundation\nmodels.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.19345v1.pdf",
        "similarity": 0.35451430809039,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-17"
    },
    {
        "new_title": "Review of multimodal machine learning approaches in healthcare",
        "new_link": "http://arxiv.org/abs/2402.02460v2",
        "new_summary": "  Machine learning methods in healthcare have traditionally focused on using\ndata from a single modality, limiting their ability to effectively replicate\nthe clinical practice of integrating multiple sources of information for\nimproved decision making. Clinicians typically rely on a variety of data\nsources including patients' demographic information, laboratory data, vital\nsigns and various imaging data modalities to make informed decisions and\ncontextualise their findings. Recent advances in machine learning have\nfacilitated the more efficient incorporation of multimodal data, resulting in\napplications that better represent the clinician's approach. Here, we provide a\nreview of multimodal machine learning approaches in healthcare, offering a\ncomprehensive overview of recent literature. We discuss the various data\nmodalities used in clinical diagnosis, with a particular emphasis on imaging\ndata. We evaluate fusion techniques, explore existing multimodal datasets and\nexamine common training strategies.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.02460v2.pdf",
        "similarity": 0.35446411486096996,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-04"
    },
    {
        "new_title": "Continual Deep Learning on the Edge via Stochastic Local Competition\n  among Subnetworks",
        "new_link": "http://arxiv.org/abs/2407.10758v1",
        "new_summary": "  Continual learning on edge devices poses unique challenges due to stringent\nresource constraints. This paper introduces a novel method that leverages\nstochastic competition principles to promote sparsity, significantly reducing\ndeep network memory footprint and computational demand. Specifically, we\npropose deep networks that comprise blocks of units that compete locally to win\nthe representation of each arising new task; competition takes place in a\nstochastic manner. This type of network organization results in sparse\ntask-specific representations from each network layer; the sparsity pattern is\nobtained during training and is different among tasks. Crucially, our method\nsparsifies both the weights and the weight gradients, thus facilitating\ntraining on edge devices. This is performed on the grounds of winning\nprobability for each unit in a block. During inference, the network retains\nonly the winning unit and zeroes-out all weights pertaining to non-winning\nunits for the task at hand. Thus, our approach is specifically tailored for\ndeployment on edge devices, providing an efficient and scalable solution for\ncontinual learning in resource-limited environments.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.10758v1.pdf",
        "similarity": 0.3544627966860891,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-15"
    },
    {
        "new_title": "Enhancing Cardiovascular Disease Risk Prediction with Machine Learning\n  Models",
        "new_link": "http://arxiv.org/abs/2401.17328v3",
        "new_summary": "  Cardiovascular disease remains a leading global cause of mortality,\nnecessitating accurate risk prediction tools. Traditional methods, such as\nQRISK and the Framingham heart score, exhibit limitations in their ability to\nincorporate comprehensive patient data, potentially resulting in incomplete\nrisk factor consideration. To address these shortcomings, this study conducts a\nmeticulous review focusing on the application of machine learning models to\nenhance predictive accuracy. Machine learning models, such as support vector\nmachines, and Random Forest, as well as deep learning techniques like\nconvolutional neural networks and recurrent neural networks, have emerged as\npromising alternatives. These models offer superior performance, accommodating\na broader spectrum of variables and providing precise subgroup-specific\npredictions. While machine learning integration holds promise for enhancing\nrisk assessment, it presents challenges such as data requirements and\ncomputational constraints. Additionally, large language models have\nrevolutionised healthcare applications, augmenting diagnostic precision and\npatient care. This study examines the core aspects of cardiovascular disease\nevent risk and presents a thorough review of traditional and machine learning\nmodels, alongside deep learning techniques, for improved accuracy. It offers a\ncomprehensive survey of relevant datasets, critically compares ML models with\nconventional approaches, and synthesizes key findings, highlighting their\nimplications for clinical practice. Furthermore, the potential of machine\nlearning and large language models in cardiovascular medicine is undeniable.\nHowever, rigorous validation and optimisation are imperative before widespread\napplication in healthcare. This integration promises more accurate and\npersonalised cardiovascular care.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.17328v3.pdf",
        "similarity": 0.35445517976246455,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-29"
    },
    {
        "new_title": "Machine learning for industrial sensing and control: A survey and\n  practical perspective",
        "new_link": "http://arxiv.org/abs/2401.13836v1",
        "new_summary": "  With the rise of deep learning, there has been renewed interest within the\nprocess industries to utilize data on large-scale nonlinear sensing and control\nproblems. We identify key statistical and machine learning techniques that have\nseen practical success in the process industries. To do so, we start with\nhybrid modeling to provide a methodological framework underlying core\napplication areas: soft sensing, process optimization, and control. Soft\nsensing contains a wealth of industrial applications of statistical and machine\nlearning methods. We quantitatively identify research trends, allowing insight\ninto the most successful techniques in practice.\n  We consider two distinct flavors for data-driven optimization and control:\nhybrid modeling in conjunction with mathematical programming techniques and\nreinforcement learning. Throughout these application areas, we discuss their\nrespective industrial requirements and challenges.\n  A common challenge is the interpretability and efficiency of purely\ndata-driven methods. This suggests a need to carefully balance deep learning\ntechniques with domain knowledge. As a result, we highlight ways prior\nknowledge may be integrated into industrial machine learning applications. The\ntreatment of methods, problems, and applications presented here is poised to\ninform and inspire practitioners and researchers to develop impactful\ndata-driven sensing, optimization, and control solutions in the process\nindustries.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.13836v1.pdf",
        "similarity": 0.3541654801072092,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-24"
    },
    {
        "new_title": "Consistency Enhancement-Based Deep Multiview Clustering via Contrastive\n  Learning",
        "new_link": "http://arxiv.org/abs/2401.12648v3",
        "new_summary": "  Multiview clustering (MVC) segregates data samples into meaningful clusters\nby synthesizing information across multiple views. Moreover, deep\nlearning-based methods have demonstrated their strong feature learning\ncapabilities in MVC scenarios. However, effectively generalizing feature\nrepresentations while maintaining consistency is still an intractable problem.\nIn addition, most existing deep clustering methods based on contrastive\nlearning overlook the consistency of the clustering representations during the\nclustering process. In this paper, we show how the above problems can be\novercome and propose a consistent enhancement-based deep MVC method via\ncontrastive learning (CCEC). Specifically, semantic connection blocks are\nincorporated into a feature representation to preserve the consistent\ninformation among multiple views. Furthermore, the representation process for\nclustering is enhanced through spectral clustering, and the consistency across\nmultiple views is improved. Experiments conducted on five datasets demonstrate\nthe effectiveness and superiority of our method in comparison with the\nstate-of-the-art (SOTA) methods. The code for this method can be accessed at\nhttps://anonymous.4open.science/r/CCEC-E84E/.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.12648v3.pdf",
        "similarity": 0.3541350938980825,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-23"
    },
    {
        "new_title": "EVCL: Elastic Variational Continual Learning with Weight Consolidation",
        "new_link": "http://arxiv.org/abs/2406.15972v1",
        "new_summary": "  Continual learning aims to allow models to learn new tasks without forgetting\nwhat has been learned before. This work introduces Elastic Variational\nContinual Learning with Weight Consolidation (EVCL), a novel hybrid model that\nintegrates the variational posterior approximation mechanism of Variational\nContinual Learning (VCL) with the regularization-based parameter-protection\nstrategy of Elastic Weight Consolidation (EWC). By combining the strengths of\nboth methods, EVCL effectively mitigates catastrophic forgetting and enables\nbetter capture of dependencies between model parameters and task-specific data.\nEvaluated on five discriminative tasks, EVCL consistently outperforms existing\nbaselines in both domain-incremental and task-incremental learning scenarios\nfor deep discriminative models.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.15972v1.pdf",
        "similarity": 0.35407996327846963,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-23"
    },
    {
        "new_title": "Smooth Deep Saliency",
        "new_link": "http://arxiv.org/abs/2404.02282v2",
        "new_summary": "  In this work, we investigate methods to reduce the noise in deep saliency\nmaps coming from convolutional downsampling, with the purpose of explaining how\na deep learning model detects tumors in scanned histological tissue samples.\nThose methods make the investigated models more interpretable for\ngradient-based saliency maps, computed in hidden layers. We test our approach\non different models trained for image classification on ImageNet1K, and models\ntrained for tumor detection on Camelyon16 and in-house real-world digital\npathology scans of stained tissue samples. Our results show that the\ncheckerboard noise in the gradient gets reduced, resulting in smoother and\ntherefore easier to interpret saliency maps.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.02282v2.pdf",
        "similarity": 0.3537953433036249,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-02"
    },
    {
        "new_title": "Deep decomposition method for the limited aperture inverse obstacle\n  scattering problem",
        "new_link": "http://arxiv.org/abs/2403.19470v1",
        "new_summary": "  In this paper, we consider a deep learning approach to the limited aperture\ninverse obstacle scattering problem. It is well known that traditional deep\nlearning relies solely on data, which may limit its performance for the inverse\nproblem when only indirect observation data and a physical model are available.\nA fundamental question arises in light of these limitations: is it possible to\nenable deep learning to work on inverse problems without labeled data and to be\naware of what it is learning? This work proposes a deep decomposition method\n(DDM) for such purposes, which does not require ground truth labels. It\naccomplishes this by providing physical operators associated with the\nscattering model to the neural network architecture. Additionally, a deep\nlearning based data completion scheme is implemented in DDM to prevent\ndistorting the solution of the inverse problem for limited aperture data.\nFurthermore, apart from addressing the ill-posedness imposed by the inverse\nproblem itself, DDM is a physics-aware machine learning technique that can have\ninterpretability property. The convergence result of DDM is theoretically\nproven. Numerical experiments are presented to demonstrate the validity of the\nproposed DDM even when the incident and observation apertures are extremely\nlimited.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.19470v1.pdf",
        "similarity": 0.3534973058038465,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-28"
    },
    {
        "new_title": "An AI Architecture with the Capability to Explain Recognition Results",
        "new_link": "http://arxiv.org/abs/2406.08740v2",
        "new_summary": "  Explainability is needed to establish confidence in machine learning results.\nSome explainable methods take a post hoc approach to explain the weights of\nmachine learning models, others highlight areas of the input contributing to\ndecisions. These methods do not adequately explain decisions, in plain terms.\nExplainable property-based systems have been shown to provide explanations in\nplain terms, however, they have not performed as well as leading unexplainable\nmachine learning methods. This research focuses on the importance of metrics to\nexplainability and contributes two methods yielding performance gains. The\nfirst method introduces a combination of explainable and unexplainable flows,\nproposing a metric to characterize explainability of a decision. The second\nmethod compares classic metrics for estimating the effectiveness of neural\nnetworks in the system, posing a new metric as the leading performer. Results\nfrom the new methods and examples from handwritten datasets are presented.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.08740v2.pdf",
        "similarity": 0.3530588017720821,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-13"
    },
    {
        "new_title": "Enhancing Readmission Prediction with Deep Learning: Extracting\n  Biomedical Concepts from Clinical Texts",
        "new_link": "http://arxiv.org/abs/2403.09722v2",
        "new_summary": "  Hospital readmission, defined as patients being re-hospitalized shortly after\ndischarge, is a critical concern as it impacts patient outcomes and healthcare\ncosts. Identifying patients at risk of readmission allows for timely\ninterventions, reducing re-hospitalization rates and overall treatment costs.\nThis study focuses on predicting patient readmission within less than 30 days\nusing text mining techniques applied to discharge report texts from electronic\nhealth records (EHR). Various machine learning and deep learning methods were\nemployed to develop a classification model for this purpose. A novel aspect of\nthis research involves leveraging the Bio-Discharge Summary Bert (BDSS) model\nalong with principal component analysis (PCA) feature extraction to preprocess\ndata for deep learning model input. Our analysis of the MIMIC-III dataset\nindicates that our approach, which combines the BDSS model with a multilayer\nperceptron (MLP), outperforms state-of-the-art methods. This model achieved a\nrecall of 94% and an area under the curve (AUC) of 75%, showcasing its\neffectiveness in predicting patient readmissions. This study contributes to the\nadvancement of predictive modeling in healthcare by integrating text mining\ntechniques with deep learning algorithms to improve patient outcomes and\noptimize resource allocation.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.09722v2.pdf",
        "similarity": 0.3530178788520665,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-12"
    },
    {
        "new_title": "Deep Learning for Satellite Image Time Series Analysis: A Review",
        "new_link": "http://arxiv.org/abs/2404.03936v2",
        "new_summary": "  Earth observation (EO) satellite missions have been providing detailed images\nabout the state of the Earth and its land cover for over 50 years. Long term\nmissions, such as NASA's Landsat, Terra, and Aqua satellites, and more\nrecently, the ESA's Sentinel missions, record images of the entire world every\nfew days. Although single images provide point-in-time data, repeated images of\nthe same area, or satellite image time series (SITS) provide information about\nthe changing state of vegetation and land use. These SITS are useful for\nmodeling dynamic processes and seasonal changes such as plant phenology. They\nhave potential benefits for many aspects of land and natural resource\nmanagement, including applications in agricultural, forest, water, and disaster\nmanagement, urban planning, and mining. However, the resulting satellite image\ntime series (SITS) are complex, incorporating information from the temporal,\nspatial, and spectral dimensions. Therefore, deep learning methods are often\ndeployed as they can analyze these complex relationships. This review presents\na summary of the state-of-the-art methods of modelling environmental,\nagricultural, and other Earth observation variables from SITS data using deep\nlearning methods. We aim to provide a resource for remote sensing experts\ninterested in using deep learning techniques to enhance Earth observation\nmodels with temporal information.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.03936v2.pdf",
        "similarity": 0.3528545470594668,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-05"
    },
    {
        "new_title": "Toward Efficient Deep Spiking Neuron Networks:A Survey On Compression",
        "new_link": "http://arxiv.org/abs/2407.08744v1",
        "new_summary": "  With the rapid development of deep learning, Deep Spiking Neural Networks\n(DSNNs) have emerged as promising due to their unique spike event processing\nand asynchronous computation. When deployed on neuromorphic chips, DSNNs offer\nsignificant power advantages over Deep Artificial Neural Networks (DANNs) and\neliminate time and energy consuming multiplications due to the binary nature of\nspikes (0 or 1). Additionally, DSNNs excel in processing temporal information,\nmaking them potentially superior for handling temporal data compared to DANNs.\nHowever, their deep network structure and numerous parameters result in high\ncomputational costs and energy consumption, limiting real-life deployment. To\nenhance DSNNs efficiency, researchers have adapted methods from DANNs, such as\npruning, quantization, and knowledge distillation, and developed specific\ntechniques like reducing spike firing and pruning time steps. While previous\nsurveys have covered DSNNs algorithms, hardware deployment, and general\noverviews, focused research on DSNNs compression and efficiency has been\nlacking. This survey addresses this gap by concentrating on efficient DSNNs and\ntheir compression methods. It begins with an exploration of DSNNs' biological\nbackground and computational units, highlighting differences from DANNs. It\nthen delves into various compression methods, including pruning, quantization,\nknowledge distillation, and reducing spike firing, and concludes with\nsuggestions for future research directions.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.08744v1.pdf",
        "similarity": 0.35238428949501516,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-03"
    },
    {
        "new_title": "Learning rate adaptive stochastic gradient descent optimization methods:\n  numerical simulations for deep learning methods for partial differential\n  equations and convergence analyses",
        "new_link": "http://arxiv.org/abs/2406.14340v1",
        "new_summary": "  It is known that the standard stochastic gradient descent (SGD) optimization\nmethod, as well as accelerated and adaptive SGD optimization methods such as\nthe Adam optimizer fail to converge if the learning rates do not converge to\nzero (as, for example, in the situation of constant learning rates). Numerical\nsimulations often use human-tuned deterministic learning rate schedules or\nsmall constant learning rates. The default learning rate schedules for SGD\noptimization methods in machine learning implementation frameworks such as\nTensorFlow and Pytorch are constant learning rates. In this work we propose and\nstudy a learning-rate-adaptive approach for SGD optimization methods in which\nthe learning rate is adjusted based on empirical estimates for the values of\nthe objective function of the considered optimization problem (the function\nthat one intends to minimize). In particular, we propose a\nlearning-rate-adaptive variant of the Adam optimizer and implement it in case\nof several neural network learning problems, particularly, in the context of\ndeep learning approximation methods for partial differential equations such as\ndeep Kolmogorov methods, physics-informed neural networks, and deep Ritz\nmethods. In each of the presented learning problems the proposed\nlearning-rate-adaptive variant of the Adam optimizer faster reduces the value\nof the objective function than the Adam optimizer with the default learning\nrate. For a simple class of quadratic minimization problems we also rigorously\nprove that a learning-rate-adaptive variant of the SGD optimization method\nconverges to the minimizer of the considered minimization problem. Our\nconvergence proof is based on an analysis of the laws of invariant measures of\nthe SGD method as well as on a more general convergence analysis for SGD with\nrandom but predictable learning rates which we develop in this work.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.14340v1.pdf",
        "similarity": 0.3522110848431785,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-20"
    },
    {
        "new_title": "Iteration and Stochastic First-order Oracle Complexities of Stochastic\n  Gradient Descent using Constant and Decaying Learning Rates",
        "new_link": "http://arxiv.org/abs/2402.15344v1",
        "new_summary": "  The performance of stochastic gradient descent (SGD), which is the simplest\nfirst-order optimizer for training deep neural networks, depends on not only\nthe learning rate but also the batch size. They both affect the number of\niterations and the stochastic first-order oracle (SFO) complexity needed for\ntraining. In particular, the previous numerical results indicated that, for SGD\nusing a constant learning rate, the number of iterations needed for training\ndecreases when the batch size increases, and the SFO complexity needed for\ntraining is minimized at a critical batch size and that it increases once the\nbatch size exceeds that size. Here, we study the relationship between batch\nsize and the iteration and SFO complexities needed for nonconvex optimization\nin deep learning with SGD using constant or decaying learning rates and show\nthat SGD using the critical batch size minimizes the SFO complexity. We also\nprovide numerical comparisons of SGD with the existing first-order optimizers\nand show the usefulness of SGD using a critical batch size. Moreover, we show\nthat measured critical batch sizes are close to the sizes estimated from our\ntheoretical results.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.15344v1.pdf",
        "similarity": 0.352048338841626,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-23"
    },
    {
        "new_title": "Hypothesis Spaces for Deep Learning",
        "new_link": "http://arxiv.org/abs/2403.03353v2",
        "new_summary": "  This paper introduces a hypothesis space for deep learning that employs deep\nneural networks (DNNs). By treating a DNN as a function of two variables, the\nphysical variable and parameter variable, we consider the primitive set of the\nDNNs for the parameter variable located in a set of the weight matrices and\nbiases determined by a prescribed depth and widths of the DNNs. We then\ncomplete the linear span of the primitive DNN set in a weak* topology to\nconstruct a Banach space of functions of the physical variable. We prove that\nthe Banach space so constructed is a reproducing kernel Banach space (RKBS) and\nconstruct its reproducing kernel. We investigate two learning models,\nregularized learning and minimum interpolation problem in the resulting RKBS,\nby establishing representer theorems for solutions of the learning models. The\nrepresenter theorems unfold that solutions of these learning models can be\nexpressed as linear combination of a finite number of kernel sessions\ndetermined by given data and the reproducing kernel.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.03353v2.pdf",
        "similarity": 0.3514879849147265,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-05"
    },
    {
        "new_title": "Deep Embedding Clustering Driven by Sample Stability",
        "new_link": "http://arxiv.org/abs/2401.15989v1",
        "new_summary": "  Deep clustering methods improve the performance of clustering tasks by\njointly optimizing deep representation learning and clustering. While numerous\ndeep clustering algorithms have been proposed, most of them rely on\nartificially constructed pseudo targets for performing clustering. This\nconstruction process requires some prior knowledge, and it is challenging to\ndetermine a suitable pseudo target for clustering. To address this issue, we\npropose a deep embedding clustering algorithm driven by sample stability\n(DECS), which eliminates the requirement of pseudo targets. Specifically, we\nstart by constructing the initial feature space with an autoencoder and then\nlearn the cluster-oriented embedding feature constrained by sample stability.\nThe sample stability aims to explore the deterministic relationship between\nsamples and all cluster centroids, pulling samples to their respective clusters\nand keeping them away from other clusters with high determinacy. We analyzed\nthe convergence of the loss using Lipschitz continuity in theory, which\nverifies the validity of the model. The experimental results on five datasets\nillustrate that the proposed method achieves superior performance compared to\nstate-of-the-art clustering approaches.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.15989v1.pdf",
        "similarity": 0.35143020528187774,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-29"
    },
    {
        "new_title": "Batch Active Learning of Reward Functions from Human Preferences",
        "new_link": "http://arxiv.org/abs/2402.15757v1",
        "new_summary": "  Data generation and labeling are often expensive in robot learning.\nPreference-based learning is a concept that enables reliable labeling by\nquerying users with preference questions. Active querying methods are commonly\nemployed in preference-based learning to generate more informative data at the\nexpense of parallelization and computation time. In this paper, we develop a\nset of novel algorithms, batch active preference-based learning methods, that\nenable efficient learning of reward functions using as few data samples as\npossible while still having short query generation times and also retaining\nparallelizability. We introduce a method based on determinantal point processes\n(DPP) for active batch generation and several heuristic-based alternatives.\nFinally, we present our experimental results for a variety of robotics tasks in\nsimulation. Our results suggest that our batch active learning algorithm\nrequires only a few queries that are computed in a short amount of time. We\nshowcase one of our algorithms in a study to learn human users' preferences.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.15757v1.pdf",
        "similarity": 0.3507085564327171,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-24"
    },
    {
        "new_title": "Unsupervised Concept Drift Detection from Deep Learning Representations\n  in Real-time",
        "new_link": "http://arxiv.org/abs/2406.17813v1",
        "new_summary": "  Concept Drift is a phenomenon in which the underlying data distribution and\nstatistical properties of a target domain change over time, leading to a\ndegradation of the model's performance. Consequently, models deployed in\nproduction require continuous monitoring through drift detection techniques.\nMost drift detection methods to date are supervised, i.e., based on\nground-truth labels. However, true labels are usually not available in many\nreal-world scenarios. Although recent efforts have been made to develop\nunsupervised methods, they often lack the required accuracy, have a complexity\nthat makes real-time implementation in production environments difficult, or\nare unable to effectively characterize drift. To address these challenges, we\npropose DriftLens, an unsupervised real-time concept drift detection framework.\nIt works on unstructured data by exploiting the distribution distances of deep\nlearning representations. DriftLens can also provide drift characterization by\nanalyzing each label separately. A comprehensive experimental evaluation is\npresented with multiple deep learning classifiers for text, image, and speech.\nResults show that (i) DriftLens performs better than previous methods in\ndetecting drift in $11/13$ use cases; (ii) it runs at least 5 times faster;\n(iii) its detected drift value is very coherent with the amount of drift\n(correlation $\\geq 0.85$); (iv) it is robust to parameter changes.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.17813v1.pdf",
        "similarity": 0.35068118011532157,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-24"
    },
    {
        "new_title": "Augmenting Automation: Intent-Based User Instruction Classification with\n  Machine Learning",
        "new_link": "http://arxiv.org/abs/2403.01242v1",
        "new_summary": "  Electric automation systems offer convenience and efficiency in controlling\nelectrical circuits and devices. Traditionally, these systems rely on\npredefined commands for control, limiting flexibility and adaptability. In this\npaper, we propose a novel approach to augment automation by introducing\nintent-based user instruction classification using machine learning techniques.\nOur system represents user instructions as intents, allowing for dynamic\ncontrol of electrical circuits without relying on predefined commands. Through\na machine learning model trained on a labeled dataset of user instructions, our\nsystem classifies intents from user input, enabling a more intuitive and\nadaptable control scheme. We present the design and implementation of our\nintent-based electric automation system, detailing the development of the\nmachine learning model for intent classification. Experimental results\ndemonstrate the effectiveness of our approach in enhancing user experience and\nexpanding the capabilities of electric automation systems. Our work contributes\nto the advancement of smart technologies by providing a more seamless\ninteraction between users and their environments.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.01242v1.pdf",
        "similarity": 0.35067242344989163,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-02"
    },
    {
        "new_title": "Scalable Bayesian Inference in the Era of Deep Learning: From Gaussian\n  Processes to Deep Neural Networks",
        "new_link": "http://arxiv.org/abs/2404.19157v1",
        "new_summary": "  Large neural networks trained on large datasets have become the dominant\nparadigm in machine learning. These systems rely on maximum likelihood point\nestimates of their parameters, precluding them from expressing model\nuncertainty. This may result in overconfident predictions and it prevents the\nuse of deep learning models for sequential decision making. This thesis\ndevelops scalable methods to equip neural networks with model uncertainty. In\nparticular, we leverage the linearised Laplace approximation to equip\npre-trained neural networks with the uncertainty estimates provided by their\ntangent linear models. This turns the problem of Bayesian inference in neural\nnetworks into one of Bayesian inference in conjugate Gaussian-linear models.\nAlas, the cost of this remains cubic in either the number of network parameters\nor in the number of observations times output dimensions. By assumption,\nneither are tractable. We address this intractability by using stochastic\ngradient descent (SGD) -- the workhorse algorithm of deep learning -- to\nperform posterior sampling in linear models and their convex duals: Gaussian\nprocesses. With this, we turn back to linearised neural networks, finding the\nlinearised Laplace approximation to present a number of incompatibilities with\nmodern deep learning practices -- namely, stochastic optimisation, early\nstopping and normalisation layers -- when used for hyperparameter learning. We\nresolve these and construct a sample-based EM algorithm for scalable\nhyperparameter learning with linearised neural networks. We apply the above\nmethods to perform linearised neural network inference with ResNet-50 (25M\nparameters) trained on Imagenet (1.2M observations and 1000 output dimensions).\nAdditionally, we apply our methods to estimate uncertainty for 3d tomographic\nreconstructions obtained with the deep image prior network.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.19157v1.pdf",
        "similarity": 0.35062357427746493,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-29"
    },
    {
        "new_title": "Real-Time Pill Identification for the Visually Impaired Using Deep\n  Learning",
        "new_link": "http://arxiv.org/abs/2405.05983v1",
        "new_summary": "  The prevalence of mobile technology offers unique opportunities for\naddressing healthcare challenges, especially for individuals with visual\nimpairments. This paper explores the development and implementation of a deep\nlearning-based mobile application designed to assist blind and visually\nimpaired individuals in real-time pill identification. Utilizing the YOLO\nframework, the application aims to accurately recognize and differentiate\nbetween various pill types through real-time image processing on mobile\ndevices. The system incorporates Text-to- Speech (TTS) to provide immediate\nauditory feedback, enhancing usability and independence for visually impaired\nusers. Our study evaluates the application's effectiveness in terms of\ndetection accuracy and user experience, highlighting its potential to improve\nmedication management and safety among the visually impaired community.\nKeywords-Deep Learning; YOLO Framework; Mobile Application; Visual Impairment;\nPill Identification; Healthcare\n",
        "pdf_link": "https://arxiv.org/pdf/2405.05983v1.pdf",
        "similarity": 0.35047762816539596,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-08"
    },
    {
        "new_title": "Forecasting with Deep Learning: Beyond Average of Average of Average\n  Performance",
        "new_link": "http://arxiv.org/abs/2406.16590v1",
        "new_summary": "  Accurate evaluation of forecasting models is essential for ensuring reliable\npredictions. Current practices for evaluating and comparing forecasting models\nfocus on summarising performance into a single score, using metrics such as\nSMAPE. We hypothesize that averaging performance over all samples dilutes\nrelevant information about the relative performance of models. Particularly,\nconditions in which this relative performance is different than the overall\naccuracy. We address this limitation by proposing a novel framework for\nevaluating univariate time series forecasting models from multiple\nperspectives, such as one-step ahead forecasting versus multi-step ahead\nforecasting. We show the advantages of this framework by comparing a\nstate-of-the-art deep learning approach with classical forecasting techniques.\nWhile classical methods (e.g. ARIMA) are long-standing approaches to\nforecasting, deep neural networks (e.g. NHITS) have recently shown\nstate-of-the-art forecasting performance in benchmark datasets. We conducted\nextensive experiments that show NHITS generally performs best, but its\nsuperiority varies with forecasting conditions. For instance, concerning the\nforecasting horizon, NHITS only outperforms classical approaches for multi-step\nahead forecasting. Another relevant insight is that, when dealing with\nanomalies, NHITS is outperformed by methods such as Theta. These findings\nhighlight the importance of aspect-based model evaluation.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.16590v1.pdf",
        "similarity": 0.3502929255298771,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-24"
    },
    {
        "new_title": "Scalable Nested Optimization for Deep Learning",
        "new_link": "http://arxiv.org/abs/2407.01526v1",
        "new_summary": "  Gradient-based optimization has been critical to the success of machine\nlearning, updating a single set of parameters to minimize a single loss. A\ngrowing number of applications rely on a generalization of this, where we have\na bilevel or nested optimization of which subsets of parameters update on\ndifferent objectives nested inside each other. We focus on motivating examples\nof hyperparameter optimization and generative adversarial networks. However,\nnaively applying classical methods often fails when we look at solving these\nnested problems on a large scale. In this thesis, we build tools for nested\noptimization that scale to deep learning setups.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.01526v1.pdf",
        "similarity": 0.3500562130334825,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-01"
    },
    {
        "new_title": "Learning Structure-Aware Representations of Dependent Types",
        "new_link": "http://arxiv.org/abs/2402.02104v1",
        "new_summary": "  Agda is a dependently-typed programming language and a proof assistant,\npivotal in proof formalization and programming language theory. This paper\nextends the Agda ecosystem into machine learning territory, and, vice versa,\nmakes Agda-related resources available to machine learning practitioners. We\nintroduce and release a novel dataset of Agda program-proofs that is elaborate\nand extensive enough to support various machine learning applications -- the\nfirst of its kind. Leveraging the dataset's ultra-high resolution, detailing\nproof states at the sub-type level, we propose a novel neural architecture\ntargeted at faithfully representing dependently-typed programs on the basis of\nstructural rather than nominal principles. We instantiate and evaluate our\narchitecture in a premise selection setup, where it achieves strong initial\nresults.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.02104v1.pdf",
        "similarity": 0.34994361671087026,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-03"
    },
    {
        "new_title": "An Efficient Multimodal Learning Framework to Comprehend Consumer\n  Preferences Using BERT and Cross-Attention",
        "new_link": "http://arxiv.org/abs/2405.07435v1",
        "new_summary": "  Today, the acquisition of various behavioral log data has enabled deeper\nunderstanding of customer preferences and future behaviors in the marketing\nfield. In particular, multimodal deep learning has achieved highly accurate\npredictions by combining multiple types of data. Many of these studies utilize\nwith feature fusion to construct multimodal models, which combines extracted\nrepresentations from each modality. However, since feature fusion treats\ninformation from each modality equally, it is difficult to perform flexible\nanalysis such as the attention mechanism that has been used extensively in\nrecent years. Therefore, this study proposes a context-aware multimodal deep\nlearning model that combines Bidirectional Encoder Representations from\nTransformers (BERT) and cross-attention Transformer, which dynamically changes\nthe attention of deep-contextualized word representations based on background\ninformation such as consumer demographic and lifestyle variables. We conduct a\ncomprehensive analysis and demonstrate the effectiveness of our model by\ncomparing it with six reference models in three categories using behavioral\nlogs stored on an online platform. In addition, we present an efficient\nmultimodal learning method by comparing the learning efficiency depending on\nthe optimizers and the prediction accuracy depending on the number of tokens in\nthe text data.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.07435v1.pdf",
        "similarity": 0.34925047438170403,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-13"
    },
    {
        "new_title": "Selecting Interpretability Techniques for Healthcare Machine Learning\n  models",
        "new_link": "http://arxiv.org/abs/2406.10213v1",
        "new_summary": "  In healthcare there is a pursuit for employing interpretable algorithms to\nassist healthcare professionals in several decision scenarios. Following the\nPredictive, Descriptive and Relevant (PDR) framework, the definition of\ninterpretable machine learning as a machine-learning model that explicitly and\nin a simple frame determines relationships either contained in data or learned\nby the model that are relevant for its functioning and the categorization of\nmodels by post-hoc, acquiring interpretability after training, or model-based,\nbeing intrinsically embedded in the algorithm design. We overview a selection\nof eight algorithms, both post-hoc and model-based, that can be used for such\npurposes.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.10213v1.pdf",
        "similarity": 0.34901503392953076,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-14"
    },
    {
        "new_title": "Interpretable Short-Term Load Forecasting via Multi-Scale Temporal\n  Decomposition",
        "new_link": "http://arxiv.org/abs/2402.11664v1",
        "new_summary": "  Rapid progress in machine learning and deep learning has enabled a wide range\nof applications in the electricity load forecasting of power systems, for\ninstance, univariate and multivariate short-term load forecasting. Though the\nstrong capabilities of learning the non-linearity of the load patterns and the\nhigh prediction accuracy have been achieved, the interpretability of typical\ndeep learning models for electricity load forecasting is less studied. This\npaper proposes an interpretable deep learning method, which learns a linear\ncombination of neural networks that each attends to an input time feature. We\nalso proposed a multi-scale time series decomposition method to deal with the\ncomplex time patterns. Case studies have been carried out on the Belgium\ncentral grid load dataset and the proposed model demonstrated better accuracy\ncompared to the frequently applied baseline model. Specifically, the proposed\nmulti-scale temporal decomposition achieves the best MSE, MAE and RMSE of 0.52,\n0.57 and 0.72 respectively. As for interpretability, on one hand, the proposed\nmethod displays generalization capability. On the other hand, it can\ndemonstrate not only the feature but also the temporal interpretability\ncompared to other baseline methods. Besides, the global time feature\ninterpretabilities are also obtained. Obtaining global feature\ninterpretabilities allows us to catch the overall patterns, trends, and\ncyclicality in load data while also revealing the significance of various\ntime-related features in forming the final outputs.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.11664v1.pdf",
        "similarity": 0.3488920123506416,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-18"
    },
    {
        "new_title": "A Framework For Gait-Based User Demography Estimation Using Inertial\n  Sensors",
        "new_link": "http://arxiv.org/abs/2402.09761v1",
        "new_summary": "  Human gait has been shown to provide crucial motion cues for various\napplications. Recognizing patterns in human gait has been widely adopted in\nvarious application areas such as security, virtual reality gaming, medical\nrehabilitation, and ailment identification. Furthermore, wearable inertial\nsensors have been widely used for not only recording gait but also to predict\nusers' demography. Machine Learning techniques such as deep learning, combined\nwith inertial sensor signals, have shown promising results in recognizing\npatterns in human gait and estimate users' demography. However, the black-box\nnature of such deep learning models hinders the researchers from uncovering the\nreasons behind the model's predictions. Therefore, we propose leveraging deep\nlearning and Layer-Wise Relevance Propagation (LRP) to identify the important\nvariables that play a vital role in identifying the users' demography such as\nage and gender. To assess the efficacy of this approach we train a deep neural\nnetwork model on a large sensor-based gait dataset consisting of 745 subjects\nto identify users' age and gender. Using LRP we identify the variables relevant\nfor characterizing the gait patterns. Thus, we enable interpretation of\nnon-linear ML models which are experts in identifying the users' demography\nbased on inertial signals. We believe this approach can not only provide\nclinicians information about the gait parameters relevant to age and gender but\nalso can be expanded to analyze and diagnose gait disorders.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.09761v1.pdf",
        "similarity": 0.34888285816280284,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-15"
    },
    {
        "new_title": "Introducing a Physics-informed Deep Learning Framework for Bridge Scour\n  Prediction",
        "new_link": "http://arxiv.org/abs/2407.01258v2",
        "new_summary": "  This paper introduces scour physics-informed neural networks (SPINNs), a\nhybrid physics-data-driven framework for bridge scour prediction using deep\nlearning. SPINNs are developed based on historical scour monitoring data and\nintegrate physics-based empirical equations into neural networks as\nsupplementary loss components. We incorporated three architectures: LSTM, CNN,\nand NLinear as the base data-driven model. Despite varying performance across\ndifferent base models and bridges, SPINNs overall outperformed pure data-driven\nmodels. In some bridge cases, SPINN reduced forecasting errors by up to 50\npercent. In this study, we also explored general models for bridge clusters,\ntrained by aggregating datasets across multiple bridges in a region. The pure\ndata-driven models mostly benefited from this approach, in particular bridges\nwith limited data. However, bridge-specific SPINNs provided more accurate\npredictions than general SPINNs for almost all case studies. Also, the\ntime-dependent empirical equations derived from SPINNs showed reasonable\naccuracy in estimating maximum scour depth, providing more accurate predictions\ncompared to HEC-18. Comparing both SPINNs and pure deep learning models with\ntraditional HEC-18 equation indicates substantial improvements in scour\nprediction accuracy. This study can pave the way for hybrid physics-machine\nlearning methodologies to be implemented for bridge scour design and\nmaintenance.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.01258v2.pdf",
        "similarity": 0.34883835547493686,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-01"
    },
    {
        "new_title": "Image Copy-Move Forgery Detection via Deep PatchMatch and Pairwise\n  Ranking Learning",
        "new_link": "http://arxiv.org/abs/2404.17310v1",
        "new_summary": "  Recent advances in deep learning algorithms have shown impressive progress in\nimage copy-move forgery detection (CMFD). However, these algorithms lack\ngeneralizability in practical scenarios where the copied regions are not\npresent in the training images, or the cloned regions are part of the\nbackground. Additionally, these algorithms utilize convolution operations to\ndistinguish source and target regions, leading to unsatisfactory results when\nthe target regions blend well with the background. To address these\nlimitations, this study proposes a novel end-to-end CMFD framework that\nintegrates the strengths of conventional and deep learning methods.\nSpecifically, the study develops a deep cross-scale PatchMatch (PM) method that\nis customized for CMFD to locate copy-move regions. Unlike existing deep\nmodels, our approach utilizes features extracted from high-resolution scales to\nseek explicit and reliable point-to-point matching between source and target\nregions. Furthermore, we propose a novel pairwise rank learning framework to\nseparate source and target regions. By leveraging the strong prior of\npoint-to-point matches, the framework can identify subtle differences and\neffectively discriminate between source and target regions, even when the\ntarget regions blend well with the background. Our framework is fully\ndifferentiable and can be trained end-to-end. Comprehensive experimental\nresults highlight the remarkable generalizability of our scheme across various\ncopy-move scenarios, significantly outperforming existing methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.17310v1.pdf",
        "similarity": 0.34881192147334616,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-26"
    },
    {
        "new_title": "Position Paper: Generalized grammar rules and structure-based\n  generalization beyond classical equivariance for lexical tasks and\n  transduction",
        "new_link": "http://arxiv.org/abs/2402.01629v1",
        "new_summary": "  Compositional generalization is one of the main properties which\ndifferentiates lexical learning in humans from state-of-art neural networks. We\npropose a general framework for building models that can generalize\ncompositionally using the concept of Generalized Grammar Rules (GGRs), a class\nof symmetry-based compositional constraints for transduction tasks, which we\nview as a transduction analogue of equivariance constraints in physics-inspired\ntasks. Besides formalizing generalized notions of symmetry for language\ntransduction, our framework is general enough to contain many existing works as\nspecial cases. We present ideas on how GGRs might be implemented, and in the\nprocess draw connections to reinforcement learning and other areas of research.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.01629v1.pdf",
        "similarity": 0.34876649523014064,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-02"
    },
    {
        "new_title": "Posterior and variational inference for deep neural networks with\n  heavy-tailed weights",
        "new_link": "http://arxiv.org/abs/2406.03369v1",
        "new_summary": "  We consider deep neural networks in a Bayesian framework with a prior\ndistribution sampling the network weights at random. Following a recent idea of\nAgapiou and Castillo (2023), who show that heavy-tailed prior distributions\nachieve automatic adaptation to smoothness, we introduce a simple Bayesian deep\nlearning prior based on heavy-tailed weights and ReLU activation. We show that\nthe corresponding posterior distribution achieves near-optimal minimax\ncontraction rates, simultaneously adaptive to both intrinsic dimension and\nsmoothness of the underlying function, in a variety of contexts including\nnonparametric regression, geometric data and Besov spaces. While most works so\nfar need a form of model selection built-in within the prior distribution, a\nkey aspect of our approach is that it does not require to sample\nhyperparameters to learn the architecture of the network. We also provide\nvariational Bayes counterparts of the results, that show that mean-field\nvariational approximations still benefit from near-optimal theoretical support.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.03369v1.pdf",
        "similarity": 0.34780830036194427,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-05"
    },
    {
        "new_title": "DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection",
        "new_link": "http://arxiv.org/abs/2402.17176v1",
        "new_summary": "  Model-X knockoff, among various feature selection methods, received much\nattention recently due to its guarantee on false discovery rate (FDR) control.\nSubsequent to its introduction in parametric design, knockoff is advanced to\nhandle arbitrary data distributions using deep learning-based generative\nmodeling. However, we observed that current implementations of the deep Model-X\nknockoff framework exhibit limitations. Notably, the \"swap property\" that\nknockoffs necessitate frequently encounter challenges on sample level, leading\nto a diminished selection power. To overcome, we develop \"Deep Dependency\nRegularized Knockoff (DeepDRK)\", a distribution-free deep learning method that\nstrikes a balance between FDR and power. In DeepDRK, a generative model\ngrounded in a transformer architecture is introduced to better achieve the\n\"swap property\". Novel efficient regularization techniques are also proposed to\nreach higher power. Our model outperforms other benchmarks in synthetic,\nsemi-synthetic, and real-world data, especially when sample size is small and\ndata distribution is complex.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.17176v1.pdf",
        "similarity": 0.34763029676919566,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-27"
    },
    {
        "new_title": "RQP-SGD: Differential Private Machine Learning through Noisy SGD and\n  Randomized Quantization",
        "new_link": "http://arxiv.org/abs/2402.06606v1",
        "new_summary": "  The rise of IoT devices has prompted the demand for deploying machine\nlearning at-the-edge with real-time, efficient, and secure data processing. In\nthis context, implementing machine learning (ML) models with real-valued weight\nparameters can prove to be impractical particularly for large models, and there\nis a need to train models with quantized discrete weights. At the same time,\nthese low-dimensional models also need to preserve privacy of the underlying\ndataset. In this work, we present RQP-SGD, a new approach for\nprivacy-preserving quantization to train machine learning models for low-memory\nML-at-the-edge. This approach combines differentially private stochastic\ngradient descent (DP-SGD) with randomized quantization, providing a measurable\nprivacy guarantee in machine learning. In particular, we study the utility\nconvergence of implementing RQP-SGD on ML tasks with convex objectives and\nquantization constraints and demonstrate its efficacy over deterministic\nquantization. Through experiments conducted on two datasets, we show the\npractical effectiveness of RQP-SGD.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.06606v1.pdf",
        "similarity": 0.347569090564014,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-09"
    },
    {
        "new_title": "Utilizing Motion Matching with Deep Reinforcement Learning for Target\n  Location Tasks",
        "new_link": "http://arxiv.org/abs/2403.15902v1",
        "new_summary": "  We present an approach using deep reinforcement learning (DRL) to directly\ngenerate motion matching queries for long-term tasks, particularly targeting\nthe reaching of specific locations. By integrating motion matching and DRL, our\nmethod demonstrates the rapid learning of policies for target location tasks\nwithin minutes on a standard desktop, employing a simple reward design.\nAdditionally, we propose a unique hit reward and obstacle curriculum scheme to\nenhance policy learning in environments with moving obstacles.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.15902v1.pdf",
        "similarity": 0.3475560675834517,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-23"
    },
    {
        "new_title": "Transforming the Bootstrap: Using Transformers to Compute Scattering\n  Amplitudes in Planar N = 4 Super Yang-Mills Theory",
        "new_link": "http://arxiv.org/abs/2405.06107v1",
        "new_summary": "  We pursue the use of deep learning methods to improve state-of-the-art\ncomputations in theoretical high-energy physics. Planar N = 4 Super Yang-Mills\ntheory is a close cousin to the theory that describes Higgs boson production at\nthe Large Hadron Collider; its scattering amplitudes are large mathematical\nexpressions containing integer coefficients. In this paper, we apply\nTransformers to predict these coefficients. The problem can be formulated in a\nlanguage-like representation amenable to standard cross-entropy training\nobjectives. We design two related experiments and show that the model achieves\nhigh accuracy (> 98%) on both tasks. Our work shows that Transformers can be\napplied successfully to problems in theoretical physics that require exact\nsolutions.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.06107v1.pdf",
        "similarity": 0.34683229954359646,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-09"
    },
    {
        "new_title": "Bridging Diversity and Uncertainty in Active learning with\n  Self-Supervised Pre-Training",
        "new_link": "http://arxiv.org/abs/2403.03728v1",
        "new_summary": "  This study addresses the integration of diversity-based and uncertainty-based\nsampling strategies in active learning, particularly within the context of\nself-supervised pre-trained models. We introduce a straightforward heuristic\ncalled TCM that mitigates the cold start problem while maintaining strong\nperformance across various data levels. By initially applying TypiClust for\ndiversity sampling and subsequently transitioning to uncertainty sampling with\nMargin, our approach effectively combines the strengths of both strategies. Our\nexperiments demonstrate that TCM consistently outperforms existing methods\nacross various datasets in both low and high data regimes.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.03728v1.pdf",
        "similarity": 0.3464708087813036,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-06"
    },
    {
        "new_title": "Federated Learning on Transcriptomic Data: Model Quality and Performance\n  Trade-Offs",
        "new_link": "http://arxiv.org/abs/2402.14527v1",
        "new_summary": "  Machine learning on large-scale genomic or transcriptomic data is important\nfor many novel health applications. For example, precision medicine tailors\nmedical treatments to patients on the basis of individual biomarkers, cellular\nand molecular states, etc. However, the data required is sensitive, voluminous,\nheterogeneous, and typically distributed across locations where dedicated\nmachine learning hardware is not available. Due to privacy and regulatory\nreasons, it is also problematic to aggregate all data at a trusted third\nparty.Federated learning is a promising solution to this dilemma, because it\nenables decentralized, collaborative machine learning without exchanging raw\ndata. In this paper, we perform comparative experiments with the federated\nlearning frameworks TensorFlow Federated and Flower. Our test case is the\ntraining of disease prognosis and cell type classification models. We train the\nmodels with distributed transcriptomic data, considering both data\nheterogeneity and architectural heterogeneity. We measure model quality,\nrobustness against privacy-enhancing noise, computational performance and\nresource overhead. Each of the federated learning frameworks has different\nstrengths. However, our experiments confirm that both frameworks can readily\nbuild models on transcriptomic data, without transferring personal raw data to\na third party with abundant computational resources.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.14527v1.pdf",
        "similarity": 0.3464644063485329,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-22"
    },
    {
        "new_title": "Deep learning enhanced mixed integer optimization: Learning to reduce\n  model dimensionality",
        "new_link": "http://arxiv.org/abs/2401.09556v2",
        "new_summary": "  This work introduces a framework to address the computational complexity\ninherent in Mixed-Integer Programming (MIP) models by harnessing the potential\nof deep learning. By employing deep learning, we construct problem-specific\nheuristics that identify and exploit common structures across MIP instances. We\ntrain deep learning models to estimate complicating binary variables for target\nMIP problem instances. The resulting reduced MIP models are solved using\nstandard off-the-shelf solvers. We present an algorithm for generating\nsynthetic data enhancing the robustness and generalizability of our models\nacross diverse MIP instances. We compare the effectiveness of (a) feed-forward\nneural networks (ANN) and (b) convolutional neural networks (CNN). To enhance\nthe framework's performance, we employ Bayesian optimization for hyperparameter\ntuning, aiming to maximize the occurrence of global optimum solutions. We apply\nthis framework to a flow-based facility location allocation MIP formulation\nthat describes long-term investment planning and medium-term tactical\nscheduling in a personalized medicine supply chain.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.09556v2.pdf",
        "similarity": 0.346335233852407,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-17"
    },
    {
        "new_title": "Privacy-Preserving and Trustworthy Deep Learning for Medical Imaging",
        "new_link": "http://arxiv.org/abs/2407.00538v1",
        "new_summary": "  The shift towards efficient and automated data analysis through Machine\nLearning (ML) has notably impacted healthcare systems, particularly Radiomics.\nRadiomics leverages ML to analyze medical images accurately and efficiently for\nprecision medicine. Current methods rely on Deep Learning (DL) to improve\nperformance and accuracy (Deep Radiomics). Given the sensitivity of medical\nimages, ensuring privacy throughout the Deep Radiomics pipeline-from data\ngeneration and collection to model training and inference-is essential,\nespecially when outsourced. Thus, Privacy-Enhancing Technologies (PETs) are\ncrucial tools for Deep Radiomics. Previous studies and systematization efforts\nhave either broadly overviewed PETs and their applications or mainly focused on\nsubsets of PETs for ML algorithms. In Deep Radiomics, where efficiency,\naccuracy, and privacy are crucial, many PETs, while theoretically applicable,\nmay not be practical without specialized optimizations or hybrid designs.\nAdditionally, not all DL models are suitable for Radiomics. Consequently, there\nis a need for specialized studies that investigate and systematize the\neffective and practical integration of PETs into the Deep Radiomics pipeline.\nThis work addresses this research gap by (1) classifying existing PETs,\npresenting practical hybrid PETS constructions, and a taxonomy illustrating\ntheir potential integration with the Deep Radiomics pipeline, with comparative\nanalyses detailing assumptions, architectural suitability, and security, (2)\nOffering technical insights, describing potential challenges and means of\ncombining PETs into the Deep Radiomics pipeline, including integration\nstrategies, subtilities, and potential challenges, (3) Proposing potential\nresearch directions, identifying challenges, and suggesting solutions to\nenhance the PETs in Deep Radiomics.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.00538v1.pdf",
        "similarity": 0.3462541621311613,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-29"
    },
    {
        "new_title": "Extracting Formulae in Many-Valued Logic from Deep Neural Networks",
        "new_link": "http://arxiv.org/abs/2401.12113v1",
        "new_summary": "  We propose a new perspective on deep ReLU networks, namely as circuit\ncounterparts of Lukasiewicz infinite-valued logic -- a many-valued (MV)\ngeneralization of Boolean logic. An algorithm for extracting formulae in MV\nlogic from deep ReLU networks is presented. As the algorithm applies to\nnetworks with general, in particular also real-valued, weights, it can be used\nto extract logical formulae from deep ReLU networks trained on data.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.12113v1.pdf",
        "similarity": 0.3462008554002213,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-22"
    },
    {
        "new_title": "Linear Recursive Feature Machines provably recover low-rank matrices",
        "new_link": "http://arxiv.org/abs/2401.04553v1",
        "new_summary": "  A fundamental problem in machine learning is to understand how neural\nnetworks make accurate predictions, while seemingly bypassing the curse of\ndimensionality. A possible explanation is that common training algorithms for\nneural networks implicitly perform dimensionality reduction - a process called\nfeature learning. Recent work posited that the effects of feature learning can\nbe elicited from a classical statistical estimator called the average gradient\nouter product (AGOP). The authors proposed Recursive Feature Machines (RFMs) as\nan algorithm that explicitly performs feature learning by alternating between\n(1) reweighting the feature vectors by the AGOP and (2) learning the prediction\nfunction in the transformed space. In this work, we develop the first\ntheoretical guarantees for how RFM performs dimensionality reduction by\nfocusing on the class of overparametrized problems arising in sparse linear\nregression and low-rank matrix recovery. Specifically, we show that RFM\nrestricted to linear models (lin-RFM) generalizes the well-studied Iteratively\nReweighted Least Squares (IRLS) algorithm. Our results shed light on the\nconnection between feature learning in neural networks and classical sparse\nrecovery algorithms. In addition, we provide an implementation of lin-RFM that\nscales to matrices with millions of missing entries. Our implementation is\nfaster than the standard IRLS algorithm as it is SVD-free. It also outperforms\ndeep linear networks for sparse linear regression and low-rank matrix\ncompletion.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.04553v1.pdf",
        "similarity": 0.3461662637510772,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-09"
    },
    {
        "new_title": "Comprehensive Survey of Model Compression and Speed up for Vision\n  Transformers",
        "new_link": "http://arxiv.org/abs/2404.10407v1",
        "new_summary": "  Vision Transformers (ViT) have marked a paradigm shift in computer vision,\noutperforming state-of-the-art models across diverse tasks. However, their\npractical deployment is hampered by high computational and memory demands. This\nstudy addresses the challenge by evaluating four primary model compression\ntechniques: quantization, low-rank approximation, knowledge distillation, and\npruning. We methodically analyze and compare the efficacy of these techniques\nand their combinations in optimizing ViTs for resource-constrained\nenvironments. Our comprehensive experimental evaluation demonstrates that these\nmethods facilitate a balanced compromise between model accuracy and\ncomputational efficiency, paving the way for wider application in edge\ncomputing devices.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.10407v1.pdf",
        "similarity": 0.34608186468534624,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-16"
    },
    {
        "new_title": "Deep Generative Modeling Reshapes Compression and Transmission: From\n  Efficiency to Resiliency",
        "new_link": "http://arxiv.org/abs/2406.06446v1",
        "new_summary": "  Information theory and machine learning are inextricably linked and have even\nbeen referred to as \"two sides of the same coin\". One particularly elegant\nconnection is the essential equivalence between probabilistic generative\nmodeling and data compression or transmission. In this article, we reveal the\ndual-functionality of deep generative models that reshapes both data\ncompression for efficiency and transmission error concealment for resiliency.\nWe present how the contextual predictive capabilities of powerful generative\nmodels can be well positioned to be strong compressors and estimators. In this\nsense, we advocate for viewing the deep generative modeling problem through the\nlens of end-to-end communications, and evaluate the compression and error\nrestoration capabilities of foundation generative models. We show that the\nkernel of many large generative models is powerful predictor that can capture\ncomplex relationships among semantic latent variables, and the communication\nviewpoints provide novel insights into semantic feature tokenization,\ncontextual learning, and usage of deep generative models. In summary, our\narticle highlights the essential connections of generative AI to source and\nchannel coding techniques, and motivates researchers to make further\nexplorations in this emerging topic.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.06446v1.pdf",
        "similarity": 0.34607465853268016,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-10"
    },
    {
        "new_title": "Machine Learning Data Practices through a Data Curation Lens: An\n  Evaluation Framework",
        "new_link": "http://arxiv.org/abs/2405.02703v1",
        "new_summary": "  Studies of dataset development in machine learning call for greater attention\nto the data practices that make model development possible and shape its\noutcomes. Many argue that the adoption of theory and practices from archives\nand data curation fields can support greater fairness, accountability,\ntransparency, and more ethical machine learning. In response, this paper\nexamines data practices in machine learning dataset development through the\nlens of data curation. We evaluate data practices in machine learning as data\ncuration practices. To do so, we develop a framework for evaluating machine\nlearning datasets using data curation concepts and principles through a rubric.\nThrough a mixed-methods analysis of evaluation results for 25 ML datasets, we\nstudy the feasibility of data curation principles to be adopted for machine\nlearning data work in practice and explore how data curation is currently\nperformed. We find that researchers in machine learning, which often emphasizes\nmodel development, struggle to apply standard data curation principles. Our\nfindings illustrate difficulties at the intersection of these fields, such as\nevaluating dimensions that have shared terms in both fields but non-shared\nmeanings, a high degree of interpretative flexibility in adapting concepts\nwithout prescriptive restrictions, obstacles in limiting the depth of data\ncuration expertise needed to apply the rubric, and challenges in scoping the\nextent of documentation dataset creators are responsible for. We propose ways\nto address these challenges and develop an overall framework for evaluation\nthat outlines how data curation concepts and methods can inform machine\nlearning data practices.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.02703v1.pdf",
        "similarity": 0.34594281392330467,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-04"
    },
    {
        "new_title": "Moments of Clarity: Streamlining Latent Spaces in Machine Learning using\n  Moment Pooling",
        "new_link": "http://arxiv.org/abs/2403.08854v1",
        "new_summary": "  Many machine learning applications involve learning a latent representation\nof data, which is often high-dimensional and difficult to directly interpret.\nIn this work, we propose \"Moment Pooling\", a natural extension of Deep Sets\nnetworks which drastically decrease latent space dimensionality of these\nnetworks while maintaining or even improving performance. Moment Pooling\ngeneralizes the summation in Deep Sets to arbitrary multivariate moments, which\nenables the model to achieve a much higher effective latent dimensionality for\na fixed latent dimension. We demonstrate Moment Pooling on the collider physics\ntask of quark/gluon jet classification by extending Energy Flow Networks (EFNs)\nto Moment EFNs. We find that Moment EFNs with latent dimensions as small as 1\nperform similarly to ordinary EFNs with higher latent dimension. This small\nlatent dimension allows for the internal representation to be directly\nvisualized and interpreted, which in turn enables the learned internal jet\nrepresentation to be extracted in closed form.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.08854v1.pdf",
        "similarity": 0.3459130015295032,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-13"
    },
    {
        "new_title": "Learning to Continually Learn with the Bayesian Principle",
        "new_link": "http://arxiv.org/abs/2405.18758v1",
        "new_summary": "  In the present era of deep learning, continual learning research is mainly\nfocused on mitigating forgetting when training a neural network with stochastic\ngradient descent on a non-stationary stream of data. On the other hand, in the\nmore classical literature of statistical machine learning, many models have\nsequential Bayesian update rules that yield the same learning outcome as the\nbatch training, i.e., they are completely immune to catastrophic forgetting.\nHowever, they are often overly simple to model complex real-world data. In this\nwork, we adopt the meta-learning paradigm to combine the strong\nrepresentational power of neural networks and simple statistical models'\nrobustness to forgetting. In our novel meta-continual learning framework,\ncontinual learning takes place only in statistical models via ideal sequential\nBayesian update rules, while neural networks are meta-learned to bridge the raw\ndata and the statistical models. Since the neural networks remain fixed during\ncontinual learning, they are protected from catastrophic forgetting. This\napproach not only achieves significantly improved performance but also exhibits\nexcellent scalability. Since our approach is domain-agnostic and\nmodel-agnostic, it can be applied to a wide range of problems and easily\nintegrated with existing model architectures.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.18758v1.pdf",
        "similarity": 0.3456539743904697,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-29"
    },
    {
        "new_title": "Enriching the Machine Learning Workloads in BigBench",
        "new_link": "http://arxiv.org/abs/2406.10843v1",
        "new_summary": "  In the era of Big Data and the growing support for Machine Learning, Deep\nLearning and Artificial Intelligence algorithms in the current software\nsystems, there is an urgent need of standardized application benchmarks that\nstress test and evaluate these new technologies. Relying on the standardized\nBigBench (TPCx-BB) benchmark, this work enriches the improved BigBench V2 with\nthree new workloads and expands the coverage of machine learning algorithms.\nOur workloads utilize multiple algorithms and compare different implementations\nfor the same algorithm across several popular libraries like MLlib, SystemML,\nScikit-learn and Pandas, demonstrating the relevance and usability of our\nbenchmark extension.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.10843v1.pdf",
        "similarity": 0.3455218132553942,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-16"
    },
    {
        "new_title": "Cheetah: Bridging the Gap Between Machine Learning and Particle\n  Accelerator Physics with High-Speed, Differentiable Simulations",
        "new_link": "http://arxiv.org/abs/2401.05815v1",
        "new_summary": "  Machine learning has emerged as a powerful solution to the modern challenges\nin accelerator physics. However, the limited availability of beam time, the\ncomputational cost of simulations, and the high-dimensionality of optimisation\nproblems pose significant challenges in generating the required data for\ntraining state-of-the-art machine learning models. In this work, we introduce\nCheetah, a PyTorch-based high-speed differentiable linear-beam dynamics code.\nCheetah enables the fast collection of large data sets by reducing computation\ntimes by multiple orders of magnitude and facilitates efficient gradient-based\noptimisation for accelerator tuning and system identification. This positions\nCheetah as a user-friendly, readily extensible tool that integrates seamlessly\nwith widely adopted machine learning tools. We showcase the utility of Cheetah\nthrough five examples, including reinforcement learning training,\ngradient-based beamline tuning, gradient-based system identification,\nphysics-informed Bayesian optimisation priors, and modular neural network\nsurrogate modelling of space charge effects. The use of such a high-speed\ndifferentiable simulation code will simplify the development of machine\nlearning-based methods for particle accelerators and fast-track their\nintegration into everyday operations of accelerator facilities.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.05815v1.pdf",
        "similarity": 0.34545361003865643,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-11"
    },
    {
        "new_title": "The Sandwich meta-framework for architecture agnostic deep\n  privacy-preserving transfer learning for non-invasive brainwave decoding",
        "new_link": "http://arxiv.org/abs/2404.06868v2",
        "new_summary": "  Machine learning has enhanced the performance of decoding signals indicating\nhuman behaviour. EEG decoding, as an exemplar indicating neural activity and\nhuman thoughts non-invasively, has been helpful in neural activity analysis and\naiding patients via brain-computer interfaces. However, training machine\nlearning algorithms on EEG encounters two primary challenges: variability\nacross data sets and privacy concerns using data from individuals and data\ncentres. Our objective is to address these challenges by integrating transfer\nlearning for data variability and federated learning for data privacy into a\nunified approach. We introduce the Sandwich as a novel deep privacy-preserving\nmeta-framework combining transfer learning and federated learning. The Sandwich\nframework comprises three components: federated networks (first layers) that\nhandle data set differences at the input level, a shared network (middle layer)\nlearning common rules and applying transfer learning, and individual\nclassifiers (final layers) for specific tasks of each data set. It enables the\ncentral network (central server) to benefit from multiple data sets, while\nlocal branches (local servers) maintain data and label privacy. We evaluated\nthe `Sandwich' meta-architecture in various configurations using the BEETL\nmotor imagery challenge, a benchmark for heterogeneous EEG data sets. Compared\nwith baseline models, our `Sandwich' implementations showed superior\nperformance. The best-performing model, the Inception Sandwich with deep set\nalignment (Inception-SD-Deepset), exceeded baseline methods by 9%. The\n`Sandwich' framework demonstrates significant advancements in federated deep\ntransfer learning for diverse tasks and data sets. It outperforms conventional\ndeep learning methods, showcasing the potential for effective use of larger,\nheterogeneous data sets with enhanced privacy as a model-agnostic\nmeta-framework.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.06868v2.pdf",
        "similarity": 0.34544918805865327,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-10"
    },
    {
        "new_title": "Towards Incremental Learning in Large Language Models: A Critical Review",
        "new_link": "http://arxiv.org/abs/2404.18311v4",
        "new_summary": "  Incremental learning is the ability of systems to acquire knowledge over\ntime, enabling their adaptation and generalization to novel tasks. It is a\ncritical ability for intelligent, real-world systems, especially when data\nchanges frequently or is limited. This review provides a comprehensive analysis\nof incremental learning in Large Language Models. It synthesizes the\nstate-of-the-art incremental learning paradigms, including continual learning,\nmeta-learning, parameter-efficient learning, and mixture-of-experts learning.\nWe demonstrate their utility for incremental learning by describing specific\nachievements from these related topics and their critical factors. An important\nfinding is that many of these approaches do not update the core model, and none\nof them update incrementally in real-time. The paper highlights current\nproblems and challenges for future research in the field. By consolidating the\nlatest relevant research developments, this review offers a comprehensive\nunderstanding of incremental learning and its implications for designing and\ndeveloping LLM-based learning systems.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.18311v4.pdf",
        "similarity": 0.34534339268824815,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-28"
    },
    {
        "new_title": "Atom-Level Optical Chemical Structure Recognition with Limited\n  Supervision",
        "new_link": "http://arxiv.org/abs/2404.01743v1",
        "new_summary": "  Identifying the chemical structure from a graphical representation, or image,\nof a molecule is a challenging pattern recognition task that would greatly\nbenefit drug development. Yet, existing methods for chemical structure\nrecognition do not typically generalize well, and show diminished effectiveness\nwhen confronted with domains where data is sparse, or costly to generate, such\nas hand-drawn molecule images. To address this limitation, we propose a new\nchemical structure recognition tool that delivers state-of-the-art performance\nand can adapt to new domains with a limited number of data samples and\nsupervision. Unlike previous approaches, our method provides atom-level\nlocalization, and can therefore segment the image into the different atoms and\nbonds. Our model is the first model to perform OCSR with atom-level entity\ndetection with only SMILES supervision. Through rigorous and extensive\nbenchmarking, we demonstrate the preeminence of our chemical structure\nrecognition approach in terms of data efficiency, accuracy, and atom-level\nentity prediction.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.01743v1.pdf",
        "similarity": 0.3453305831271384,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-02"
    },
    {
        "new_title": "Towards a Framework for Deep Learning Certification in Safety-Critical\n  Applications Using Inherently Safe Design and Run-Time Error Detection",
        "new_link": "http://arxiv.org/abs/2403.14678v1",
        "new_summary": "  Although an ever-growing number of applications employ deep learning based\nsystems for prediction, decision-making, or state estimation, almost no\ncertification processes have been established that would allow such systems to\nbe deployed in safety-critical applications. In this work we consider\nreal-world problems arising in aviation and other safety-critical areas, and\ninvestigate their requirements for a certified model. To this end, we\ninvestigate methodologies from the machine learning research community aimed\ntowards verifying robustness and reliability of deep learning systems, and\nevaluate these methodologies with regard to their applicability to real-world\nproblems. Then, we establish a new framework towards deep learning\ncertification based on (i) inherently safe design, and (ii) run-time error\ndetection. Using a concrete use case from aviation, we show how deep learning\nmodels can recover disentangled variables through the use of weakly-supervised\nrepresentation learning. We argue that such a system design is inherently less\nprone to common model failures, and can be verified to encode underlying\nmechanisms governing the data. Then, we investigate four techniques related to\nthe run-time safety of a model, namely (i) uncertainty quantification, (ii)\nout-of-distribution detection, (iii) feature collapse, and (iv) adversarial\nattacks. We evaluate each for their applicability and formulate a set of\ndesiderata that a certified model should fulfill. Finally, we propose a novel\nmodel structure that exhibits all desired properties discussed in this work,\nand is able to make regression and uncertainty predictions, as well as detect\nout-of-distribution inputs, while requiring no regression labels to train. We\nconclude with a discussion of the current state and expected future progress of\ndeep learning certification, and its industrial and social implications.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.14678v1.pdf",
        "similarity": 0.345272142686746,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-12"
    },
    {
        "new_title": "AnimalFormer: Multimodal Vision Framework for Behavior-based Precision\n  Livestock Farming",
        "new_link": "http://arxiv.org/abs/2406.09711v1",
        "new_summary": "  We introduce a multimodal vision framework for precision livestock farming,\nharnessing the power of GroundingDINO, HQSAM, and ViTPose models. This\nintegrated suite enables comprehensive behavioral analytics from video data\nwithout invasive animal tagging. GroundingDINO generates accurate bounding\nboxes around livestock, while HQSAM segments individual animals within these\nboxes. ViTPose estimates key body points, facilitating posture and movement\nanalysis. Demonstrated on a sheep dataset with grazing, running, sitting,\nstanding, and walking activities, our framework extracts invaluable insights:\nactivity and grazing patterns, interaction dynamics, and detailed postural\nevaluations. Applicable across species and video resolutions, this framework\nrevolutionizes non-invasive livestock monitoring for activity detection,\ncounting, health assessments, and posture analyses. It empowers data-driven\nfarm management, optimizing animal welfare and productivity through AI-powered\nbehavioral understanding.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.09711v1.pdf",
        "similarity": 0.3452611410686257,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-14"
    },
    {
        "new_title": "Leveraging Interpolation Models and Error Bounds for Verifiable\n  Scientific Machine Learning",
        "new_link": "http://arxiv.org/abs/2404.03586v1",
        "new_summary": "  Effective verification and validation techniques for modern scientific\nmachine learning workflows are challenging to devise. Statistical methods are\nabundant and easily deployed, but often rely on speculative assumptions about\nthe data and methods involved. Error bounds for classical interpolation\ntechniques can provide mathematically rigorous estimates of accuracy, but often\nare difficult or impractical to determine computationally. In this work, we\npresent a best-of-both-worlds approach to verifiable scientific machine\nlearning by demonstrating that (1) multiple standard interpolation techniques\nhave informative error bounds that can be computed or estimated efficiently;\n(2) comparative performance among distinct interpolants can aid in validation\ngoals; (3) deploying interpolation methods on latent spaces generated by deep\nlearning techniques enables some interpretability for black-box models. We\npresent a detailed case study of our approach for predicting lift-drag ratios\nfrom airfoil images. Code developed for this work is available in a public\nGithub repository.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.03586v1.pdf",
        "similarity": 0.345218005154236,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-04"
    },
    {
        "new_title": "Large Language Model-Based Interpretable Machine Learning Control in\n  Building Energy Systems",
        "new_link": "http://arxiv.org/abs/2402.09584v1",
        "new_summary": "  The potential of Machine Learning Control (MLC) in HVAC systems is hindered\nby its opaque nature and inference mechanisms, which is challenging for users\nand modelers to fully comprehend, ultimately leading to a lack of trust in\nMLC-based decision-making. To address this challenge, this paper investigates\nand explores Interpretable Machine Learning (IML), a branch of Machine Learning\n(ML) that enhances transparency and understanding of models and their\ninferences, to improve the credibility of MLC and its industrial application in\nHVAC systems. Specifically, we developed an innovative framework that combines\nthe principles of Shapley values and the in-context learning feature of Large\nLanguage Models (LLMs). While the Shapley values are instrumental in dissecting\nthe contributions of various features in ML models, LLM provides an in-depth\nunderstanding of rule-based parts in MLC; combining them, LLM further packages\nthese insights into a coherent, human-understandable narrative. The paper\npresents a case study to demonstrate the feasibility of the developed IML\nframework for model predictive control-based precooling under demand response\nevents in a virtual testbed. The results indicate that the developed framework\ngenerates and explains the control signals in accordance with the rule-based\nrationale.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.09584v1.pdf",
        "similarity": 0.34508425850402435,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-14"
    },
    {
        "new_title": "Position: Topological Deep Learning is the New Frontier for Relational\n  Learning",
        "new_link": "http://arxiv.org/abs/2402.08871v2",
        "new_summary": "  Topological deep learning (TDL) is a rapidly evolving field that uses\ntopological features to understand and design deep learning models. This paper\nposits that TDL is the new frontier for relational learning. TDL may complement\ngraph representation learning and geometric deep learning by incorporating\ntopological concepts, and can thus provide a natural choice for various machine\nlearning settings. To this end, this paper discusses open problems in TDL,\nranging from practical benefits to theoretical foundations. For each problem,\nit outlines potential solutions and future research opportunities. At the same\ntime, this paper serves as an invitation to the scientific community to\nactively participate in TDL research to unlock the potential of this emerging\nfield.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.08871v2.pdf",
        "similarity": 0.34466455828213766,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-14"
    },
    {
        "new_title": "On the Application of Egocentric Computer Vision to Industrial Scenarios",
        "new_link": "http://arxiv.org/abs/2406.07738v1",
        "new_summary": "  Egocentric vision aims to capture and analyse the world from the first-person\nperspective. We explore the possibilities for egocentric wearable devices to\nimprove and enhance industrial use cases w.r.t. data collection, annotation,\nlabelling and downstream applications. This would contribute to easier data\ncollection and allow users to provide additional context. We envision that this\napproach could serve as a supplement to the traditional industrial Machine\nVision workflow. Code, Dataset and related resources will be available at:\nhttps://github.com/Vivek9Chavan/EgoVis24\n",
        "pdf_link": "https://arxiv.org/pdf/2406.07738v1.pdf",
        "similarity": 0.34429861861111855,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-11"
    },
    {
        "new_title": "Quality Scalable Quantization Methodology for Deep Learning on Edge",
        "new_link": "http://arxiv.org/abs/2407.11260v1",
        "new_summary": "  Deep Learning Architectures employ heavy computations and bulk of the\ncomputational energy is taken up by the convolution operations in the\nConvolutional Neural Networks. The objective of our proposed work is to reduce\nthe energy consumption and size of CNN for using machine learning techniques in\nedge computing on ubiquitous computing devices. We propose Systematic Quality\nScalable Design Methodology consisting of Quality Scalable Quantization on a\nhigher abstraction level and Quality Scalable Multipliers at lower abstraction\nlevel. The first component consists of parameter compression where we\napproximate representation of values in filters of deep learning models by\nencoding in 3 bits. A shift and scale based on-chip decoding hardware is\nproposed which can decode these 3-bit representations to recover approximate\nfilter values. The size of the DNN model is reduced this way and can be sent\nover a communication channel to be decoded on the edge computing devices. This\nway power is reduced by limiting data bits by approximation. In the second\ncomponent we propose a quality scalable multiplier which reduces the number of\npartial products by converting numbers in canonic sign digit representations\nand further approximating the number by reducing least significant bits. These\nquantized CNNs provide almost same ac-curacy as network with original weights\nwith little or no fine-tuning. The hardware for the adaptive multipliers\nutilize gate clocking for reducing energy consumption during multiplications.\nThe proposed methodology greatly reduces the memory and power requirements of\nDNN models making it a feasible approach to deploy Deep Learning on edge\ncomputing. The experiments done on LeNet and ConvNets show an increase upto 6%\nof zeros and memory savings upto 82.4919% while keeping the accuracy near the\nstate of the art.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.11260v1.pdf",
        "similarity": 0.3439621238583602,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-15"
    },
    {
        "new_title": "Parametric PDE Control with Deep Reinforcement Learning and\n  Differentiable L0-Sparse Polynomial Policies",
        "new_link": "http://arxiv.org/abs/2403.15267v1",
        "new_summary": "  Optimal control of parametric partial differential equations (PDEs) is\ncrucial in many applications in engineering and science. In recent years, the\nprogress in scientific machine learning has opened up new frontiers for the\ncontrol of parametric PDEs. In particular, deep reinforcement learning (DRL)\nhas the potential to solve high-dimensional and complex control problems in a\nlarge variety of applications. Most DRL methods rely on deep neural network\n(DNN) control policies. However, for many dynamical systems, DNN-based control\npolicies tend to be over-parametrized, which means they need large amounts of\ntraining data, show limited robustness, and lack interpretability. In this\nwork, we leverage dictionary learning and differentiable L$_0$ regularization\nto learn sparse, robust, and interpretable control policies for parametric\nPDEs. Our sparse policy architecture is agnostic to the DRL method and can be\nused in different policy-gradient and actor-critic DRL algorithms without\nchanging their policy-optimization procedure. We test our approach on the\nchallenging tasks of controlling parametric Kuramoto-Sivashinsky and\nconvection-diffusion-reaction PDEs. We show that our method (1) outperforms\nbaseline DNN-based DRL policies, (2) allows for the derivation of interpretable\nequations of the learned optimal control laws, and (3) generalizes to unseen\nparameters of the PDE without retraining the policies.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.15267v1.pdf",
        "similarity": 0.3439417033719519,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-22"
    },
    {
        "new_title": "Exploring the efficacy of a hybrid approach with modal decomposition\n  over fully deep learning models for flow dynamics forecasting",
        "new_link": "http://arxiv.org/abs/2404.17884v1",
        "new_summary": "  Fluid dynamics problems are characterized by being multidimensional and\nnonlinear, causing the experiments and numerical simulations being complex,\ntime-consuming and monetarily expensive. In this sense, there is a need to find\nnew ways to obtain data in a more economical manner. Thus, in this work we\nstudy the application of time series forecasting to fluid dynamics problems,\nwhere the aim is to predict the flow dynamics using only past information. We\nfocus our study on models based on deep learning that do not require a high\namount of data for training, as this is the problem we are trying to address.\nSpecifically in this work we have tested three autoregressive models where two\nof them are fully based on deep learning and the other one is a hybrid model\nthat combines modal decomposition with deep learning. We ask these models to\ngenerate $200$ time-ahead predictions of two datasets coming from a numerical\nsimulation and experimental measurements, where the latter is characterized by\nbeing turbulent. We show how the hybrid model generates more reliable\npredictions in the experimental case, as it is physics-informed in the sense\nthat the modal decomposition extracts the physics in a way that allows us to\npredict it.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.17884v1.pdf",
        "similarity": 0.3437890794675498,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-27"
    },
    {
        "new_title": "Deep Time Series Models: A Comprehensive Survey and Benchmark",
        "new_link": "http://arxiv.org/abs/2407.13278v1",
        "new_summary": "  Time series, characterized by a sequence of data points arranged in a\ndiscrete-time order, are ubiquitous in real-world applications. Different from\nother modalities, time series present unique challenges due to their complex\nand dynamic nature, including the entanglement of nonlinear patterns and\ntime-variant trends. Analyzing time series data is of great significance in\nreal-world scenarios and has been widely studied over centuries. Recent years\nhave witnessed remarkable breakthroughs in the time series community, with\ntechniques shifting from traditional statistical methods to advanced deep\nlearning models. In this paper, we delve into the design of deep time series\nmodels across various analysis tasks and review the existing literature from\ntwo perspectives: basic modules and model architectures. Further, we develop\nand release Time Series Library (TSLib) as a fair benchmark of deep time series\nmodels for diverse analysis tasks, which implements 24 mainstream models,\ncovers 30 datasets from different domains, and supports five prevalent analysis\ntasks. Based on TSLib, we thoroughly evaluate 12 advanced deep time series\nmodels on different tasks. Empirical results indicate that models with specific\nstructures are well-suited for distinct analytical tasks, which offers insights\nfor research and adoption of deep time series models. Code is available at\nhttps://github.com/thuml/Time-Series-Library.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.13278v1.pdf",
        "similarity": 0.34374807940536667,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-18"
    },
    {
        "new_title": "Mission Critical -- Satellite Data is a Distinct Modality in Machine\n  Learning",
        "new_link": "http://arxiv.org/abs/2402.01444v1",
        "new_summary": "  Satellite data has the potential to inspire a seismic shift for machine\nlearning -- one in which we rethink existing practices designed for traditional\ndata modalities. As machine learning for satellite data (SatML) gains traction\nfor its real-world impact, our field is at a crossroads. We can either continue\napplying ill-suited approaches, or we can initiate a new research agenda that\ncenters around the unique characteristics and challenges of satellite data.\nThis position paper argues that satellite data constitutes a distinct modality\nfor machine learning research and that we must recognize it as such to advance\nthe quality and impact of SatML research across theory, methods, and\ndeployment. We outline critical discussion questions and actionable suggestions\nto transform SatML from merely an intriguing application area to a dedicated\nresearch discipline that helps move the needle on big challenges for machine\nlearning and society.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.01444v1.pdf",
        "similarity": 0.34287859525353437,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-02"
    },
    {
        "new_title": "A Role of Environmental Complexity on Representation Learning in Deep\n  Reinforcement Learning Agents",
        "new_link": "http://arxiv.org/abs/2407.03436v1",
        "new_summary": "  The environments where individuals live can present diverse navigation\nchallenges, resulting in varying navigation abilities and strategies. Inspired\nby differing urban layouts and the Dual Solutions Paradigm test used for human\nnavigators, we developed a simulated navigation environment to train deep\nreinforcement learning agents in a shortcut usage task. We modulated the\nfrequency of exposure to a shortcut and navigation cue, leading to the\ndevelopment of artificial agents with differing abilities. We examined the\nencoded representations in artificial neural networks driving these agents,\nrevealing intricate dynamics in representation learning, and correlated them\nwith shortcut use preferences. Furthermore, we demonstrated methods to analyze\nrepresentations across a population of nodes, which proved effective in finding\npatterns in what would otherwise be noisy single-node data. These techniques\nmay also have broader applications in studying neural activity. From our\nobservations in representation learning dynamics, we propose insights for human\nnavigation learning, emphasizing the importance of navigation challenges in\ndeveloping strong landmark knowledge over repeated exposures to landmarks\nalone.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.03436v1.pdf",
        "similarity": 0.3428642747877988,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-03"
    },
    {
        "new_title": "Layout-Agnostic Scene Text Image Synthesis with Diffusion Models",
        "new_link": "http://arxiv.org/abs/2406.01062v4",
        "new_summary": "  While diffusion models have significantly advanced the quality of image\ngeneration their capability to accurately and coherently render text within\nthese images remains a substantial challenge. Conventional diffusion-based\nmethods for scene text generation are typically limited by their reliance on an\nintermediate layout output. This dependency often results in a constrained\ndiversity of text styles and fonts an inherent limitation stemming from the\ndeterministic nature of the layout generation phase. To address these\nchallenges this paper introduces SceneTextGen a novel diffusion-based model\nspecifically designed to circumvent the need for a predefined layout stage. By\ndoing so SceneTextGen facilitates a more natural and varied representation of\ntext. The novelty of SceneTextGen lies in its integration of three key\ncomponents: a character-level encoder for capturing detailed typographic\nproperties coupled with a character-level instance segmentation model and a\nword-level spotting model to address the issues of unwanted text generation and\nminor character inaccuracies. We validate the performance of our method by\ndemonstrating improved character recognition rates on generated images across\ndifferent public visual text datasets in comparison to both standard diffusion\nbased methods and text specific methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.01062v4.pdf",
        "similarity": 0.3427849088169794,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-03"
    },
    {
        "new_title": "Forecasting VIX using Bayesian Deep Learning",
        "new_link": "http://arxiv.org/abs/2401.17042v1",
        "new_summary": "  Recently, deep learning techniques are gradually replacing traditional\nstatistical and machine learning models as the first choice for price\nforecasting tasks. In this paper, we leverage probabilistic deep learning for\ninferring the volatility index VIX. We employ the probabilistic counterpart of\nWaveNet, Temporal Convolutional Network (TCN), and Transformers. We show that\nTCN outperforms all models with an RMSE around 0.189. In addition, it has been\nwell known that modern neural networks provide inaccurate uncertainty\nestimates. For solving this problem, we use the standard deviation scaling to\ncalibrate the networks. Furthermore, we found out that MNF with Gaussian prior\noutperforms Reparameterization Trick and Flipout models in terms of precision\nand uncertainty predictions. Finally, we claim that MNF with Cauchy and\nLogUniform prior distributions yield well calibrated TCN and WaveNet networks\nbeing the former that best infer the VIX values.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.17042v1.pdf",
        "similarity": 0.34238576277532917,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-30"
    },
    {
        "new_title": "Vertical Symbolic Regression via Deep Policy Gradient",
        "new_link": "http://arxiv.org/abs/2402.00254v1",
        "new_summary": "  Vertical Symbolic Regression (VSR) recently has been proposed to expedite the\ndiscovery of symbolic equations with many independent variables from\nexperimental data. VSR reduces the search spaces following the vertical\ndiscovery path by building from reduced-form equations involving a subset of\nindependent variables to full-fledged ones. Proved successful by many symbolic\nregressors, deep neural networks are expected to further scale up VSR.\nNevertheless, directly combining VSR with deep neural networks will result in\ndifficulty in passing gradients and other engineering issues. We propose\nVertical Symbolic Regression using Deep Policy Gradient (VSR-DPG) and\ndemonstrate that VSR-DPG can recover ground-truth equations involving multiple\ninput variables, significantly beyond both deep reinforcement learning-based\napproaches and previous VSR variants. Our VSR-DPG models symbolic regression as\na sequential decision-making process, in which equations are built from\nrepeated applications of grammar rules. The integrated deep model is trained to\nmaximize a policy gradient objective. Experimental results demonstrate that our\nVSR-DPG significantly outperforms popular baselines in identifying both\nalgebraic equations and ordinary differential equations on a series of\nbenchmarks.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.00254v1.pdf",
        "similarity": 0.3414658248220433,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-01"
    },
    {
        "new_title": "Seismic First Break Picking in a Higher Dimension Using Deep Graph\n  Learning",
        "new_link": "http://arxiv.org/abs/2404.08408v1",
        "new_summary": "  Contemporary automatic first break (FB) picking methods typically analyze 1D\nsignals, 2D source gathers, or 3D source-receiver gathers. Utilizing\nhigher-dimensional data, such as 2D or 3D, incorporates global features,\nimproving the stability of local picking. Despite the benefits,\nhigh-dimensional data requires structured input and increases computational\ndemands. Addressing this, we propose a novel approach using deep graph learning\ncalled DGL-FB, constructing a large graph to efficiently extract information.\nIn this graph, each seismic trace is represented as a node, connected by edges\nthat reflect similarities. To manage the size of the graph, we develop a\nsubgraph sampling technique to streamline model training and inference. Our\nproposed framework, DGL-FB, leverages deep graph learning for FB picking. It\nencodes subgraphs into global features using a deep graph encoder.\nSubsequently, the encoded global features are combined with local node signals\nand fed into a ResUNet-based 1D segmentation network for FB detection. Field\nsurvey evaluations of DGL-FB show superior accuracy and stability compared to a\n2D U-Net-based benchmark method.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.08408v1.pdf",
        "similarity": 0.34107343004456037,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-12"
    },
    {
        "new_title": "Quantum Hamiltonian Embedding of Images for Data Reuploading Classifiers",
        "new_link": "http://arxiv.org/abs/2407.14055v1",
        "new_summary": "  When applying quantum computing to machine learning tasks, one of the first\nconsiderations is the design of the quantum machine learning model itself.\nConventionally, the design of quantum machine learning algorithms relies on the\n``quantisation\" of classical learning algorithms, such as using quantum linear\nalgebra to implement important subroutines of classical algorithms, if not the\nentire algorithm, seeking to achieve quantum advantage through possible\nrun-time accelerations brought by quantum computing. However, recent research\nhas started questioning whether quantum advantage via speedup is the right goal\nfor quantum machine learning [1]. Research also has been undertaken to exploit\nproperties that are unique to quantum systems, such as quantum contextuality,\nto better design quantum machine learning models [2]. In this paper, we take an\nalternative approach by incorporating the heuristics and empirical evidences\nfrom the design of classical deep learning algorithms to the design of quantum\nneural networks. We first construct a model based on the data reuploading\ncircuit [3] with the quantum Hamiltonian data embedding unitary [4]. Through\nnumerical experiments on images datasets, including the famous MNIST and\nFashionMNIST datasets, we demonstrate that our model outperforms the quantum\nconvolutional neural network (QCNN)[5] by a large margin (up to over 40% on\nMNIST test set). Based on the model design process and numerical results, we\nthen laid out six principles for designing quantum machine learning models,\nespecially quantum neural networks.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.14055v1.pdf",
        "similarity": 0.34102773546788734,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-19"
    },
    {
        "new_title": "PhilHumans: Benchmarking Machine Learning for Personal Health",
        "new_link": "http://arxiv.org/abs/2405.02770v2",
        "new_summary": "  The use of machine learning in Healthcare has the potential to improve\npatient outcomes as well as broaden the reach and affordability of Healthcare.\nThe history of other application areas indicates that strong benchmarks are\nessential for the development of intelligent systems. We present Personal\nHealth Interfaces Leveraging HUman-MAchine Natural interactions (PhilHumans), a\nholistic suite of benchmarks for machine learning across different Healthcare\nsettings - talk therapy, diet coaching, emergency care, intensive care,\nobstetric sonography - as well as different learning settings, such as action\nanticipation, timeseries modeling, insight mining, language modeling, computer\nvision, reinforcement learning and program synthesis\n",
        "pdf_link": "https://arxiv.org/pdf/2405.02770v2.pdf",
        "similarity": 0.3403797546125651,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-04"
    },
    {
        "new_title": "LADDER: Revisiting the Cosmic Distance Ladder with Deep Learning\n  Approaches and Exploring its Applications",
        "new_link": "http://arxiv.org/abs/2401.17029v2",
        "new_summary": "  We investigate the prospect of reconstructing the ''cosmic distance ladder''\nof the Universe using a novel deep learning framework called LADDER - Learning\nAlgorithm for Deep Distance Estimation and Reconstruction. LADDER is trained on\nthe apparent magnitude data from the Pantheon Type Ia supernovae compilation,\nincorporating the full covariance information among data points, to produce\npredictions along with corresponding errors. After employing several validation\ntests with a number of deep learning models, we pick LADDER as the best\nperforming one. We then demonstrate applications of our method in the\ncosmological context, including serving as a model-independent tool for\nconsistency checks for other datasets like baryon acoustic oscillations,\ncalibration of high-redshift datasets such as gamma ray bursts, and use as a\nmodel-independent mock catalog generator for future probes. Our analysis\nadvocates for careful consideration of machine learning techniques applied to\ncosmological contexts.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.17029v2.pdf",
        "similarity": 0.3403694453330629,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-30"
    },
    {
        "new_title": "End-to-End Model-based Deep Learning for Dual-Energy Computed Tomography\n  Material Decomposition",
        "new_link": "http://arxiv.org/abs/2406.00479v1",
        "new_summary": "  Dual energy X-ray Computed Tomography (DECT) enables to automatically\ndecompose materials in clinical images without the manual segmentation using\nthe dependency of the X-ray linear attenuation with energy. In this work we\npropose a deep learning procedure called End-to-End Material Decomposition\n(E2E-DEcomp) for quantitative material decomposition which directly convert the\nCT projection data into material images. The algorithm is based on\nincorporating the knowledge of the spectral model DECT system into the deep\nlearning training loss and combining a data-learned prior in the material image\ndomain. Furthermore, the training does not require any energy-based images in\nthe dataset but rather only sinogram and material images. We show the\neffectiveness of the proposed direct E2E-DEcomp method on the AAPM spectral CT\ndataset (Sidky and Pan, 2023) compared with state of the art supervised deep\nlearning networks.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.00479v1.pdf",
        "similarity": 0.34003455824284995,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-01"
    },
    {
        "new_title": "Energy-Guided Data Sampling for Traffic Prediction with Mini Training\n  Datasets",
        "new_link": "http://arxiv.org/abs/2403.18710v2",
        "new_summary": "  Recent endeavors aimed at forecasting future traffic flow states through deep\nlearning encounter various challenges and yield diverse outcomes. A notable\nobstacle arises from the substantial data requirements of deep learning models,\na resource often scarce in traffic flow systems. Despite the abundance of\ndomain knowledge concerning traffic flow dynamics, prevailing deep learning\nmethodologies frequently fail to fully exploit it. To address these issues, we\npropose an innovative solution that merges Convolutional Neural Networks (CNNs)\nwith Long Short-Term Memory (LSTM) architecture to enhance the prediction of\ntraffic flow dynamics. A key revelation of our research is the feasibility of\nsampling training data for large traffic systems from simulations conducted on\nsmaller traffic systems. This insight suggests the potential for referencing a\nmacroscopic-level distribution to inform the sampling of microscopic data. Such\nsampling is facilitated by the observed scale invariance in the normalized\nenergy distribution of the statistical mechanics model, thereby streamlining\nthe data generation process for large-scale traffic systems. Our simulations\ndemonstrate promising agreement between predicted and actual traffic flow\ndynamics, underscoring the efficacy of our proposed approach.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.18710v2.pdf",
        "similarity": 0.33997357556976665,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-27"
    },
    {
        "new_title": "Taming Differentiable Logics with Coq Formalisation",
        "new_link": "http://arxiv.org/abs/2403.13700v2",
        "new_summary": "  For performance and verification in machine learning, new methods have\nrecently been proposed that optimise learning systems to satisfy formally\nexpressed logical properties. Among these methods, differentiable logics (DLs)\nare used to translate propositional or first-order formulae into loss functions\ndeployed for optimisation in machine learning. At the same time, recent\nattempts to give programming language support for verification of neural\nnetworks showed that DLs can be used to compile verification properties to\nmachine-learning backends. This situation is calling for stronger guarantees\nabout the soundness of such compilers, the soundness and compositionality of\nDLs, and the differentiability and performance of the resulting loss functions.\nIn this paper, we propose an approach to formalise existing DLs using the\nMathematical Components library in the Coq proof assistant. Thanks to this\nformalisation, we are able to give uniform semantics to otherwise disparate\nDLs, give formal proofs to existing informal arguments, find errors in previous\nwork, and provide formal proofs to missing conjectured properties. This work is\nmeant as a stepping stone for the development of programming language support\nfor verification of machine learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.13700v2.pdf",
        "similarity": 0.33957960921587865,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-20"
    },
    {
        "new_title": "Real-time Yemeni Currency Detection",
        "new_link": "http://arxiv.org/abs/2406.13034v1",
        "new_summary": "  Banknote recognition is a major problem faced by visually Challenged people.\nSo we propose a application to help the visually Challenged people to identify\nthe different types of Yemenian currencies through deep learning technique. As\nmoney has a significant role in daily life for any business transactions,\nreal-time detection and recognition of banknotes become necessary for a person,\nespecially blind or visually impaired, or for a system that sorts the data.\nThis paper presents a real-time Yemeni currency detection system for visually\nimpaired persons. The proposed system exploits the deep learning approach to\nfacilitate the visually impaired people to prosperously recognize banknotes.\nFor real-time recognition, we have deployed the system into a mobile\napplication.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.13034v1.pdf",
        "similarity": 0.33943298168315217,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-18"
    },
    {
        "new_title": "Cross-Task Multi-Branch Vision Transformer for Facial Expression and\n  Mask Wearing Classification",
        "new_link": "http://arxiv.org/abs/2404.14606v2",
        "new_summary": "  With wearing masks becoming a new cultural norm, facial expression\nrecognition (FER) while taking masks into account has become a significant\nchallenge. In this paper, we propose a unified multi-branch vision transformer\nfor facial expression recognition and mask wearing classification tasks. Our\napproach extracts shared features for both tasks using a dual-branch\narchitecture that obtains multi-scale feature representations. Furthermore, we\npropose a cross-task fusion phase that processes tokens for each task with\nseparate branches, while exchanging information using a cross attention module.\nOur proposed framework reduces the overall complexity compared with using\nseparate networks for both tasks by the simple yet effective cross-task fusion\nphase. Extensive experiments demonstrate that our proposed model performs\nbetter than or on par with different state-of-the-art methods on both facial\nexpression recognition and facial mask wearing classification task.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.14606v2.pdf",
        "similarity": 0.3393047123989411,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-22"
    },
    {
        "new_title": "Enhancing supply chain security with automated machine learning",
        "new_link": "http://arxiv.org/abs/2406.13166v1",
        "new_summary": "  This study tackles the complexities of global supply chains, which are\nincreasingly vulnerable to disruptions caused by port congestion, material\nshortages, and inflation. To address these challenges, we explore the\napplication of machine learning methods, which excel in predicting and\noptimizing solutions based on large datasets. Our focus is on enhancing supply\nchain security through fraud detection, maintenance prediction, and material\nbackorder forecasting. We introduce an automated machine learning framework\nthat streamlines data analysis, model construction, and hyperparameter\noptimization for these tasks. By automating these processes, our framework\nimproves the efficiency and effectiveness of supply chain security measures.\nOur research identifies key factors that influence machine learning\nperformance, including sampling methods, categorical encoding, feature\nselection, and hyperparameter optimization. We demonstrate the importance of\nconsidering these factors when applying machine learning to supply chain\nchallenges. Traditional mathematical programming models often struggle to cope\nwith the complexity of large-scale supply chain problems. Our study shows that\nmachine learning methods can provide a viable alternative, particularly when\ndealing with extensive datasets and complex patterns. The automated machine\nlearning framework presented in this study offers a novel approach to supply\nchain security, contributing to the existing body of knowledge in the field.\nIts comprehensive automation of machine learning processes makes it a valuable\ncontribution to the domain of supply chain management.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.13166v1.pdf",
        "similarity": 0.33894155580191737,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-19"
    },
    {
        "new_title": "Scalable Bayesian Learning with posteriors",
        "new_link": "http://arxiv.org/abs/2406.00104v1",
        "new_summary": "  Although theoretically compelling, Bayesian learning with modern machine\nlearning models is computationally challenging since it requires approximating\na high dimensional posterior distribution. In this work, we (i) introduce\nposteriors, an easily extensible PyTorch library hosting general-purpose\nimplementations making Bayesian learning accessible and scalable to large data\nand parameter regimes; (ii) present a tempered framing of stochastic gradient\nMarkov chain Monte Carlo, as implemented in posteriors, that transitions\nseamlessly into optimization and unveils a minor modification to deep ensembles\nto ensure they are asymptotically unbiased for the Bayesian posterior, and\n(iii) demonstrate and compare the utility of Bayesian approximations through\nexperiments including an investigation into the cold posterior effect and\napplications with large language models.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.00104v1.pdf",
        "similarity": 0.338914418818796,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-31"
    },
    {
        "new_title": "A real-time, robust and versatile visual-SLAM framework based on deep\n  learning networks",
        "new_link": "http://arxiv.org/abs/2405.03413v3",
        "new_summary": "  This paper explores how deep learning techniques can improve visual-based\nSLAM performance in challenging environments. By combining deep feature\nextraction and deep matching methods, we introduce a versatile hybrid visual\nSLAM system designed to enhance adaptability in challenging scenarios, such as\nlow-light conditions, dynamic lighting, weak-texture areas, and severe jitter.\nOur system supports multiple modes, including monocular, stereo,\nmonocular-inertial, and stereo-inertial configurations. We also perform\nanalysis how to combine visual SLAM with deep learning methods to enlighten\nother researches. Through extensive experiments on both public datasets and\nself-sampled data, we demonstrate the superiority of the SL-SLAM system over\ntraditional approaches. The experimental results show that SL-SLAM outperforms\nstate-of-the-art SLAM algorithms in terms of localization accuracy and tracking\nrobustness. For the benefit of community, we make public the source code at\nhttps://github.com/zzzzxxxx111/SLslam.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.03413v3.pdf",
        "similarity": 0.33891005807574487,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-06"
    },
    {
        "new_title": "Learning Run-time Safety Monitors for Machine Learning Components",
        "new_link": "http://arxiv.org/abs/2406.16220v1",
        "new_summary": "  For machine learning components used as part of autonomous systems (AS) in\ncarrying out critical tasks it is crucial that assurance of the models can be\nmaintained in the face of post-deployment changes (such as changes in the\noperating environment of the system). A critical part of this is to be able to\nmonitor when the performance of the model at runtime (as a result of changes)\nposes a safety risk to the system. This is a particularly difficult challenge\nwhen ground truth is unavailable at runtime. In this paper we introduce a\nprocess for creating safety monitors for ML components through the use of\ndegraded datasets and machine learning. The safety monitor that is created is\ndeployed to the AS in parallel to the ML component to provide a prediction of\nthe safety risk associated with the model output. We demonstrate the viability\nof our approach through some initial experiments using publicly available speed\nsign datasets.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.16220v1.pdf",
        "similarity": 0.3385514617784215,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-23"
    },
    {
        "new_title": "The Promise of Analog Deep Learning: Recent Advances, Challenges and\n  Opportunities",
        "new_link": "http://arxiv.org/abs/2406.12911v1",
        "new_summary": "  Much of the present-day Artificial Intelligence (AI) utilizes artificial\nneural networks, which are sophisticated computational models designed to\nrecognize patterns and solve complex problems by learning from data. However, a\nmajor bottleneck occurs during a device's calculation of weighted sums for\nforward propagation and optimization procedure for backpropagation, especially\nfor deep neural networks, or networks with numerous layers. Exploration into\ndifferent methods of implementing neural networks is necessary for further\nadvancement of the area. While a great deal of research into AI hardware in\nboth directions, analog and digital implementation widely exists, much of the\nexisting survey works lacks discussion on the progress of analog deep learning.\nTo this end, we attempt to evaluate and specify the advantages and\ndisadvantages, along with the current progress with regards to deep learning,\nfor analog implementations. In this paper, our focus lies on the comprehensive\nexamination of eight distinct analog deep learning methodologies across\nmultiple key parameters. These parameters include attained accuracy levels,\napplication domains, algorithmic advancements, computational speed, and\nconsiderations of energy efficiency and power consumption. We also identify the\nneural network-based experiments implemented using these hardware devices and\ndiscuss comparative performance achieved by the different analog deep learning\nmethods along with an analysis of their current limitations. Overall, we find\nthat Analog Deep Learning has great potential for future consumer-level\napplications, but there is still a long road ahead in terms of scalability.\nMost of the current implementations are more proof of concept and are not yet\npractically deployable for large-scale models.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.12911v1.pdf",
        "similarity": 0.3383536502463413,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-13"
    },
    {
        "new_title": "An Ensemble Framework for Explainable Geospatial Machine Learning Models",
        "new_link": "http://arxiv.org/abs/2403.03328v1",
        "new_summary": "  Analyzing spatial varying effect is pivotal in geographic analysis. Yet,\naccurately capturing and interpreting this variability is challenging due to\nthe complexity and non-linearity of geospatial data. Herein, we introduce an\nintegrated framework that merges local spatial weighting scheme, Explainable\nArtificial Intelligence (XAI), and cutting-edge machine learning technologies\nto bridge the gap between traditional geographic analysis models and general\nmachine learning approaches. Through tests on synthetic datasets, this\nframework is verified to enhance the interpretability and accuracy of\npredictions in both geographic regression and classification by elucidating\nspatial variability. It significantly boosts prediction precision, offering a\nnovel approach to understanding spatial phenomena.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.03328v1.pdf",
        "similarity": 0.33827961528761163,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-05"
    },
    {
        "new_title": "Assessing Patient Eligibility for Inspire Therapy through Machine\n  Learning and Deep Learning Models",
        "new_link": "http://arxiv.org/abs/2402.01067v1",
        "new_summary": "  Inspire therapy is an FDA-approved internal neurostimulation treatment for\nobstructive sleep apnea. However, not all patients respond to this therapy,\nposing a challenge even for experienced otolaryngologists to determine\ncandidacy. This paper makes the first attempt to leverage both machine learning\nand deep learning techniques in discerning patient responsiveness to Inspire\ntherapy using medical data and videos captured through Drug-Induced Sleep\nEndoscopy (DISE), an essential procedure for Inspire therapy. To achieve this,\nwe gathered and annotated three datasets from 127 patients. Two of these\ndatasets comprise endoscopic videos focused on the Base of the Tongue and\nVelopharynx. The third dataset composes the patient's clinical information. By\nutilizing these datasets, we benchmarked and compared the performance of six\ndeep learning models and five classical machine learning algorithms. The\nresults demonstrate the potential of employing machine learning and deep\nlearning techniques to determine a patient's eligibility for Inspire therapy,\npaving the way for future advancements in this field.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.01067v1.pdf",
        "similarity": 0.3382678813490515,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-01"
    },
    {
        "new_title": "Overcoming Saturation in Density Ratio Estimation by Iterated\n  Regularization",
        "new_link": "http://arxiv.org/abs/2402.13891v2",
        "new_summary": "  Estimating the ratio of two probability densities from finitely many samples,\nis a central task in machine learning and statistics. In this work, we show\nthat a large class of kernel methods for density ratio estimation suffers from\nerror saturation, which prevents algorithms from achieving fast error\nconvergence rates on highly regular learning problems. To resolve saturation,\nwe introduce iterated regularization in density ratio estimation to achieve\nfast error rates. Our methods outperform its non-iteratively regularized\nversions on benchmarks for density ratio estimation as well as on large-scale\nevaluations for importance-weighted ensembling of deep unsupervised domain\nadaptation models.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.13891v2.pdf",
        "similarity": 0.3380269555435825,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-21"
    },
    {
        "new_title": "Cascaded Self-supervised Learning for Subject-independent EEG-based\n  Emotion Recognition",
        "new_link": "http://arxiv.org/abs/2403.04041v1",
        "new_summary": "  EEG-based Emotion recognition holds significant promise for applications in\nhuman-computer interaction, medicine, and neuroscience. While deep learning has\nshown potential in this field, current approaches usually rely on large-scale\nhigh-quality labeled datasets, limiting the performance of deep learning.\nSelf-supervised learning offers a solution by automatically generating labels,\nbut its inter-subject generalizability remains under-explored. For this reason,\nour interest lies in offering a self-supervised learning paradigm with better\ninter-subject generalizability. Inspired by recent efforts in combining\nlow-level and high-level tasks in deep learning, we propose a cascaded\nself-supervised architecture for EEG emotion recognition. Then, we introduce a\nlow-level task, time-to-frequency reconstruction (TFR). This task leverages the\ninherent time-frequency relationship in EEG signals. Our architecture\nintegrates it with the high-level contrastive learning modules, performing\nself-supervised learning for EEG-based emotion recognition. Experiment on DEAP\nand DREAMER datasets demonstrates superior performance of our method over\nsimilar works. The outcome results also highlight the indispensability of the\nTFR task and the robustness of our method to label scarcity, validating the\neffectiveness of the proposed method.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.04041v1.pdf",
        "similarity": 0.3375281828635813,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-06"
    },
    {
        "new_title": "Online Classification with Predictions",
        "new_link": "http://arxiv.org/abs/2405.14066v1",
        "new_summary": "  We study online classification when the learner has access to predictions\nabout future examples. We design an online learner whose expected regret is\nnever worse than the worst-case regret, gracefully improves with the quality of\nthe predictions, and can be significantly better than the worst-case regret\nwhen the predictions of future examples are accurate. As a corollary, we show\nthat if the learner is always guaranteed to observe data where future examples\nare easily predictable, then online learning can be as easy as transductive\nonline learning. Our results complement recent work in online algorithms with\npredictions and smoothed online classification, which go beyond a worse-case\nanalysis by using machine-learned predictions and distributional assumptions\nrespectively.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.14066v1.pdf",
        "similarity": 0.3374588330657613,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-22"
    },
    {
        "new_title": "Automated Machine Learning for Multi-Label Classification",
        "new_link": "http://arxiv.org/abs/2402.18198v1",
        "new_summary": "  Automated machine learning (AutoML) aims to select and configure machine\nlearning algorithms and combine them into machine learning pipelines tailored\nto a dataset at hand. For supervised learning tasks, most notably binary and\nmultinomial classification, aka single-label classification (SLC), such AutoML\napproaches have shown promising results. However, the task of multi-label\nclassification (MLC), where data points are associated with a set of class\nlabels instead of a single class label, has received much less attention so\nfar. In the context of multi-label classification, the data-specific selection\nand configuration of multi-label classifiers are challenging even for experts\nin the field, as it is a high-dimensional optimization problem with multi-level\nhierarchical dependencies. While for SLC, the space of machine learning\npipelines is already huge, the size of the MLC search space outnumbers the one\nof SLC by several orders.\n  In the first part of this thesis, we devise a novel AutoML approach for\nsingle-label classification tasks optimizing pipelines of machine learning\nalgorithms, consisting of two algorithms at most. This approach is then\nextended first to optimize pipelines of unlimited length and eventually\nconfigure the complex hierarchical structures of multi-label classification\nmethods. Furthermore, we investigate how well AutoML approaches that form the\nstate of the art for single-label classification tasks scale with the increased\nproblem complexity of AutoML for multi-label classification.\n  In the second part, we explore how methods for SLC and MLC could be\nconfigured more flexibly to achieve better generalization performance and how\nto increase the efficiency of execution-based AutoML systems.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.18198v1.pdf",
        "similarity": 0.33744669239248054,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-28"
    },
    {
        "new_title": "Exploring Lightweight Federated Learning for Distributed Load\n  Forecasting",
        "new_link": "http://arxiv.org/abs/2404.03320v1",
        "new_summary": "  Federated Learning (FL) is a distributed learning scheme that enables deep\nlearning to be applied to sensitive data streams and applications in a\nprivacy-preserving manner. This paper focuses on the use of FL for analyzing\nsmart energy meter data with the aim to achieve comparable accuracy to\nstate-of-the-art methods for load forecasting while ensuring the privacy of\nindividual meter data. We show that with a lightweight fully connected deep\nneural network, we are able to achieve forecasting accuracy comparable to\nexisting schemes, both at each meter source and at the aggregator, by utilising\nthe FL framework. The use of lightweight models further reduces the energy and\nresource consumption caused by complex deep-learning models, making this\napproach ideally suited for deployment across resource-constrained smart meter\nsystems. With our proposed lightweight model, we are able to achieve an overall\naverage load forecasting RMSE of 0.17, with the model having a negligible\nenergy overhead of 50 mWh when performing training and inference on an Arduino\nUno platform.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.03320v1.pdf",
        "similarity": 0.337382374073123,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-04"
    },
    {
        "new_title": "Enabling Multi-Agent Transfer Reinforcement Learning via Scenario\n  Independent Representation",
        "new_link": "http://arxiv.org/abs/2402.08184v1",
        "new_summary": "  Multi-Agent Reinforcement Learning (MARL) algorithms are widely adopted in\ntackling complex tasks that require collaboration and competition among agents\nin dynamic Multi-Agent Systems (MAS). However, learning such tasks from scratch\nis arduous and may not always be feasible, particularly for MASs with a large\nnumber of interactive agents due to the extensive sample complexity. Therefore,\nreusing knowledge gained from past experiences or other agents could\nefficiently accelerate the learning process and upscale MARL algorithms. In\nthis study, we introduce a novel framework that enables transfer learning for\nMARL through unifying various state spaces into fixed-size inputs that allow\none unified deep-learning policy viable in different scenarios within a MAS. We\nevaluated our approach in a range of scenarios within the StarCraft Multi-Agent\nChallenge (SMAC) environment, and the findings show significant enhancements in\nmulti-agent learning performance using maneuvering skills learned from other\nscenarios compared to agents learning from scratch. Furthermore, we adopted\nCurriculum Transfer Learning (CTL), enabling our deep learning policy to\nprogressively acquire knowledge and skills across pre-designed homogeneous\nlearning scenarios organized by difficulty levels. This process promotes inter-\nand intra-agent knowledge transfer, leading to high multi-agent learning\nperformance in more complicated heterogeneous scenarios.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.08184v1.pdf",
        "similarity": 0.3371230830958831,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-13"
    },
    {
        "new_title": "Outlier detection in maritime environments using AIS data and deep\n  recurrent architectures",
        "new_link": "http://arxiv.org/abs/2406.09966v1",
        "new_summary": "  A methodology based on deep recurrent models for maritime surveillance, over\npublicly available Automatic Identification System (AIS) data, is presented in\nthis paper. The setup employs a deep Recurrent Neural Network (RNN)-based\nmodel, for encoding and reconstructing the observed ships' motion patterns. Our\napproach is based on a thresholding mechanism, over the calculated errors\nbetween observed and reconstructed motion patterns of maritime vessels.\nSpecifically, a deep-learning framework, i.e. an encoder-decoder architecture,\nis trained using the observed motion patterns, enabling the models to learn and\npredict the expected trajectory, which will be compared to the effective ones.\nOur models, particularly the bidirectional GRU with recurrent dropouts,\nshowcased superior performance in capturing the temporal dynamics of maritime\ndata, illustrating the potential of deep learning to enhance maritime\nsurveillance capabilities. Our work lays a solid foundation for future research\nin this domain, highlighting a path toward improved maritime safety through the\ninnovative application of technology.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.09966v1.pdf",
        "similarity": 0.3370611919974209,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-14"
    },
    {
        "new_title": "Seismic Traveltime Tomography with Label-free Learning",
        "new_link": "http://arxiv.org/abs/2402.00310v2",
        "new_summary": "  Deep learning techniques have been used to build velocity models (VMs) for\nseismic traveltime tomography and have shown encouraging performance in recent\nyears. However, they need to generate labeled samples (i.e., pairs of input and\nlabel) to train the deep neural network (NN) with end-to-end learning, and the\nreal labels for field data inversion are usually missing or very expensive.\nSome traditional tomographic methods can be implemented quickly, but their\neffectiveness is often limited by prior assumptions. To avoid generating and/or\ncollecting labeled samples, we propose a novel method by integrating deep\nlearning and dictionary learning to enhance the VMs with low resolution by\nusing the traditional tomography-least square method (LSQR). We first design a\ntype of shallow and simple NN to reduce computational cost followed by\nproposing a two-step strategy to enhance the VMs with low resolution: (1)\nWarming up. An initial dictionary is trained from the estimation by LSQR\nthrough dictionary learning method; (2) Dictionary optimization. The initial\ndictionary obtained in the warming-up step will be optimized by the NN, and\nthen it will be used to reconstruct high-resolution VMs with the reference\nslowness and the estimation by LSQR. Furthermore, we design a loss function to\nminimize traveltime misfit to ensure that NN training is label-free, and the\noptimized dictionary can be obtained after each epoch of NN training. We\ndemonstrate the effectiveness of the proposed method through the numerical\ntests on both synthetic and field data.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.00310v2.pdf",
        "similarity": 0.3367534640261602,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-01"
    },
    {
        "new_title": "Leveraging Hybrid Intelligence Towards Sustainable and Energy-Efficient\n  Machine Learning",
        "new_link": "http://arxiv.org/abs/2407.10580v1",
        "new_summary": "  Hybrid intelligence aims to enhance decision-making, problem-solving, and\noverall system performance by combining the strengths of both, human cognitive\nabilities and artificial intelligence. With the rise of Large Language Models\n(LLM), progressively participating as smart agents to accelerate machine\nlearning development, Hybrid Intelligence is becoming an increasingly important\ntopic for effective interaction between humans and machines. This paper\npresents an approach to leverage Hybrid Intelligence towards sustainable and\nenergy-aware machine learning. When developing machine learning models, final\nmodel performance commonly rules the optimization process while the efficiency\nof the process itself is often neglected. Moreover, in recent times, energy\nefficiency has become equally crucial due to the significant environmental\nimpact of complex and large-scale computational processes. The contribution of\nthis work covers the interactive inclusion of secondary knowledge sources\nthrough Human-in-the-loop (HITL) and LLM agents to stress out and further\nresolve inefficiencies in the machine learning development process.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.10580v1.pdf",
        "similarity": 0.33662471176156167,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-15"
    },
    {
        "new_title": "Exploring the Adversarial Frontier: Quantifying Robustness via\n  Adversarial Hypervolume",
        "new_link": "http://arxiv.org/abs/2403.05100v1",
        "new_summary": "  The escalating threat of adversarial attacks on deep learning models,\nparticularly in security-critical fields, has underscored the need for robust\ndeep learning systems. Conventional robustness evaluations have relied on\nadversarial accuracy, which measures a model's performance under a specific\nperturbation intensity. However, this singular metric does not fully\nencapsulate the overall resilience of a model against varying degrees of\nperturbation. To address this gap, we propose a new metric termed adversarial\nhypervolume, assessing the robustness of deep learning models comprehensively\nover a range of perturbation intensities from a multi-objective optimization\nstandpoint. This metric allows for an in-depth comparison of defense mechanisms\nand recognizes the trivial improvements in robustness afforded by less potent\ndefensive strategies. Additionally, we adopt a novel training algorithm that\nenhances adversarial robustness uniformly across various perturbation\nintensities, in contrast to methods narrowly focused on optimizing adversarial\naccuracy. Our extensive empirical studies validate the effectiveness of the\nadversarial hypervolume metric, demonstrating its ability to reveal subtle\ndifferences in robustness that adversarial accuracy overlooks. This research\ncontributes a new measure of robustness and establishes a standard for\nassessing and benchmarking the resilience of current and future defensive\nmodels against adversarial threats.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.05100v1.pdf",
        "similarity": 0.33639181697160764,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-08"
    },
    {
        "new_title": "Normalization and effective learning rates in reinforcement learning",
        "new_link": "http://arxiv.org/abs/2407.01800v1",
        "new_summary": "  Normalization layers have recently experienced a renaissance in the deep\nreinforcement learning and continual learning literature, with several works\nhighlighting diverse benefits such as improving loss landscape conditioning and\ncombatting overestimation bias. However, normalization brings with it a subtle\nbut important side effect: an equivalence between growth in the norm of the\nnetwork parameters and decay in the effective learning rate. This becomes\nproblematic in continual learning settings, where the resulting effective\nlearning rate schedule may decay to near zero too quickly relative to the\ntimescale of the learning problem. We propose to make the learning rate\nschedule explicit with a simple re-parameterization which we call\nNormalize-and-Project (NaP), which couples the insertion of normalization\nlayers with weight projection, ensuring that the effective learning rate\nremains constant throughout training. This technique reveals itself as a\npowerful analytical tool to better understand learning rate schedules in deep\nreinforcement learning, and as a means of improving robustness to\nnonstationarity in synthetic plasticity loss benchmarks along with both the\nsingle-task and sequential variants of the Arcade Learning Environment. We also\nshow that our approach can be easily applied to popular architectures such as\nResNets and transformers while recovering and in some cases even slightly\nimproving the performance of the base model in common stationary benchmarks.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.01800v1.pdf",
        "similarity": 0.3360495299681729,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-01"
    },
    {
        "new_title": "Finding the Missing Data: A BERT-inspired Approach Against Package Loss\n  in Wireless Sensing",
        "new_link": "http://arxiv.org/abs/2403.12400v1",
        "new_summary": "  Despite the development of various deep learning methods for Wi-Fi sensing,\npackage loss often results in noncontinuous estimation of the Channel State\nInformation (CSI), which negatively impacts the performance of the learning\nmodels. To overcome this challenge, we propose a deep learning model based on\nBidirectional Encoder Representations from Transformers (BERT) for CSI\nrecovery, named CSI-BERT. CSI-BERT can be trained in an self-supervised manner\non the target dataset without the need for additional data. Furthermore, unlike\ntraditional interpolation methods that focus on one subcarrier at a time,\nCSI-BERT captures the sequential relationships across different subcarriers.\nExperimental results demonstrate that CSI-BERT achieves lower error rates and\nfaster speed compared to traditional interpolation methods, even when facing\nwith high loss rates. Moreover, by harnessing the recovered CSI obtained from\nCSI-BERT, other deep learning models like Residual Network and Recurrent Neural\nNetwork can achieve an average increase in accuracy of approximately 15\\% in\nWi-Fi sensing tasks. The collected dataset WiGesture and code for our model are\npublicly available at https://github.com/RS2002/CSI-BERT.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.12400v1.pdf",
        "similarity": 0.33573769175756,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-19"
    },
    {
        "new_title": "Information-Theoretic Foundations for Machine Learning",
        "new_link": "http://arxiv.org/abs/2407.12288v2",
        "new_summary": "  The staggering progress of machine learning in the past decade has been a\nsight to behold. In retrospect, it is both remarkable and unsettling that these\nmilestones were achievable with little to no rigorous theory to guide\nexperimentation. Despite this fact, practitioners have been able to guide their\nfuture experimentation via observations from previous large-scale empirical\ninvestigations. However, alluding to Plato's Allegory of the cave, it is likely\nthat the observations which form the field's notion of reality are but shadows\nrepresenting fragments of that reality. In this work, we propose a theoretical\nframework which attempts to answer what exists outside of the cave. To the\ntheorist, we provide a framework which is mathematically rigorous and leaves\nopen many interesting ideas for future exploration. To the practitioner, we\nprovide a framework whose results are very intuitive, general, and which will\nhelp form principles to guide future investigations. Concretely, we provide a\ntheoretical framework rooted in Bayesian statistics and Shannon's information\ntheory which is general enough to unify the analysis of many phenomena in\nmachine learning. Our framework characterizes the performance of an optimal\nBayesian learner, which considers the fundamental limits of information.\nThroughout this work, we derive very general theoretical results and apply them\nto derive insights specific to settings ranging from data which is\nindependently and identically distributed under an unknown distribution, to\ndata which is sequential, to data which exhibits hierarchical structure\namenable to meta-learning. We conclude with a section dedicated to\ncharacterizing the performance of misspecified algorithms. These results are\nexciting and particularly relevant as we strive to overcome increasingly\ndifficult machine learning challenges in this endlessly complex world.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.12288v2.pdf",
        "similarity": 0.3356571516080096,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-17"
    },
    {
        "new_title": "Detecting Hidden Triggers: Mapping Non-Markov Reward Functions to Markov",
        "new_link": "http://arxiv.org/abs/2401.11325v2",
        "new_summary": "  Many Reinforcement Learning algorithms assume a Markov reward function to\nguarantee optimality. However, not all reward functions are known to be Markov.\nIn this paper, we propose a framework for mapping non-Markov reward functions\ninto equivalent Markov ones by learning a Reward Machine - a specialized reward\nautomaton. Unlike the general practice of learning Reward Machines, we do not\nrequire a set of high-level propositional symbols from which to learn. Rather,\nwe learn \\emph{hidden triggers} directly from data that encode them. We\ndemonstrate the importance of learning Reward Machines versus their\nDeterministic Finite-State Automata counterparts, for this task, given their\nability to model reward dependencies in a single automaton. We formalize this\ndistinction in our learning objective. Our mapping process is constructed as an\nInteger Linear Programming problem. We prove that our mappings provide\nconsistent expectations for the underlying process. We empirically validate our\napproach by learning black-box non-Markov Reward functions in the Officeworld\nDomain. Additionally, we demonstrate the effectiveness of learning dependencies\nbetween rewards in a new domain, Breakfastworld.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.11325v2.pdf",
        "similarity": 0.33542823699667695,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-20"
    },
    {
        "new_title": "Deep Learning-Based Computational Model for Disease Identification in\n  Cocoa Pods (Theobroma cacao L.)",
        "new_link": "http://arxiv.org/abs/2401.01247v1",
        "new_summary": "  The early identification of diseases in cocoa pods is an important task to\nguarantee the production of high-quality cocoa. The use of artificial\nintelligence techniques such as machine learning, computer vision and deep\nlearning are promising solutions to help identify and classify diseases in\ncocoa pods. In this paper we introduce the development and evaluation of a deep\nlearning computational model applied to the identification of diseases in cocoa\npods, focusing on \"monilia\" and \"black pod\" diseases. An exhaustive review of\nstate-of-the-art of computational models was carried out, based on scientific\narticles related to the identification of plant diseases using computer vision\nand deep learning techniques. As a result of the search, EfficientDet-Lite4, an\nefficient and lightweight model for object detection, was selected. A dataset,\nincluding images of both healthy and diseased cocoa pods, has been utilized to\ntrain the model to detect and pinpoint disease manifestations with considerable\naccuracy. Significant enhancements in the model training and evaluation\ndemonstrate the capability of recognizing and classifying diseases through\nimage analysis. Furthermore, the functionalities of the model were integrated\ninto an Android native mobile with an user-friendly interface, allowing to\nyounger or inexperienced farmers a fast and accuracy identification of health\nstatus of cocoa pods\n",
        "pdf_link": "https://arxiv.org/pdf/2401.01247v1.pdf",
        "similarity": 0.33505547594066654,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-02"
    },
    {
        "new_title": "Acquiring Diverse Skills using Curriculum Reinforcement Learning with\n  Mixture of Experts",
        "new_link": "http://arxiv.org/abs/2403.06966v2",
        "new_summary": "  Reinforcement learning (RL) is a powerful approach for acquiring a\ngood-performing policy. However, learning diverse skills is challenging in RL\ndue to the commonly used Gaussian policy parameterization. We propose\n\\textbf{Di}verse \\textbf{Skil}l \\textbf{L}earning (Di-SkilL\\footnote{Videos and\ncode are available on the project webpage:\n\\url{https://alrhub.github.io/di-skill-website/}}), an RL method for learning\ndiverse skills using Mixture of Experts, where each expert formalizes a skill\nas a contextual motion primitive. Di-SkilL optimizes each expert and its\nassociate context distribution to a maximum entropy objective that incentivizes\nlearning diverse skills in similar contexts. The per-expert context\ndistribution enables automatic curricula learning, allowing each expert to\nfocus on its best-performing sub-region of the context space. To overcome hard\ndiscontinuities and multi-modalities without any prior knowledge of the\nenvironment's unknown context probability space, we leverage energy-based\nmodels to represent the per-expert context distributions and demonstrate how we\ncan efficiently train them using the standard policy gradient objective. We\nshow on challenging robot simulation tasks that Di-SkilL can learn diverse and\nperformant skills.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.06966v2.pdf",
        "similarity": 0.3350029071087449,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-11"
    },
    {
        "new_title": "Opportunities in deep learning methods development for computational\n  biology",
        "new_link": "http://arxiv.org/abs/2406.08686v1",
        "new_summary": "  Advances in molecular technologies underlie an enormous growth in the size of\ndata sets pertaining to biology and biomedicine. These advances parallel those\nin the deep learning subfield of machine learning. Components in the\ndifferentiable programming toolbox that makes deep learning possible are\nallowing computer scientists to address an increasingly large array of problems\nwith flexible and effective tools. However many of these tools have not fully\nproliferated into the computational biology and bioinformatics fields. In this\nperspective we survey some of these advances and highlight exemplary examples\nof their utilization in the biosciences, with the goal of increasing awareness\namong practitioners of emerging opportunities to blend expert knowledge with\nnewly emerging deep learning architectural tools.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.08686v1.pdf",
        "similarity": 0.33499020437064436,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-12"
    },
    {
        "new_title": "Unsupervised Adaptive Deep Learning Method For BCI Motor Imagery\n  Decoding",
        "new_link": "http://arxiv.org/abs/2403.15438v1",
        "new_summary": "  In the context of Brain-Computer Interfaces, we propose an adaptive method\nthat reaches offline performance level while being usable online without\nrequiring supervision. Interestingly, our method does not require retraining\nthe model, as it consists in using a frozen efficient deep learning backbone\nwhile continuously realigning data, both at input and latent spaces, based on\nstreaming observations. We demonstrate its efficiency for Motor Imagery brain\ndecoding from electroencephalography data, considering challenging\ncross-subject scenarios. For reproducibility, we share the code of our\nexperiments.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.15438v1.pdf",
        "similarity": 0.3349140550968135,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-15"
    },
    {
        "new_title": "Removing the need for ground truth UWB data collection: self-supervised\n  ranging error correction using deep reinforcement learning",
        "new_link": "http://arxiv.org/abs/2403.19262v1",
        "new_summary": "  Indoor positioning using UWB technology has gained interest due to its\ncentimeter-level accuracy potential. However, multipath effects and\nnon-line-of-sight conditions cause ranging errors between anchors and tags.\nExisting approaches for mitigating these ranging errors rely on collecting\nlarge labeled datasets, making them impractical for real-world deployments.\nThis paper proposes a novel self-supervised deep reinforcement learning\napproach that does not require labeled ground truth data. A reinforcement\nlearning agent uses the channel impulse response as a state and predicts\ncorrections to minimize the error between corrected and estimated ranges. The\nagent learns, self-supervised, by iteratively improving corrections that are\ngenerated by combining the predictability of trajectories with filtering and\nsmoothening. Experiments on real-world UWB measurements demonstrate comparable\nperformance to state-of-the-art supervised methods, overcoming data dependency\nand lack of generalizability limitations. This makes self-supervised deep\nreinforcement learning a promising solution for practical and scalable\nUWB-ranging error correction.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.19262v1.pdf",
        "similarity": 0.33486093129667976,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-28"
    },
    {
        "new_title": "A Novel Stochastic Transformer-based Approach for Post-Traumatic Stress\n  Disorder Detection using Audio Recording of Clinical Interviews",
        "new_link": "http://arxiv.org/abs/2403.19441v1",
        "new_summary": "  Post-traumatic stress disorder (PTSD) is a mental disorder that can be\ndeveloped after witnessing or experiencing extremely traumatic events. PTSD can\naffect anyone, regardless of ethnicity, or culture. An estimated one in every\neleven people will experience PTSD during their lifetime. The\nClinician-Administered PTSD Scale (CAPS) and the PTSD Check List for Civilians\n(PCL-C) interviews are gold standards in the diagnosis of PTSD. These\nquestionnaires can be fooled by the subject's responses. This work proposes a\ndeep learning-based approach that achieves state-of-the-art performances for\nPTSD detection using audio recordings during clinical interviews. Our approach\nis based on MFCC low-level features extracted from audio recordings of clinical\ninterviews, followed by deep high-level learning using a Stochastic\nTransformer. Our proposed approach achieves state-of-the-art performances with\nan RMSE of 2.92 on the eDAIC dataset thanks to the stochastic depth, stochastic\ndeep learning layers, and stochastic activation function.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.19441v1.pdf",
        "similarity": 0.3343587899297422,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-28"
    },
    {
        "new_title": "Speech Emotion Recognition Using CNN and Its Use Case in Digital\n  Healthcare",
        "new_link": "http://arxiv.org/abs/2406.10741v1",
        "new_summary": "  The process of identifying human emotion and affective states from speech is\nknown as speech emotion recognition (SER). This is based on the observation\nthat tone and pitch in the voice frequently convey underlying emotion. Speech\nrecognition includes the ability to recognize emotions, which is becoming\nincreasingly popular and in high demand. With the help of appropriate factors\n(such modalities, emotions, intensities, repetitions, etc.) found in the data,\nmy research seeks to use the Convolutional Neural Network (CNN) to distinguish\nemotions from audio recordings and label them in accordance with the range of\ndifferent emotions. I have developed a machine learning model to identify\nemotions from supplied audio files with the aid of machine learning methods.\nThe evaluation is mostly focused on precision, recall, and F1 score, which are\ncommon machine learning metrics. To properly set up and train the machine\nlearning framework, the main objective is to investigate the influence and\ncross-relation of all input and output parameters. To improve the ability to\nrecognize intentions, a key condition for communication, I have evaluated\nemotions using my specialized machine learning algorithm via voice that would\naddress the emotional state from voice with the help of digital healthcare,\nbridging the gap between human and artificial intelligence (AI).\n",
        "pdf_link": "https://arxiv.org/pdf/2406.10741v1.pdf",
        "similarity": 0.33425906177812253,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-15"
    },
    {
        "new_title": "A Review of Deep Learning Methods for Photoplethysmography Data",
        "new_link": "http://arxiv.org/abs/2401.12783v1",
        "new_summary": "  Photoplethysmography (PPG) is a highly promising device due to its advantages\nin portability, user-friendly operation, and non-invasive capabilities to\nmeasure a wide range of physiological information. Recent advancements in deep\nlearning have demonstrated remarkable outcomes by leveraging PPG signals for\ntasks related to personal health management and other multifaceted\napplications. In this review, we systematically reviewed papers that applied\ndeep learning models to process PPG data between January 1st of 2017 and July\n31st of 2023 from Google Scholar, PubMed and Dimensions. Each paper is analyzed\nfrom three key perspectives: tasks, models, and data. We finally extracted 193\npapers where different deep learning frameworks were used to process PPG\nsignals. Based on the tasks addressed in these papers, we categorized them into\ntwo major groups: medical-related, and non-medical-related. The medical-related\ntasks were further divided into seven subgroups, including blood pressure\nanalysis, cardiovascular monitoring and diagnosis, sleep health, mental health,\nrespiratory monitoring and analysis, blood glucose analysis, as well as others.\nThe non-medical-related tasks were divided into four subgroups, which encompass\nsignal processing, biometric identification, electrocardiogram reconstruction,\nand human activity recognition. In conclusion, significant progress has been\nmade in the field of using deep learning methods to process PPG data recently.\nThis allows for a more thorough exploration and utilization of the information\ncontained in PPG signals. However, challenges remain, such as limited quantity\nand quality of publicly available databases, a lack of effective validation in\nreal-world scenarios, and concerns about the interpretability, scalability, and\ncomplexity of deep learning models. Moreover, there are still emerging research\nareas that require further investigation.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.12783v1.pdf",
        "similarity": 0.3338292543833406,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-23"
    },
    {
        "new_title": "Fog enabled distributed training architecture for federated learning",
        "new_link": "http://arxiv.org/abs/2402.12906v1",
        "new_summary": "  The amount of data being produced at every epoch of second is increasing\nevery moment. Various sensors, cameras and smart gadgets produce continuous\ndata throughout its installation. Processing and analyzing raw data at a cloud\nserver faces several challenges such as bandwidth, congestion, latency, privacy\nand security. Fog computing brings computational resources closer to IoT that\naddresses some of these issues. These IoT devices have low computational\ncapability, which is insufficient to train machine learning. Mining hidden\npatterns and inferential rules from continuously growing data is crucial for\nvarious applications. Due to growing privacy concerns, privacy preserving\nmachine learning is another aspect that needs to be inculcated. In this paper,\nwe have proposed a fog enabled distributed training architecture for machine\nlearning tasks using resources constrained devices. The proposed architecture\ntrains machine learning model on rapidly changing data using online learning.\nThe network is inlined with privacy preserving federated learning training.\nFurther, the learning capability of architecture is tested on a real world IIoT\nuse case. We trained a neural network model for human position detection in\nIIoT setup on rapidly changing data.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.12906v1.pdf",
        "similarity": 0.33382528976763487,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-20"
    },
    {
        "new_title": "Discounted Adaptive Online Learning: Towards Better Regularization",
        "new_link": "http://arxiv.org/abs/2402.02720v2",
        "new_summary": "  We study online learning in adversarial nonstationary environments. Since the\nfuture can be very different from the past, a critical challenge is to\ngracefully forget the history while new data comes in. To formalize this\nintuition, we revisit the discounted regret in online convex optimization, and\npropose an adaptive (i.e., instance optimal), FTRL-based algorithm that\nimproves the widespread non-adaptive baseline -- gradient descent with a\nconstant learning rate. From a practical perspective, this refines the\nclassical idea of regularization in lifelong learning: we show that designing\ngood regularizers can be guided by the principled theory of adaptive online\noptimization.\n  Complementing this result, we also consider the (Gibbs and Cand\\`es,\n2021)-style online conformal prediction problem, where the goal is to\nsequentially predict the uncertainty sets of a black-box machine learning\nmodel. We show that the FTRL nature of our algorithm can simplify the\nconventional gradient-descent-based analysis, leading to instance-dependent\nperformance guarantees.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.02720v2.pdf",
        "similarity": 0.33355572782404336,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-05"
    },
    {
        "new_title": "ResSurv: Cancer Survival Analysis Prediction Model Based on Residual\n  Networks",
        "new_link": "http://arxiv.org/abs/2405.06992v1",
        "new_summary": "  Survival prediction is an important branch of cancer prognosis analysis. The\nmodel that predicts survival risk through TCGA genomics data can discover genes\nrelated to cancer and provide diagnosis and treatment recommendations based on\npatient characteristics. We found that deep learning models based on Cox\nproportional hazards often suffer from overfitting when dealing with\nhigh-throughput data. Moreover, we found that as the number of network layers\nincreases, the experimental results will not get better, and network\ndegradation will occur. Based on this problem, we propose a new framework based\non Deep Residual Learning. Combine the ideas of Cox proportional hazards and\nResidual. And name it ResSurv. First, ResSurv is a feed-forward deep learning\nnetwork stacked by multiple basic ResNet Blocks. In each ResNet Block, we add a\nNormalization Layer to prevent gradient disappearance and gradient explosion.\nSecondly, for the loss function of the neural network, we inherited the Cox\nproportional hazards methods, applied the semi-parametric of the CPH model to\nthe neural network, combined with the partial likelihood model, established the\nloss function, and performed backpropagation and gradient update. Finally, we\ncompared ResSurv networks of different depths and found that we can effectively\nextract high-dimensional features. Ablation experiments and comparative\nexperiments prove that our model has reached SOTA(state of the art) in the\nfield of deep learning, and our network can effectively extract deep\ninformation.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.06992v1.pdf",
        "similarity": 0.3332984385274924,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-11"
    },
    {
        "new_title": "Counterfactual Explanations for Deep Learning-Based Traffic Forecasting",
        "new_link": "http://arxiv.org/abs/2405.00456v1",
        "new_summary": "  Deep learning models are widely used in traffic forecasting and have achieved\nstate-of-the-art prediction accuracy. However, the black-box nature of those\nmodels makes the results difficult to interpret by users. This study aims to\nleverage an Explainable AI approach, counterfactual explanations, to enhance\nthe explainability and usability of deep learning-based traffic forecasting\nmodels. Specifically, the goal is to elucidate relationships between various\ninput contextual features and their corresponding predictions. We present a\ncomprehensive framework that generates counterfactual explanations for traffic\nforecasting and provides usable insights through the proposed scenario-driven\ncounterfactual explanations. The study first implements a deep learning model\nto predict traffic speed based on historical traffic data and contextual\nvariables. Counterfactual explanations are then used to illuminate how\nalterations in these input variables affect predicted outcomes, thereby\nenhancing the transparency of the deep learning model. We investigated the\nimpact of contextual features on traffic speed prediction under varying spatial\nand temporal conditions. The scenario-driven counterfactual explanations\nintegrate two types of user-defined constraints, directional and weighting\nconstraints, to tailor the search for counterfactual explanations to specific\nuse cases. These tailored explanations benefit machine learning practitioners\nwho aim to understand the model's learning mechanisms and domain experts who\nseek insights for real-world applications. The results showcase the\neffectiveness of counterfactual explanations in revealing traffic patterns\nlearned by deep learning models, showing its potential for interpreting\nblack-box deep learning models used for spatiotemporal predictions in general.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.00456v1.pdf",
        "similarity": 0.3332096294098593,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-01"
    },
    {
        "new_title": "Learning in Feature Spaces via Coupled Covariances: Asymmetric Kernel\n  SVD and Nystr\u00f6m method",
        "new_link": "http://arxiv.org/abs/2406.08748v1",
        "new_summary": "  In contrast with Mercer kernel-based approaches as used e.g., in Kernel\nPrincipal Component Analysis (KPCA), it was previously shown that Singular\nValue Decomposition (SVD) inherently relates to asymmetric kernels and\nAsymmetric Kernel Singular Value Decomposition (KSVD) has been proposed.\nHowever, the existing formulation to KSVD cannot work with infinite-dimensional\nfeature mappings, the variational objective can be unbounded, and needs further\nnumerical evaluation and exploration towards machine learning. In this work, i)\nwe introduce a new asymmetric learning paradigm based on coupled covariance\neigenproblem (CCE) through covariance operators, allowing infinite-dimensional\nfeature maps. The solution to CCE is ultimately obtained from the SVD of the\ninduced asymmetric kernel matrix, providing links to KSVD. ii) Starting from\nthe integral equations corresponding to a pair of coupled adjoint\neigenfunctions, we formalize the asymmetric Nystr\\\"om method through a finite\nsample approximation to speed up training. iii) We provide the first empirical\nevaluations verifying the practical utility and benefits of KSVD and compare\nwith methods resorting to symmetrization or linear SVD across multiple tasks.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.08748v1.pdf",
        "similarity": 0.3329983626360113,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-13"
    },
    {
        "new_title": "Nuclear Norm Regularization for Deep Learning",
        "new_link": "http://arxiv.org/abs/2405.14544v1",
        "new_summary": "  Penalizing the nuclear norm of a function's Jacobian encourages it to locally\nbehave like a low-rank linear map. Such functions vary locally along only a\nhandful of directions, making the Jacobian nuclear norm a natural regularizer\nfor machine learning problems. However, this regularizer is intractable for\nhigh-dimensional problems, as it requires computing a large Jacobian matrix and\ntaking its singular value decomposition. We show how to efficiently penalize\nthe Jacobian nuclear norm using techniques tailor-made for deep learning. We\nprove that for functions parametrized as compositions $f = g \\circ h$, one may\nequivalently penalize the average squared Frobenius norm of $Jg$ and $Jh$. We\nthen propose a denoising-style approximation that avoids the Jacobian\ncomputations altogether. Our method is simple, efficient, and accurate,\nenabling Jacobian nuclear norm regularization to scale to high-dimensional deep\nlearning problems. We complement our theory with an empirical study of our\nregularizer's performance and investigate applications to denoising and\nrepresentation learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.14544v1.pdf",
        "similarity": 0.3329605614900053,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-23"
    },
    {
        "new_title": "Multi-task Magnetic Resonance Imaging Reconstruction using Meta-learning",
        "new_link": "http://arxiv.org/abs/2403.19966v2",
        "new_summary": "  Using single-task deep learning methods to reconstruct Magnetic Resonance\nImaging (MRI) data acquired with different imaging sequences is inherently\nchallenging. The trained deep learning model typically lacks generalizability,\nand the dissimilarity among image datasets with different types of contrast\nleads to suboptimal learning performance. This paper proposes a meta-learning\napproach to efficiently learn image features from multiple MR image datasets.\nOur algorithm can perform multi-task learning to simultaneously reconstruct MR\nimages acquired using different imaging sequences with different image\ncontrasts. The experiment results demonstrate the ability of our new\nmeta-learning reconstruction method to successfully reconstruct\nhighly-undersampled k-space data from multiple MRI datasets simultaneously,\noutperforming other compelling reconstruction methods previously developed for\nsingle-task learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.19966v2.pdf",
        "similarity": 0.3328617088334253,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-29"
    },
    {
        "new_title": "Enhanced Mortality Prediction in ICU Stroke Patients via Deep Learning",
        "new_link": "http://arxiv.org/abs/2407.14211v1",
        "new_summary": "  Background: Stroke is second-leading cause of disability and death among\nadults. Approximately 17 million people suffer from a stroke annually, with\nabout 85% being ischemic strokes. Predicting mortality of ischemic stroke\npatients in intensive care unit (ICU) is crucial for optimizing treatment\nstrategies, allocating resources, and improving survival rates. Methods: We\nacquired data on ICU ischemic stroke patients from MIMIC-IV database, including\ndiagnoses, vital signs, laboratory tests, medications, procedures, treatments,\nand clinical notes. Stroke patients were randomly divided into training (70%,\nn=2441), test (15%, n=523), and validation (15%, n=523) sets. To address data\nimbalances, we applied Synthetic Minority Over-sampling Technique (SMOTE). We\nselected 30 features for model development, significantly reducing feature\nnumber from 1095 used in the best study. We developed a deep learning model to\nassess mortality risk and implemented several baseline machine learning models\nfor comparison. Results: XGB-DL model, combining XGBoost for feature selection\nand deep learning, effectively minimized false positives. Model AUROC improved\nfrom 0.865 (95% CI: 0.821 - 0.905) on first day to 0.903 (95% CI: 0.868 -\n0.936) by fourth day using data from 3,646 ICU mortality patients in the\nMIMIC-IV database with 0.945 AUROC (95% CI: 0.944 - 0.947) during training.\nAlthough other ML models also performed well in terms of AUROC, we chose Deep\nLearning for its higher specificity. Conclusions: Through enhanced feature\nselection and data cleaning, proposed model demonstrates a 13% AUROC\nimprovement compared to existing models while reducing feature number from 1095\nin previous studies to 30.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.14211v1.pdf",
        "similarity": 0.33286060818189195,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-19"
    },
    {
        "new_title": "Transfer Learning for Molecular Property Predictions from Small Data\n  Sets",
        "new_link": "http://arxiv.org/abs/2404.13393v1",
        "new_summary": "  Machine learning has emerged as a new tool in chemistry to bypass expensive\nexperiments or quantum-chemical calculations, for example, in high-throughput\nscreening applications. However, many machine learning studies rely on small\ndata sets, making it difficult to efficiently implement powerful deep learning\narchitectures such as message passing neural networks. In this study, we\nbenchmark common machine learning models for the prediction of molecular\nproperties on small data sets, for which the best results are obtained with the\nmessage passing neural network PaiNN, as well as SOAP molecular descriptors\nconcatenated to a set of simple molecular descriptors tailored to gradient\nboosting with regression trees. To further improve the predictive capabilities\nof PaiNN, we present a transfer learning strategy that uses large data sets to\npre-train the respective models and allows to obtain more accurate models after\nfine-tuning on the original data sets. The pre-training labels are obtained\nfrom computationally cheap ab initio or semi-empirical models and corrected by\nsimple linear regression on the target data set to obtain labels that are close\nto those of the original data. This strategy is tested on the Harvard Oxford\nPhotovoltaics data set (HOPV, HOMO-LUMO-gaps), for which excellent results are\nobtained, and on the Freesolv data set (solvation energies), where this method\nis unsuccessful due to a complex underlying learning task and the dissimilar\nmethods used to obtain pre-training and fine-tuning labels. Finally, we find\nthat the final training results do not improve monotonically with the size of\nthe pre-training data set, but pre-training with fewer data points can lead to\nmore biased pre-trained models and higher accuracy after fine-tuning.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.13393v1.pdf",
        "similarity": 0.33269568975811453,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-20"
    },
    {
        "new_title": "On Convolutional Vision Transformers for Yield Prediction",
        "new_link": "http://arxiv.org/abs/2402.05557v1",
        "new_summary": "  While a variety of methods offer good yield prediction on histogrammed remote\nsensing data, vision Transformers are only sparsely represented in the\nliterature. The Convolution vision Transformer (CvT) is being tested to\nevaluate vision Transformers that are currently achieving state-of-the-art\nresults in many other vision tasks. CvT combines some of the advantages of\nconvolution with the advantages of dynamic attention and global context fusion\nof Transformers. It performs worse than widely tested methods such as XGBoost\nand CNNs, but shows that Transformers have potential to improve yield\nprediction.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.05557v1.pdf",
        "similarity": 0.3319212528447183,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-08"
    },
    {
        "new_title": "Using Large Language Models to Automate and Expedite Reinforcement\n  Learning with Reward Machine",
        "new_link": "http://arxiv.org/abs/2402.07069v1",
        "new_summary": "  We present LARL-RM (Large language model-generated Automaton for\nReinforcement Learning with Reward Machine) algorithm in order to encode\nhigh-level knowledge into reinforcement learning using automaton to expedite\nthe reinforcement learning. Our method uses Large Language Models (LLM) to\nobtain high-level domain-specific knowledge using prompt engineering instead of\nproviding the reinforcement learning algorithm directly with the high-level\nknowledge which requires an expert to encode the automaton. We use\nchain-of-thought and few-shot methods for prompt engineering and demonstrate\nthat our method works using these approaches. Additionally, LARL-RM allows for\nfully closed-loop reinforcement learning without the need for an expert to\nguide and supervise the learning since LARL-RM can use the LLM directly to\ngenerate the required high-level knowledge for the task at hand. We also show\nthe theoretical guarantee of our algorithm to converge to an optimal policy. We\ndemonstrate that LARL-RM speeds up the convergence by 30% by implementing our\nmethod in two case studies.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.07069v1.pdf",
        "similarity": 0.33173685707637207,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-11"
    },
    {
        "new_title": "Traditional Machine Learning Models and Bidirectional Encoder\n  Representations From Transformer (BERT)-Based Automatic Classification of\n  Tweets About Eating Disorders: Algorithm Development and Validation Study",
        "new_link": "http://arxiv.org/abs/2402.05571v1",
        "new_summary": "  Background: Eating disorders are increasingly prevalent, and social networks\noffer valuable information.\n  Objective: Our goal was to identify efficient machine learning models for\ncategorizing tweets related to eating disorders.\n  Methods: Over three months, we collected tweets about eating disorders. A\n2,000-tweet subset was labeled for: (1) being written by individuals with\neating disorders, (2) promoting eating disorders, (3) informativeness, and (4)\nscientific content. Both traditional machine learning and deep learning models\nwere employed for classification, assessing accuracy, F1 score, and\ncomputational time.\n  Results: From 1,058,957 collected tweets, transformer-based bidirectional\nencoder representations achieved the highest F1 scores (71.1%-86.4%) across all\nfour categories.\n  Conclusions: Transformer-based models outperform traditional techniques in\nclassifying eating disorder-related tweets, though they require more\ncomputational resources.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.05571v1.pdf",
        "similarity": 0.33171102635515654,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-08"
    },
    {
        "new_title": "Federated Learning of Socially Appropriate Agent Behaviours in Simulated\n  Home Environments",
        "new_link": "http://arxiv.org/abs/2403.07586v1",
        "new_summary": "  As social robots become increasingly integrated into daily life, ensuring\ntheir behaviours align with social norms is crucial. For their widespread\nopen-world application, it is important to explore Federated Learning (FL)\nsettings where individual robots can learn about their unique environments\nwhile also learning from each others' experiences. In this paper, we present a\nnovel FL benchmark that evaluates different strategies, using multi-label\nregression objectives, where each client individually learns to predict the\nsocial appropriateness of different robot actions while also sharing their\nlearning with others. Furthermore, splitting the training data by different\ncontexts such that each client incrementally learns across contexts, we present\na novel Federated Continual Learning (FCL) benchmark that adapts FL-based\nmethods to use state-of-the-art Continual Learning (CL) methods to continually\nlearn socially appropriate agent behaviours under different contextual\nsettings. Federated Averaging (FedAvg) of weights emerges as a robust FL\nstrategy while rehearsal-based FCL enables incrementally learning the social\nappropriateness of robot actions, across contextual splits.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.07586v1.pdf",
        "similarity": 0.3315848689622318,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-12"
    },
    {
        "new_title": "A Gap in Time: The Challenge of Processing Heterogeneous IoT Point Data\n  in Buildings",
        "new_link": "http://arxiv.org/abs/2405.14267v1",
        "new_summary": "  The growing need for sustainable energy solutions has driven the integration\nof digitalized buildings into the power grid, utilizing Internet-of-Things\ntechnology to optimize building performance and energy efficiency. However,\nincorporating IoT point data within deep-learning frameworks for energy\nmanagement presents a complex challenge, predominantly due to the inherent data\nheterogeneity. This paper comprehensively analyzes the multifaceted\nheterogeneity present in real-world building IoT data streams. We meticulously\ndissect the heterogeneity across multiple dimensions, encompassing ontology,\netiology, temporal irregularity, spatial diversity, and their combined effects\non the IoT point data distribution. In addition, experiments using\nstate-of-the-art forecasting models are conducted to evaluate their impacts on\nthe performance of deep-learning models for forecasting tasks. By charting the\ndiversity along these dimensions, we illustrate the challenges and delineate\npathways for future research to leverage this heterogeneity as a resource\nrather than a roadblock. This exploration sets the stage for advancing the\npredictive abilities of deep-learning algorithms and catalyzing the evolution\nof intelligent energy-efficient buildings.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.14267v1.pdf",
        "similarity": 0.33149707397387024,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-23"
    },
    {
        "new_title": "On Adversarial Examples for Text Classification by Perturbing Latent\n  Representations",
        "new_link": "http://arxiv.org/abs/2405.03789v1",
        "new_summary": "  Recently, with the advancement of deep learning, several applications in text\nclassification have advanced significantly. However, this improvement comes\nwith a cost because deep learning is vulnerable to adversarial examples. This\nweakness indicates that deep learning is not very robust. Fortunately, the\ninput of a text classifier is discrete. Hence, it can prevent the classifier\nfrom state-of-the-art attacks. Nonetheless, previous works have generated\nblack-box attacks that successfully manipulate the discrete values of the input\nto find adversarial examples. Therefore, instead of changing the discrete\nvalues, we transform the input into its embedding vector containing real values\nto perform the state-of-the-art white-box attacks. Then, we convert the\nperturbed embedding vector back into a text and name it an adversarial example.\nIn summary, we create a framework that measures the robustness of a text\nclassifier by using the gradients of the classifier.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.03789v1.pdf",
        "similarity": 0.33144355214474386,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-06"
    },
    {
        "new_title": "Leverage Multi-source Traffic Demand Data Fusion with Transformer Model\n  for Urban Parking Prediction",
        "new_link": "http://arxiv.org/abs/2405.01055v1",
        "new_summary": "  The escalation in urban private car ownership has worsened the urban parking\npredicament, necessitating effective parking availability prediction for urban\nplanning and management. However, the existing prediction methods suffer from\nlow prediction accuracy with the lack of spatial-temporal correlation features\nrelated to parking volume, and neglect of flow patterns and correlations\nbetween similar parking lots within certain areas. To address these challenges,\nthis study proposes a parking availability prediction framework integrating\nspatial-temporal deep learning with multi-source data fusion, encompassing\ntraffic demand data from multiple sources (e.g., metro, bus, taxi services),\nand parking lot data. The framework is based on the Transformer as the\nspatial-temporal deep learning model and leverages K-means clustering to\nestablish parking cluster zones, extracting and integrating traffic demand\ncharacteristics from various transportation modes (i.e., metro, bus, online\nride-hailing, and taxi) connected to parking lots. Real-world empirical data\nwas used to verify the effectiveness of the proposed method compared with\ndifferent machine learning, deep learning, and traditional statistical models\nfor predicting parking availability. Experimental results reveal that, with the\nproposed pipeline, the developed Transformer model outperforms other models in\nterms of various metrics, e.g., Mean Squared Error (MSE), Mean Absolute Error\n(MAE), and Mean Absolute Percentage Error (MAPE). By fusing multi-source\ndemanding data with spatial-temporal deep learning techniques, this approach\noffers the potential to develop parking availability prediction systems that\nfurnish more accurate and timely information to both drivers and urban\nplanners, thereby fostering more efficient and sustainable urban mobility.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.01055v1.pdf",
        "similarity": 0.33123902880770856,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-02"
    },
    {
        "new_title": "Deep functional multiple index models with an application to SER",
        "new_link": "http://arxiv.org/abs/2403.17562v1",
        "new_summary": "  Speech Emotion Recognition (SER) plays a crucial role in advancing\nhuman-computer interaction and speech processing capabilities. We introduce a\nnovel deep-learning architecture designed specifically for the functional data\nmodel known as the multiple-index functional model. Our key innovation lies in\nintegrating adaptive basis layers and an automated data transformation search\nwithin the deep learning framework. Simulations for this new model show good\nperformances. This allows us to extract features tailored for chunk-level SER,\nbased on Mel Frequency Cepstral Coefficients (MFCCs). We demonstrate the\neffectiveness of our approach on the benchmark IEMOCAP database, achieving good\nperformance compared to existing methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.17562v1.pdf",
        "similarity": 0.331216899867107,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-26"
    },
    {
        "new_title": "A Study of Plasticity Loss in On-Policy Deep Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2405.19153v1",
        "new_summary": "  Continual learning with deep neural networks presents challenges distinct\nfrom both the fixed-dataset and convex continual learning regimes. One such\nchallenge is plasticity loss, wherein a neural network trained in an online\nfashion displays a degraded ability to fit new tasks. This problem has been\nextensively studied in both supervised learning and off-policy reinforcement\nlearning (RL), where a number of remedies have been proposed. Still, plasticity\nloss has received less attention in the on-policy deep RL setting. Here we\nperform an extensive set of experiments examining plasticity loss and a variety\nof mitigation methods in on-policy deep RL. We demonstrate that plasticity loss\nis pervasive under domain shift in this regime, and that a number of methods\ndeveloped to resolve it in other settings fail, sometimes even resulting in\nperformance that is worse than performing no intervention at all. In contrast,\nwe find that a class of ``regenerative'' methods are able to consistently\nmitigate plasticity loss in a variety of contexts, including in gridworld tasks\nand more challenging environments like Montezuma's Revenge and ProcGen.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.19153v1.pdf",
        "similarity": 0.33071166633906063,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-29"
    },
    {
        "new_title": "Deepfake Audio Detection Using Spectrogram-based Feature and Ensemble of\n  Deep Learning Models",
        "new_link": "http://arxiv.org/abs/2407.01777v1",
        "new_summary": "  In this paper, we propose a deep learning based system for the task of\ndeepfake audio detection. In particular, the draw input audio is first\ntransformed into various spectrograms using three transformation methods of\nShort-time Fourier Transform (STFT), Constant-Q Transform (CQT), Wavelet\nTransform (WT) combined with different auditory-based filters of Mel,\nGammatone, linear filters (LF), and discrete cosine transform (DCT). Given the\nspectrograms, we evaluate a wide range of classification models based on three\ndeep learning approaches. The first approach is to train directly the\nspectrograms using our proposed baseline models of CNN-based model\n(CNN-baseline), RNN-based model (RNN-baseline), C-RNN model (C-RNN baseline).\nMeanwhile, the second approach is transfer learning from computer vision models\nsuch as ResNet-18, MobileNet-V3, EfficientNet-B0, DenseNet-121, SuffleNet-V2,\nSwint, Convnext-Tiny, GoogLeNet, MNASsnet, RegNet. In the third approach, we\nleverage the state-of-the-art audio pre-trained models of Whisper, Seamless,\nSpeechbrain, and Pyannote to extract audio embeddings from the input\nspectrograms. Then, the audio embeddings are explored by a Multilayer\nperceptron (MLP) model to detect the fake or real audio samples. Finally,\nhigh-performance deep learning models from these approaches are fused to\nachieve the best performance. We evaluated our proposed models on ASVspoof 2019\nbenchmark dataset. Our best ensemble model achieved an Equal Error Rate (EER)\nof 0.03, which is highly competitive to top-performing systems in the\nASVspoofing 2019 challenge. Experimental results also highlight the potential\nof selective spectrograms and deep learning approaches to enhance the task of\naudio deepfake detection.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.01777v1.pdf",
        "similarity": 0.33064119878115183,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-01"
    },
    {
        "new_title": "Learning Spatio-Temporal Patterns of Polar Ice Layers With\n  Physics-Informed Graph Neural Network",
        "new_link": "http://arxiv.org/abs/2406.15299v1",
        "new_summary": "  Learning spatio-temporal patterns of polar ice layers is crucial for\nmonitoring the change in ice sheet balance and evaluating ice dynamic\nprocesses. While a few researchers focus on learning ice layer patterns from\nechogram images captured by airborne snow radar sensors via different\nconvolutional neural networks, the noise in the echogram images proves to be a\nmajor obstacle. Instead, we focus on geometric deep learning based on graph\nneural networks to learn the spatio-temporal patterns from thickness\ninformation of shallow ice layers and make predictions for deep layers. In this\npaper, we propose a physics-informed hybrid graph neural network that combines\nthe GraphSAGE framework for graph feature learning with the long short-term\nmemory (LSTM) structure for learning temporal changes, and introduce\nmeasurements of physical ice properties from Model Atmospheric Regional (MAR)\nweather model as physical node features. We found that our proposed network can\nconsistently outperform the current non-inductive or non-physical model in\npredicting deep ice layer thickness.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.15299v1.pdf",
        "similarity": 0.3299989584134766,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-21"
    },
    {
        "new_title": "Kernel Neural Operators (KNOs) for Scalable, Memory-efficient,\n  Geometrically-flexible Operator Learning",
        "new_link": "http://arxiv.org/abs/2407.00809v1",
        "new_summary": "  This paper introduces the Kernel Neural Operator (KNO), a novel operator\nlearning technique that uses deep kernel-based integral operators in\nconjunction with quadrature for function-space approximation of operators (maps\nfrom functions to functions). KNOs use parameterized, closed-form,\nfinitely-smooth, and compactly-supported kernels with trainable sparsity\nparameters within the integral operators to significantly reduce the number of\nparameters that must be learned relative to existing neural operators.\nMoreover, the use of quadrature for numerical integration endows the KNO with\ngeometric flexibility that enables operator learning on irregular geometries.\nNumerical results demonstrate that on existing benchmarks the training and test\naccuracy of KNOs is higher than popular operator learning techniques while\nusing at least an order of magnitude fewer trainable parameters. KNOs thus\nrepresent a new paradigm of low-memory, geometrically-flexible, deep operator\nlearning, while retaining the implementation simplicity and transparency of\ntraditional kernel methods from both scientific computing and machine learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.00809v1.pdf",
        "similarity": 0.32999722366865053,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-30"
    },
    {
        "new_title": "Client-supervised Federated Learning: Towards One-model-for-all\n  Personalization",
        "new_link": "http://arxiv.org/abs/2403.19499v1",
        "new_summary": "  Personalized Federated Learning (PerFL) is a new machine learning paradigm\nthat delivers personalized models for diverse clients under federated learning\nsettings. Most PerFL methods require extra learning processes on a client to\nadapt a globally shared model to the client-specific personalized model using\nits own local data. However, the model adaptation process in PerFL is still an\nopen challenge in the stage of model deployment and test time. This work\ntackles the challenge by proposing a novel federated learning framework to\nlearn only one robust global model to achieve competitive performance to those\npersonalized models on unseen/test clients in the FL system. Specifically, we\ndesign a new Client-Supervised Federated Learning (FedCS) to unravel clients'\nbias on instances' latent representations so that the global model can learn\nboth client-specific and client-agnostic knowledge. Experimental study shows\nthat the FedCS can learn a robust FL global model for the changing data\ndistributions of unseen/test clients. The FedCS's global model can be directly\ndeployed to the test clients while achieving comparable performance to other\npersonalized FL methods that require model adaptation.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.19499v1.pdf",
        "similarity": 0.3297283360044143,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-28"
    },
    {
        "new_title": "Error Detection and Constraint Recovery in Hierarchical Multi-Label\n  Classification without Prior Knowledge",
        "new_link": "http://arxiv.org/abs/2407.15192v1",
        "new_summary": "  Recent advances in Hierarchical Multi-label Classification (HMC),\nparticularly neurosymbolic-based approaches, have demonstrated improved\nconsistency and accuracy by enforcing constraints on a neural model during\ntraining. However, such work assumes the existence of such constraints\na-priori. In this paper, we relax this strong assumption and present an\napproach based on Error Detection Rules (EDR) that allow for learning\nexplainable rules about the failure modes of machine learning models. We show\nthat these rules are not only effective in detecting when a machine learning\nclassifier has made an error but also can be leveraged as constraints for HMC,\nthereby allowing the recovery of explainable constraints even if they are not\nprovided. We show that our approach is effective in detecting machine learning\nerrors and recovering constraints, is noise tolerant, and can function as a\nsource of knowledge for neurosymbolic models on multiple datasets, including a\nnewly introduced military vehicle recognition dataset.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.15192v1.pdf",
        "similarity": 0.3296846568128782,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-21"
    },
    {
        "new_title": "Physics Sensor Based Deep Learning Fall Detection System",
        "new_link": "http://arxiv.org/abs/2403.06994v1",
        "new_summary": "  Fall detection based on embedded sensor is a practical and popular research\ndirection in recent years. In terms of a specific application: fall detection\nmethods based upon physics sensors such as [gyroscope and accelerator] have\nbeen exploited using traditional hand crafted features and feed them in machine\nlearning models like Markov chain or just threshold based classification\nmethods. In this paper, we build a complete system named TSFallDetect including\ndata receiving device based on embedded sensor, mobile deep-learning model\ndeploying platform, and a simple server, which will be used to gather models\nand data for future expansion. On the other hand, we exploit the sequential\ndeep-learning methods to address this falling motion prediction problem based\non data collected by inertial and film pressure sensors. We make a empirical\nstudy based on existing datasets and our datasets collected from our system\nseparately, which shows that the deep-learning model has more potential\nadvantage than other traditional methods, and we proposed a new deep-learning\nmodel based on the time series data to predict the fall, and it may be superior\nto other sequential models in this particular field.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.06994v1.pdf",
        "similarity": 0.3295145935992648,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-29"
    },
    {
        "new_title": "Neural Network Learning and Quantum Gravity",
        "new_link": "http://arxiv.org/abs/2403.03245v1",
        "new_summary": "  The landscape of low-energy effective field theories stemming from string\ntheory is too vast for a systematic exploration. However, the meadows of the\nstring landscape may be fertile ground for the application of machine learning\ntechniques. Employing neural network learning may allow for inferring novel,\nundiscovered properties that consistent theories in the landscape should\npossess, or checking conjectural statements about alleged characteristics\nthereof. The aim of this work is to describe to what extent the string\nlandscape can be explored with neural network-based learning. Our analysis is\nmotivated by recent studies that show that the string landscape is\ncharacterized by finiteness properties, emerging from its underlying tame,\no-minimal structures. Indeed, employing these results, we illustrate that any\nlow-energy effective theory of string theory is endowed with certain\nstatistical learnability properties. Consequently, several learning problems\ntherein formulated, including interpolations and multi-class classification\nproblems, can be concretely addressed with machine learning, delivering results\nwith sufficiently high accuracy.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.03245v1.pdf",
        "similarity": 0.32936672703305425,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-05"
    },
    {
        "new_title": "A Coding-Theoretic Analysis of Hyperspherical Prototypical Learning\n  Geometry",
        "new_link": "http://arxiv.org/abs/2407.07664v1",
        "new_summary": "  Hyperspherical Prototypical Learning (HPL) is a supervised approach to\nrepresentation learning that designs class prototypes on the unit hypersphere.\nThe prototypes bias the representations to class separation in a scale\ninvariant and known geometry. Previous approaches to HPL have either of the\nfollowing shortcomings: (i) they follow an unprincipled optimisation procedure;\nor (ii) they are theoretically sound, but are constrained to only one possible\nlatent dimension. In this paper, we address both shortcomings. To address (i),\nwe present a principled optimisation procedure whose solution we show is\noptimal. To address (ii), we construct well-separated prototypes in a wide\nrange of dimensions using linear block codes. Additionally, we give a full\ncharacterisation of the optimal prototype placement in terms of achievable and\nconverse bounds, showing that our proposed methods are near-optimal.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.07664v1.pdf",
        "similarity": 0.32929585733683436,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-10"
    },
    {
        "new_title": "Unifying Low Dimensional Observations in Deep Learning Through the Deep\n  Linear Unconstrained Feature Model",
        "new_link": "http://arxiv.org/abs/2404.06106v1",
        "new_summary": "  Modern deep neural networks have achieved high performance across various\ntasks. Recently, researchers have noted occurrences of low-dimensional\nstructure in the weights, Hessian's, gradients, and feature vectors of these\nnetworks, spanning different datasets and architectures when trained to\nconvergence. In this analysis, we theoretically demonstrate these observations\narising, and show how they can be unified within a generalized unconstrained\nfeature model that can be considered analytically. Specifically, we consider a\npreviously described structure called Neural Collapse, and its multi-layer\ncounterpart, Deep Neural Collapse, which emerges when the network approaches\nglobal optima. This phenomenon explains the other observed low-dimensional\nbehaviours on a layer-wise level, such as the bulk and outlier structure seen\nin Hessian spectra, and the alignment of gradient descent with the outlier\neigenspace of the Hessian. Empirical results in both the deep linear\nunconstrained feature model and its non-linear equivalent support these\npredicted observations.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.06106v1.pdf",
        "similarity": 0.32921462347039254,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-09"
    },
    {
        "new_title": "MobilityDL: A Review of Deep Learning From Trajectory Data",
        "new_link": "http://arxiv.org/abs/2402.00732v1",
        "new_summary": "  Trajectory data combines the complexities of time series, spatial data, and\n(sometimes irrational) movement behavior. As data availability and computing\npower have increased, so has the popularity of deep learning from trajectory\ndata. This review paper provides the first comprehensive overview of deep\nlearning approaches for trajectory data. We have identified eight specific\nmobility use cases which we analyze with regards to the deep learning models\nand the training data used. Besides a comprehensive quantitative review of the\nliterature since 2018, the main contribution of our work is the data-centric\nanalysis of recent work in this field, placing it along the mobility data\ncontinuum which ranges from detailed dense trajectories of individual movers\n(quasi-continuous tracking data), to sparse trajectories (such as check-in\ndata), and aggregated trajectories (crowd information).\n",
        "pdf_link": "https://arxiv.org/pdf/2402.00732v1.pdf",
        "similarity": 0.3291129246165054,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-01"
    },
    {
        "new_title": "DG-RePlAce: A Dataflow-Driven GPU-Accelerated Analytical Global\n  Placement Framework for Machine Learning Accelerators",
        "new_link": "http://arxiv.org/abs/2404.13049v2",
        "new_summary": "  Global placement is a fundamental step in VLSI physical design. The wide use\nof 2D processing element (PE) arrays in machine learning accelerators poses new\nchallenges of scalability and Quality of Results (QoR) for state-of-the-art\nacademic global placers. In this work, we develop DG-RePlAce, a new and fast\nGPU-accelerated global placement framework built on top of the OpenROAD\ninfrastructure, which exploits the inherent dataflow and datapath structures of\nmachine learning accelerators. Experimental results with a variety of machine\nlearning accelerators using a commercial 12nm enablement show that, compared\nwith RePlAce (DREAMPlace), our approach achieves an average reduction in routed\nwirelength by 10% (7%) and total negative slack (TNS) by 31% (34%), with faster\nglobal placement and on-par total runtimes relative to DREAMPlace. Empirical\nstudies on the TILOS MacroPlacement Benchmarks further demonstrate that\npost-route improvements over RePlAce and DREAMPlace may reach beyond the\nmotivating application to machine learning accelerators.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.13049v2.pdf",
        "similarity": 0.32904548829860697,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-16"
    },
    {
        "new_title": "Minimum-Norm Interpolation Under Covariate Shift",
        "new_link": "http://arxiv.org/abs/2404.00522v2",
        "new_summary": "  Transfer learning is a critical part of real-world machine learning\ndeployments and has been extensively studied in experimental works with\noverparameterized neural networks. However, even in the simplest setting of\nlinear regression a notable gap still exists in the theoretical understanding\nof transfer learning. In-distribution research on high-dimensional linear\nregression has led to the identification of a phenomenon known as\n\\textit{benign overfitting}, in which linear interpolators overfit to noisy\ntraining labels and yet still generalize well. This behavior occurs under\nspecific conditions on the source covariance matrix and input data dimension.\nTherefore, it is natural to wonder how such high-dimensional linear models\nbehave under transfer learning. We prove the first non-asymptotic excess risk\nbounds for benignly-overfit linear interpolators in the transfer learning\nsetting. From our analysis, we propose a taxonomy of \\textit{beneficial} and\n\\textit{malignant} covariate shifts based on the degree of\noverparameterization. We follow our analysis with empirical studies that show\nthese beneficial and malignant covariate shifts for linear interpolators on\nreal image data, and for fully-connected neural networks in settings where the\ninput data dimension is larger than the training sample size.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.00522v2.pdf",
        "similarity": 0.32902737552477784,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-31"
    },
    {
        "new_title": "Bridging Expert Knowledge with Deep Learning Techniques for Just-In-Time\n  Defect Prediction",
        "new_link": "http://arxiv.org/abs/2403.11079v1",
        "new_summary": "  Just-In-Time (JIT) defect prediction aims to automatically predict whether a\ncommit is defective or not, and has been widely studied in recent years. In\ngeneral, most studies can be classified into two categories: 1) simple models\nusing traditional machine learning classifiers with hand-crafted features, and\n2) complex models using deep learning techniques to automatically extract\nfeatures from commit contents. Hand-crafted features used by simple models are\nbased on expert knowledge but may not fully represent the semantic meaning of\nthe commits. On the other hand, deep learning-based features used by complex\nmodels represent the semantic meaning of commits but may not reflect useful\nexpert knowledge. Simple models and complex models seem complementary to each\nother to some extent. To utilize the advantages of both simple and complex\nmodels, we propose a model fusion framework that adopts both early fusions on\nthe feature level and late fusions on the decision level. We propose SimCom++\nby adopting the best early and late fusion strategies. The experimental results\nshow that SimCom++ can significantly outperform the baselines by 5.7--26.9\\%.\nIn addition, our experimental results confirm that the simple model and complex\nmodel are complementary to each other.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.11079v1.pdf",
        "similarity": 0.3287242893258058,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-17"
    },
    {
        "new_title": "Onboard Out-of-Calibration Detection of Deep Learning Models using\n  Conformal Prediction",
        "new_link": "http://arxiv.org/abs/2405.02634v1",
        "new_summary": "  The black box nature of deep learning models complicate their usage in\ncritical applications such as remote sensing. Conformal prediction is a method\nto ensure trust in such scenarios. Subject to data exchangeability, conformal\nprediction provides finite sample coverage guarantees in the form of a\nprediction set that is guaranteed to contain the true class within a user\ndefined error rate. In this letter we show that conformal prediction algorithms\nare related to the uncertainty of the deep learning model and that this\nrelation can be used to detect if the deep learning model is\nout-of-calibration. Popular classification models like Resnet50, Densenet161,\nInceptionV3, and MobileNetV2 are applied on remote sensing datasets such as the\nEuroSAT to demonstrate how under noisy scenarios the model outputs become\nuntrustworthy. Furthermore an out-of-calibration detection procedure relating\nthe model uncertainty and the average size of the conformal prediction set is\npresented.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.02634v1.pdf",
        "similarity": 0.32845556997370884,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-04"
    },
    {
        "new_title": "Confidence Intervals and Simultaneous Confidence Bands Based on Deep\n  Learning",
        "new_link": "http://arxiv.org/abs/2406.14009v1",
        "new_summary": "  Deep learning models have significantly improved prediction accuracy in\nvarious fields, gaining recognition across numerous disciplines. Yet, an aspect\nof deep learning that remains insufficiently addressed is the assessment of\nprediction uncertainty. Producing reliable uncertainty estimators could be\ncrucial in practical terms. For instance, predictions associated with a high\ndegree of uncertainty could be sent for further evaluation. Recent works in\nuncertainty quantification of deep learning predictions, including Bayesian\nposterior credible intervals and a frequentist confidence-interval estimation,\nhave proven to yield either invalid or overly conservative intervals.\nFurthermore, there is currently no method for quantifying uncertainty that can\naccommodate deep neural networks for survival (time-to-event) data that\ninvolves right-censored outcomes. In this work, we provide a valid\nnon-parametric bootstrap method that correctly disentangles data uncertainty\nfrom the noise inherent in the adopted optimization algorithm, ensuring that\nthe resulting point-wise confidence intervals or the simultaneous confidence\nbands are accurate (i.e., valid and not overly conservative). The proposed\nad-hoc method can be easily integrated into any deep neural network without\ninterfering with the training process. The utility of the proposed approach is\nillustrated by constructing simultaneous confidence bands for survival curves\nderived from deep neural networks for survival data with right censoring.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.14009v1.pdf",
        "similarity": 0.3284490560263046,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-20"
    },
    {
        "new_title": "Phase determination with and without deep learning",
        "new_link": "http://arxiv.org/abs/2403.09786v1",
        "new_summary": "  Detection of phase transitions is a critical task in statistical physics,\ntraditionally pursued through analytic methods and direct numerical\nsimulations. Recently, machine-learning techniques have emerged as promising\ntools in this context, with a particular focus on supervised and unsupervised\nlearning methods, along with non-learning approaches. In this work, we study\nthe performance of unsupervised learning in detecting phase transitions in the\n$J_1$-$J_2$ Ising model on the square lattice. The model is chosen due to its\nsimplicity and complexity, thus providing an understanding of the application\nof machine-learning techniques in both straightforward and challenging\nscenarios. We propose a simple method based on a direct comparison of\nconfigurations. The reconstruction error, defined as the mean-squared distance\nbetween two configurations, is used to determine the critical temperatures\n($T_c$). The results from the comparison of configurations are contrasted with\nthat of the configurations generated by variational autoencoders. Our findings\nhighlight that for certain systems, a simpler method can yield results\ncomparable to more complex neural networks. This work contributes to the\nbroader understanding of machine-learning applications in statistical physics\nand introduces an efficient approach to the detection of phase transitions\nusing machine determination techniques.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.09786v1.pdf",
        "similarity": 0.32816278144401423,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-14"
    },
    {
        "new_title": "Provable Contrastive Continual Learning",
        "new_link": "http://arxiv.org/abs/2405.18756v1",
        "new_summary": "  Continual learning requires learning incremental tasks with dynamic data\ndistributions. So far, it has been observed that employing a combination of\ncontrastive loss and distillation loss for training in continual learning\nyields strong performance. To the best of our knowledge, however, this\ncontrastive continual learning framework lacks convincing theoretical\nexplanations. In this work, we fill this gap by establishing theoretical\nperformance guarantees, which reveal how the performance of the model is\nbounded by training losses of previous tasks in the contrastive continual\nlearning framework. Our theoretical explanations further support the idea that\npre-training can benefit continual learning. Inspired by our theoretical\nanalysis of these guarantees, we propose a novel contrastive continual learning\nalgorithm called CILA, which uses adaptive distillation coefficients for\ndifferent tasks. These distillation coefficients are easily computed by the\nratio between average distillation losses and average contrastive losses from\nprevious tasks. Our method shows great improvement on standard benchmarks and\nachieves new state-of-the-art performance.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.18756v1.pdf",
        "similarity": 0.32807041046515145,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-29"
    },
    {
        "new_title": "Motion Consistency Loss for Monocular Visual Odometry with\n  Attention-Based Deep Learning",
        "new_link": "http://arxiv.org/abs/2401.10857v1",
        "new_summary": "  Deep learning algorithms have driven expressive progress in many complex\ntasks. The loss function is a core component of deep learning techniques,\nguiding the learning process of neural networks. This paper contributes by\nintroducing a consistency loss for visual odometry with deep learning-based\napproaches. The motion consistency loss explores repeated motions that appear\nin consecutive overlapped video clips. Experimental results show that our\napproach increased the performance of a model on the KITTI odometry benchmark.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.10857v1.pdf",
        "similarity": 0.32787704144575786,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-19"
    },
    {
        "new_title": "Motion-Informed Deep Learning for Brain MR Image Reconstruction\n  Framework",
        "new_link": "http://arxiv.org/abs/2405.17756v1",
        "new_summary": "  Motion artifacts in Magnetic Resonance Imaging (MRI) are one of the\nfrequently occurring artifacts due to patient movements during scanning. Motion\nis estimated to be present in approximately 30% of clinical MRI scans; however,\nmotion has not been explicitly modeled within deep learning image\nreconstruction models. Deep learning (DL) algorithms have been demonstrated to\nbe effective for both the image reconstruction task and the motion correction\ntask, but the two tasks are considered separately. The image reconstruction\ntask involves removing undersampling artifacts such as noise and aliasing\nartifacts, whereas motion correction involves removing artifacts including\nblurring, ghosting, and ringing. In this work, we propose a novel method to\nsimultaneously accelerate imaging and correct motion. This is achieved by\nintegrating a motion module into the deep learning-based MRI reconstruction\nprocess, enabling real-time detection and correction of motion. We model motion\nas a tightly integrated auxiliary layer in the deep learning model during\ntraining, making the deep learning model 'motion-informed'. During inference,\nimage reconstruction is performed from undersampled raw k-space data using a\ntrained motion-informed DL model. Experimental results demonstrate that the\nproposed motion-informed deep learning image reconstruction network\noutperformed the conventional image reconstruction network for motion-degraded\nMRI datasets.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.17756v1.pdf",
        "similarity": 0.32776924906200783,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-28"
    },
    {
        "new_title": "$C^*$-Algebraic Machine Learning: Moving in a New Direction",
        "new_link": "http://arxiv.org/abs/2402.02637v2",
        "new_summary": "  Machine learning has a long collaborative tradition with several fields of\nmathematics, such as statistics, probability and linear algebra. We propose a\nnew direction for machine learning research: $C^*$-algebraic ML $-$ a\ncross-fertilization between $C^*$-algebra and machine learning. The\nmathematical concept of $C^*$-algebra is a natural generalization of the space\nof complex numbers. It enables us to unify existing learning strategies, and\nconstruct a new framework for more diverse and information-rich data models. We\nexplain why and how to use $C^*$-algebras in machine learning, and provide\ntechnical considerations that go into the design of $C^*$-algebraic learning\nmodels in the contexts of kernel methods and neural networks. Furthermore, we\ndiscuss open questions and challenges in $C^*$-algebraic ML and give our\nthoughts for future development and applications.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.02637v2.pdf",
        "similarity": 0.3277277295916498,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-04"
    },
    {
        "new_title": "Learning Multimodal Behaviors from Scratch with Diffusion Policy\n  Gradient",
        "new_link": "http://arxiv.org/abs/2406.00681v1",
        "new_summary": "  Deep reinforcement learning (RL) algorithms typically parameterize the policy\nas a deep network that outputs either a deterministic action or a stochastic\none modeled as a Gaussian distribution, hence restricting learning to a single\nbehavioral mode. Meanwhile, diffusion models emerged as a powerful framework\nfor multimodal learning. However, the use of diffusion policies in online RL is\nhindered by the intractability of policy likelihood approximation, as well as\nthe greedy objective of RL methods that can easily skew the policy to a single\nmode. This paper presents Deep Diffusion Policy Gradient (DDiffPG), a novel\nactor-critic algorithm that learns from scratch multimodal policies\nparameterized as diffusion models while discovering and maintaining versatile\nbehaviors. DDiffPG explores and discovers multiple modes through off-the-shelf\nunsupervised clustering combined with novelty-based intrinsic motivation.\nDDiffPG forms a multimodal training batch and utilizes mode-specific Q-learning\nto mitigate the inherent greediness of the RL objective, ensuring the\nimprovement of the diffusion policy across all modes. Our approach further\nallows the policy to be conditioned on mode-specific embeddings to explicitly\ncontrol the learned modes. Empirical studies validate DDiffPG's capability to\nmaster multimodal behaviors in complex, high-dimensional continuous control\ntasks with sparse rewards, also showcasing proof-of-concept dynamic online\nreplanning when navigating mazes with unseen obstacles.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.00681v1.pdf",
        "similarity": 0.3270546941225367,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-02"
    },
    {
        "new_title": "Generalized Cauchy-Schwarz Divergence and Its Deep Learning Applications",
        "new_link": "http://arxiv.org/abs/2405.04061v3",
        "new_summary": "  Divergence measures play a central role and become increasingly essential in\ndeep learning, yet efficient measures for multiple (more than two)\ndistributions are rarely explored. This becomes particularly crucial in areas\nwhere the simultaneous management of multiple distributions is both inevitable\nand essential. Examples include clustering, multi-source domain adaptation or\ngeneralization, and multi-view learning, among others. While computing the mean\nof pairwise distances between any two distributions is a prevalent method to\nquantify the total divergence among multiple distributions, it is imperative to\nacknowledge that this approach is not straightforward and necessitates\nsignificant computational resources. In this study, we introduce a new\ndivergence measure tailored for multiple distributions named the generalized\nCauchy-Schwarz divergence (GCSD). Additionally, we furnish a kernel-based\nclosed-form sample estimator, making it convenient and straightforward to use\nin various machine-learning applications. Finally, we explore its profound\nimplications in the realm of deep learning by applying it to tackle two\nthoughtfully chosen machine-learning tasks: deep clustering and multi-source\ndomain adaptation. Our extensive experimental investigations confirm the\nrobustness and effectiveness of GCSD in both scenarios. The findings also\nunderscore the innovative potential of GCSD and its capability to significantly\npropel machine learning methodologies that necessitate the quantification of\nmultiple distributions.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.04061v3.pdf",
        "similarity": 0.32703532888368425,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-07"
    },
    {
        "new_title": "Combining Radiomics and Machine Learning Approaches for Objective ASD\n  Diagnosis: Verifying White Matter Associations with ASD",
        "new_link": "http://arxiv.org/abs/2405.16248v1",
        "new_summary": "  Autism Spectrum Disorder is a condition characterized by a typical brain\ndevelopment leading to impairments in social skills, communication abilities,\nrepetitive behaviors, and sensory processing. There have been many studies\ncombining brain MRI images with machine learning algorithms to achieve\nobjective diagnosis of autism, but the correlation between white matter and\nautism has not been fully utilized. To address this gap, we develop a\ncomputer-aided diagnostic model focusing on white matter regions in brain MRI\nby employing radiomics and machine learning methods. This study introduced a\nMultiUNet model for segmenting white matter, leveraging the UNet architecture\nand utilizing manually segmented MRI images as the training data. Subsequently,\nwe extracted white matter features using the Pyradiomics toolkit and applied\ndifferent machine learning models such as Support Vector Machine, Random\nForest, Logistic Regression, and K-Nearest Neighbors to predict autism. The\nprediction sets all exceeded 80% accuracy. Additionally, we employed\nConvolutional Neural Network to analyze segmented white matter images,\nachieving a prediction accuracy of 86.84%. Notably, Support Vector Machine\ndemonstrated the highest prediction accuracy at 89.47%. These findings not only\nunderscore the efficacy of the models but also establish a link between white\nmatter abnormalities and autism. Our study contributes to a comprehensive\nevaluation of various diagnostic models for autism and introduces a\ncomputer-aided diagnostic algorithm for early and objective autism diagnosis\nbased on MRI white matter regions.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.16248v1.pdf",
        "similarity": 0.32691769425480366,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-25"
    },
    {
        "new_title": "Attention Graph for Multi-Robot Social Navigation with Deep\n  Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2401.17914v1",
        "new_summary": "  Learning robot navigation strategies among pedestrian is crucial for domain\nbased applications. Combining perception, planning and prediction allows us to\nmodel the interactions between robots and pedestrians, resulting in impressive\noutcomes especially with recent approaches based on deep reinforcement learning\n(RL). However, these works do not consider multi-robot scenarios. In this\npaper, we present MultiSoc, a new method for learning multi-agent socially\naware navigation strategies using RL. Inspired by recent works on multi-agent\ndeep RL, our method leverages graph-based representation of agent interactions,\ncombining the positions and fields of view of entities (pedestrians and\nagents). Each agent uses a model based on two Graph Neural Network combined\nwith attention mechanisms. First an edge-selector produces a sparse graph, then\na crowd coordinator applies node attention to produce a graph representing the\ninfluence of each entity on the others. This is incorporated into a model-free\nRL framework to learn multi-agent policies. We evaluate our approach on\nsimulation and provide a series of experiments in a set of various conditions\n(number of agents / pedestrians). Empirical results show that our method learns\nfaster than social navigation deep RL mono-agent techniques, and enables\nefficient multi-agent implicit coordination in challenging crowd navigation\nwith multiple heterogeneous humans. Furthermore, by incorporating customizable\nmeta-parameters, we can adjust the neighborhood density to take into account in\nour navigation strategy.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.17914v1.pdf",
        "similarity": 0.32689560044560034,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-31"
    },
    {
        "new_title": "Deep Learning-based Depth Estimation Methods from Monocular Image and\n  Videos: A Comprehensive Survey",
        "new_link": "http://arxiv.org/abs/2406.19675v1",
        "new_summary": "  Estimating depth from single RGB images and videos is of widespread interest\ndue to its applications in many areas, including autonomous driving, 3D\nreconstruction, digital entertainment, and robotics. More than 500 deep\nlearning-based papers have been published in the past 10 years, which indicates\nthe growing interest in the task. This paper presents a comprehensive survey of\nthe existing deep learning-based methods, the challenges they address, and how\nthey have evolved in their architecture and supervision methods. It provides a\ntaxonomy for classifying the current work based on their input and output\nmodalities, network architectures, and learning methods. It also discusses the\nmajor milestones in the history of monocular depth estimation, and different\npipelines, datasets, and evaluation metrics used in existing methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.19675v1.pdf",
        "similarity": 0.3268702022485824,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-28"
    },
    {
        "new_title": "Advancing Deep Active Learning & Data Subset Selection: Unifying\n  Principles with Information-Theory Intuitions",
        "new_link": "http://arxiv.org/abs/2401.04305v3",
        "new_summary": "  At its core, this thesis aims to enhance the practicality of deep learning by\nimproving the label and training efficiency of deep learning models. To this\nend, we investigate data subset selection techniques, specifically active\nlearning and active sampling, grounded in information-theoretic principles.\nActive learning improves label efficiency, while active sampling enhances\ntraining efficiency. Supervised deep learning models often require extensive\ntraining with labeled data. Label acquisition can be expensive and\ntime-consuming, and training large models is resource-intensive, hindering the\nadoption outside academic research and \"big tech.\" Existing methods for data\nsubset selection in deep learning often rely on heuristics or lack a principled\ninformation-theoretic foundation. In contrast, this thesis examines several\nobjectives for data subset selection and their applications within deep\nlearning, striving for a more principled approach inspired by information\ntheory. We begin by disentangling epistemic and aleatoric uncertainty in single\nforward-pass deep neural networks, which provides helpful intuitions and\ninsights into different forms of uncertainty and their relevance for data\nsubset selection. We then propose and investigate various approaches for active\nlearning and data subset selection in (Bayesian) deep learning. Finally, we\nrelate various existing and proposed approaches to approximations of\ninformation quantities in weight or prediction space. Underpinning this work is\na principled and practical notation for information-theoretic quantities that\nincludes both random variables and observed outcomes. This thesis demonstrates\nthe benefits of working from a unified perspective and highlights the potential\nimpact of our contributions to the practical application of deep learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.04305v3.pdf",
        "similarity": 0.3267669189167967,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-09"
    },
    {
        "new_title": "Decomposing weather forecasting into advection and convection with\n  neural networks",
        "new_link": "http://arxiv.org/abs/2405.06590v1",
        "new_summary": "  Operational weather forecasting models have advanced for decades on both the\nexplicit numerical solvers and the empirical physical parameterization schemes.\nHowever, the involved high computational costs and uncertainties in these\nexisting schemes are requiring potential improvements through alternative\nmachine learning methods. Previous works use a unified model to learn the\ndynamics and physics of the atmospheric model. Contrarily, we propose a simple\nyet effective machine learning model that learns the horizontal movement in the\ndynamical core and vertical movement in the physical parameterization\nseparately. By replacing the advection with a graph attention network and the\nconvection with a multi-layer perceptron, our model provides a new and\nefficient perspective to simulate the transition of variables in atmospheric\nmodels. We also assess the model's performance over a 5-day iterative\nforecasting. Under the same input variables and training methods, our model\noutperforms existing data-driven methods with a significantly-reduced number of\nparameters with a resolution of 5.625 deg. Overall, this work aims to\ncontribute to the ongoing efforts that leverage machine learning techniques for\nimproving both the accuracy and efficiency of global weather forecasting.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.06590v1.pdf",
        "similarity": 0.3266717970598684,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-10"
    },
    {
        "new_title": "Task agnostic continual learning with Pairwise layer architecture",
        "new_link": "http://arxiv.org/abs/2405.13632v1",
        "new_summary": "  Most of the dominant approaches to continual learning are based on either\nmemory replay, parameter isolation, or regularization techniques that require\ntask boundaries to calculate task statistics. We propose a static\narchitecture-based method that doesn't use any of these. We show that we can\nimprove the continual learning performance by replacing the final layer of our\nnetworks with our pairwise interaction layer. The pairwise interaction layer\nuses sparse representations from a Winner-take-all style activation function to\nfind the relevant correlations in the hidden layer representations. The\nnetworks using this architecture show competitive performance in MNIST and\nFashionMNIST-based continual image classification experiments. We demonstrate\nthis in an online streaming continual learning setup where the learning system\ncannot access task labels or boundaries.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.13632v1.pdf",
        "similarity": 0.3263441762126357,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-22"
    },
    {
        "new_title": "Curvature Augmented Manifold Embedding and Learning",
        "new_link": "http://arxiv.org/abs/2403.14813v1",
        "new_summary": "  A new dimensional reduction (DR) and data visualization method,\nCurvature-Augmented Manifold Embedding and Learning (CAMEL), is proposed. The\nkey novel contribution is to formulate the DR problem as a mechanistic/physics\nmodel, where the force field among nodes (data points) is used to find an\nn-dimensional manifold representation of the data sets. Compared with many\nexisting attractive-repulsive force-based methods, one unique contribution of\nthe proposed method is to include a non-pairwise force. A new force field model\nis introduced and discussed, inspired by the multi-body potential in\nlattice-particle physics and Riemann curvature in topology. A\ncurvature-augmented force is included in CAMEL. Following this, CAMEL\nformulation for unsupervised learning, supervised learning, semi-supervised\nlearning/metric learning, and inverse learning are provided. Next, CAMEL is\napplied to many benchmark datasets by comparing existing models, such as tSNE,\nUMAP, TRIMAP, and PacMap. Both visual comparison and metrics-based evaluation\nare performed. 14 open literature and self-proposed metrics are employed for a\ncomprehensive comparison. Conclusions and future work are suggested based on\nthe current investigation. Related code and demonstration are available on\nhttps://github.com/ymlasu/CAMEL for interested readers to reproduce the results\nand other applications.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.14813v1.pdf",
        "similarity": 0.3261141786317167,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-21"
    },
    {
        "new_title": "Parametric Matrix Models",
        "new_link": "http://arxiv.org/abs/2401.11694v4",
        "new_summary": "  We present a general class of machine learning algorithms called parametric\nmatrix models. In contrast with most existing machine learning models that\nimitate the biology of neurons, parametric matrix models use matrix equations\nthat emulate the physics of quantum systems. Similar to how physics problems\nare usually solved, parametric matrix models learn the governing equations that\nlead to the desired outputs. Parametric matrix models can be efficiently\ntrained from empirical data, and the equations may use algebraic, differential,\nor integral relations. While originally designed for scientific computing, we\nprove that parametric matrix models are universal function approximators that\ncan be applied to general machine learning problems. After introducing the\nunderlying theory, we apply parametric matrix models to a series of different\nchallenges that show their performance for a wide range of problems. For all\nthe challenges tested here, parametric matrix models produce accurate results\nwithin an efficient and interpretable computational framework that allows for\ninput feature extrapolation.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.11694v4.pdf",
        "similarity": 0.32583587078405674,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-22"
    },
    {
        "new_title": "Towards a theory of learning dynamics in deep state space models",
        "new_link": "http://arxiv.org/abs/2407.07279v1",
        "new_summary": "  State space models (SSMs) have shown remarkable empirical performance on many\nlong sequence modeling tasks, but a theoretical understanding of these models\nis still lacking. In this work, we study the learning dynamics of linear SSMs\nto understand how covariance structure in data, latent state size, and\ninitialization affect the evolution of parameters throughout learning with\ngradient descent. We show that focusing on the learning dynamics in the\nfrequency domain affords analytical solutions under mild assumptions, and we\nestablish a link between one-dimensional SSMs and the dynamics of deep linear\nfeed-forward networks. Finally, we analyze how latent state\nover-parameterization affects convergence time and describe future work in\nextending our results to the study of deep SSMs with nonlinear connections.\nThis work is a step toward a theory of learning dynamics in deep state space\nmodels.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.07279v1.pdf",
        "similarity": 0.32569161383779216,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-10"
    },
    {
        "new_title": "Q-Star Meets Scalable Posterior Sampling: Bridging Theory and Practice\n  via HyperAgent",
        "new_link": "http://arxiv.org/abs/2402.10228v5",
        "new_summary": "  We propose HyperAgent, a reinforcement learning (RL) algorithm based on the\nhypermodel framework for exploration in RL. HyperAgent allows for the efficient\nincremental approximation of posteriors associated with an optimal action-value\nfunction ($Q^\\star$) without the need for conjugacy and follows the greedy\npolicies w.r.t. these approximate posterior samples. We demonstrate that\nHyperAgent offers robust performance in large-scale deep RL benchmarks. It can\nsolve Deep Sea hard exploration problems with episodes that optimally scale\nwith problem size and exhibits significant efficiency gains in the Atari suite.\nImplementing HyperAgent requires minimal code addition to well-established deep\nRL frameworks like DQN. We theoretically prove that, under tabular assumptions,\nHyperAgent achieves logarithmic per-step computational complexity while\nattaining sublinear regret, matching the best known randomized tabular RL\nalgorithm.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.10228v5.pdf",
        "similarity": 0.32560047326328667,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-05"
    },
    {
        "new_title": "Adjacent Leader Decentralized Stochastic Gradient Descent",
        "new_link": "http://arxiv.org/abs/2405.11389v1",
        "new_summary": "  This work focuses on the decentralized deep learning optimization framework.\nWe propose Adjacent Leader Decentralized Gradient Descent (AL-DSGD), for\nimproving final model performance, accelerating convergence, and reducing the\ncommunication overhead of decentralized deep learning optimizers. AL-DSGD\nrelies on two main ideas. Firstly, to increase the influence of the strongest\nlearners on the learning system it assigns weights to different neighbor\nworkers according to both their performance and the degree when averaging among\nthem, and it applies a corrective force on the workers dictated by both the\ncurrently best-performing neighbor and the neighbor with the maximal degree.\nSecondly, to alleviate the problem of the deterioration of the convergence\nspeed and performance of the nodes with lower degrees, AL-DSGD relies on\ndynamic communication graphs, which effectively allows the workers to\ncommunicate with more nodes while keeping the degrees of the nodes low.\nExperiments demonstrate that AL-DSGD accelerates the convergence of the\ndecentralized state-of-the-art techniques and improves their test performance\nespecially in the communication constrained environments. We also theoretically\nprove the convergence of the proposed scheme. Finally, we release to the\ncommunity a highly general and concise PyTorch-based library for distributed\ntraining of deep learning models that supports easy implementation of any\ndistributed deep learning approach ((a)synchronous, (de)centralized).\n",
        "pdf_link": "https://arxiv.org/pdf/2405.11389v1.pdf",
        "similarity": 0.3254997398643501,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-18"
    },
    {
        "new_title": "Better than classical? The subtle art of benchmarking quantum machine\n  learning models",
        "new_link": "http://arxiv.org/abs/2403.07059v2",
        "new_summary": "  Benchmarking models via classical simulations is one of the main ways to\njudge ideas in quantum machine learning before noise-free hardware is\navailable. However, the huge impact of the experimental design on the results,\nthe small scales within reach today, as well as narratives influenced by the\ncommercialisation of quantum technologies make it difficult to gain robust\ninsights. To facilitate better decision-making we develop an open-source\npackage based on the PennyLane software framework and use it to conduct a\nlarge-scale study that systematically tests 12 popular quantum machine learning\nmodels on 6 binary classification tasks used to create 160 individual datasets.\nWe find that overall, out-of-the-box classical machine learning models\noutperform the quantum classifiers. Moreover, removing entanglement from a\nquantum model often results in as good or better performance, suggesting that\n\"quantumness\" may not be the crucial ingredient for the small learning tasks\nconsidered here. Our benchmarks also unlock investigations beyond simplistic\nleaderboard comparisons, and we identify five important questions for quantum\nmodel design that follow from our results.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.07059v2.pdf",
        "similarity": 0.32548571934097625,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-11"
    },
    {
        "new_title": "The Art of the Steal: Purloining Deep Learning Models Developed for an\n  Ultrasound Scanner to a Competitor Machine",
        "new_link": "http://arxiv.org/abs/2407.03512v1",
        "new_summary": "  A transfer function approach has recently proven effective for calibrating\ndeep learning (DL) algorithms in quantitative ultrasound (QUS), addressing data\nshifts at both the acquisition and machine levels. Expanding on this approach,\nwe develop a strategy to 'steal' the functionality of a DL model from one\nultrasound machine and implement it on another, in the context of QUS. This\ndemonstrates the ease with which the functionality of a DL model can be\ntransferred between machines, highlighting the security risks associated with\ndeploying such models in a commercial scanner for clinical use. The proposed\nmethod is a black-box unsupervised domain adaptation technique that integrates\nthe transfer function approach with an iterative schema. It does not utilize\nany information related to model internals of the victim machine but it solely\nrelies on the availability of input-output interface. Additionally, we assume\nthe availability of unlabelled data from the testing machine, i.e., the\nperpetrator machine. This scenario could become commonplace as companies begin\ndeploying their DL functionalities for clinical use. Competing companies might\nacquire the victim machine and, through the input-output interface, replicate\nthe functionality onto their own machines. In the experiments, we used a\nSonixOne and a Verasonics machine. The victim model was trained on SonixOne\ndata, and its functionality was then transferred to the Verasonics machine. The\nproposed method successfully transferred the functionality to the Verasonics\nmachine, achieving a remarkable 98\\% classification accuracy in a binary\ndecision task. This study underscores the need to establish security measures\nprior to deploying DL models in clinical settings.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.03512v1.pdf",
        "similarity": 0.32541481511162146,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-03"
    },
    {
        "new_title": "TorchSurv: A Lightweight Package for Deep Survival Analysis",
        "new_link": "http://arxiv.org/abs/2404.10761v2",
        "new_summary": "  TorchSurv is a Python package that serves as a companion tool to perform deep\nsurvival modeling within the PyTorch environment. Unlike existing libraries\nthat impose specific parametric forms, TorchSurv enables the use of custom\nPyTorch-based deep survival models. With its lightweight design, minimal input\nrequirements, full PyTorch backend, and freedom from restrictive survival model\nparameterizations, TorchSurv facilitates efficient deep survival model\nimplementation and is particularly beneficial for high-dimensional and complex\ninput data scenarios.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.10761v2.pdf",
        "similarity": 0.3250380948003184,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-16"
    },
    {
        "new_title": "Misclassification bounds for PAC-Bayesian sparse deep learning",
        "new_link": "http://arxiv.org/abs/2405.01304v1",
        "new_summary": "  Recently, there has been a significant focus on exploring the theoretical\naspects of deep learning, especially regarding its performance in\nclassification tasks. Bayesian deep learning has emerged as a unified\nprobabilistic framework, seeking to integrate deep learning with Bayesian\nmethodologies seamlessly. However, there exists a gap in the theoretical\nunderstanding of Bayesian approaches in deep learning for classification. This\nstudy presents an attempt to bridge that gap. By leveraging PAC-Bayes bounds\ntechniques, we present theoretical results on the prediction or\nmisclassification error of a probabilistic approach utilizing Spike-and-Slab\npriors for sparse deep learning in classification. We establish non-asymptotic\nresults for the prediction error. Additionally, we demonstrate that, by\nconsidering different architectures, our results can achieve minimax optimal\nrates in both low and high-dimensional settings, up to a logarithmic factor.\nMoreover, our additional logarithmic term yields slight improvements over\nprevious works. Additionally, we propose and analyze an automated model\nselection approach aimed at optimally choosing a network architecture with\nguaranteed optimality.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.01304v1.pdf",
        "similarity": 0.3250058100843608,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-02"
    },
    {
        "new_title": "Between Randomness and Arbitrariness: Some Lessons for Reliable Machine\n  Learning at Scale",
        "new_link": "http://arxiv.org/abs/2406.09548v1",
        "new_summary": "  To develop rigorous knowledge about ML models -- and the systems in which\nthey are embedded -- we need reliable measurements. But reliable measurement is\nfundamentally challenging, and touches on issues of reproducibility,\nscalability, uncertainty quantification, epistemology, and more. This\ndissertation addresses criteria needed to take reliability seriously: both\ncriteria for designing meaningful metrics, and for methodologies that ensure\nthat we can dependably and efficiently measure these metrics at scale and in\npractice. In doing so, this dissertation articulates a research vision for a\nnew field of scholarship at the intersection of machine learning, law, and\npolicy. Within this frame, we cover topics that fit under three different\nthemes: (1) quantifying and mitigating sources of arbitrariness in ML, (2)\ntaming randomness in uncertainty estimation and optimization algorithms, in\norder to achieve scalability without sacrificing reliability, and (3) providing\nmethods for evaluating generative-AI systems, with specific focuses on\nquantifying memorization in language models and training latent diffusion\nmodels on open-licensed data. By making contributions in these three themes,\nthis dissertation serves as an empirical proof by example that research on\nreliable measurement for machine learning is intimately and inescapably bound\nup with research in law and policy. These different disciplines pose similar\nresearch questions about reliable measurement in machine learning. They are, in\nfact, two complementary sides of the same research vision, which, broadly\nconstrued, aims to construct machine-learning systems that cohere with broader\nsocietal values.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.09548v1.pdf",
        "similarity": 0.3249877769408593,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-13"
    },
    {
        "new_title": "Mathematical Algorithm Design for Deep Learning under Societal and\n  Judicial Constraints: The Algorithmic Transparency Requirement",
        "new_link": "http://arxiv.org/abs/2401.10310v1",
        "new_summary": "  Deep learning still has drawbacks in terms of trustworthiness, which\ndescribes a comprehensible, fair, safe, and reliable method. To mitigate the\npotential risk of AI, clear obligations associated to trustworthiness have been\nproposed via regulatory guidelines, e.g., in the European AI Act. Therefore, a\ncentral question is to what extent trustworthy deep learning can be realized.\nEstablishing the described properties constituting trustworthiness requires\nthat the factors influencing an algorithmic computation can be retraced, i.e.,\nthe algorithmic implementation is transparent. Motivated by the observation\nthat the current evolution of deep learning models necessitates a change in\ncomputing technology, we derive a mathematical framework which enables us to\nanalyze whether a transparent implementation in a computing model is feasible.\nWe exemplarily apply our trustworthiness framework to analyze deep learning\napproaches for inverse problems in digital and analog computing models\nrepresented by Turing and Blum-Shub-Smale Machines, respectively. Based on\nprevious results, we find that Blum-Shub-Smale Machines have the potential to\nestablish trustworthy solvers for inverse problems under fairly general\nconditions, whereas Turing machines cannot guarantee trustworthiness to the\nsame degree.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.10310v1.pdf",
        "similarity": 0.3247676667966571,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-18"
    },
    {
        "new_title": "Personalized Federated Learning via Sequential Layer Expansion in\n  Representation Learning",
        "new_link": "http://arxiv.org/abs/2404.17799v1",
        "new_summary": "  Federated learning ensures the privacy of clients by conducting distributed\ntraining on individual client devices and sharing only the model weights with a\ncentral server. However, in real-world scenarios, the heterogeneity of data\namong clients necessitates appropriate personalization methods. In this paper,\nwe aim to address this heterogeneity using a form of parameter decoupling known\nas representation learning. Representation learning divides deep learning\nmodels into 'base' and 'head' components. The base component, capturing common\nfeatures across all clients, is shared with the server, while the head\ncomponent, capturing unique features specific to individual clients, remains\nlocal. We propose a new representation learning-based approach that suggests\ndecoupling the entire deep learning model into more densely divided parts with\nthe application of suitable scheduling methods, which can benefit not only data\nheterogeneity but also class heterogeneity. In this paper, we compare and\nanalyze two layer scheduling approaches, namely forward (\\textit{Vanilla}) and\nbackward (\\textit{Anti}), in the context of data and class heterogeneity among\nclients. Our experimental results show that the proposed algorithm, when\ncompared to existing personalized federated learning algorithms, achieves\nincreased accuracy, especially under challenging conditions, while reducing\ncomputation costs.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.17799v1.pdf",
        "similarity": 0.32475896678659505,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-27"
    },
    {
        "new_title": "Exploring Thermography Technology: A Comprehensive Facial Dataset for\n  Face Detection, Recognition, and Emotion",
        "new_link": "http://arxiv.org/abs/2407.09494v1",
        "new_summary": "  This dataset includes 6823 thermal images captured using a UNI-T UTi165A\ncamera for face detection, recognition, and emotion analysis. It consists of\n2485 facial recognition images depicting emotions (happy, sad, angry, natural,\nsurprised), 2054 images for face recognition, and 2284 images for face\ndetection. The dataset covers various conditions, color palettes, shooting\nangles, and zoom levels, with a temperature range of -10{\\deg}C to 400{\\deg}C\nand a resolution of 19,200 pixels. It serves as a valuable resource for\nadvancing thermal imaging technology, aiding in algorithm development, and\nbenchmarking for facial recognition across different palettes. Additionally, it\ncontributes to facial motion recognition, fostering interdisciplinary\ncollaboration in computer vision, psychology, and neuroscience. The dataset\npromotes transparency in thermal face detection and recognition research, with\napplications in security, healthcare, and human-computer interaction.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.09494v1.pdf",
        "similarity": 0.32472357161663434,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-28"
    },
    {
        "new_title": "Deep learning enables automated assessments of inflammatory response in\n  zebrafish exposed to different pollutants",
        "new_link": "http://arxiv.org/abs/2406.00603v1",
        "new_summary": "  In the field of environmental toxicology, rapid and precise assessment of the\ninflammatory response to pollutants in biological models is critical. This\nstudy leverages the power of deep learning to enable automated assessments of\nzebrafish, a model organism widely used for its translational relevance to\nhuman disease pathways. We present an innovative approach to assessing\ninflammatory responses in zebrafish exposed to various pollutants through an\nend-to-end deep learning model. The model employs a Unet-based architecture to\nautomatically process high-throughput lateral zebrafish images, segmenting\nspecific regions and quantifying neutrophils as inflammation markers. Alongside\nimaging, qPCR analysis offers gene expression insights, revealing the molecular\nimpact of exposure on inflammatory pathways. Moreover, the deep learning model\nwas packaged as a user-friendly executable file (.exe), facilitating widespread\napplication by enabling use on virtually any computer without the need for\nspecialized software or training.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.00603v1.pdf",
        "similarity": 0.324687563474932,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-02"
    },
    {
        "new_title": "Rapid modelling of reactive transport in porous media using machine\n  learning: limitations and solutions",
        "new_link": "http://arxiv.org/abs/2405.14548v1",
        "new_summary": "  Reactive transport in porous media plays a pivotal role in subsurface\nreservoir processes, influencing fluid properties and geochemical\ncharacteristics. However, coupling fluid flow and transport with geochemical\nreactions is computationally intensive, requiring geochemical calculations at\neach grid cell and each time step within a discretized simulation domain.\nAlthough recent advancements have integrated machine learning techniques as\nsurrogates for geochemical simulations, ensuring computational efficiency and\naccuracy remains a challenge. This chapter investigates machine learning models\nas replacements for a geochemical module in a reactive transport in porous\nmedia simulation. We test this approach on a well-documented cation exchange\nproblem. While the surrogate models excel in isolated predictions, they fall\nshort in rollout predictions over successive time steps. By introducing\nmodifications, including physics-based constraints and tailored dataset\ngeneration strategies, we show that machine learning surrogates can achieve\naccurate rollout predictions. Our findings emphasize that, when judiciously\ndesigned, machine learning surrogates can substantially expedite the cation\nexchange problem without compromising accuracy, offering significant potential\nfor a range of reactive transport applications.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.14548v1.pdf",
        "similarity": 0.32457012015551495,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-23"
    },
    {
        "new_title": "SIMAP: A simplicial-map layer for neural networks",
        "new_link": "http://arxiv.org/abs/2403.15083v1",
        "new_summary": "  In this paper, we present SIMAP, a novel layer integrated into deep learning\nmodels, aimed at enhancing the interpretability of the output. The SIMAP layer\nis an enhanced version of Simplicial-Map Neural Networks (SMNNs), an\nexplainable neural network based on support sets and simplicial maps (functions\nused in topology to transform shapes while preserving their structural\nconnectivity). The novelty of the methodology proposed in this paper is\ntwo-fold: Firstly, SIMAP layers work in combination with other deep learning\narchitectures as an interpretable layer substituting classic dense final\nlayers. Secondly, unlike SMNNs, the support set is based on a fixed maximal\nsimplex, the barycentric subdivision being efficiently computed with a\nmatrix-based multiplication algorithm.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.15083v1.pdf",
        "similarity": 0.3241278953388849,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-22"
    },
    {
        "new_title": "Nonlinear time-series embedding by monotone variational inequality",
        "new_link": "http://arxiv.org/abs/2406.06894v1",
        "new_summary": "  In the wild, we often encounter collections of sequential data such as\nelectrocardiograms, motion capture, genomes, and natural language, and\nsequences may be multichannel or symbolic with nonlinear dynamics. We introduce\na new method to learn low-dimensional representations of nonlinear time series\nwithout supervision and can have provable recovery guarantees. The learned\nrepresentation can be used for downstream machine-learning tasks such as\nclustering and classification. The method is based on the assumption that the\nobserved sequences arise from a common domain, but each sequence obeys its own\nautoregressive models that are related to each other through low-rank\nregularization. We cast the problem as a computationally efficient convex\nmatrix parameter recovery problem using monotone Variational Inequality and\nencode the common domain assumption via low-rank constraint across the learned\nrepresentations, which can learn the geometry for the entire domain as well as\nfaithful representations for the dynamics of each individual sequence using the\ndomain information in totality. We show the competitive performance of our\nmethod on real-world time-series data with the baselines and demonstrate its\neffectiveness for symbolic text modeling and RNA sequence clustering.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.06894v1.pdf",
        "similarity": 0.32397698713164136,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-11"
    },
    {
        "new_title": "Evaluating Serverless Machine Learning Performance on Google Cloud Run",
        "new_link": "http://arxiv.org/abs/2406.16250v1",
        "new_summary": "  End-users can get functions-as-a-service from serverless platforms, which\npromise lower hosting costs, high availability, fault tolerance, and dynamic\nflexibility for hosting individual functions known as microservices. Machine\nlearning tools are seen to be reliably useful, and the services created using\nthese tools are in increasing demand on a large scale. The serverless platforms\nare uniquely suited for hosting these machine learning services to be used for\nlarge-scale applications. These platforms are well known for their cost\nefficiency, fault tolerance, resource scaling, robust APIs for communication,\nand global reach. However, machine learning services are different from the\nweb-services in that these serverless platforms were originally designed to\nhost web services. We aimed to understand how these serverless platforms handle\nmachine learning workloads with our study. We examine machine learning\nperformance on one of the serverless platforms - Google Cloud Run, which is a\nGPU-less infrastructure that is not designed for machine learning application\ndeployment.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.16250v1.pdf",
        "similarity": 0.3235472589166359,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-24"
    },
    {
        "new_title": "Automatic Assessment of Dysarthria Using Audio-visual Vowel Graph\n  Attention Network",
        "new_link": "http://arxiv.org/abs/2405.03254v2",
        "new_summary": "  Automatic assessment of dysarthria remains a highly challenging task due to\nhigh variability in acoustic signals and the limited data. Currently, research\non the automatic assessment of dysarthria primarily focuses on two approaches:\none that utilizes expert features combined with machine learning, and the other\nthat employs data-driven deep learning methods to extract representations.\nResearch has demonstrated that expert features are effective in representing\npathological characteristics, while deep learning methods excel at uncovering\nlatent features. Therefore, integrating the advantages of expert features and\ndeep learning to construct a neural network architecture based on expert\nknowledge may be beneficial for interpretability and assessment performance. In\nthis context, the present paper proposes a vowel graph attention network based\non audio-visual information, which effectively integrates the strengths of\nexpert knowledges and deep learning. Firstly, various features were combined as\ninputs, including knowledge based acoustical features and deep learning based\npre-trained representations. Secondly, the graph network structure based on\nvowel space theory was designed, allowing for a deep exploration of spatial\ncorrelations among vowels. Finally, visual information was incorporated into\nthe model to further enhance its robustness and generalizability. The method\nexhibited superior performance in regression experiments targeting Frenchay\nscores compared to existing approaches.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.03254v2.pdf",
        "similarity": 0.32325064328188446,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-06"
    },
    {
        "new_title": "Representation Learning For Efficient Deep Multi-Agent Reinforcement\n  Learning",
        "new_link": "http://arxiv.org/abs/2406.02890v1",
        "new_summary": "  Sample efficiency remains a key challenge in multi-agent reinforcement\nlearning (MARL). A promising approach is to learn a meaningful latent\nrepresentation space through auxiliary learning objectives alongside the MARL\nobjective to aid in learning a successful control policy. In our work, we\npresent MAPO-LSO (Multi-Agent Policy Optimization with Latent Space\nOptimization) which applies a form of comprehensive representation learning\ndevised to supplement MARL training. Specifically, MAPO-LSO proposes a\nmulti-agent extension of transition dynamics reconstruction and self-predictive\nlearning that constructs a latent state optimization scheme that can be\ntrivially extended to current state-of-the-art MARL algorithms. Empirical\nresults demonstrate MAPO-LSO to show notable improvements in sample efficiency\nand learning performance compared to its vanilla MARL counterpart without any\nadditional MARL hyperparameter tuning on a diverse suite of MARL tasks.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.02890v1.pdf",
        "similarity": 0.3232407452911353,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-05"
    },
    {
        "new_title": "Data Debiasing with Datamodels (D3M): Improving Subgroup Robustness via\n  Data Selection",
        "new_link": "http://arxiv.org/abs/2406.16846v1",
        "new_summary": "  Machine learning models can fail on subgroups that are underrepresented\nduring training. While techniques such as dataset balancing can improve\nperformance on underperforming groups, they require access to training group\nannotations and can end up removing large portions of the dataset. In this\npaper, we introduce Data Debiasing with Datamodels (D3M), a debiasing approach\nwhich isolates and removes specific training examples that drive the model's\nfailures on minority groups. Our approach enables us to efficiently train\ndebiased classifiers while removing only a small number of examples, and does\nnot require training group annotations or additional hyperparameter tuning.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.16846v1.pdf",
        "similarity": 0.32320169837665064,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-24"
    },
    {
        "new_title": "Deep Neural Networks are Adaptive to Function Regularity and Data\n  Distribution in Approximation and Estimation",
        "new_link": "http://arxiv.org/abs/2406.05320v1",
        "new_summary": "  Deep learning has exhibited remarkable results across diverse areas. To\nunderstand its success, substantial research has been directed towards its\ntheoretical foundations. Nevertheless, the majority of these studies examine\nhow well deep neural networks can model functions with uniform regularity. In\nthis paper, we explore a different angle: how deep neural networks can adapt to\ndifferent regularity in functions across different locations and scales and\nnonuniform data distributions. More precisely, we focus on a broad class of\nfunctions defined by nonlinear tree-based approximation. This class encompasses\na range of function types, such as functions with uniform regularity and\ndiscontinuous functions. We develop nonparametric approximation and estimation\ntheories for this function class using deep ReLU networks. Our results show\nthat deep neural networks are adaptive to different regularity of functions and\nnonuniform data distributions at different locations and scales. We apply our\nresults to several function classes, and derive the corresponding approximation\nand generalization errors. The validity of our results is demonstrated through\nnumerical experiments.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.05320v1.pdf",
        "similarity": 0.32319726326430837,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-08"
    },
    {
        "new_title": "Learned harmonic mean estimation of the Bayesian evidence with\n  normalizing flows",
        "new_link": "http://arxiv.org/abs/2405.05969v1",
        "new_summary": "  We present the learned harmonic mean estimator with normalizing flows - a\nrobust, scalable and flexible estimator of the Bayesian evidence for model\ncomparison. Since the estimator is agnostic to sampling strategy and simply\nrequires posterior samples, it can be applied to compute the evidence using any\nMarkov chain Monte Carlo (MCMC) sampling technique, including saved down MCMC\nchains, or any variational inference approach. The learned harmonic mean\nestimator was recently introduced, where machine learning techniques were\ndeveloped to learn a suitable internal importance sampling target distribution\nto solve the issue of exploding variance of the original harmonic mean\nestimator. In this article we present the use of normalizing flows as the\ninternal machine learning technique within the learned harmonic mean estimator.\nNormalizing flows can be elegantly coupled with the learned harmonic mean to\nprovide an approach that is more robust, flexible and scalable than the machine\nlearning models considered previously. We perform a series of numerical\nexperiments, applying our method to benchmark problems and to a cosmological\nexample in up to 21 dimensions. We find the learned harmonic mean estimator is\nin agreement with ground truth values and nested sampling estimates. The\nopen-source harmonic Python package implementing the learned harmonic mean, now\nwith normalizing flows included, is publicly available.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.05969v1.pdf",
        "similarity": 0.3230323877809933,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-09"
    },
    {
        "new_title": "Biological Neurons Compete with Deep Reinforcement Learning in Sample\n  Efficiency in a Simulated Gameworld",
        "new_link": "http://arxiv.org/abs/2405.16946v1",
        "new_summary": "  How do biological systems and machine learning algorithms compare in the\nnumber of samples required to show significant improvements in completing a\ntask? We compared the learning efficiency of in vitro biological neural\nnetworks to the state-of-the-art deep reinforcement learning (RL) algorithms in\na simplified simulation of the game `Pong'. Using DishBrain, a system that\nembodies in vitro neural networks with in silico computation using a\nhigh-density multi-electrode array, we contrasted the learning rate and the\nperformance of these biological systems against time-matched learning from\nthree state-of-the-art deep RL algorithms (i.e., DQN, A2C, and PPO) in the same\ngame environment. This allowed a meaningful comparison between biological\nneural systems and deep RL. We find that when samples are limited to a\nreal-world time course, even these very simple biological cultures outperformed\ndeep RL algorithms across various game performance characteristics, implying a\nhigher sample efficiency. Ultimately, even when tested across multiple types of\ninformation input to assess the impact of higher dimensional data input,\nbiological neurons showcased faster learning than all deep reinforcement\nlearning agents.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.16946v1.pdf",
        "similarity": 0.3226649476733553,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-27"
    },
    {
        "new_title": "Farthest Point Sampling in Property Designated Chemical Feature Space as\n  a General Strategy for Enhancing the Machine Learning Model Performance for\n  Small Scale Chemical Dataset",
        "new_link": "http://arxiv.org/abs/2404.11348v1",
        "new_summary": "  Machine learning model development in chemistry and materials science often\ngrapples with the challenge of small scale, unbalanced labelled datasets, a\ncommon limitation in scientific experiments. These dataset imbalances can\nprecipitate overfit ting and diminish model generalization. Our study explores\nthe efficacy of the farthest point sampling (FPS) strategy within target ed\nchemical feature spaces, demonstrating its capacity to generate\nwell-distributed training datasets and consequently enhance model performance.\nWe rigorously evaluated this strategy across various machine learning models,\nincluding artificial neural net works (ANN), support vector machines (SVM), and\nrandom forests (RF), using datasets encapsulating physicochemical properties\nlike standard boiling points and enthalpy of vaporization. Our findings reveal\nthat FPS-based models consistently surpass those trained via random sampling,\nexhibiting superior predictive accuracy and robustness, alongside a marked\nreduction in overfitting. This improvement is particularly pronounced in\nsmaller training datasets, attributable to increased diversity within the\ntraining data's chemical feature space. Consequently, FPS emerges as a\nuniversally effective and adaptable approach in approaching high performance\nmachine learning models by small and biased experimental datasets prevalent in\nchemistry and materials science.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.11348v1.pdf",
        "similarity": 0.32264841228228364,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-17"
    },
    {
        "new_title": "Deep learning for the design of non-Hermitian topolectrical circuits",
        "new_link": "http://arxiv.org/abs/2402.09978v1",
        "new_summary": "  Non-Hermitian topological phases can produce some remarkable properties,\ncompared with their Hermitian counterpart, such as the breakdown of\nconventional bulk-boundary correspondence and the non-Hermitian topological\nedge mode. Here, we introduce several algorithms with multi-layer perceptron\n(MLP), and convolutional neural network (CNN) in the field of deep learning, to\npredict the winding of eigenvalues non-Hermitian Hamiltonians. Subsequently, we\nuse the smallest module of the periodic circuit as one unit to construct\nhigh-dimensional circuit data features. Further, we use the Dense Convolutional\nNetwork (DenseNet), a type of convolutional neural network that utilizes dense\nconnections between layers to design a non-Hermitian topolectrical Chern\ncircuit, as the DenseNet algorithm is more suitable for processing\nhigh-dimensional data. Our results demonstrate the effectiveness of the deep\nlearning network in capturing the global topological characteristics of a\nnon-Hermitian system based on training data.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.09978v1.pdf",
        "similarity": 0.32261398832483856,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-15"
    },
    {
        "new_title": "Deep Image Restoration For Image Anti-Forensics",
        "new_link": "http://arxiv.org/abs/2405.02751v1",
        "new_summary": "  While image forensics is concerned with whether an image has been tampered\nwith, image anti-forensics attempts to prevent image forensics methods from\ndetecting tampered images. The competition between these two fields started\nlong before the advancement of deep learning. JPEG compression, blurring and\nnoising, which are simple methods by today's standards, have long been used for\nanti-forensics and have been the subject of much research in both forensics and\nanti-forensics. Although these traditional methods are old, they make it\ndifficult to detect fake images and are used for data augmentation in training\ndeep image forgery detection models. In addition to making the image difficult\nto detect, these methods leave traces on the image and consequently degrade the\nimage quality. Separate image forensics methods have also been developed to\ndetect these traces. In this study, we go one step further and improve the\nimage quality after these methods with deep image restoration models and make\nit harder to detect the forged image. We evaluate the impact of these methods\non image quality. We then test both our proposed methods with deep learning and\nmethods without deep learning on the two best existing image manipulation\ndetection models. In the obtained results, we show how existing image forgery\ndetection models fail against the proposed methods. Code implementation will be\npublicly available at https://github.com/99eren99/DIRFIAF .\n",
        "pdf_link": "https://arxiv.org/pdf/2405.02751v1.pdf",
        "similarity": 0.3223178616092889,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-04"
    },
    {
        "new_title": "QLingNet: An efficient and flexible modeling framework for subsonic\n  airfoils",
        "new_link": "http://arxiv.org/abs/2405.08248v1",
        "new_summary": "  Artificial intelligence techniques are considered an effective means to\naccelerate flow field simulations. However, current deep learning methods\nstruggle to achieve generalization to flow field resolutions while ensuring\ncomputational efficiency. This paper presents a deep learning approach for\nrapid prediction of two types of subsonic flow fields with different\nresolutions. Unlike convolutional neural networks, the constructed feature\nextractor integrates features of different spatial scales along the channel\ndimension, reducing the sensitivity of the deep learning model to resolution\nwhile improving computational efficiency. Additionally, to ensure consistency\nbetween the input and output resolutions of the deep learning model, a memory\npooling strategy is proposed, which ensures accurate reconstruction of flow\nfields at any resolution. By conducting extensive qualitative and quantitative\nanalyses on a given test dataset, it is demonstrated that the proposed deep\nlearning model can achieve a three-order-of-magnitude speedup compared to\nCPU-based solvers while adapting to flow fields of arbitrary resolutions.\nMoreover, the prediction accuracy for pressure exceeds 99\\%, laying the\nfoundation for the development of large-scale models in the field of\naerodynamics.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.08248v1.pdf",
        "similarity": 0.3222933473434794,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-14"
    },
    {
        "new_title": "Towards MLOps: A DevOps Tools Recommender System for Machine Learning\n  System",
        "new_link": "http://arxiv.org/abs/2402.12867v1",
        "new_summary": "  Applying DevOps practices to machine learning system is termed as MLOps and\nmachine learning systems evolve on new data unlike traditional systems on\nrequirements. The objective of MLOps is to establish a connection between\ndifferent open-source tools to construct a pipeline that can automatically\nperform steps to construct a dataset, train the machine learning model and\ndeploy the model to the production as well as store different versions of model\nand dataset. Benefits of MLOps is to make sure the fast delivery of the new\ntrained models to the production to have accurate results. Furthermore, MLOps\npractice impacts the overall quality of the software products and is completely\ndependent on open-source tools and selection of relevant open-source tools is\nconsidered as challenged while a generalized method to select an appropriate\nopen-source tools is desirable. In this paper, we present a framework for\nrecommendation system that processes the contextual information (e.g., nature\nof data, type of the data) of the machine learning project and recommends a\nrelevant toolchain (tech-stack) for the operationalization of machine learning\nsystems. To check the applicability of the proposed framework, four different\napproaches i.e., rule-based, random forest, decision trees and k-nearest\nneighbors were investigated where precision, recall and f-score is measured,\nthe random forest out classed other approaches with highest f-score value of\n0.66.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.12867v1.pdf",
        "similarity": 0.322232423535206,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-20"
    },
    {
        "new_title": "Computing low-thrust transfers in the asteroid belt, a comparison\n  between astrodynamical manipulations and a machine learning approach",
        "new_link": "http://arxiv.org/abs/2405.18918v1",
        "new_summary": "  Low-thrust trajectories play a crucial role in optimizing scientific output\nand cost efficiency in asteroid belt missions. Unlike high-thrust transfers,\nlow-thrust trajectories require solving complex optimal control problems. This\ncomplexity grows exponentially with the number of asteroids visited due to\norbital mechanics intricacies. In the literature, methods for approximating\nlow-thrust transfers without full optimization have been proposed, including\nanalytical and machine learning techniques. In this work, we propose new\nanalytical approximations and compare their accuracy and performance to machine\nlearning methods. While analytical approximations leverage orbit theory to\nestimate trajectory costs, machine learning employs a more black-box approach,\nutilizing neural networks to predict optimal transfers based on various\nattributes. We build a dataset of about 3 million transfers, found by solving\nthe time and fuel optimal control problems, for different time of flights,\nwhich we also release open-source. Comparison between the two methods on this\ndatabase reveals the superiority of machine learning, especially for longer\ntransfers. Despite challenges such as multi revolution transfers, both\napproaches maintain accuracy within a few percent in the final mass errors, on\na database of trajectories involving numerous asteroids. This work contributes\nto the efficient exploration of mission opportunities in the asteroid belt,\nproviding insights into the strengths and limitations of different\napproximation strategies.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.18918v1.pdf",
        "similarity": 0.32143804861687325,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-29"
    },
    {
        "new_title": "On Security Weaknesses and Vulnerabilities in Deep Learning Systems",
        "new_link": "http://arxiv.org/abs/2406.08688v1",
        "new_summary": "  The security guarantee of AI-enabled software systems (particularly using\ndeep learning techniques as a functional core) is pivotal against the\nadversarial attacks exploiting software vulnerabilities. However, little\nattention has been paid to a systematic investigation of vulnerabilities in\nsuch systems. A common situation learned from the open source software\ncommunity is that deep learning engineers frequently integrate off-the-shelf or\nopen-source learning frameworks into their ecosystems. In this work, we\nspecifically look into deep learning (DL) framework and perform the first\nsystematic study of vulnerabilities in DL systems through a comprehensive\nanalysis of identified vulnerabilities from Common Vulnerabilities and\nExposures (CVE) and open-source DL tools, including TensorFlow, Caffe, OpenCV,\nKeras, and PyTorch. We propose a two-stream data analysis framework to explore\nvulnerability patterns from various databases. We investigate the unique DL\nframeworks and libraries development ecosystems that appear to be decentralized\nand fragmented. By revisiting the Common Weakness Enumeration (CWE) List, which\nprovides the traditional software vulnerability related practices, we observed\nthat it is more challenging to detect and fix the vulnerabilities throughout\nthe DL systems lifecycle. Moreover, we conducted a large-scale empirical study\nof 3,049 DL vulnerabilities to better understand the patterns of vulnerability\nand the challenges in fixing them. We have released the full replication\npackage at https://github.com/codelzz/Vulnerabilities4DLSystem. We anticipate\nthat our study can advance the development of secure DL systems.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.08688v1.pdf",
        "similarity": 0.32131311977645133,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-12"
    },
    {
        "new_title": "Hessian-Free Laplace in Bayesian Deep Learning",
        "new_link": "http://arxiv.org/abs/2403.10671v1",
        "new_summary": "  The Laplace approximation (LA) of the Bayesian posterior is a Gaussian\ndistribution centered at the maximum a posteriori estimate. Its appeal in\nBayesian deep learning stems from the ability to quantify uncertainty post-hoc\n(i.e., after standard network parameter optimization), the ease of sampling\nfrom the approximate posterior, and the analytic form of model evidence.\nHowever, an important computational bottleneck of LA is the necessary step of\ncalculating and inverting the Hessian matrix of the log posterior. The Hessian\nmay be approximated in a variety of ways, with quality varying with a number of\nfactors including the network, dataset, and inference task. In this paper, we\npropose an alternative framework that sidesteps Hessian calculation and\ninversion. The Hessian-free Laplace (HFL) approximation uses curvature of both\nthe log posterior and network prediction to estimate its variance. Only two\npoint estimates are needed: the standard maximum a posteriori parameter and the\noptimal parameter under a loss regularized by the network prediction. We show\nthat, under standard assumptions of LA in Bayesian deep learning, HFL targets\nthe same variance as LA, and can be efficiently amortized in a pre-trained\nnetwork. Experiments demonstrate comparable performance to that of exact and\napproximate Hessians, with excellent coverage for in-between uncertainty.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.10671v1.pdf",
        "similarity": 0.3212821350731213,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-15"
    },
    {
        "new_title": "Predicting ATP binding sites in protein sequences using Deep Learning\n  and Natural Language Processing",
        "new_link": "http://arxiv.org/abs/2402.01829v1",
        "new_summary": "  Predicting ATP-Protein Binding sites in genes is of great significance in the\nfield of Biology and Medicine. The majority of research in this field has been\nconducted through time- and resource-intensive 'wet experiments' in\nlaboratories. Over the years, researchers have been investigating computational\nmethods computational methods to accomplish the same goals, utilising the\nstrength of advanced Deep Learning and NLP algorithms. In this paper, we\npropose to develop methods to classify ATP-Protein binding sites. We conducted\nvarious experiments mainly using PSSMs and several word embeddings as features.\nWe used 2D CNNs and LightGBM classifiers as our chief Deep Learning Algorithms.\nThe MP3Vec and BERT models have also been subjected to testing in our study.\nThe outcomes of our experiments demonstrated improvement over the\nstate-of-the-art benchmarks.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.01829v1.pdf",
        "similarity": 0.3208701364067956,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-02"
    },
    {
        "new_title": "Towards Enhancing the Reproducibility of Deep Learning Bugs: An\n  Empirical Study",
        "new_link": "http://arxiv.org/abs/2401.03069v2",
        "new_summary": "  Context: Deep learning has achieved remarkable progress in various domains.\nHowever, like any software system, deep learning systems contain bugs, some of\nwhich can have severe impacts, as evidenced by crashes involving autonomous\nvehicles. Despite substantial advancements in deep learning techniques, little\nresearch has focused on reproducing deep learning bugs, which is an essential\nstep for their resolution. Existing literature suggests that only 3% of deep\nlearning bugs are reproducible, underscoring the need for further research.\n  Objective: This paper examines the reproducibility of deep learning bugs. We\nidentify edit actions and useful information that could improve the\nreproducibility of deep learning bugs.\n  Method: First, we construct a dataset of 668 deep-learning bugs from Stack\nOverflow and GitHub across three frameworks and 22 architectures. Second, out\nof the 668 bugs, we select 165 bugs using stratified sampling and attempt to\ndetermine their reproducibility. While reproducing these bugs, we identify edit\nactions and useful information for their reproduction. Third, we used the\nApriori algorithm to identify useful information and edit actions required to\nreproduce specific types of bugs. Finally, we conducted a user study involving\n22 developers to assess the effectiveness of our findings in real-life\nsettings.\n  Results: We successfully reproduced 148 out of 165 bugs attempted. We\nidentified ten edit actions and five useful types of component information that\ncan help us reproduce the deep learning bugs. With the help of our findings,\nthe developers were able to reproduce 22.92% more bugs and reduce their\nreproduction time by 24.35%.\n  Conclusions: Our research addresses the critical issue of deep learning bug\nreproducibility. Practitioners and researchers can leverage our findings to\nimprove deep learning bug reproducibility.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.03069v2.pdf",
        "similarity": 0.32010520484504035,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-05"
    },
    {
        "new_title": "Inference via Interpolation: Contrastive Representations Provably Enable\n  Planning and Inference",
        "new_link": "http://arxiv.org/abs/2403.04082v2",
        "new_summary": "  Given time series data, how can we answer questions like \"what will happen in\nthe future?\" and \"how did we get here?\" These sorts of probabilistic inference\nquestions are challenging when observations are high-dimensional. In this\npaper, we show how these questions can have compact, closed form solutions in\nterms of learned representations. The key idea is to apply a variant of\ncontrastive learning to time series data. Prior work already shows that the\nrepresentations learned by contrastive learning encode a probability ratio. By\nextending prior work to show that the marginal distribution over\nrepresentations is Gaussian, we can then prove that joint distribution of\nrepresentations is also Gaussian. Taken together, these results show that\nrepresentations learned via temporal contrastive learning follow a Gauss-Markov\nchain, a graphical model where inference (e.g., prediction, planning) over\nrepresentations corresponds to inverting a low-dimensional matrix. In one\nspecial case, inferring intermediate representations will be equivalent to\ninterpolating between the learned representations. We validate our theory using\nnumerical simulations on tasks up to 46-dimensions.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.04082v2.pdf",
        "similarity": 0.3199378145940074,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-06"
    },
    {
        "new_title": "Enhancing Fairness and Performance in Machine Learning Models: A\n  Multi-Task Learning Approach with Monte-Carlo Dropout and Pareto Optimality",
        "new_link": "http://arxiv.org/abs/2404.08230v1",
        "new_summary": "  This paper considers the need for generalizable bias mitigation techniques in\nmachine learning due to the growing concerns of fairness and discrimination in\ndata-driven decision-making procedures across a range of industries. While many\nexisting methods for mitigating bias in machine learning have succeeded in\nspecific cases, they often lack generalizability and cannot be easily applied\nto different data types or models. Additionally, the trade-off between accuracy\nand fairness remains a fundamental tension in the field. To address these\nissues, we propose a bias mitigation method based on multi-task learning,\nutilizing the concept of Monte-Carlo dropout and Pareto optimality from\nmulti-objective optimization. This method optimizes accuracy and fairness while\nimproving the model's explainability without using sensitive information. We\ntest this method on three datasets from different domains and show how it can\ndeliver the most desired trade-off between model fairness and performance. This\nallows for tuning in specific domains where one metric may be more important\nthan another. With the framework we introduce in this paper, we aim to enhance\nthe fairness-performance trade-off and offer a solution to bias mitigation\nmethods' generalizability issues in machine learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.08230v1.pdf",
        "similarity": 0.3199369211585272,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-12"
    },
    {
        "new_title": "The CAST package for training and assessment of spatial prediction\n  models in R",
        "new_link": "http://arxiv.org/abs/2404.06978v1",
        "new_summary": "  One key task in environmental science is to map environmental variables\ncontinuously in space or even in space and time. Machine learning algorithms\nare frequently used to learn from local field observations to make spatial\npredictions by estimating the value of the variable of interest in places where\nit has not been measured. However, the application of machine learning\nstrategies for spatial mapping involves additional challenges compared to\n\"non-spatial\" prediction tasks that often originate from spatial\nautocorrelation and from training data that are not independent and identically\ndistributed.\n  In the past few years, we developed a number of methods to support the\napplication of machine learning for spatial data which involves the development\nof suitable cross-validation strategies for performance assessment and model\nselection, spatial feature selection, and methods to assess the area of\napplicability of the trained models. The intention of the CAST package is to\nsupport the application of machine learning strategies for predictive mapping\nby implementing such methods and making them available for easy integration\ninto modelling workflows.\n  Here we introduce the CAST package and its core functionalities. At the case\nstudy of mapping plant species richness, we will go through the different steps\nof the modelling workflow and show how CAST can be used to support more\nreliable spatial predictions.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.06978v1.pdf",
        "similarity": 0.3193800316124261,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-10"
    },
    {
        "new_title": "Research on geometric figure classification algorithm based on Deep\n  Learning",
        "new_link": "http://arxiv.org/abs/2404.16561v1",
        "new_summary": "  In recent years, with the rapid development of computer information\ntechnology, the development of artificial intelligence has been accelerating.\nThe traditional geometry recognition technology is relatively backward and the\nrecognition rate is low. In the face of massive information database, the\ntraditional algorithm model inevitably has the problems of low recognition\naccuracy and poor performance. Deep learning theory has gradually become a very\nimportant part of machine learning. The implementation of convolutional neural\nnetwork (CNN) reduces the difficulty of graphics generation algorithm. In this\npaper, using the advantages of lenet-5 architecture sharing weights and feature\nextraction and classification, the proposed geometric pattern recognition\nalgorithm model is faster in the training data set. By constructing the shared\nfeature parameters of the algorithm model, the cross-entropy loss function is\nused in the recognition process to improve the generalization of the model and\nimprove the average recognition accuracy of the test data set.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.16561v1.pdf",
        "similarity": 0.3192676813900431,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-25"
    },
    {
        "new_title": "Querying Easily Flip-flopped Samples for Deep Active Learning",
        "new_link": "http://arxiv.org/abs/2401.09787v2",
        "new_summary": "  Active learning is a machine learning paradigm that aims to improve the\nperformance of a model by strategically selecting and querying unlabeled data.\nOne effective selection strategy is to base it on the model's predictive\nuncertainty, which can be interpreted as a measure of how informative a sample\nis. The sample's distance to the decision boundary is a natural measure of\npredictive uncertainty, but it is often intractable to compute, especially for\ncomplex decision boundaries formed in multiclass classification tasks. To\naddress this issue, this paper proposes the {\\it least disagree metric} (LDM),\ndefined as the smallest probability of disagreement of the predicted label, and\nan estimator for LDM proven to be asymptotically consistent under mild\nassumptions. The estimator is computationally efficient and can be easily\nimplemented for deep learning models using parameter perturbation. The\nLDM-based active learning is performed by querying unlabeled data with the\nsmallest LDM. Experimental results show that our LDM-based active learning\nalgorithm obtains state-of-the-art overall performance on all considered\ndatasets and deep architectures.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.09787v2.pdf",
        "similarity": 0.31904622584340914,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-18"
    },
    {
        "new_title": "Advancing fNIRS Neuroimaging through Synthetic Data Generation and\n  Machine Learning Applications",
        "new_link": "http://arxiv.org/abs/2405.11242v1",
        "new_summary": "  This study presents an integrated approach for advancing functional\nNear-Infrared Spectroscopy (fNIRS) neuroimaging through the synthesis of data\nand application of machine learning models. By addressing the scarcity of\nhigh-quality neuroimaging datasets, this work harnesses Monte Carlo simulations\nand parametric head models to generate a comprehensive synthetic dataset,\nreflecting a wide spectrum of conditions. We developed a containerized\nenvironment employing Docker and Xarray for standardized and reproducible data\nanalysis, facilitating meaningful comparisons across different signal\nprocessing modalities. Additionally, a cloud-based infrastructure is\nestablished for scalable data generation and processing, enhancing the\naccessibility and quality of neuroimaging data. The combination of synthetic\ndata generation with machine learning techniques holds promise for improving\nthe accuracy, efficiency, and applicability of fNIRS tomography, potentially\nrevolutionizing diagnostics and treatment strategies for neurological\nconditions. The methodologies and infrastructure developed herein set new\nstandards in data simulation and analysis, paving the way for future research\nin neuroimaging and the broader biomedical engineering field.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.11242v1.pdf",
        "similarity": 0.31864675682636356,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-18"
    },
    {
        "new_title": "Interpretable Machine Learning Enhances Disease Prognosis: Applications\n  on COVID-19 and Onward",
        "new_link": "http://arxiv.org/abs/2405.11672v2",
        "new_summary": "  In response to the COVID-19 pandemic, the integration of interpretable\nmachine learning techniques has garnered significant attention, offering\ntransparent and understandable insights crucial for informed clinical decision\nmaking. This literature review delves into the applications of interpretable\nmachine learning in predicting the prognosis of respiratory diseases,\nparticularly focusing on COVID-19 and its implications for future research and\nclinical practice. We reviewed various machine learning models that are not\nonly capable of incorporating existing clinical domain knowledge but also have\nthe learning capability to explore new information from the data. These models\nand experiences not only aid in managing the current crisis but also hold\npromise for addressing future disease outbreaks. By harnessing interpretable\nmachine learning, healthcare systems can enhance their preparedness and\nresponse capabilities, thereby improving patient outcomes and mitigating the\nimpact of respiratory diseases in the years to come.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.11672v2.pdf",
        "similarity": 0.31856324791368534,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-19"
    },
    {
        "new_title": "Harnessing the Power of Beta Scoring in Deep Active Learning for\n  Multi-Label Text Classification",
        "new_link": "http://arxiv.org/abs/2401.07395v1",
        "new_summary": "  Within the scope of natural language processing, the domain of multi-label\ntext classification is uniquely challenging due to its expansive and uneven\nlabel distribution. The complexity deepens due to the demand for an extensive\nset of annotated data for training an advanced deep learning model, especially\nin specialized fields where the labeling task can be labor-intensive and often\nrequires domain-specific knowledge. Addressing these challenges, our study\nintroduces a novel deep active learning strategy, capitalizing on the Beta\nfamily of proper scoring rules within the Expected Loss Reduction framework. It\ncomputes the expected increase in scores using the Beta Scoring Rules, which\nare then transformed into sample vector representations. These vector\nrepresentations guide the diverse selection of informative samples, directly\nlinking this process to the model's expected proper score. Comprehensive\nevaluations across both synthetic and real datasets reveal our method's\ncapability to often outperform established acquisition techniques in\nmulti-label text classification, presenting encouraging outcomes across various\narchitectural and dataset scenarios.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.07395v1.pdf",
        "similarity": 0.31849799669484286,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-15"
    },
    {
        "new_title": "Impact of Architectural Modifications on Deep Learning Adversarial\n  Robustness",
        "new_link": "http://arxiv.org/abs/2405.01934v1",
        "new_summary": "  Rapid advancements of deep learning are accelerating adoption in a wide\nvariety of applications, including safety-critical applications such as\nself-driving vehicles, drones, robots, and surveillance systems. These\nadvancements include applying variations of sophisticated techniques that\nimprove the performance of models. However, such models are not immune to\nadversarial manipulations, which can cause the system to misbehave and remain\nunnoticed by experts. The frequency of modifications to existing deep learning\nmodels necessitates thorough analysis to determine the impact on models'\nrobustness. In this work, we present an experimental evaluation of the effects\nof model modifications on deep learning model robustness using adversarial\nattacks. Our methodology involves examining the robustness of variations of\nmodels against various adversarial attacks. By conducting our experiments, we\naim to shed light on the critical issue of maintaining the reliability and\nsafety of deep learning models in safety- and security-critical applications.\nOur results indicate the pressing demand for an in-depth assessment of the\neffects of model changes on the robustness of models.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.01934v1.pdf",
        "similarity": 0.31829028682724725,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-03"
    },
    {
        "new_title": "Revisiting the Performance of Deep Learning-Based Vulnerability\n  Detection on Realistic Datasets",
        "new_link": "http://arxiv.org/abs/2407.03093v1",
        "new_summary": "  The impact of software vulnerabilities on everyday software systems is\nsignificant. Despite deep learning models being proposed for vulnerability\ndetection, their reliability is questionable. Prior evaluations show high\nrecall/F1 scores of up to 99%, but these models underperform in practical\nscenarios, particularly when assessed on entire codebases rather than just the\nfixing commit. This paper introduces Real-Vul, a comprehensive dataset\nrepresenting real-world scenarios for evaluating vulnerability detection\nmodels. Evaluating DeepWukong, LineVul, ReVeal, and IVDetect shows a\nsignificant drop in performance, with precision decreasing by up to 95\npercentage points and F1 scores by up to 91 points. Furthermore, Model\nperformance fluctuates based on vulnerability characteristics, with better F1\nscores for information leaks or code injection than for path resolution or\npredictable return values. The results highlight a significant performance gap\nthat needs addressing before deploying deep learning-based vulnerability\ndetection in practical settings. Overfitting is identified as a key issue, and\nan augmentation technique is proposed, potentially improving performance by up\nto 30%. Contributions include a dataset creation approach for better model\nevaluation, Real-Vul dataset, and empirical evidence of deep learning models\nstruggling in real-world settings.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.03093v1.pdf",
        "similarity": 0.31808744681440965,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-03"
    },
    {
        "new_title": "Machine Learning for Soccer Match Result Prediction",
        "new_link": "http://arxiv.org/abs/2403.07669v1",
        "new_summary": "  Machine learning has become a common approach to predicting the outcomes of\nsoccer matches, and the body of literature in this domain has grown\nsubstantially in the past decade and a half. This chapter discusses available\ndatasets, the types of models and features, and ways of evaluating model\nperformance in this application domain. The aim of this chapter is to give a\nbroad overview of the current state and potential future developments in\nmachine learning for soccer match results prediction, as a resource for those\ninterested in conducting future studies in the area. Our main findings are that\nwhile gradient-boosted tree models such as CatBoost, applied to soccer-specific\nratings such as pi-ratings, are currently the best-performing models on\ndatasets containing only goals as the match features, there needs to be a more\nthorough comparison of the performance of deep learning models and Random\nForest on a range of datasets with different types of features. Furthermore,\nnew rating systems using both player- and team-level information and\nincorporating additional information from, e.g., spatiotemporal tracking and\nevent data, could be investigated further. Finally, the interpretability of\nmatch result prediction models needs to be enhanced for them to be more useful\nfor team management.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.07669v1.pdf",
        "similarity": 0.31800208895899335,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-12"
    },
    {
        "new_title": "A critical appraisal of water table depth estimation: Challenges and\n  opportunities within machine learning",
        "new_link": "http://arxiv.org/abs/2405.04579v2",
        "new_summary": "  Fine-resolution spatial patterns of water table depth (WTD) play a crucial\nrole in shaping ecological resilience, hydrological connectivity, and\nanthropocentric objectives. Generally, a large-scale (e.g., continental or\nglobal) spatial map of static WTD can be simulated using either\nphysically-based (PB) or machine learning-based (ML) models. We construct three\nfine-resolution (500 m) ML simulations of WTD, using the XGBoost algorithm and\nmore than 20 million real and proxy observations of WTD, across the United\nStates and Canada. The three ML models were constrained using known physical\nrelations between WTD's drivers and WTD and were trained by sequentially adding\nreal and proxy observations of WTD. We interpret the black box of our\nphysically constrained ML models and compare it against available literature in\ngroundwater hydrology. Through an extensive (pixel-by-pixel) evaluation, we\ndemonstrate that our models can more accurately predict unseen real and proxy\nobservations of WTD across most of North America's ecoregions compared to three\navailable PB simulations of WTD. However, we still argue that large-scale WTD\nestimation is far from being a solved problem. We reason that due to biased\nobservational data mainly collected from low-elevation floodplains, the\nmisspecification of equations within physically-based models, and the\nover-flexibility of machine learning models, verifiably accurate simulations of\nWTD do not yet exist. Ultimately, we thoroughly discuss future directions that\nmay help hydrogeologists decide how to proceed with WTD estimations, with a\nparticular focus on the application of machine learning and the use of proxy\nsatellite data.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.04579v2.pdf",
        "similarity": 0.31746413087116065,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-30"
    },
    {
        "new_title": "Geometric Neural Operators (GNPs) for Data-Driven Deep Learning of\n  Non-Euclidean Operators",
        "new_link": "http://arxiv.org/abs/2404.10843v1",
        "new_summary": "  We introduce Geometric Neural Operators (GNPs) for accounting for geometric\ncontributions in data-driven deep learning of operators. We show how GNPs can\nbe used (i) to estimate geometric properties, such as the metric and\ncurvatures, (ii) to approximate Partial Differential Equations (PDEs) on\nmanifolds, (iii) learn solution maps for Laplace-Beltrami (LB) operators, and\n(iv) to solve Bayesian inverse problems for identifying manifold shapes. The\nmethods allow for handling geometries of general shape including point-cloud\nrepresentations. The developed GNPs provide approaches for incorporating the\nroles of geometry in data-driven learning of operators.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.10843v1.pdf",
        "similarity": 0.3173535566869157,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-16"
    },
    {
        "new_title": "Federated Learning driven Large Language Models for Swarm Intelligence:\n  A Survey",
        "new_link": "http://arxiv.org/abs/2406.09831v1",
        "new_summary": "  Federated learning (FL) offers a compelling framework for training large\nlanguage models (LLMs) while addressing data privacy and decentralization\nchallenges. This paper surveys recent advancements in the federated learning of\nlarge language models, with a particular focus on machine unlearning, a crucial\naspect for complying with privacy regulations like the Right to be Forgotten.\nMachine unlearning in the context of federated LLMs involves systematically and\nsecurely removing individual data contributions from the learned model without\nretraining from scratch. We explore various strategies that enable effective\nunlearning, such as perturbation techniques, model decomposition, and\nincremental learning, highlighting their implications for maintaining model\nperformance and data privacy. Furthermore, we examine case studies and\nexperimental results from recent literature to assess the effectiveness and\nefficiency of these approaches in real-world scenarios. Our survey reveals a\ngrowing interest in developing more robust and scalable federated unlearning\nmethods, suggesting a vital area for future research in the intersection of AI\nethics and distributed machine learning technologies.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.09831v1.pdf",
        "similarity": 0.3172848105832172,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-14"
    },
    {
        "new_title": "A Survey on Deep Stereo Matching in the Twenties",
        "new_link": "http://arxiv.org/abs/2407.07816v1",
        "new_summary": "  Stereo matching is close to hitting a half-century of history, yet witnessed\na rapid evolution in the last decade thanks to deep learning. While previous\nsurveys in the late 2010s covered the first stage of this revolution, the last\nfive years of research brought further ground-breaking advancements to the\nfield. This paper aims to fill this gap in a two-fold manner: first, we offer\nan in-depth examination of the latest developments in deep stereo matching,\nfocusing on the pioneering architectural designs and groundbreaking paradigms\nthat have redefined the field in the 2020s; second, we present a thorough\nanalysis of the critical challenges that have emerged alongside these advances,\nproviding a comprehensive taxonomy of these issues and exploring the\nstate-of-the-art techniques proposed to address them. By reviewing both the\narchitectural innovations and the key challenges, we offer a holistic view of\ndeep stereo matching and highlight the specific areas that require further\ninvestigation. To accompany this survey, we maintain a regularly updated\nproject page that catalogs papers on deep stereo matching in our\nAwesome-Deep-Stereo-Matching\n(https://github.com/fabiotosi92/Awesome-Deep-Stereo-Matching) repository.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.07816v1.pdf",
        "similarity": 0.31724689161817476,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-10"
    },
    {
        "new_title": "Survey of Computerized Adaptive Testing: A Machine Learning Perspective",
        "new_link": "http://arxiv.org/abs/2404.00712v2",
        "new_summary": "  Computerized Adaptive Testing (CAT) provides an efficient and tailored method\nfor assessing the proficiency of examinees, by dynamically adjusting test\nquestions based on their performance. Widely adopted across diverse fields like\neducation, healthcare, sports, and sociology, CAT has revolutionized testing\npractices. While traditional methods rely on psychometrics and statistics, the\nincreasing complexity of large-scale testing has spurred the integration of\nmachine learning techniques. This paper aims to provide a machine\nlearning-focused survey on CAT, presenting a fresh perspective on this adaptive\ntesting method. By examining the test question selection algorithm at the heart\nof CAT's adaptivity, we shed light on its functionality. Furthermore, we delve\ninto cognitive diagnosis models, question bank construction, and test control\nwithin CAT, exploring how machine learning can optimize these components.\nThrough an analysis of current methods, strengths, limitations, and challenges,\nwe strive to develop robust, fair, and efficient CAT systems. By bridging\npsychometric-driven CAT research with machine learning, this survey advocates\nfor a more inclusive and interdisciplinary approach to the future of adaptive\ntesting.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.00712v2.pdf",
        "similarity": 0.3171502231979855,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-31"
    },
    {
        "new_title": "Meta-Learning Loss Functions for Deep Neural Networks",
        "new_link": "http://arxiv.org/abs/2406.09713v2",
        "new_summary": "  Humans can often quickly and efficiently solve complex new learning tasks\ngiven only a small set of examples. In contrast, modern artificially\nintelligent systems often require thousands or millions of observations in\norder to solve even the most basic tasks. Meta-learning aims to resolve this\nissue by leveraging past experiences from similar learning tasks to embed the\nappropriate inductive biases into the learning system. Historically methods for\nmeta-learning components such as optimizers, parameter initializations, and\nmore have led to significant performance increases. This thesis aims to explore\nthe concept of meta-learning to improve performance, through the\noften-overlooked component of the loss function. The loss function is a vital\ncomponent of a learning system, as it represents the primary learning\nobjective, where success is determined and quantified by the system's ability\nto optimize for that objective successfully.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.09713v2.pdf",
        "similarity": 0.3170173379789698,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-14"
    },
    {
        "new_title": "Your Instructions Are Not Always Helpful: Assessing the Efficacy of\n  Instruction Fine-tuning for Software Vulnerability Detection",
        "new_link": "http://arxiv.org/abs/2401.07466v1",
        "new_summary": "  Software, while beneficial, poses potential cybersecurity risks due to\ninherent vulnerabilities. Detecting these vulnerabilities is crucial, and deep\nlearning has shown promise as an effective tool for this task due to its\nability to perform well without extensive feature engineering. However, a\nchallenge in deploying deep learning for vulnerability detection is the limited\navailability of training data. Recent research highlights the deep learning\nefficacy in diverse tasks. This success is attributed to instruction\nfine-tuning, a technique that remains under-explored in the context of\nvulnerability detection. This paper investigates the capability of models,\nspecifically a recent language model, to generalize beyond the programming\nlanguages used in their training data. It also examines the role of natural\nlanguage instructions in enhancing this generalization. Our study evaluates the\nmodel performance on a real-world dataset to predict vulnerable code. We\npresent key insights and lessons learned, contributing to understanding the\ndeep learning application in software vulnerability detection.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.07466v1.pdf",
        "similarity": 0.31679818473764154,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-15"
    },
    {
        "new_title": "Deep Dict: Deep Learning-based Lossy Time Series Compressor for IoT Data",
        "new_link": "http://arxiv.org/abs/2401.10396v1",
        "new_summary": "  We propose Deep Dict, a deep learning-based lossy time series compressor\ndesigned to achieve a high compression ratio while maintaining decompression\nerror within a predefined range. Deep Dict incorporates two essential\ncomponents: the Bernoulli transformer autoencoder (BTAE) and a distortion\nconstraint. BTAE extracts Bernoulli representations from time series data,\nreducing the size of the representations compared to conventional autoencoders.\nThe distortion constraint limits the prediction error of BTAE to the desired\nrange. Moreover, in order to address the limitations of common regression\nlosses such as L1/L2, we introduce a novel loss function called quantized\nentropy loss (QEL). QEL takes into account the specific characteristics of the\nproblem, enhancing robustness to outliers and alleviating optimization\nchallenges. Our evaluation of Deep Dict across ten diverse time series datasets\nfrom various domains reveals that Deep Dict outperforms state-of-the-art lossy\ncompressors in terms of compression ratio by a significant margin by up to\n53.66%.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.10396v1.pdf",
        "similarity": 0.3165943847846619,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-18"
    },
    {
        "new_title": "Evaluating Large Language Models for Anxiety and Depression\n  Classification using Counseling and Psychotherapy Transcripts",
        "new_link": "http://arxiv.org/abs/2407.13228v1",
        "new_summary": "  We aim to evaluate the efficacy of traditional machine learning and large\nlanguage models (LLMs) in classifying anxiety and depression from long\nconversational transcripts. We fine-tune both established transformer models\n(BERT, RoBERTa, Longformer) and more recent large models (Mistral-7B), trained\na Support Vector Machine with feature engineering, and assessed GPT models\nthrough prompting. We observe that state-of-the-art models fail to enhance\nclassification outcomes compared to traditional machine learning methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.13228v1.pdf",
        "similarity": 0.31628642452827355,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-18"
    },
    {
        "new_title": "Deep Feature Gaussian Processes for Single-Scene Aerosol Optical Depth\n  Reconstruction",
        "new_link": "http://arxiv.org/abs/2405.17262v1",
        "new_summary": "  Remote sensing data provide a low-cost solution for large-scale monitoring of\nair pollution via the retrieval of aerosol optical depth (AOD), but is often\nlimited by cloud contamination. Existing methods for AOD reconstruction rely on\ntemporal information. However, for remote sensing data at high spatial\nresolution, multi-temporal observations are often unavailable. In this letter,\nwe take advantage of deep representation learning from convolutional neural\nnetworks and propose Deep Feature Gaussian Processes (DFGP) for single-scene\nAOD reconstruction. By using deep learning, we transform the variables to a\nfeature space with better explainable power. By using Gaussian processes, we\nexplicitly consider the correlation between observed AOD and missing AOD in\nspatial and feature domains. Experiments on two AOD datasets with real-world\ncloud patterns showed that the proposed method outperformed deep CNN and random\nforest, achieving R$^2$ of 0.7431 on MODIS AOD and R$^2$ of 0.9211 on EMIT AOD,\ncompared to deep CNN's R$^2$ of 0.6507 and R$^2$ of 0.8619. The proposed\nmethods increased R$^2$ by over 0.35 compared to the popular random forest in\nAOD reconstruction. The data and code used in this study are available at\n\\url{https://skrisliu.com/dfgp}.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.17262v1.pdf",
        "similarity": 0.3162650091304255,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-27"
    },
    {
        "new_title": "Byzantine-tolerant distributed learning of finite mixture models",
        "new_link": "http://arxiv.org/abs/2407.13980v1",
        "new_summary": "  This paper proposes two split-and-conquer (SC) learning estimators for finite\nmixture models that are tolerant to Byzantine failures. In SC learning,\nindividual machines obtain local estimates, which are then transmitted to a\ncentral server for aggregation. During this communication, the server may\nreceive malicious or incorrect information from some local machines, a scenario\nknown as Byzantine failures. While SC learning approaches have been devised to\nmitigate Byzantine failures in statistical models with Euclidean parameters,\ndeveloping Byzantine-tolerant methods for finite mixture models with\nnon-Euclidean parameters requires a distinct strategy. Our proposed\ndistance-based methods are hyperparameter tuning free, unlike existing methods,\nand are resilient to Byzantine failures while achieving high statistical\nefficiency. We validate the effectiveness of our methods both theoretically and\nempirically via experiments on simulated and real data from machine learning\napplications for digit recognition. The code for the experiment can be found at\nhttps://github.com/SarahQiong/RobustSCGMM.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.13980v1.pdf",
        "similarity": 0.3159761789261269,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-19"
    },
    {
        "new_title": "Persian Slang Text Conversion to Formal and Deep Learning of Persian\n  Short Texts on Social Media for Sentiment Classification",
        "new_link": "http://arxiv.org/abs/2403.06023v1",
        "new_summary": "  The lack of a suitable tool for the analysis of conversational texts in the\nPersian language has made various analyses of these texts, including Sentiment\nAnalysis, difficult. In this research, we tried to make the understanding of\nthese texts easier for the machine by providing PSC, Persian Slang Converter, a\ntool for converting conversational texts into formal ones, and by using the\nmost up-to-date and best deep learning methods along with the PSC, the\nsentiment learning of short Persian language texts for the machine in a better\nway. be made More than 10 million unlabeled texts from various social networks\nand movie subtitles (as Conversational texts) and about 10 million news texts\n(as formal texts) have been used for training unsupervised models and formal\nimplementation of the tool. 60,000 texts from the comments of Instagram social\nnetwork users with positive, negative, and neutral labels are considered\nsupervised data for training the emotion classification model of short texts.\nUsing the formal tool, 57% of the words of the corpus of conversation were\nconverted. Finally, by using the formalizer, FastText model, and deep LSTM\nnetwork, an accuracy of 81.91 was obtained on the test data.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.06023v1.pdf",
        "similarity": 0.31596441264111913,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-09"
    },
    {
        "new_title": "Learning on Multimodal Graphs: A Survey",
        "new_link": "http://arxiv.org/abs/2402.05322v1",
        "new_summary": "  Multimodal data pervades various domains, including healthcare, social media,\nand transportation, where multimodal graphs play a pivotal role. Machine\nlearning on multimodal graphs, referred to as multimodal graph learning (MGL),\nis essential for successful artificial intelligence (AI) applications. The\nburgeoning research in this field encompasses diverse graph data types and\nmodalities, learning techniques, and application scenarios. This survey paper\nconducts a comparative analysis of existing works in multimodal graph learning,\nelucidating how multimodal learning is achieved across different graph types\nand exploring the characteristics of prevalent learning techniques.\nAdditionally, we delineate significant applications of multimodal graph\nlearning and offer insights into future directions in this domain.\nConsequently, this paper serves as a foundational resource for researchers\nseeking to comprehend existing MGL techniques and their applicability across\ndiverse scenarios.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.05322v1.pdf",
        "similarity": 0.31590896205580193,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-07"
    },
    {
        "new_title": "On the Conflict of Robustness and Learning in Collaborative Machine\n  Learning",
        "new_link": "http://arxiv.org/abs/2402.13700v1",
        "new_summary": "  Collaborative Machine Learning (CML) allows participants to jointly train a\nmachine learning model while keeping their training data private. In scenarios\nwhere privacy is a strong requirement, such as health-related applications,\nsafety is also a primary concern. This means that privacy-preserving CML\nprocesses must produce models that output correct and reliable decisions\n\\emph{even in the presence of potentially untrusted participants}. In response\nto this issue, researchers propose to use \\textit{robust aggregators} that rely\non metrics which help filter out malicious contributions that could compromise\nthe training process. In this work, we formalize the landscape of robust\naggregators in the literature. Our formalization allows us to show that\nexisting robust aggregators cannot fulfill their goal: either they use\ndistance-based metrics that cannot accurately identify targeted malicious\nupdates; or propose methods whose success is in direct conflict with the\nability of CML participants to learn from others and therefore cannot eliminate\nthe risk of manipulation without preventing learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.13700v1.pdf",
        "similarity": 0.3157468222441519,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-21"
    },
    {
        "new_title": "Active Statistical Inference",
        "new_link": "http://arxiv.org/abs/2403.03208v2",
        "new_summary": "  Inspired by the concept of active learning, we propose active\ninference$\\unicode{x2013}$a methodology for statistical inference with\nmachine-learning-assisted data collection. Assuming a budget on the number of\nlabels that can be collected, the methodology uses a machine learning model to\nidentify which data points would be most beneficial to label, thus effectively\nutilizing the budget. It operates on a simple yet powerful intuition:\nprioritize the collection of labels for data points where the model exhibits\nuncertainty, and rely on the model's predictions where it is confident. Active\ninference constructs provably valid confidence intervals and hypothesis tests\nwhile leveraging any black-box machine learning model and handling any data\ndistribution. The key point is that it achieves the same level of accuracy with\nfar fewer samples than existing baselines relying on non-adaptively-collected\ndata. This means that for the same number of collected samples, active\ninference enables smaller confidence intervals and more powerful p-values. We\nevaluate active inference on datasets from public opinion research, census\nanalysis, and proteomics.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.03208v2.pdf",
        "similarity": 0.3156941167448972,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-05"
    },
    {
        "new_title": "A Protein Structure Prediction Approach Leveraging Transformer and CNN\n  Integration",
        "new_link": "http://arxiv.org/abs/2402.19095v2",
        "new_summary": "  Proteins are essential for life, and their structure determines their\nfunction. The protein secondary structure is formed by the folding of the\nprotein primary structure, and the protein tertiary structure is formed by the\nbending and folding of the secondary structure. Therefore, the study of protein\nsecondary structure is very helpful to the overall understanding of protein\nstructure. Although the accuracy of protein secondary structure prediction has\ncontinuously improved with the development of machine learning and deep\nlearning, progress in the field of protein structure prediction, unfortunately,\nremains insufficient to meet the large demand for protein information.\nTherefore, based on the advantages of deep learning-based methods in feature\nextraction and learning ability, this paper adopts a two-dimensional fusion\ndeep neural network model, DstruCCN, which uses Convolutional Neural Networks\n(CCN) and a supervised Transformer protein language model for single-sequence\nprotein structure prediction. The training features of the two are combined to\npredict the protein Transformer binding site matrix, and then the\nthree-dimensional structure is reconstructed using energy minimization.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.19095v2.pdf",
        "similarity": 0.3154724113900786,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-29"
    },
    {
        "new_title": "Improving Smart Contract Security with Contrastive Learning-based\n  Vulnerability Detection",
        "new_link": "http://arxiv.org/abs/2404.17839v1",
        "new_summary": "  Currently, smart contract vulnerabilities (SCVs) have emerged as a major\nfactor threatening the transaction security of blockchain. Existing\nstate-of-the-art methods rely on deep learning to mitigate this threat. They\ntreat each input contract as an independent entity and feed it into a deep\nlearning model to learn vulnerability patterns by fitting vulnerability labels.\nIt is a pity that they disregard the correlation between contracts, failing to\nconsider the commonalities between contracts of the same type and the\ndifferences among contracts of different types. As a result, the performance of\nthese methods falls short of the desired level.\n  To tackle this problem, we propose a novel Contrastive Learning Enhanced\nAutomated Recognition Approach for Smart Contract Vulnerabilities, named Clear.\nIn particular, Clear employs a contrastive learning (CL) model to capture the\nfine-grained correlation information among contracts and generates correlation\nlabels based on the relationships between contracts to guide the training\nprocess of the CL model. Finally, it combines the correlation and the semantic\ninformation of the contract to detect SCVs. Through an empirical evaluation of\na large-scale real-world dataset of over 40K smart contracts and compare 13\nstate-of-the-art baseline methods. We show that Clear achieves (1) optimal\nperformance over all baseline methods; (2) 9.73%-39.99% higher F1-score than\nexisting deep learning methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.17839v1.pdf",
        "similarity": 0.3154204152158836,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-27"
    },
    {
        "new_title": "Knowledge-Reuse Transfer Learning Methods in Molecular and Material\n  Science",
        "new_link": "http://arxiv.org/abs/2403.12982v1",
        "new_summary": "  Molecules and materials are the foundation for the development of modern\nadvanced industries such as energy storage systems and semiconductor devices.\nHowever, traditional trial-and-error methods or theoretical calculations are\nhighly resource-intensive, and extremely long R&D (Research and Development)\nperiods cannot meet the urgent need for molecules/materials in industrial\ndevelopment. Machine learning (ML) methods based on big data are expected to\nbreak this dilemma. However, the difficulty in constructing large-scale\ndatasets of new molecules/materials due to the high cost of data acquisition\nand annotation limits the development of machine learning. The application of\ntransfer learning lowers the data requirements for model training, which makes\ntransfer learning stand out in researches addressing data quality issues. In\nthis review, we summarize recent advances in transfer learning related to\nmolecular and materials science. We focus on the application of transfer\nlearning methods for the discovery of advanced molecules/materials,\nparticularly, the construction of transfer learning frameworks for different\nsystems, and how transfer learning can enhance the performance of models. In\naddition, the challenges of transfer learning are also discussed.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.12982v1.pdf",
        "similarity": 0.3151071367021181,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-02"
    },
    {
        "new_title": "DeepPhysiNet: Bridging Deep Learning and Atmospheric Physics for\n  Accurate and Continuous Weather Modeling",
        "new_link": "http://arxiv.org/abs/2401.04125v1",
        "new_summary": "  Accurate weather forecasting holds significant importance to human\nactivities. Currently, there are two paradigms for weather forecasting:\nNumerical Weather Prediction (NWP) and Deep Learning-based Prediction (DLP).\nNWP utilizes atmospheric physics for weather modeling but suffers from poor\ndata utilization and high computational costs, while DLP can learn weather\npatterns from vast amounts of data directly but struggles to incorporate\nphysical laws. Both paradigms possess their respective strengths and\nweaknesses, and are incompatible, because physical laws adopted in NWP describe\nthe relationship between coordinates and meteorological variables, while DLP\ndirectly learns the relationships between meteorological variables without\nconsideration of coordinates. To address these problems, we introduce the\nDeepPhysiNet framework, incorporating physical laws into deep learning models\nfor accurate and continuous weather system modeling. First, we construct\nphysics networks based on multilayer perceptrons (MLPs) for individual\nmeteorological variable, such as temperature, pressure, and wind speed. Physics\nnetworks establish relationships between variables and coordinates by taking\ncoordinates as input and producing variable values as output. The physical laws\nin the form of Partial Differential Equations (PDEs) can be incorporated as a\npart of loss function. Next, we construct hyper-networks based on deep learning\nmethods to directly learn weather patterns from a large amount of\nmeteorological data. The output of hyper-networks constitutes a part of the\nweights for the physics networks. Experimental results demonstrate that, upon\nsuccessful integration of physical laws, DeepPhysiNet can accomplish multiple\ntasks simultaneously, not only enhancing forecast accuracy but also obtaining\ncontinuous spatiotemporal resolution results, which is unattainable by either\nthe NWP or DLP.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.04125v1.pdf",
        "similarity": 0.31500492550315623,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-04"
    },
    {
        "new_title": "Exact representation and efficient approximations of linear model\n  predictive control laws via HardTanh type deep neural networks",
        "new_link": "http://arxiv.org/abs/2401.05076v1",
        "new_summary": "  Deep neural networks have revolutionized many fields, including image\nprocessing, inverse problems, text mining and more recently, give very\npromising results in systems and control. Neural networks with hidden layers\nhave a strong potential as an approximation framework of predictive control\nlaws as they usually yield better approximation quality and smaller memory\nrequirements than existing explicit (multi-parametric) approaches. In this\npaper, we first show that neural networks with HardTanh activation functions\ncan exactly represent predictive control laws of linear time-invariant systems.\nWe derive theoretical bounds on the minimum number of hidden layers and neurons\nthat a HardTanh neural network should have to exactly represent a given\npredictive control law. The choice of HardTanh deep neural networks is\nparticularly suited for linear predictive control laws as they usually require\nless hidden layers and neurons than deep neural networks with ReLU units for\nrepresenting exactly continuous piecewise affine (or equivalently min-max)\nmaps. In the second part of the paper we bring the physics of the model and\nstandard optimization techniques into the architecture design, in order to\neliminate the disadvantages of the black-box HardTanh learning. More\nspecifically, we design trainable unfolded HardTanh deep architectures for\nlearning linear predictive control laws based on two standard iterative\noptimization algorithms, i.e., projected gradient descent and accelerated\nprojected gradient descent. We also study the performance of the proposed\nHardTanh type deep neural networks on a linear model predictive control\napplication.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.05076v1.pdf",
        "similarity": 0.3149406990662876,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-10"
    },
    {
        "new_title": "Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential\n  Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2402.07107v3",
        "new_summary": "  We present a novel statistical approach to incorporating uncertainty\nawareness in model-free distributional reinforcement learning involving\nquantile regression-based deep Q networks. The proposed algorithm,\n$\\textit{Calibrated Evidential Quantile Regression in Deep Q Networks\n(CEQR-DQN)}$, aims to address key challenges associated with separately\nestimating aleatoric and epistemic uncertainty in stochastic environments. It\ncombines deep evidential learning with quantile calibration based on principles\nof conformal inference to provide explicit, sample-free computations of\n$\\textit{global}$ uncertainty as opposed to $\\textit{local}$ estimates based on\nsimple variance, overcoming limitations of traditional methods in computational\nand statistical efficiency and handling of out-of-distribution (OOD)\nobservations. Tested on a suite of miniaturized Atari games (i.e., MinAtar),\nCEQR-DQN is shown to surpass similar existing frameworks in scores and learning\nspeed. Its ability to rigorously evaluate uncertainty improves exploration\nstrategies and can serve as a blueprint for other algorithms requiring\nuncertainty awareness.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.07107v3.pdf",
        "similarity": 0.3144747405576825,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-11"
    },
    {
        "new_title": "Deeper Learning in Astronomy",
        "new_link": "http://arxiv.org/abs/2403.19937v1",
        "new_summary": "  It is well known that the best way to understand astronomical data is through\nmachine learning, where a \"black box\" is set up, inside which a kind of\nartificial intelligence learns how to interpret the features in the data. We\nsuggest that perhaps there may be some merit to a new approach in which humans\nare used instead of machines to understand the data. This may even apply to\nfields other than astronomy.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.19937v1.pdf",
        "similarity": 0.31440152736732596,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-29"
    },
    {
        "new_title": "In-context Exploration-Exploitation for Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2403.06826v1",
        "new_summary": "  In-context learning is a promising approach for online policy learning of\noffline reinforcement learning (RL) methods, which can be achieved at inference\ntime without gradient optimization. However, this method is hindered by\nsignificant computational costs resulting from the gathering of large training\ntrajectory sets and the need to train large Transformer models. We address this\nchallenge by introducing an In-context Exploration-Exploitation (ICEE)\nalgorithm, designed to optimize the efficiency of in-context policy learning.\nUnlike existing models, ICEE performs an exploration-exploitation trade-off at\ninference time within a Transformer model, without the need for explicit\nBayesian inference. Consequently, ICEE can solve Bayesian optimization problems\nas efficiently as Gaussian process biased methods do, but in significantly less\ntime. Through experiments in grid world environments, we demonstrate that ICEE\ncan learn to solve new RL tasks using only tens of episodes, marking a\nsubstantial improvement over the hundreds of episodes needed by the previous\nin-context learning method.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.06826v1.pdf",
        "similarity": 0.314378427232065,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-11"
    },
    {
        "new_title": "Case-Enhanced Vision Transformer: Improving Explanations of Image\n  Similarity with a ViT-based Similarity Metric",
        "new_link": "http://arxiv.org/abs/2407.16981v1",
        "new_summary": "  This short paper presents preliminary research on the Case-Enhanced Vision\nTransformer (CEViT), a similarity measurement method aimed at improving the\nexplainability of similarity assessments for image data. Initial experimental\nresults suggest that integrating CEViT into k-Nearest Neighbor (k-NN)\nclassification yields classification accuracy comparable to state-of-the-art\ncomputer vision models, while adding capabilities for illustrating differences\nbetween classes. CEViT explanations can be influenced by prior cases, to\nillustrate aspects of similarity relevant to those cases.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.16981v1.pdf",
        "similarity": 0.31424382168040843,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-24"
    },
    {
        "new_title": "t-DGR: A Trajectory-Based Deep Generative Replay Method for Continual\n  Learning in Decision Making",
        "new_link": "http://arxiv.org/abs/2401.02576v2",
        "new_summary": "  Deep generative replay has emerged as a promising approach for continual\nlearning in decision-making tasks. This approach addresses the problem of\ncatastrophic forgetting by leveraging the generation of trajectories from\npreviously encountered tasks to augment the current dataset. However, existing\ndeep generative replay methods for continual learning rely on autoregressive\nmodels, which suffer from compounding errors in the generated trajectories. In\nthis paper, we propose a simple, scalable, and non-autoregressive method for\ncontinual learning in decision-making tasks using a generative model that\ngenerates task samples conditioned on the trajectory timestep. We evaluate our\nmethod on Continual World benchmarks and find that our approach achieves\nstate-of-the-art performance on the average success rate metric among continual\nlearning methods. Code is available at https://github.com/WilliamYue37/t-DGR.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.02576v2.pdf",
        "similarity": 0.3142264808883123,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-04"
    },
    {
        "new_title": "AI, Meet Human: Learning Paradigms for Hybrid Decision Making Systems",
        "new_link": "http://arxiv.org/abs/2402.06287v1",
        "new_summary": "  Everyday we increasingly rely on machine learning models to automate and\nsupport high-stake tasks and decisions. This growing presence means that humans\nare now constantly interacting with machine learning-based systems, training\nand using models everyday. Several different techniques in computer science\nliterature account for the human interaction with machine learning systems, but\ntheir classification is sparse and the goals varied. This survey proposes a\ntaxonomy of Hybrid Decision Making Systems, providing both a conceptual and\ntechnical framework for understanding how current computer science literature\nmodels interaction between humans and machines.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.06287v1.pdf",
        "similarity": 0.314103116626676,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-09"
    },
    {
        "new_title": "A Comparison of Deep Learning Architectures for Spacecraft Anomaly\n  Detection",
        "new_link": "http://arxiv.org/abs/2403.12864v2",
        "new_summary": "  Spacecraft operations are highly critical, demanding impeccable reliability\nand safety. Ensuring the optimal performance of a spacecraft requires the early\ndetection and mitigation of anomalies, which could otherwise result in unit or\nmission failures. With the advent of deep learning, a surge of interest has\nbeen seen in leveraging these sophisticated algorithms for anomaly detection in\nspace operations. This study aims to compare the efficacy of various deep\nlearning architectures in detecting anomalies in spacecraft data. The deep\nlearning models under investigation include Convolutional Neural Networks\n(CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM)\nnetworks, and Transformer-based architectures. Each of these models was trained\nand validated using a comprehensive dataset sourced from multiple spacecraft\nmissions, encompassing diverse operational scenarios and anomaly types. Initial\nresults indicate that while CNNs excel in identifying spatial patterns and may\nbe effective for some classes of spacecraft data, LSTMs and RNNs show a marked\nproficiency in capturing temporal anomalies seen in time-series spacecraft\ntelemetry. The Transformer-based architectures, given their ability to focus on\nboth local and global contexts, have showcased promising results, especially in\nscenarios where anomalies are subtle and span over longer durations.\nAdditionally, considerations such as computational efficiency, ease of\ndeployment, and real-time processing capabilities were evaluated. While CNNs\nand LSTMs demonstrated a balance between accuracy and computational demands,\nTransformer architectures, though highly accurate, require significant\ncomputational resources. In conclusion, the choice of deep learning\narchitecture for spacecraft anomaly detection is highly contingent on the\nnature of the data, the type of anomalies, and operational constraints.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.12864v2.pdf",
        "similarity": 0.31386008595720727,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-19"
    },
    {
        "new_title": "DemOpts: Fairness corrections in COVID-19 case prediction models",
        "new_link": "http://arxiv.org/abs/2405.09483v2",
        "new_summary": "  COVID-19 forecasting models have been used to inform decision making around\nresource allocation and intervention decisions e.g., hospital beds or\nstay-at-home orders. State of the art deep learning models often use multimodal\ndata such as mobility or socio-demographic data to enhance COVID-19 case\nprediction models. Nevertheless, related work has revealed under-reporting bias\nin COVID-19 cases as well as sampling bias in mobility data for certain\nminority racial and ethnic groups, which could in turn affect the fairness of\nthe COVID-19 predictions along race labels. In this paper, we show that state\nof the art deep learning models output mean prediction errors that are\nsignificantly different across racial and ethnic groups; and which could, in\nturn, support unfair policy decisions. We also propose a novel de-biasing\nmethod, DemOpts, to increase the fairness of deep learning based forecasting\nmodels trained on potentially biased datasets. Our results show that DemOpts\ncan achieve better error parity that other state of the art de-biasing\napproaches, thus effectively reducing the differences in the mean error\ndistributions across more racial and ethnic groups.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.09483v2.pdf",
        "similarity": 0.3138015876983337,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-15"
    },
    {
        "new_title": "Optimization of geological carbon storage operations with multimodal\n  latent dynamic model and deep reinforcement learning",
        "new_link": "http://arxiv.org/abs/2406.04575v1",
        "new_summary": "  Maximizing storage performance in geological carbon storage (GCS) is crucial\nfor commercial deployment, but traditional optimization demands\nresource-intensive simulations, posing computational challenges. This study\nintroduces the multimodal latent dynamic (MLD) model, a deep learning framework\nfor fast flow prediction and well control optimization in GCS. The MLD model\nincludes a representation module for compressed latent representations, a\ntransition module for system state evolution, and a prediction module for flow\nresponses. A novel training strategy combining regression loss and\njoint-embedding consistency loss enhances temporal consistency and multi-step\nprediction accuracy. Unlike existing models, the MLD supports diverse input\nmodalities, allowing comprehensive data interactions. The MLD model, resembling\na Markov decision process (MDP), can train deep reinforcement learning agents,\nspecifically using the soft actor-critic (SAC) algorithm, to maximize net\npresent value (NPV) through continuous interactions. The approach outperforms\ntraditional methods, achieving the highest NPV while reducing computational\nresources by over 60%. It also demonstrates strong generalization performance,\nproviding improved decisions for new scenarios based on knowledge from previous\nones.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.04575v1.pdf",
        "similarity": 0.3137686910417022,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-07"
    },
    {
        "new_title": "Faster Machine Unlearning via Natural Gradient Descent",
        "new_link": "http://arxiv.org/abs/2407.08169v1",
        "new_summary": "  We address the challenge of efficiently and reliably deleting data from\nmachine learning models trained using Empirical Risk Minimization (ERM), a\nprocess known as machine unlearning. To avoid retraining models from scratch,\nwe propose a novel algorithm leveraging Natural Gradient Descent (NGD). Our\ntheoretical framework ensures strong privacy guarantees for convex models,\nwhile a practical Min/Max optimization algorithm is developed for non-convex\nmodels. Comprehensive evaluations show significant improvements in privacy,\ncomputational efficiency, and generalization compared to state-of-the-art\nmethods, advancing both the theoretical and practical aspects of machine\nunlearning.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.08169v1.pdf",
        "similarity": 0.31370080045389054,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-11"
    },
    {
        "new_title": "Constrained Meta Agnostic Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2406.14047v1",
        "new_summary": "  Meta-Reinforcement Learning (Meta-RL) aims to acquire meta-knowledge for\nquick adaptation to diverse tasks. However, applying these policies in\nreal-world environments presents a significant challenge in balancing rapid\nadaptability with adherence to environmental constraints. Our novel approach,\nConstraint Model Agnostic Meta Learning (C-MAML), merges meta learning with\nconstrained optimization to address this challenge. C-MAML enables rapid and\nefficient task adaptation by incorporating task-specific constraints directly\ninto its meta-algorithm framework during the training phase. This fusion\nresults in safer initial parameters for learning new tasks. We demonstrate the\neffectiveness of C-MAML in simulated locomotion with wheeled robot tasks of\nvarying complexity, highlighting its practicality and robustness in dynamic\nenvironments.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.14047v1.pdf",
        "similarity": 0.3132408805879853,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-20"
    },
    {
        "new_title": "ILILT: Implicit Learning of Inverse Lithography Technologies",
        "new_link": "http://arxiv.org/abs/2405.03574v1",
        "new_summary": "  Lithography, transferring chip design masks to the silicon wafer, is the most\nimportant phase in modern semiconductor manufacturing flow. Due to the\nlimitations of lithography systems, Extensive design optimizations are required\nto tackle the design and silicon mismatch. Inverse lithography technology (ILT)\nis one of the promising solutions to perform pre-fabrication optimization,\ntermed mask optimization. Because of mask optimization problems' constrained\nnon-convexity, numerical ILT solvers rely heavily on good initialization to\navoid getting stuck on sub-optimal solutions. Machine learning (ML) techniques\nare hence proposed to generate mask initialization for ILT solvers with\none-shot inference, targeting faster and better convergence during ILT. This\npaper addresses the question of \\textit{whether ML models can directly generate\nhigh-quality optimized masks without engaging ILT solvers in the loop}. We\npropose an implicit learning ILT framework: ILILT, which leverages the implicit\nlayer learning method and lithography-conditioned inputs to ground the model.\nTrained to understand the ILT optimization procedure, ILILT can outperform the\nstate-of-the-art machine learning solutions, significantly improving efficiency\nand quality.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.03574v1.pdf",
        "similarity": 0.3129714068546309,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-06"
    },
    {
        "new_title": "A Primer on Temporal Graph Learning",
        "new_link": "http://arxiv.org/abs/2401.03988v2",
        "new_summary": "  This document aims to familiarize readers with temporal graph learning (TGL)\nthrough a concept-first approach. We have systematically presented vital\nconcepts essential for understanding the workings of a TGL framework. In\naddition to qualitative explanations, we have incorporated mathematical\nformulations where applicable, enhancing the clarity of the text. Since TGL\ninvolves temporal and spatial learning, we introduce relevant learning\narchitectures ranging from recurrent and convolutional neural networks to\ntransformers and graph neural networks. We also discuss classical time series\nforecasting methods to inspire interpretable learning solutions for TGL.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.03988v2.pdf",
        "similarity": 0.31297041147300103,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-08"
    },
    {
        "new_title": "Neural Operators Meet Energy-based Theory: Operator Learning for\n  Hamiltonian and Dissipative PDEs",
        "new_link": "http://arxiv.org/abs/2402.09018v1",
        "new_summary": "  The operator learning has received significant attention in recent years,\nwith the aim of learning a mapping between function spaces. Prior works have\nproposed deep neural networks (DNNs) for learning such a mapping, enabling the\nlearning of solution operators of partial differential equations (PDEs).\nHowever, these works still struggle to learn dynamics that obeys the laws of\nphysics. This paper proposes Energy-consistent Neural Operators (ENOs), a\ngeneral framework for learning solution operators of PDEs that follows the\nenergy conservation or dissipation law from observed solution trajectories. We\nintroduce a novel penalty function inspired by the energy-based theory of\nphysics for training, in which the energy functional is modeled by another DNN,\nallowing one to bias the outputs of the DNN-based solution operators to ensure\nenergetic consistency without explicit PDEs. Experiments on multiple physical\nsystems show that ENO outperforms existing DNN models in predicting solutions\nfrom data, especially in super-resolution settings.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.09018v1.pdf",
        "similarity": 0.3129249701707042,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-14"
    },
    {
        "new_title": "On Characterizing and Mitigating Imbalances in Multi-Instance Partial\n  Label Learning",
        "new_link": "http://arxiv.org/abs/2407.10000v1",
        "new_summary": "  Multi-Instance Partial Label Learning (MI-PLL) is a weakly-supervised\nlearning setting encompassing partial label learning, latent structural\nlearning, and neurosymbolic learning. Differently from supervised learning, in\nMI-PLL, the inputs to the classifiers at training-time are tuples of instances\n$\\textbf{x}$, while the supervision signal is generated by a function $\\sigma$\nover the gold labels of $\\textbf{x}$. The gold labels are hidden during\ntraining. In this paper, we focus on characterizing and mitigating learning\nimbalances, i.e., differences in the errors occurring when classifying\ninstances of different classes (aka class-specific risks), under MI-PLL. The\nphenomenon of learning imbalances has been extensively studied in the context\nof long-tail learning; however, the nature of MI-PLL introduces new challenges.\nOur contributions are as follows. From a theoretical perspective, we\ncharacterize the learning imbalances by deriving class-specific risk bounds\nthat depend upon the function $\\sigma$. Our theory reveals that learning\nimbalances exist in MI-PLL even when the hidden labels are uniformly\ndistributed. On the practical side, we introduce a technique for estimating the\nmarginal of the hidden labels using only MI-PLL data. Then, we introduce\nalgorithms that mitigate imbalances at training- and testing-time, by treating\nthe marginal of the hidden labels as a constraint. The first algorithm relies\non a novel linear programming formulation of MI-PLL for pseudo-labeling. The\nsecond one adjusts a model's scores based on robust optimal transport. We\ndemonstrate the effectiveness of our techniques using strong neurosymbolic and\nlong-tail learning baselines, discussing also open challenges.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.10000v1.pdf",
        "similarity": 0.31287747008779077,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-13"
    },
    {
        "new_title": "Classifier-guided neural blind deconvolution: a physics-informed\n  denoising module for bearing fault diagnosis under heavy noise",
        "new_link": "http://arxiv.org/abs/2404.15341v1",
        "new_summary": "  Blind deconvolution (BD) has been demonstrated as an efficacious approach for\nextracting bearing fault-specific features from vibration signals under strong\nbackground noise. Despite BD's desirable feature in adaptability and\nmathematical interpretability, a significant challenge persists: How to\neffectively integrate BD with fault-diagnosing classifiers? This issue arises\nbecause the traditional BD method is solely designed for feature extraction\nwith its own optimizer and objective function. When BD is combined with\ndownstream deep learning classifiers, the different learning objectives will be\nin conflict. To address this problem, this paper introduces classifier-guided\nBD (ClassBD) for joint learning of BD-based feature extraction and deep\nlearning-based fault classification. Firstly, we present a time and frequency\nneural BD that employs neural networks to implement conventional BD, thereby\nfacilitating the seamless integration of BD and the deep learning classifier\nfor co-optimization of model parameters. Subsequently, we develop a unified\nframework to use a deep learning classifier to guide the learning of BD\nfilters. In addition, we devise a physics-informed loss function composed of\nkurtosis, $l_2/l_4$ norm, and a cross-entropy loss to jointly optimize the BD\nfilters and deep learning classifier. Consequently, the fault labels provide\nuseful information to direct BD to extract features that distinguish classes\namidst strong noise. To the best of our knowledge, this is the first of its\nkind that BD is successfully applied to bearing fault diagnosis. Experimental\nresults from three datasets demonstrate that ClassBD outperforms other\nstate-of-the-art methods under noisy conditions.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.15341v1.pdf",
        "similarity": 0.3127817222357095,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-11"
    },
    {
        "new_title": "Bayesian Deep Learning Via Expectation Maximization and Turbo Deep\n  Approximate Message Passing",
        "new_link": "http://arxiv.org/abs/2402.07366v2",
        "new_summary": "  Efficient learning and model compression algorithm for deep neural network\n(DNN) is a key workhorse behind the rise of deep learning (DL). In this work,\nwe propose a message passing based Bayesian deep learning algorithm called\nEM-TDAMP to avoid the drawbacks of traditional stochastic gradient descent\n(SGD) based learning algorithms and regularization-based model compression\nmethods. Specifically, we formulate the problem of DNN learning and compression\nas a sparse Bayesian inference problem, in which group sparse prior is employed\nto achieve structured model compression. Then, we propose an expectation\nmaximization (EM) framework to estimate posterior distributions for parameters\n(E-step) and update hyperparameters (M-step), where the E-step is realized by a\nnewly proposed turbo deep approximate message passing (TDAMP) algorithm. We\nfurther extend the EM-TDAMP and propose a novel Bayesian federated learning\nframework, in which and the clients perform TDAMP to efficiently calculate the\nlocal posterior distributions based on the local data, and the central server\nfirst aggregates the local posterior distributions to update the global\nposterior distributions and then update hyperparameters based on EM to\naccelerate convergence. We detail the application of EM-TDAMP to Boston housing\nprice prediction and handwriting recognition, and present extensive numerical\nresults to demonstrate the advantages of EM-TDAMP.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.07366v2.pdf",
        "similarity": 0.31265563314830064,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-12"
    },
    {
        "new_title": "A Deep Learning Approach Towards Student Performance Prediction in\n  Online Courses: Challenges Based on a Global Perspective",
        "new_link": "http://arxiv.org/abs/2402.01655v1",
        "new_summary": "  Analyzing and evaluating students' progress in any learning environment is\nstressful and time consuming if done using traditional analysis methods. This\nis further exasperated by the increasing number of students due to the shift of\nfocus toward integrating the Internet technologies in education and the focus\nof academic institutions on moving toward e-Learning, blended, or online\nlearning models. As a result, the topic of student performance prediction has\nbecome a vibrant research area in recent years. To address this, machine\nlearning and data mining techniques have emerged as a viable solution. To that\nend, this work proposes the use of deep learning techniques (CNN and RNN-LSTM)\nto predict the students' performance at the midpoint stage of the online course\ndelivery using three distinct datasets collected from three different regions\nof the world. Experimental results show that deep learning models have\npromising performance as they outperform other optimized traditional ML models\nin two of the three considered datasets while also having comparable\nperformance for the third dataset.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.01655v1.pdf",
        "similarity": 0.31255131853974355,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-10"
    },
    {
        "new_title": "Unveiling Hidden Energy Anomalies: Harnessing Deep Learning to Optimize\n  Energy Management in Sports Facilities",
        "new_link": "http://arxiv.org/abs/2402.08742v1",
        "new_summary": "  Anomaly detection in sport facilities has gained significant attention due to\nits potential to promote energy saving and optimizing operational efficiency.\nIn this research article, we investigate the role of machine learning,\nparticularly deep learning, in anomaly detection for sport facilities. We\nexplore the challenges and perspectives of utilizing deep learning methods for\nthis task, aiming to address the drawbacks and limitations of conventional\napproaches. Our proposed approach involves feature extraction from the data\ncollected in sport facilities. We present a problem formulation using Deep\nFeedforward Neural Networks (DFNN) and introduce threshold estimation\ntechniques to identify anomalies effectively. Furthermore, we propose methods\nto reduce false alarms, ensuring the reliability and accuracy of anomaly\ndetection. To evaluate the effectiveness of our approach, we conduct\nexperiments on aquatic center dataset at Qatar University. The results\ndemonstrate the superiority of our deep learning-based method over conventional\ntechniques, highlighting its potential in real-world applications. Typically,\n94.33% accuracy and 92.92% F1-score have been achieved using the proposed\nscheme.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.08742v1.pdf",
        "similarity": 0.31247102037371316,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-13"
    },
    {
        "new_title": "Boolean Logic as an Error feedback mechanism",
        "new_link": "http://arxiv.org/abs/2401.16418v1",
        "new_summary": "  The notion of Boolean logic backpropagation was introduced to build neural\nnetworks with weights and activations being Boolean numbers. Most of\ncomputations can be done with Boolean logic instead of real arithmetic, both\nduring training and inference phases. But the underlying discrete optimization\nproblem is NP-hard, and the Boolean logic has no guarantee. In this work we\npropose the first convergence analysis, under standard non-convex assumptions.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.16418v1.pdf",
        "similarity": 0.312320232525619,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-29"
    },
    {
        "new_title": "Weakly Augmented Variational Autoencoder in Time Series Anomaly\n  Detection",
        "new_link": "http://arxiv.org/abs/2401.03341v1",
        "new_summary": "  Due to their unsupervised training and uncertainty estimation, deep\nVariational Autoencoders (VAEs) have become powerful tools for\nreconstruction-based Time Series Anomaly Detection (TSAD). Existing VAE-based\nTSAD methods, either statistical or deep, tune meta-priors to estimate the\nlikelihood probability for effectively capturing spatiotemporal dependencies in\nthe data. However, these methods confront the challenge of inherent data\nscarcity, which is often the case in anomaly detection tasks. Such scarcity\neasily leads to latent holes, discontinuous regions in latent space, resulting\nin non-robust reconstructions on these discontinuous spaces. We propose a novel\ngenerative framework that combines VAEs with self-supervised learning (SSL) to\naddress this issue.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.03341v1.pdf",
        "similarity": 0.3120911680639765,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-07"
    },
    {
        "new_title": "Stacking-based deep neural network for player scouting in football 1",
        "new_link": "http://arxiv.org/abs/2403.08835v1",
        "new_summary": "  Datascouting is one of the most known data applications in professional\nsport, and specifically football. Its objective is to analyze huge database of\nplayers in order to detect high potentials that can be then individually\nconsidered by human scouts. In this paper, we propose a stacking-based deep\nlearning model to detect high potential football players. Applied on\nopen-source database, our model obtains significantly better results that\nclassical statistical methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.08835v1.pdf",
        "similarity": 0.31174642567361766,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-13"
    },
    {
        "new_title": "Open-world Machine Learning: A Review and New Outlooks",
        "new_link": "http://arxiv.org/abs/2403.01759v2",
        "new_summary": "  Machine learning has achieved remarkable success in many applications.\nHowever, existing studies are largely based on the closed-world assumption,\nwhich assumes that the environment is stationary, and the model is fixed once\ndeployed. In many real-world applications, this fundamental and rather naive\nassumption may not hold because an open environment is complex, dynamic, and\nfull of unknowns. In such cases, rejecting unknowns, discovering novelties, and\nthen incrementally learning them, could enable models to be safe and evolve\ncontinually as biological systems do. This paper provides a holistic view of\nopen-world machine learning by investigating unknown rejection, novel class\ndiscovery, and class-incremental learning in a unified paradigm. The\nchallenges, principles, and limitations of current methodologies are discussed\nin detail. Finally, we discuss several potential directions for future\nresearch. This paper aims to provide a comprehensive introduction to the\nemerging open-world machine learning paradigm, to help researchers build more\npowerful AI systems in their respective fields, and to promote the development\nof artificial general intelligence.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.01759v2.pdf",
        "similarity": 0.3116184475071736,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-04"
    },
    {
        "new_title": "Attribution Regularization for Multimodal Paradigms",
        "new_link": "http://arxiv.org/abs/2404.02359v1",
        "new_summary": "  Multimodal machine learning has gained significant attention in recent years\ndue to its potential for integrating information from multiple modalities to\nenhance learning and decision-making processes. However, it is commonly\nobserved that unimodal models outperform multimodal models, despite the latter\nhaving access to richer information. Additionally, the influence of a single\nmodality often dominates the decision-making process, resulting in suboptimal\nperformance. This research project aims to address these challenges by\nproposing a novel regularization term that encourages multimodal models to\neffectively utilize information from all modalities when making decisions. The\nfocus of this project lies in the video-audio domain, although the proposed\nregularization technique holds promise for broader applications in embodied AI\nresearch, where multiple modalities are involved. By leveraging this\nregularization term, the proposed approach aims to mitigate the issue of\nunimodal dominance and improve the performance of multimodal machine learning\nsystems. Through extensive experimentation and evaluation, the effectiveness\nand generalizability of the proposed technique will be assessed. The findings\nof this research project have the potential to significantly contribute to the\nadvancement of multimodal machine learning and facilitate its application in\nvarious domains, including multimedia analysis, human-computer interaction, and\nembodied AI research.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.02359v1.pdf",
        "similarity": 0.3115774829869428,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-02"
    },
    {
        "new_title": "Quantifying Distribution Shifts and Uncertainties for Enhanced Model\n  Robustness in Machine Learning Applications",
        "new_link": "http://arxiv.org/abs/2405.01978v1",
        "new_summary": "  Distribution shifts, where statistical properties differ between training and\ntest datasets, present a significant challenge in real-world machine learning\napplications where they directly impact model generalization and robustness. In\nthis study, we explore model adaptation and generalization by utilizing\nsynthetic data to systematically address distributional disparities. Our\ninvestigation aims to identify the prerequisites for successful model\nadaptation across diverse data distributions, while quantifying the associated\nuncertainties. Specifically, we generate synthetic data using the Van der Waals\nequation for gases and employ quantitative measures such as Kullback-Leibler\ndivergence, Jensen-Shannon distance, and Mahalanobis distance to assess data\nsimilarity. These metrics en able us to evaluate both model accuracy and\nquantify the associated uncertainty in predictions arising from data\ndistribution shifts. Our findings suggest that utilizing statistical measures,\nsuch as the Mahalanobis distance, to determine whether model predictions fall\nwithin the low-error \"interpolation regime\" or the high-error \"extrapolation\nregime\" provides a complementary method for assessing distribution shift and\nmodel uncertainty. These insights hold significant value for enhancing model\nrobustness and generalization, essential for the successful deployment of\nmachine learning applications in real-world scenarios.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.01978v1.pdf",
        "similarity": 0.31133479339000225,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-03"
    },
    {
        "new_title": "A Hierarchical Framework with Spatio-Temporal Consistency Learning for\n  Emergence Detection in Complex Adaptive Systems",
        "new_link": "http://arxiv.org/abs/2401.10300v1",
        "new_summary": "  Emergence, a global property of complex adaptive systems (CASs) constituted\nby interactive agents, is prevalent in real-world dynamic systems, e.g.,\nnetwork-level traffic congestions. Detecting its formation and evaporation\nhelps to monitor the state of a system, allowing to issue a warning signal for\nharmful emergent phenomena. Since there is no centralized controller of CAS,\ndetecting emergence based on each agent's local observation is desirable but\nchallenging. Existing works are unable to capture emergence-related spatial\npatterns, and fail to model the nonlinear relationships among agents. This\npaper proposes a hierarchical framework with spatio-temporal consistency\nlearning to solve these two problems by learning the system representation and\nagent representations, respectively. Especially, spatio-temporal encoders are\ntailored to capture agents' nonlinear relationships and the system's complex\nevolution. Representations of the agents and the system are learned by\npreserving the intrinsic spatio-temporal consistency in a self-supervised\nmanner. Our method achieves more accurate detection than traditional methods\nand deep learning methods on three datasets with well-known yet hard-to-detect\nemergent behaviors. Notably, our hierarchical framework is generic, which can\nemploy other deep learning methods for agent-level and system-level detection.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.10300v1.pdf",
        "similarity": 0.31131007265917704,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-18"
    },
    {
        "new_title": "3D-UGCN: A Unified Graph Convolutional Network for Robust 3D Human Pose\n  Estimation from Monocular RGB Images",
        "new_link": "http://arxiv.org/abs/2407.16137v1",
        "new_summary": "  Human pose estimation remains a multifaceted challenge in computer vision,\npivotal across diverse domains such as behavior recognition, human-computer\ninteraction, and pedestrian tracking. This paper proposes an improved method\nbased on the spatial-temporal graph convolution net-work (UGCN) to address the\nissue of missing human posture skeleton sequences in single-view videos. We\npresent the improved UGCN, which allows the network to process 3D human pose\ndata and improves the 3D human pose skeleton sequence, thereby resolving the\nocclusion issue.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.16137v1.pdf",
        "similarity": 0.31054975967815335,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-23"
    },
    {
        "new_title": "High Throughput Phenotyping of Physician Notes with Large Language and\n  Hybrid NLP Models",
        "new_link": "http://arxiv.org/abs/2403.05920v1",
        "new_summary": "  Deep phenotyping is the detailed description of patient signs and symptoms\nusing concepts from an ontology. The deep phenotyping of the numerous physician\nnotes in electronic health records requires high throughput methods. Over the\npast thirty years, progress toward making high throughput phenotyping feasible.\nIn this study, we demonstrate that a large language model and a hybrid NLP\nmodel (combining word vectors with a machine learning classifier) can perform\nhigh throughput phenotyping on physician notes with high accuracy. Large\nlanguage models will likely emerge as the preferred method for high throughput\ndeep phenotyping of physician notes.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.05920v1.pdf",
        "similarity": 0.31054066382745193,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-09"
    },
    {
        "new_title": "Hard ASH: Sparsity and the right optimizer make a continual learner",
        "new_link": "http://arxiv.org/abs/2404.17651v1",
        "new_summary": "  In class incremental learning, neural networks typically suffer from\ncatastrophic forgetting. We show that an MLP featuring a sparse activation\nfunction and an adaptive learning rate optimizer can compete with established\nregularization techniques in the Split-MNIST task. We highlight the\neffectiveness of the Adaptive SwisH (ASH) activation function in this context\nand introduce a novel variant, Hard Adaptive SwisH (Hard ASH) to further\nenhance the learning retention.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.17651v1.pdf",
        "similarity": 0.3102634766583422,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-26"
    },
    {
        "new_title": "On Stronger Computational Separations Between Multimodal and Unimodal\n  Machine Learning",
        "new_link": "http://arxiv.org/abs/2404.02254v2",
        "new_summary": "  Recently, multimodal machine learning has enjoyed huge empirical success\n(e.g. GPT-4). Motivated to develop theoretical justification for this empirical\nsuccess, Lu (NeurIPS '23, ALT '24) introduces a theory of multimodal learning,\nand considers possible \\textit{separations} between theoretical models of\nmultimodal and unimodal learning. In particular, Lu (ALT '24) shows a\ncomputational separation, which is relevant to \\textit{worst-case} instances of\nthe learning task. In this paper, we give a stronger \\textit{average-case}\ncomputational separation, where for ``typical'' instances of the learning task,\nunimodal learning is computationally hard, but multimodal learning is easy. We\nthen question how ``natural'' the average-case separation is. Would it be\nencountered in practice? To this end, we prove that under basic conditions, any\ngiven computational separation between average-case unimodal and multimodal\nlearning tasks implies a corresponding cryptographic key agreement protocol. We\nsuggest to interpret this as evidence that very strong \\textit{computational}\nadvantages of multimodal learning may arise \\textit{infrequently} in practice,\nsince they exist only for the ``pathological'' case of inherently cryptographic\ndistributions. However, this does not apply to possible (super-polynomial)\n\\textit{statistical} advantages.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.02254v2.pdf",
        "similarity": 0.30997216035248304,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-02"
    },
    {
        "new_title": "Predictive Analytics of Varieties of Potatoes",
        "new_link": "http://arxiv.org/abs/2404.03701v2",
        "new_summary": "  We explore the application of machine learning algorithms to predict the\nsuitability of Russet potato clones for advancement in breeding trials.\nLeveraging data from manually collected trials in the state of Oregon, we\ninvestigate the potential of a wide variety of state-of-the-art binary\nclassification models. We conduct a comprehensive analysis of the dataset that\nincludes preprocessing, feature engineering, and imputation to address missing\nvalues. We focus on several key metrics such as accuracy, F1-score, and\nMatthews correlation coefficient (MCC) for model evaluation. The top-performing\nmodels, namely the multi-layer perceptron classifier (MLPC), histogram-based\ngradient boosting classifier (HGBC), and a support vector machine classifier\n(SVC), demonstrate consistent and significant results. Variable selection\nfurther enhances model performance and identifies influential features in\npredicting trial outcomes. The findings emphasize the potential of machine\nlearning in streamlining the selection process for potato varieties, offering\nbenefits such as increased efficiency, substantial cost savings, and judicious\nresource utilization. Our study contributes insights into precision agriculture\nand showcases the relevance of advanced technologies for informed\ndecision-making in breeding programs.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.03701v2.pdf",
        "similarity": 0.3096621733438971,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-04"
    },
    {
        "new_title": "Automatic Domain Adaptation by Transformers in In-Context Learning",
        "new_link": "http://arxiv.org/abs/2405.16819v1",
        "new_summary": "  Selecting or designing an appropriate domain adaptation algorithm for a given\nproblem remains challenging. This paper presents a Transformer model that can\nprovably approximate and opt for domain adaptation methods for a given dataset\nin the in-context learning framework, where a foundation model performs new\ntasks without updating its parameters at test time. Specifically, we prove that\nTransformers can approximate instance-based and feature-based unsupervised\ndomain adaptation algorithms and automatically select an algorithm suited for a\ngiven dataset. Numerical results indicate that in-context learning demonstrates\nan adaptive domain adaptation surpassing existing methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.16819v1.pdf",
        "similarity": 0.30939553793918945,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-27"
    },
    {
        "new_title": "Federated Online Adaptation for Deep Stereo",
        "new_link": "http://arxiv.org/abs/2405.14873v1",
        "new_summary": "  We introduce a novel approach for adapting deep stereo networks in a\ncollaborative manner. By building over principles of federated learning, we\ndevelop a distributed framework allowing for demanding the optimization process\nto a number of clients deployed in different environments. This makes it\npossible, for a deep stereo network running on resourced-constrained devices,\nto capitalize on the adaptation process carried out by other instances of the\nsame architecture, and thus improve its accuracy in challenging environments\neven when it cannot carry out adaptation on its own. Experimental results show\nhow federated adaptation performs equivalently to on-device adaptation, and\neven better when dealing with challenging environments.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.14873v1.pdf",
        "similarity": 0.30910427443229804,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-23"
    },
    {
        "new_title": "Deep Learning Algorithms Used in Intrusion Detection Systems -- A Review",
        "new_link": "http://arxiv.org/abs/2402.17020v1",
        "new_summary": "  The increase in network attacks has necessitated the development of robust\nand efficient intrusion detection systems (IDS) capable of identifying\nmalicious activities in real-time. In the last five years, deep learning\nalgorithms have emerged as powerful tools in this domain, offering enhanced\ndetection capabilities compared to traditional methods. This review paper\nstudies recent advancements in the application of deep learning techniques,\nincluding Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN),\nDeep Belief Networks (DBN), Deep Neural Networks (DNN), Long Short-Term Memory\n(LSTM), autoencoders (AE), Multi-Layer Perceptrons (MLP), Self-Normalizing\nNetworks (SNN) and hybrid models, within network intrusion detection systems.\nwe delve into the unique architectures, training models, and classification\nmethodologies tailored for network traffic analysis and anomaly detection.\nFurthermore, we analyze the strengths and limitations of each deep learning\napproach in terms of detection accuracy, computational efficiency, scalability,\nand adaptability to evolving threats. Additionally, this paper highlights\nprominent datasets and benchmarking frameworks commonly utilized for evaluating\nthe performance of deep learning-based IDS. This review will provide\nresearchers and industry practitioners with valuable insights into the\nstate-of-the-art deep learning algorithms for enhancing the security framework\nof network environments through intrusion detection.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.17020v1.pdf",
        "similarity": 0.30899500922881495,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-26"
    },
    {
        "new_title": "A First Step in Using Machine Learning Methods to Enhance Interaction\n  Analysis for Embodied Learning Environments",
        "new_link": "http://arxiv.org/abs/2405.06203v1",
        "new_summary": "  Investigating children's embodied learning in mixed-reality environments,\nwhere they collaboratively simulate scientific processes, requires analyzing\ncomplex multimodal data to interpret their learning and coordination behaviors.\nLearning scientists have developed Interaction Analysis (IA) methodologies for\nanalyzing such data, but this requires researchers to watch hours of videos to\nextract and interpret students' learning patterns. Our study aims to simplify\nresearchers' tasks, using Machine Learning and Multimodal Learning Analytics to\nsupport the IA processes. Our study combines machine learning algorithms and\nmultimodal analyses to support and streamline researcher efforts in developing\na comprehensive understanding of students' scientific engagement through their\nmovements, gaze, and affective responses in a simulated scenario. To facilitate\nan effective researcher-AI partnership, we present an initial case study to\ndetermine the feasibility of visually representing students' states, actions,\ngaze, affect, and movement on a timeline. Our case study focuses on a specific\nscience scenario where students learn about photosynthesis. The timeline allows\nus to investigate the alignment of critical learning moments identified by\nmultimodal and interaction analysis, and uncover insights into students'\ntemporal learning progressions.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.06203v1.pdf",
        "similarity": 0.30793015359101966,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-10"
    },
    {
        "new_title": "Artificial intelligence and machine learning applications for cultured\n  meat",
        "new_link": "http://arxiv.org/abs/2407.09982v1",
        "new_summary": "  Cultured meat has the potential to provide a complementary meat industry with\nreduced environmental, ethical, and health impacts. However, major\ntechnological challenges remain which require time- and resource-intensive\nresearch and development efforts. Machine learning has the potential to\naccelerate cultured meat technology by streamlining experiments, predicting\noptimal results, and reducing experimentation time and resources. However, the\nuse of machine learning in cultured meat is in its infancy. This review covers\nthe work available to date on the use of machine learning in cultured meat and\nexplores future possibilities. We address four major areas of cultured meat\nresearch and development: establishing cell lines, cell culture media design,\nmicroscopy and image analysis, and bioprocessing and food processing\noptimization. This review aims to provide the foundation necessary for both\ncultured meat and machine learning scientists to identify research\nopportunities at the intersection between cultured meat and machine learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.09982v1.pdf",
        "similarity": 0.30792593819094405,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-30"
    },
    {
        "new_title": "Applications of interpretable deep learning in neuroimaging: a\n  comprehensive review",
        "new_link": "http://arxiv.org/abs/2406.17792v1",
        "new_summary": "  Clinical adoption of deep learning models has been hindered, in part, because\nthe black-box nature of neural networks leads to concerns regarding their\ntrustworthiness and reliability. These concerns are particularly relevant in\nthe field of neuroimaging due to the complex brain phenotypes and inter-subject\nheterogeneity often encountered. The challenge can be addressed by\ninterpretable deep learning (iDL) methods that enable the visualisation and\ninterpretation of the inner workings of deep learning models. This study\nsystematically reviewed the literature on neuroimaging applications of iDL\nmethods and critically analysed how iDL explanation properties were evaluated.\nSeventy-five studies were included, and ten categories of iDL methods were\nidentified. We also reviewed five properties of iDL explanations that were\nanalysed in the included studies: biological validity, robustness, continuity,\nselectivity, and downstream task performance. We found that the most popular\niDL approaches used in the literature may be sub-optimal for neuroimaging data,\nand we discussed possible future directions for the field.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.17792v1.pdf",
        "similarity": 0.3078059443622859,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-30"
    },
    {
        "new_title": "Comparison of Machine Learning Classification Algorithms and Application\n  to the Framingham Heart Study",
        "new_link": "http://arxiv.org/abs/2402.15005v1",
        "new_summary": "  The use of machine learning algorithms in healthcare can amplify social\ninjustices and health inequities. While the exacerbation of biases can occur\nand compound during the problem selection, data collection, and outcome\ndefinition, this research pertains to some generalizability impediments that\noccur during the development and the post-deployment of machine learning\nclassification algorithms. Using the Framingham coronary heart disease data as\na case study, we show how to effectively select a probability cutoff to convert\na regression model for a dichotomous variable into a classifier. We then\ncompare the sampling distribution of the predictive performance of eight\nmachine learning classification algorithms under four training/testing\nscenarios to test their generalizability and their potential to perpetuate\nbiases. We show that both the Extreme Gradient Boosting, and Support Vector\nMachine are flawed when trained on an unbalanced dataset. We introduced and\nshow that the double discriminant scoring of type I is the most generalizable\nas it consistently outperforms the other classification algorithms regardless\nof the training/testing scenario. Finally, we introduce a methodology to\nextract an optimal variable hierarchy for a classification algorithm, and\nillustrate it on the overall, male and female Framingham coronary heart disease\ndata.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.15005v1.pdf",
        "similarity": 0.3075976381335093,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-22"
    },
    {
        "new_title": "Indian Stock Market Prediction using Augmented Financial Intelligence ML",
        "new_link": "http://arxiv.org/abs/2407.02236v1",
        "new_summary": "  This paper presents price prediction models using Machine Learning algorithms\naugmented with Superforecasters predictions, aimed at enhancing investment\ndecisions. Five Machine Learning models are built, including Bidirectional\nLSTM, ARIMA, a combination of CNN and LSTM, GRU, and a model built using LSTM\nand GRU algorithms. The models are evaluated using the Mean Absolute Error to\ndetermine their predictive accuracy. Additionally, the paper suggests\nincorporating human intelligence by identifying Superforecasters and tracking\ntheir predictions to anticipate unpredictable shifts or changes in stock prices\n. The predictions made by these users can further enhance the accuracy of stock\nprice predictions when combined with Machine Learning and Natural Language\nProcessing techniques. Predicting the price of any commodity can be a\nsignificant task but predicting the price of a stock in the stock market deals\nwith much more uncertainty. Recognising the limited knowledge and exposure to\nstocks among certain investors, this paper proposes price prediction models\nusing Machine Learning algorithms. In this work, five Machine learning models\nare built using Bidirectional LSTM, ARIMA, a combination of CNN and LSTM, GRU\nand the last one is built using LSTM and GRU algorithms. Later these models are\nassessed using MAE scores to find which model is predicting with the highest\naccuracy. In addition to this, this paper also suggests the use of human\nintelligence to closely predict the shift in price patterns in the stock market\nThe main goal is to identify Superforecasters and track their predictions to\nanticipate unpredictable shifts or changes in stock prices. By leveraging the\ncombined power of Machine Learning and the Human Intelligence, predictive\naccuracy can be significantly increased.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.02236v1.pdf",
        "similarity": 0.3074324316781043,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-02"
    },
    {
        "new_title": "Mitigating Covariate Shift in Misspecified Regression with Applications\n  to Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2401.12216v1",
        "new_summary": "  A pervasive phenomenon in machine learning applications is distribution\nshift, where training and deployment conditions for a machine learning model\ndiffer. As distribution shift typically results in a degradation in\nperformance, much attention has been devoted to algorithmic interventions that\nmitigate these detrimental effects. In this paper, we study the effect of\ndistribution shift in the presence of model misspecification, specifically\nfocusing on $L_{\\infty}$-misspecified regression and adversarial covariate\nshift, where the regression target remains fixed while the covariate\ndistribution changes arbitrarily. We show that empirical risk minimization, or\nstandard least squares regression, can result in undesirable misspecification\namplification where the error due to misspecification is amplified by the\ndensity ratio between the training and testing distributions. As our main\nresult, we develop a new algorithm -- inspired by robust optimization\ntechniques -- that avoids this undesirable behavior, resulting in no\nmisspecification amplification while still obtaining optimal statistical rates.\nAs applications, we use this regression procedure to obtain new guarantees in\noffline and online reinforcement learning with misspecification and establish\nnew separations between previously studied structural conditions and notions of\ncoverage.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.12216v1.pdf",
        "similarity": 0.3070572620213245,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-22"
    },
    {
        "new_title": "IFTD: Image Feature Triangle Descriptor for Loop Detection in Driving\n  Scenes",
        "new_link": "http://arxiv.org/abs/2406.07937v1",
        "new_summary": "  In this work, we propose a fast and robust Image Feature Triangle Descriptor\n(IFTD) based on the STD method, aimed at improving the efficiency and accuracy\nof place recognition in driving scenarios. We extract keypoints from BEV\nprojection image of point cloud and construct these keypoints into triangle\ndescriptors. By matching these feature triangles, we achieved precise place\nrecognition and calculated the 4-DOF pose estimation between two keyframes.\nFurthermore, we employ image similarity inspection to perform the final place\nrecognition. Experimental results on three public datasets demonstrate that our\nIFTD can achieve greater robustness and accuracy than state-of-the-art methods\nwith low computational overhead.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.07937v1.pdf",
        "similarity": 0.30695492113858475,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-12"
    },
    {
        "new_title": "KID-PPG: Knowledge Informed Deep Learning for Extracting Heart Rate from\n  a Smartwatch",
        "new_link": "http://arxiv.org/abs/2405.09559v1",
        "new_summary": "  Accurate extraction of heart rate from photoplethysmography (PPG) signals\nremains challenging due to motion artifacts and signal degradation. Although\ndeep learning methods trained as a data-driven inference problem offer\npromising solutions, they often underutilize existing knowledge from the\nmedical and signal processing community. In this paper, we address three\nshortcomings of deep learning models: motion artifact removal, degradation\nassessment, and physiologically plausible analysis of the PPG signal. We\npropose KID-PPG, a knowledge-informed deep learning model that integrates\nexpert knowledge through adaptive linear filtering, deep probabilistic\ninference, and data augmentation. We evaluate KID-PPG on the PPGDalia dataset,\nachieving an average mean absolute error of 2.85 beats per minute, surpassing\nexisting reproducible methods. Our results demonstrate a significant\nperformance improvement in heart rate tracking through the incorporation of\nprior knowledge into deep learning models. This approach shows promise in\nenhancing various biomedical applications by incorporating existing expert\nknowledge in deep learning models.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.09559v1.pdf",
        "similarity": 0.30650934977931393,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-02"
    },
    {
        "new_title": "Regularized Gradient Clipping Provably Trains Wide and Deep Neural\n  Networks",
        "new_link": "http://arxiv.org/abs/2404.08624v1",
        "new_summary": "  In this work, we instantiate a regularized form of the gradient clipping\nalgorithm and prove that it can converge to the global minima of deep neural\nnetwork loss functions provided that the net is of sufficient width. We present\nempirical evidence that our theoretically founded regularized gradient clipping\nalgorithm is also competitive with the state-of-the-art deep-learning\nheuristics. Hence the algorithm presented here constitutes a new approach to\nrigorous deep learning.\n  The modification we do to standard gradient clipping is designed to leverage\nthe PL* condition, a variant of the Polyak-Lojasiewicz inequality which was\nrecently proven to be true for various neural networks for any depth within a\nneighborhood of the initialisation.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.08624v1.pdf",
        "similarity": 0.3064855324383995,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-12"
    },
    {
        "new_title": "Large Language Models as Agents in Two-Player Games",
        "new_link": "http://arxiv.org/abs/2402.08078v1",
        "new_summary": "  By formally defining the training processes of large language models (LLMs),\nwhich usually encompasses pre-training, supervised fine-tuning, and\nreinforcement learning with human feedback, within a single and unified machine\nlearning paradigm, we can glean pivotal insights for advancing LLM\ntechnologies. This position paper delineates the parallels between the training\nmethods of LLMs and the strategies employed for the development of agents in\ntwo-player games, as studied in game theory, reinforcement learning, and\nmulti-agent systems. We propose a re-conceptualization of LLM learning\nprocesses in terms of agent learning in language-based games. This framework\nunveils innovative perspectives on the successes and challenges in LLM\ndevelopment, offering a fresh understanding of addressing alignment issues\namong other strategic considerations. Furthermore, our two-player game approach\nsheds light on novel data preparation and machine learning techniques for\ntraining LLMs.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.08078v1.pdf",
        "similarity": 0.306391227600682,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-12"
    },
    {
        "new_title": "Dyna-LfLH: Learning Agile Navigation in Dynamic Environments from\n  Learned Hallucination",
        "new_link": "http://arxiv.org/abs/2403.17231v1",
        "new_summary": "  This paper presents a self-supervised learning method to safely learn a\nmotion planner for ground robots to navigate environments with dense and\ndynamic obstacles. When facing highly-cluttered, fast-moving, hard-to-predict\nobstacles, classical motion planners may not be able to keep up with limited\nonboard computation. For learning-based planners, high-quality demonstrations\nare difficult to acquire for imitation learning while reinforcement learning\nbecomes inefficient due to the high probability of collision during\nexploration. To safely and efficiently provide training data, the Learning from\nHallucination (LfH) approaches synthesize difficult navigation environments\nbased on past successful navigation experiences in relatively easy or\ncompletely open ones, but unfortunately cannot address dynamic obstacles. In\nour new Dynamic Learning from Learned Hallucination (Dyna-LfLH), we design and\nlearn a novel latent distribution and sample dynamic obstacles from it, so the\ngenerated training data can be used to learn a motion planner to navigate in\ndynamic environments. Dyna-LfLH is evaluated on a ground robot in both\nsimulated and physical environments and achieves up to 25% better success rate\ncompared to baselines.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.17231v1.pdf",
        "similarity": 0.3060443753938802,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-25"
    },
    {
        "new_title": "HETAL: Efficient Privacy-preserving Transfer Learning with Homomorphic\n  Encryption",
        "new_link": "http://arxiv.org/abs/2403.14111v1",
        "new_summary": "  Transfer learning is a de facto standard method for efficiently training\nmachine learning models for data-scarce problems by adding and fine-tuning new\nclassification layers to a model pre-trained on large datasets. Although\nnumerous previous studies proposed to use homomorphic encryption to resolve the\ndata privacy issue in transfer learning in the machine learning as a service\nsetting, most of them only focused on encrypted inference. In this study, we\npresent HETAL, an efficient Homomorphic Encryption based Transfer Learning\nalgorithm, that protects the client's privacy in training tasks by encrypting\nthe client data using the CKKS homomorphic encryption scheme. HETAL is the\nfirst practical scheme that strictly provides encrypted training, adopting\nvalidation-based early stopping and achieving the accuracy of nonencrypted\ntraining. We propose an efficient encrypted matrix multiplication algorithm,\nwhich is 1.8 to 323 times faster than prior methods, and a highly precise\nsoftmax approximation algorithm with increased coverage. The experimental\nresults for five well-known benchmark datasets show total training times of\n567-3442 seconds, which is less than an hour.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.14111v1.pdf",
        "similarity": 0.3053685094570891,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-21"
    },
    {
        "new_title": "PETScML: Second-order solvers for training regression problems in\n  Scientific Machine Learning",
        "new_link": "http://arxiv.org/abs/2403.12188v1",
        "new_summary": "  In recent years, we have witnessed the emergence of scientific machine\nlearning as a data-driven tool for the analysis, by means of deep-learning\ntechniques, of data produced by computational science and engineering\napplications. At the core of these methods is the supervised training algorithm\nto learn the neural network realization, a highly non-convex optimization\nproblem that is usually solved using stochastic gradient methods. However,\ndistinct from deep-learning practice, scientific machine-learning training\nproblems feature a much larger volume of smooth data and better\ncharacterizations of the empirical risk functions, which make them suited for\nconventional solvers for unconstrained optimization. We introduce a lightweight\nsoftware framework built on top of the Portable and Extensible Toolkit for\nScientific computation to bridge the gap between deep-learning software and\nconventional solvers for unconstrained minimization. We empirically demonstrate\nthe superior efficacy of a trust region method based on the Gauss-Newton\napproximation of the Hessian in improving the generalization errors arising\nfrom regression tasks when learning surrogate models for a wide range of\nscientific machine-learning techniques and test cases. All the conventional\nsecond-order solvers tested, including L-BFGS and inexact Newton with\nline-search, compare favorably, either in terms of cost or accuracy, with the\nadaptive first-order methods used to validate the surrogate models.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.12188v1.pdf",
        "similarity": 0.30525559152976395,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-18"
    },
    {
        "new_title": "SigDLA: A Deep Learning Accelerator Extension for Signal Processing",
        "new_link": "http://arxiv.org/abs/2407.12565v1",
        "new_summary": "  Deep learning and signal processing are closely correlated in many IoT\nscenarios such as anomaly detection to empower intelligence of things. Many IoT\nprocessors utilize digital signal processors (DSPs) for signal processing and\nbuild deep learning frameworks on this basis. While deep learning is usually\nmuch more computing-intensive than signal processing, the computing efficiency\nof deep learning on DSPs is limited due to the lack of native hardware support.\nIn this case, we present a contrary strategy and propose to enable signal\nprocessing on top of a classical deep learning accelerator (DLA). With the\nobservation that irregular data patterns such as butterfly operations in FFT\nare the major barrier that hinders the deployment of signal processing on DLAs,\nwe propose a programmable data shuffling fabric and have it inserted between\nthe input buffer and computing array of DLAs such that the irregular data is\nreorganized and the processing is converted to be regular. With the online data\nshuffling, the proposed architecture, SigDLA, can adapt to various signal\nprocessing tasks without affecting the deep learning processing. Moreover, we\nbuild a reconfigurable computing array to suit the various data width\nrequirements of both signal processing and deep learning. According to our\nexperiments, SigDLA achieves an average performance speedup of 4.4$\\times$,\n1.4$\\times$, and 1.52$\\times$, and average energy reduction of 4.82$\\times$,\n3.27$\\times$, and 2.15$\\times$ compared to an embedded ARM processor with\ncustomized DSP instructions, a DSP processor, and an independent DSP-DLA\narchitecture respectively with 17% more chip area over the original DLAs.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.12565v1.pdf",
        "similarity": 0.3052531134823218,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-17"
    },
    {
        "new_title": "Active ML for 6G: Towards Efficient Data Generation, Acquisition, and\n  Annotation",
        "new_link": "http://arxiv.org/abs/2406.03630v1",
        "new_summary": "  This paper explores the integration of active machine learning (ML) for 6G\nnetworks, an area that remains under-explored yet holds potential. Unlike\npassive ML systems, active ML can be made to interact with the network\nenvironment. It actively selects informative and representative data points for\ntraining, thereby reducing the volume of data needed while accelerating the\nlearning process. While active learning research mainly focuses on data\nannotation, we call for a network-centric active learning framework that\nconsiders both annotation (i.e., what is the label) and data acquisition (i.e.,\nwhich and how many samples to collect). Moreover, we explore the synergy\nbetween generative artificial intelligence (AI) and active learning to overcome\nexisting limitations in both active learning and generative AI. This paper also\nfeatures a case study on a mmWave throughput prediction problem to demonstrate\nthe practical benefits and improved performance of active learning for 6G\nnetworks. Furthermore, we discuss how the implications of active learning\nextend to numerous 6G network use cases. We highlight the potential of active\nlearning based 6G networks to enhance computational efficiency, data annotation\nand acquisition efficiency, adaptability, and overall network intelligence. We\nconclude with a discussion on challenges and future research directions for\nactive learning in 6G networks, including development of novel query\nstrategies, distributed learning integration, and inclusion of human- and\nmachine-in-the-loop learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.03630v1.pdf",
        "similarity": 0.30516095355767714,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-05"
    },
    {
        "new_title": "Weight Clipping for Deep Continual and Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2407.01704v1",
        "new_summary": "  Many failures in deep continual and reinforcement learning are associated\nwith increasing magnitudes of the weights, making them hard to change and\npotentially causing overfitting. While many methods address these learning\nfailures, they often change the optimizer or the architecture, a complexity\nthat hinders widespread adoption in various systems. In this paper, we focus on\nlearning failures that are associated with increasing weight norm and we\npropose a simple technique that can be easily added on top of existing learning\nsystems: clipping neural network weights to limit them to a specific range. We\nstudy the effectiveness of weight clipping in a series of supervised and\nreinforcement learning experiments. Our empirical results highlight the\nbenefits of weight clipping for generalization, addressing loss of plasticity\nand policy collapse, and facilitating learning with a large replay ratio.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.01704v1.pdf",
        "similarity": 0.30477388549201745,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-01"
    },
    {
        "new_title": "Breast Cancer Image Classification Method Based on Deep Transfer\n  Learning",
        "new_link": "http://arxiv.org/abs/2404.09226v1",
        "new_summary": "  To address the issues of limited samples, time-consuming feature design, and\nlow accuracy in detection and classification of breast cancer pathological\nimages, a breast cancer image classification model algorithm combining deep\nlearning and transfer learning is proposed. This algorithm is based on the\nDenseNet structure of deep neural networks, and constructs a network model by\nintroducing attention mechanisms, and trains the enhanced dataset using\nmulti-level transfer learning. Experimental results demonstrate that the\nalgorithm achieves an efficiency of over 84.0\\% in the test set, with a\nsignificantly improved classification accuracy compared to previous models,\nmaking it applicable to medical breast cancer detection tasks.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.09226v1.pdf",
        "similarity": 0.3047208647385699,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-14"
    },
    {
        "new_title": "Accelerating Multilevel Markov Chain Monte Carlo Using Machine Learning\n  Models",
        "new_link": "http://arxiv.org/abs/2405.11179v1",
        "new_summary": "  This work presents an efficient approach for accelerating multilevel Markov\nChain Monte Carlo (MCMC) sampling for large-scale problems using low-fidelity\nmachine learning models. While conventional techniques for large-scale Bayesian\ninference often substitute computationally expensive high-fidelity models with\nmachine learning models, thereby introducing approximation errors, our approach\noffers a computationally efficient alternative by augmenting high-fidelity\nmodels with low-fidelity ones within a hierarchical framework. The multilevel\napproach utilizes the low-fidelity machine learning model (MLM) for inexpensive\nevaluation of proposed samples thereby improving the acceptance of samples by\nthe high-fidelity model. The hierarchy in our multilevel algorithm is derived\nfrom geometric multigrid hierarchy. We utilize an MLM to acclerate the coarse\nlevel sampling. Training machine learning model for the coarsest level\nsignificantly reduces the computational cost associated with generating\ntraining data and training the model. We present an MCMC algorithm to\naccelerate the coarsest level sampling using MLM and account for the\napproximation error introduced. We provide theoretical proofs of detailed\nbalance and demonstrate that our multilevel approach constitutes a consistent\nMCMC algorithm. Additionally, we derive conditions on the accuracy of the\nmachine learning model to facilitate more efficient hierarchical sampling. Our\ntechnique is demonstrated on a standard benchmark inference problem in\ngroundwater flow, where we estimate the probability density of a quantity of\ninterest using a four-level MCMC algorithm. Our proposed algorithm accelerates\nmultilevel sampling by a factor of two while achieving similar accuracy\ncompared to sampling using the standard multilevel algorithm.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.11179v1.pdf",
        "similarity": 0.30463119391346216,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-18"
    },
    {
        "new_title": "Edge-based Parametric Digital Twins for Intelligent Building Indoor\n  Climate Modeling",
        "new_link": "http://arxiv.org/abs/2403.04326v1",
        "new_summary": "  Digital transformation in the built environment generates vast data for\ndeveloping data-driven models to optimize building operations. This study\npresents an integrated solution utilizing edge computing, digital twins, and\ndeep learning to enhance the understanding of climate in buildings. Parametric\ndigital twins, created using an ontology, ensure consistent data representation\nacross diverse service systems equipped by different buildings. Based on\ncreated digital twins and collected data, deep learning methods are employed to\ndevelop predictive models for identifying patterns in indoor climate and\nproviding insights. Both the parametric digital twin and deep learning models\nare deployed on edge for low latency and privacy compliance. As a\ndemonstration, a case study was conducted in a historic building in\n\\\"Osterg\\\"otland, Sweden, to compare the performance of five deep learning\narchitectures. The results indicate that the time-series dense encoder model\nexhibited strong competitiveness in performing multi-horizon forecasts of\nindoor temperature and relative humidity with low computational costs.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.04326v1.pdf",
        "similarity": 0.30425070609622595,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-07"
    },
    {
        "new_title": "Arbitrary Polynomial Separations in Trainable Quantum Machine Learning",
        "new_link": "http://arxiv.org/abs/2402.08606v1",
        "new_summary": "  Recent theoretical results in quantum machine learning have demonstrated a\ngeneral trade-off between the expressive power of quantum neural networks\n(QNNs) and their trainability; as a corollary of these results, practical\nexponential separations in expressive power over classical machine learning\nmodels are believed to be infeasible as such QNNs take a time to train that is\nexponential in the model size. We here circumvent these negative results by\nconstructing a hierarchy of efficiently trainable QNNs that exhibit\nunconditionally provable, polynomial memory separations of arbitrary constant\ndegree over classical neural networks in performing a classical sequence\nmodeling task. Furthermore, each unit cell of the introduced class of QNNs is\ncomputationally efficient, implementable in constant time on a quantum device.\nThe classical networks we prove a separation over include well-known examples\nsuch as recurrent neural networks and Transformers. We show that quantum\ncontextuality is the source of the expressivity separation, suggesting that\nother classical sequence learning problems with long-time correlations may be a\nregime where practical advantages in quantum machine learning may exist.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.08606v1.pdf",
        "similarity": 0.3041800680491957,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-13"
    },
    {
        "new_title": "Integrating Human Expertise in Continuous Spaces: A Novel Interactive\n  Bayesian Optimization Framework with Preference Expected Improvement",
        "new_link": "http://arxiv.org/abs/2401.12662v1",
        "new_summary": "  Interactive Machine Learning (IML) seeks to integrate human expertise into\nmachine learning processes. However, most existing algorithms cannot be applied\nto Realworld Scenarios because their state spaces and/or action spaces are\nlimited to discrete values. Furthermore, the interaction of all existing\nmethods is restricted to deciding between multiple proposals. We therefore\npropose a novel framework based on Bayesian Optimization (BO). Interactive\nBayesian Optimization (IBO) enables collaboration between machine learning\nalgorithms and humans. This framework captures user preferences and provides an\ninterface for users to shape the strategy by hand. Additionally, we've\nincorporated a new acquisition function, Preference Expected Improvement (PEI),\nto refine the system's efficiency using a probabilistic model of the user\npreferences. Our approach is geared towards ensuring that machines can benefit\nfrom human expertise, aiming for a more aligned and effective learning process.\nIn the course of this work, we applied our method to simulations and in a real\nworld task using a Franka Panda robot to show human-robot collaboration.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.12662v1.pdf",
        "similarity": 0.3041041891084492,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-23"
    },
    {
        "new_title": "Analysis of the rate of convergence of an over-parametrized\n  convolutional neural network image classifier learned by gradient descent",
        "new_link": "http://arxiv.org/abs/2405.07619v1",
        "new_summary": "  Image classification based on over-parametrized convolutional neural networks\nwith a global average-pooling layer is considered. The weights of the network\nare learned by gradient descent. A bound on the rate of convergence of the\ndifference between the misclassification risk of the newly introduced\nconvolutional neural network estimate and the minimal possible value is\nderived.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.07619v1.pdf",
        "similarity": 0.3038241866186871,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-13"
    },
    {
        "new_title": "Deep reinforcement learning with symmetric data augmentation applied for\n  aircraft lateral attitude tracking control",
        "new_link": "http://arxiv.org/abs/2407.11077v1",
        "new_summary": "  Symmetry is an essential property in some dynamical systems that can be\nexploited for state transition prediction and control policy optimization. This\npaper develops two symmetry-integrated Reinforcement Learning (RL) algorithms\nbased on standard Deep Deterministic Policy Gradient (DDPG),which leverage\nenvironment symmetry to augment explored transition samples of a Markov\nDecision Process(MDP). The firstly developed algorithm is named as Deep\nDeterministic Policy Gradient with Symmetric Data Augmentation (DDPG-SDA),\nwhich enriches dataset of standard DDPG algorithm by symmetric data\naugmentation method under symmetry assumption of a dynamical system. To further\nimprove sample utilization efficiency, the second developed RL algorithm\nincorporates one extra critic network, which is independently trained with\naugmented dataset. A two-step approximate policy iteration method is proposed\nto integrate training for two critic networks and one actor network. The\nresulting RL algorithm is named as Deep Deterministic Policy Gradient with\nSymmetric Critic Augmentation (DDPG-SCA). Simulation results demonstrate\nenhanced sample efficiency and tracking performance of developed two RL\nalgorithms in aircraft lateral tracking control task.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.11077v1.pdf",
        "similarity": 0.303725368414717,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-13"
    },
    {
        "new_title": "Future Directions in the Theory of Graph Machine Learning",
        "new_link": "http://arxiv.org/abs/2402.02287v4",
        "new_summary": "  Machine learning on graphs, especially using graph neural networks (GNNs),\nhas seen a surge in interest due to the wide availability of graph data across\na broad spectrum of disciplines, from life to social and engineering sciences.\nDespite their practical success, our theoretical understanding of the\nproperties of GNNs remains highly incomplete. Recent theoretical advancements\nprimarily focus on elucidating the coarse-grained expressive power of GNNs,\npredominantly employing combinatorial techniques. However, these studies do not\nperfectly align with practice, particularly in understanding the generalization\nbehavior of GNNs when trained with stochastic first-order optimization\ntechniques. In this position paper, we argue that the graph machine learning\ncommunity needs to shift its attention to developing a balanced theory of graph\nmachine learning, focusing on a more thorough understanding of the interplay of\nexpressive power, generalization, and optimization.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.02287v4.pdf",
        "similarity": 0.3036917851862058,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-03"
    },
    {
        "new_title": "A novel method for identifying rice seed purity based on hybrid machine\n  learning algorithms",
        "new_link": "http://arxiv.org/abs/2406.07581v1",
        "new_summary": "  In the grain industry, the identification of seed purity is a crucial task as\nit is an important factor in evaluating the quality of seeds. For rice seeds,\nthis property allows for the reduction of unexpected influences of other\nvarieties on rice yield, nutrient composition, and price. However, in practice,\nthey are often mixed with seeds from others. This study proposes a novel method\nfor automatically identifying the rice seed purity of a certain rice variety\nbased on hybrid machine learning algorithms. The main idea is to use deep\nlearning architectures for extracting important features from the raw data and\nthen use machine learning algorithms for classification. Several experiments\nare conducted following a practical implementation to evaluate the performance\nof the proposed model. The obtained results show that the novel method improves\nsignificantly the performance of existing methods. Thus, it can be applied to\ndesign effective identification systems for rice seed purity.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.07581v1.pdf",
        "similarity": 0.3035869906999265,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-09"
    },
    {
        "new_title": "Sparse Meets Dense: A Hybrid Approach to Enhance Scientific Document\n  Retrieval",
        "new_link": "http://arxiv.org/abs/2401.04055v1",
        "new_summary": "  Traditional information retrieval is based on sparse bag-of-words vector\nrepresentations of documents and queries. More recent deep-learning approaches\nhave used dense embeddings learned using a transformer-based large language\nmodel. We show that on a classic benchmark on scientific document retrieval in\nthe medical domain of cystic fibrosis, that both of these models perform\nroughly equivalently. Notably, dense vectors from the state-of-the-art SPECTER2\nmodel do not significantly enhance performance. However, a hybrid model that we\npropose combining these methods yields significantly better results,\nunderscoring the merits of integrating classical and contemporary deep learning\ntechniques in information retrieval in the domain of specialized scientific\ndocuments.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.04055v1.pdf",
        "similarity": 0.3034749138000137,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-08"
    },
    {
        "new_title": "Spatiotemporal Observer Design for Predictive Learning of\n  High-Dimensional Data",
        "new_link": "http://arxiv.org/abs/2402.15284v1",
        "new_summary": "  Although deep learning-based methods have shown great success in\nspatiotemporal predictive learning, the framework of those models is designed\nmainly by intuition. How to make spatiotemporal forecasting with theoretical\nguarantees is still a challenging issue. In this work, we tackle this problem\nby applying domain knowledge from the dynamical system to the framework design\nof deep learning models. An observer theory-guided deep learning architecture,\ncalled Spatiotemporal Observer, is designed for predictive learning of high\ndimensional data. The characteristics of the proposed framework are twofold:\nfirstly, it provides the generalization error bound and convergence guarantee\nfor spatiotemporal prediction; secondly, dynamical regularization is introduced\nto enable the model to learn system dynamics better during training. Further\nexperimental results show that this framework could capture the spatiotemporal\ndynamics and make accurate predictions in both one-step-ahead and\nmulti-step-ahead forecasting scenarios.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.15284v1.pdf",
        "similarity": 0.3032101266030292,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-23"
    },
    {
        "new_title": "A snapshot review on soft-materials assembly design utilizing machine\n  learning methods",
        "new_link": "http://arxiv.org/abs/2405.03805v1",
        "new_summary": "  Since the surge of data in materials science research and the advancement in\nmachine learning methods, an increasing number of researchers are introducing\nmachine learning techniques into the next generation of materials discovery,\nranging from neural-network learned potentials to automated characterization\ntechniques for experimental images. In this snapshot review, we first summarize\nthe landscape of techniques for soft materials assembly design that do not\nemploy machine learning or artificial intelligence and then discuss specific\nmachine-learning and artificial-intelligence-based methods that enhance the\ndesign pipeline, such as high-throughput crystal-structure characterization and\nthe inverse design of building blocks for materials assembly and properties.\nAdditionally, we survey the landscape of current developments of scientific\nsoftware, especially in the context of their compatibility with traditional\nmolecular dynamics engines such as LAMMPS and HOOMD-blue.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.03805v1.pdf",
        "similarity": 0.30306707842430664,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-06"
    },
    {
        "new_title": "Diverse Explanations from Data-driven and Domain-driven Perspectives for\n  Machine Learning Models",
        "new_link": "http://arxiv.org/abs/2402.00347v1",
        "new_summary": "  Explanations of machine learning models are important, especially in\nscientific areas such as chemistry, biology, and physics, where they guide\nfuture laboratory experiments and resource requirements. These explanations can\nbe derived from well-trained machine learning models (data-driven perspective)\nor specific domain knowledge (domain-driven perspective). However, there exist\ninconsistencies between these perspectives due to accurate yet misleading\nmachine learning models and various stakeholders with specific needs, wants, or\naims. This paper calls attention to these inconsistencies and suggests a way to\nfind an accurate model with expected explanations that reinforce physical laws\nand meet stakeholders' requirements from a set of equally-good models, also\nknown as Rashomon sets. Our goal is to foster a comprehensive understanding of\nthese inconsistencies and ultimately contribute to the integration of\neXplainable Artificial Intelligence (XAI) into scientific domains.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.00347v1.pdf",
        "similarity": 0.30302891981157704,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-01"
    },
    {
        "new_title": "On Designing Consistent Covariance Recovery from a Deep Learning Visual\n  Odometry Engine",
        "new_link": "http://arxiv.org/abs/2403.13170v1",
        "new_summary": "  Deep learning techniques have significantly advanced in providing accurate\nvisual odometry solutions by leveraging large datasets. However, generating\nuncertainty estimates for these methods remains a challenge. Traditional sensor\nfusion approaches in a Bayesian framework are well-established, but deep\nlearning techniques with millions of parameters lack efficient methods for\nuncertainty estimation.\n  This paper addresses the issue of uncertainty estimation for pre-trained\ndeep-learning models in monocular visual odometry. We propose formulating a\nfactor graph on an implicit layer of the deep learning network to recover\nrelative covariance estimates, which allows us to determine the covariance of\nthe Visual Odometry (VO) solution. We showcase the consistency of the deep\nlearning engine's covariance approximation with an empirical analysis of the\ncovariance model on the EUROC datasets to demonstrate the correctness of our\nformulation.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.13170v1.pdf",
        "similarity": 0.30281171759889924,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-19"
    },
    {
        "new_title": "Deep learning from strongly mixing observations: Sparse-penalized\n  regularization and minimax optimality",
        "new_link": "http://arxiv.org/abs/2406.08321v1",
        "new_summary": "  The explicit regularization and optimality of deep neural networks estimators\nfrom independent data have made considerable progress recently. The study of\nsuch properties on dependent data is still a challenge. In this paper, we carry\nout deep learning from strongly mixing observations, and deal with the squared\nand a broad class of loss functions. We consider sparse-penalized\nregularization for deep neural network predictor. For a general framework that\nincludes, regression estimation, classification, time series\nprediction,$\\cdots$, oracle inequality for the expected excess risk is\nestablished and a bound on the class of H\\\"older smooth functions is provided.\nFor nonparametric regression from strong mixing data and sub-exponentially\nerror, we provide an oracle inequality for the $L_2$ error and investigate an\nupper bound of this error on a class of H\\\"older composition functions. For the\nspecific case of nonparametric autoregression with Gaussian and Laplace errors,\na lower bound of the $L_2$ error on this H\\\"older composition class is\nestablished. Up to logarithmic factor, this bound matches its upper bound; so,\nthe deep neural network estimator attains the minimax optimal rate.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.08321v1.pdf",
        "similarity": 0.30256230091267255,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-12"
    },
    {
        "new_title": "DLOVE: A new Security Evaluation Tool for Deep Learning Based\n  Watermarking Techniques",
        "new_link": "http://arxiv.org/abs/2407.06552v1",
        "new_summary": "  Recent developments in Deep Neural Network (DNN) based watermarking\ntechniques have shown remarkable performance. The state-of-the-art DNN-based\ntechniques not only surpass the robustness of classical watermarking techniques\nbut also show their robustness against many image manipulation techniques. In\nthis paper, we performed a detailed security analysis of different DNN-based\nwatermarking techniques. We propose a new class of attack called the Deep\nLearning-based OVErwriting (DLOVE) attack, which leverages adversarial machine\nlearning and overwrites the original embedded watermark with a targeted\nwatermark in a watermarked image. To the best of our knowledge, this attack is\nthe first of its kind. We have considered scenarios where watermarks are used\nto devise and formulate an adversarial attack in white box and black box\nsettings. To show adaptability and efficiency, we launch our DLOVE attack\nanalysis on seven different watermarking techniques, HiDDeN, ReDMark, PIMoG,\nStegastamp, Aparecium, Distortion Agostic Deep Watermarking and Hiding Images\nin an Image. All these techniques use different approaches to create\nimperceptible watermarked images. Our attack analysis on these watermarking\ntechniques with various constraints highlights the vulnerabilities of DNN-based\nwatermarking. Extensive experimental results validate the capabilities of\nDLOVE. We propose DLOVE as a benchmark security analysis tool to test the\nrobustness of future deep learning-based watermarking techniques.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.06552v1.pdf",
        "similarity": 0.30194158470477145,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-09"
    },
    {
        "new_title": "A Unified Framework for Human-Allied Learning of Probabilistic Circuits",
        "new_link": "http://arxiv.org/abs/2405.02413v1",
        "new_summary": "  Probabilistic Circuits (PCs) have emerged as an efficient framework for\nrepresenting and learning complex probability distributions. Nevertheless, the\nexisting body of research on PCs predominantly concentrates on data-driven\nparameter learning, often neglecting the potential of knowledge-intensive\nlearning, a particular issue in data-scarce/knowledge-rich domains such as\nhealthcare. To bridge this gap, we propose a novel unified framework that can\nsystematically integrate diverse domain knowledge into the parameter learning\nprocess of PCs. Experiments on several benchmarks as well as real world\ndatasets show that our proposed framework can both effectively and efficiently\nleverage domain knowledge to achieve superior performance compared to purely\ndata-driven learning approaches.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.02413v1.pdf",
        "similarity": 0.3018450952602897,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-03"
    },
    {
        "new_title": "Federated Learning for Non-factorizable Models using Deep Generative\n  Prior Approximations",
        "new_link": "http://arxiv.org/abs/2405.16055v1",
        "new_summary": "  Federated learning (FL) allows for collaborative model training across\ndecentralized clients while preserving privacy by avoiding data sharing.\nHowever, current FL methods assume conditional independence between client\nmodels, limiting the use of priors that capture dependence, such as Gaussian\nprocesses (GPs). We introduce the Structured Independence via deep Generative\nModel Approximation (SIGMA) prior which enables FL for non-factorizable models\nacross clients, expanding the applicability of FL to fields such as spatial\nstatistics, epidemiology, environmental science, and other domains where\nmodeling dependencies is crucial. The SIGMA prior is a pre-trained deep\ngenerative model that approximates the desired prior and induces a specified\nconditional independence structure in the latent variables, creating an\napproximate model suitable for FL settings. We demonstrate the SIGMA prior's\neffectiveness on synthetic data and showcase its utility in a real-world\nexample of FL for spatial data, using a conditional autoregressive prior to\nmodel spatial dependence across Australia. Our work enables new FL applications\nin domains where modeling dependent data is essential for accurate predictions\nand decision-making.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.16055v1.pdf",
        "similarity": 0.3018437138261798,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-25"
    },
    {
        "new_title": "JetTrain: IDE-Native Machine Learning Experiments",
        "new_link": "http://arxiv.org/abs/2402.10857v1",
        "new_summary": "  Integrated development environments (IDEs) are prevalent code-writing and\ndebugging tools. However, they have yet to be widely adopted for launching\nmachine learning (ML) experiments. This work aims to fill this gap by\nintroducing JetTrain, an IDE-integrated tool that delegates specific tasks from\nan IDE to remote computational resources. A user can write and debug code\nlocally and then seamlessly run it remotely using on-demand hardware. We argue\nthat this approach can lower the entry barrier for ML training problems and\nincrease experiment throughput.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.10857v1.pdf",
        "similarity": 0.30167048268182306,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-16"
    },
    {
        "new_title": "Evaluation of deep learning models for Australian climate extremes:\n  prediction of streamflow and floods",
        "new_link": "http://arxiv.org/abs/2407.15882v1",
        "new_summary": "  In recent years, climate extremes such as floods have created significant\nenvironmental and economic hazards for Australia, causing damage to the\nenvironment and economy and losses of human and animal lives. An efficient\nmethod of forecasting floods is crucial to limit this damage. Techniques for\nflood prediction are currently based on hydrological, and hydrodynamic\n(physically-based) numerical models. Machine learning methods that include deep\nlearning offer certain advantages over conventional physically based\napproaches, including flexibility and accuracy. Deep learning methods have been\npromising for predicting small to medium-sized climate extreme events over a\nshort time horizon; however, large flooding events present a critical\nchallenge. We present an ensemble-based machine learning approach that\naddresses large-scale extreme flooding challenges using a switching mechanism\nmotivated by extreme-value theory for long-short-term-memory (LSTM) deep\nlearning models. We use a multivariate and multi-step time-series prediction\napproach to predict streamflow for multiple days ahead in the major catchments\nof Australia. The ensemble framework also employs static information to enrich\nthe time-series information, allowing for regional modelling across catchments.\nOur results demonstrate enhanced prediction of streamflow extremes, with\nnotable efficacy for large flooding scenarios in the selected Australian\ncatchments. Through comparative analysis, our methodology underscores the\npotential for deep learning models to revolutionise flood forecasting across\ndiverse regions.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.15882v1.pdf",
        "similarity": 0.30158556356842436,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-20"
    },
    {
        "new_title": "An Enhanced Analysis of Traffic Intelligence in Smart Cities Using\n  Sustainable Deep Radial Function",
        "new_link": "http://arxiv.org/abs/2402.09432v1",
        "new_summary": "  Smart cities have revolutionized urban living by incorporating sophisticated\ntechnologies to optimize various aspects of urban infrastructure, such as\ntransportation systems. Effective traffic management is a crucial component of\nsmart cities, as it has a direct impact on the quality of life of residents and\ntourists. Utilizing deep radial basis function (RBF) networks, this paper\ndescribes a novel strategy for enhancing traffic intelligence in smart cities.\nTraditional methods of traffic analysis frequently rely on simplistic models\nthat are incapable of capturing the intricate patterns and dynamics of urban\ntraffic systems. Deep learning techniques, such as deep RBF networks, have the\npotential to extract valuable insights from traffic data and enable more\nprecise predictions and decisions. In this paper, we propose an RBF based\nmethod for enhancing smart city traffic intelligence. Deep RBF networks combine\nthe adaptability and generalization capabilities of deep learning with the\ndiscriminative capability of radial basis functions. The proposed method can\neffectively learn intricate relationships and nonlinear patterns in traffic\ndata by leveraging the hierarchical structure of deep neural networks. The deep\nRBF model can learn to predict traffic conditions, identify congestion\npatterns, and make informed recommendations for optimizing traffic management\nstrategies by incorporating these rich and diverse data To evaluate the\nefficacy of our proposed method, extensive experiments and comparisons with\nreal world traffic datasets from a smart city environment were conducted. In\nterms of prediction accuracy and efficiency, the results demonstrate that the\ndeep RBF based approach outperforms conventional traffic analysis methods.\nSmart city traffic intelligence is enhanced by the model capacity to capture\nnonlinear relationships and manage large scale data sets.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.09432v1.pdf",
        "similarity": 0.301520786954275,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-24"
    },
    {
        "new_title": "Model-based deep reinforcement learning for accelerated learning from\n  flow simulations",
        "new_link": "http://arxiv.org/abs/2402.16543v2",
        "new_summary": "  In recent years, deep reinforcement learning has emerged as a technique to\nsolve closed-loop flow control problems. Employing simulation-based\nenvironments in reinforcement learning enables a priori end-to-end optimization\nof the control system, provides a virtual testbed for safety-critical control\napplications, and allows to gain a deep understanding of the control\nmechanisms. While reinforcement learning has been applied successfully in a\nnumber of rather simple flow control benchmarks, a major bottleneck toward\nreal-world applications is the high computational cost and turnaround time of\nflow simulations. In this contribution, we demonstrate the benefits of\nmodel-based reinforcement learning for flow control applications. Specifically,\nwe optimize the policy by alternating between trajectories sampled from flow\nsimulations and trajectories sampled from an ensemble of environment models.\nThe model-based learning reduces the overall training time by up to $85\\%$ for\nthe fluidic pinball test case. Even larger savings are expected for more\ndemanding flow simulations.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.16543v2.pdf",
        "similarity": 0.3013994976432548,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-26"
    },
    {
        "new_title": "Constructive Universal Approximation Theorems for Deep Joint-Equivariant\n  Networks by Schur's Lemma",
        "new_link": "http://arxiv.org/abs/2405.13682v1",
        "new_summary": "  We present a unified constructive universal approximation theorem covering a\nwide range of learning machines including both shallow and deep neural networks\nbased on the group representation theory. Constructive here means that the\ndistribution of parameters is given in a closed-form expression (called the\nridgelet transform). Contrary to the case of shallow models, expressive power\nanalysis of deep models has been conducted in a case-by-case manner. Recently,\nSonoda et al. (2023a,b) developed a systematic method to show a constructive\napproximation theorem from scalar-valued joint-group-invariant feature maps,\ncovering a formal deep network. However, each hidden layer was formalized as an\nabstract group action, so it was not possible to cover real deep networks\ndefined by composites of nonlinear activation function. In this study, we\nextend the method for vector-valued joint-group-equivariant feature maps, so to\ncover such real networks.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.13682v1.pdf",
        "similarity": 0.3011603702356539,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-22"
    },
    {
        "new_title": "Advances of Deep Learning in Protein Science: A Comprehensive Survey",
        "new_link": "http://arxiv.org/abs/2403.05314v1",
        "new_summary": "  Protein representation learning plays a crucial role in understanding the\nstructure and function of proteins, which are essential biomolecules involved\nin various biological processes. In recent years, deep learning has emerged as\na powerful tool for protein modeling due to its ability to learn complex\npatterns and representations from large-scale protein data. This comprehensive\nsurvey aims to provide an overview of the recent advances in deep learning\ntechniques applied to protein science. The survey begins by introducing the\ndevelopments of deep learning based protein models and emphasizes the\nimportance of protein representation learning in drug discovery, protein\nengineering, and function annotation. It then delves into the fundamentals of\ndeep learning, including convolutional neural networks, recurrent neural\nnetworks, attention models, and graph neural networks in modeling protein\nsequences, structures, and functions, and explores how these techniques can be\nused to extract meaningful features and capture intricate relationships within\nprotein data. Next, the survey presents various applications of deep learning\nin the field of proteins, including protein structure prediction,\nprotein-protein interaction prediction, protein function prediction, etc.\nFurthermore, it highlights the challenges and limitations of these deep\nlearning techniques and also discusses potential solutions and future\ndirections for overcoming these challenges. This comprehensive survey provides\na valuable resource for researchers and practitioners in the field of proteins\nwho are interested in harnessing the power of deep learning techniques. By\nconsolidating the latest advancements and discussing potential avenues for\nimprovement, this review contributes to the ongoing progress in protein\nresearch and paves the way for future breakthroughs in the field.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.05314v1.pdf",
        "similarity": 0.3010464008763428,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-08"
    },
    {
        "new_title": "Collaborative Learning with Different Labeling Functions",
        "new_link": "http://arxiv.org/abs/2402.10445v3",
        "new_summary": "  We study a variant of Collaborative PAC Learning, in which we aim to learn an\naccurate classifier for each of the $n$ data distributions, while minimizing\nthe number of samples drawn from them in total. Unlike in the usual\ncollaborative learning setup, it is not assumed that there exists a single\nclassifier that is simultaneously accurate for all distributions.\n  We show that, when the data distributions satisfy a weaker realizability\nassumption, which appeared in [Crammer and Mansour, 2012] in the context of\nmulti-task learning, sample-efficient learning is still feasible. We give a\nlearning algorithm based on Empirical Risk Minimization (ERM) on a natural\naugmentation of the hypothesis class, and the analysis relies on an upper bound\non the VC dimension of this augmented class.\n  In terms of the computational efficiency, we show that ERM on the augmented\nhypothesis class is NP-hard, which gives evidence against the existence of\ncomputationally efficient learners in general. On the positive side, for two\nspecial cases, we give learners that are both sample- and\ncomputationally-efficient.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.10445v3.pdf",
        "similarity": 0.30082214175227867,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-16"
    },
    {
        "new_title": "More Flexible PAC-Bayesian Meta-Learning by Learning Learning Algorithms",
        "new_link": "http://arxiv.org/abs/2402.04054v2",
        "new_summary": "  We introduce a new framework for studying meta-learning methods using\nPAC-Bayesian theory. Its main advantage over previous work is that it allows\nfor more flexibility in how the transfer of knowledge between tasks is\nrealized. For previous approaches, this could only happen indirectly, by means\nof learning prior distributions over models. In contrast, the new\ngeneralization bounds that we prove express the process of meta-learning much\nmore directly as learning the learning algorithm that should be used for future\ntasks. The flexibility of our framework makes it suitable to analyze a wide\nrange of meta-learning mechanisms and even design new mechanisms. Other than\nour theoretical contributions we also show empirically that our framework\nimproves the prediction quality in practical meta-learning mechanisms.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.04054v2.pdf",
        "similarity": 0.3007975072602449,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-06"
    },
    {
        "new_title": "MosquitoFusion: A Multiclass Dataset for Real-Time Detection of\n  Mosquitoes, Swarms, and Breeding Sites Using Deep Learning",
        "new_link": "http://arxiv.org/abs/2404.01501v1",
        "new_summary": "  In this paper, we present an integrated approach to real-time mosquito\ndetection using our multiclass dataset (MosquitoFusion) containing 1204 diverse\nimages and leverage cutting-edge technologies, specifically computer vision, to\nautomate the identification of Mosquitoes, Swarms, and Breeding Sites. The\npre-trained YOLOv8 model, trained on this dataset, achieved a mean Average\nPrecision (mAP@50) of 57.1%, with precision at 73.4% and recall at 50.5%. The\nintegration of Geographic Information Systems (GIS) further enriches the depth\nof our analysis, providing valuable insights into spatial patterns. The dataset\nand code are available at https://github.com/faiyazabdullah/MosquitoFusion.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.01501v1.pdf",
        "similarity": 0.3007439535321689,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-01"
    },
    {
        "new_title": "Multi-Objective Learning for Deformable Image Registration",
        "new_link": "http://arxiv.org/abs/2402.16658v1",
        "new_summary": "  Deformable image registration (DIR) involves optimization of multiple\nconflicting objectives, however, not many existing DIR algorithms are\nmulti-objective (MO). Further, while there has been progress in the design of\ndeep learning algorithms for DIR, there is no work in the direction of MO DIR\nusing deep learning. In this paper, we fill this gap by combining a recently\nproposed approach for MO training of neural networks with a well-known deep\nneural network for DIR and create a deep learning based MO DIR approach. We\nevaluate the proposed approach for DIR of pelvic magnetic resonance imaging\n(MRI) scans. We experimentally demonstrate that the proposed MO DIR approach --\nproviding multiple registration outputs for each patient that each correspond\nto a different trade-off between the objectives -- has additional desirable\nproperties from a clinical use point-of-view as compared to providing a single\nDIR output. The experiments also show that the proposed MO DIR approach\nprovides a better spread of DIR outputs across the entire trade-off front than\nsimply training multiple neural networks with weights for each objective\nsampled from a grid of possible values.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.16658v1.pdf",
        "similarity": 0.300106877985298,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-23"
    },
    {
        "new_title": "Assessment of Sports Concussion in Female Athletes: A Role for\n  Neuroinformatics?",
        "new_link": "http://arxiv.org/abs/2401.13045v2",
        "new_summary": "  Over the past decade, the intricacies of sports-related concussions among\nfemale athletes have become readily apparent. Traditional clinical methods for\ndiagnosing concussions suffer limitations when applied to female athletes,\noften failing to capture subtle changes in brain structure and function.\nAdvanced neuroinformatics techniques and machine learning models have become\ninvaluable assets in this endeavor. While these technologies have been\nextensively employed in understanding concussion in male athletes, there\nremains a significant gap in our comprehension of their effectiveness for\nfemale athletes. With its remarkable data analysis capacity, machine learning\noffers a promising avenue to bridge this deficit. By harnessing the power of\nmachine learning, researchers can link observed phenotypic neuroimaging data to\nsex-specific biological mechanisms, unraveling the mysteries of concussions in\nfemale athletes. Furthermore, embedding methods within machine learning enable\nexamining brain architecture and its alterations beyond the conventional\nanatomical reference frame. In turn, allows researchers to gain deeper insights\ninto the dynamics of concussions, treatment responses, and recovery processes.\nTo guarantee that female athletes receive the optimal care they deserve,\nresearchers must employ advanced neuroimaging techniques and sophisticated\nmachine-learning models. These tools enable an in-depth investigation of the\nunderlying mechanisms responsible for concussion symptoms stemming from\nneuronal dysfunction in female athletes. This paper endeavors to address the\ncrucial issue of sex differences in multimodal neuroimaging experimental design\nand machine learning approaches within female athlete populations, ultimately\nensuring that they receive the tailored care they require when facing the\nchallenges of concussions.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.13045v2.pdf",
        "similarity": 0.29994841425478086,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-23"
    },
    {
        "new_title": "In value-based deep reinforcement learning, a pruned network is a good\n  network",
        "new_link": "http://arxiv.org/abs/2402.12479v3",
        "new_summary": "  Recent work has shown that deep reinforcement learning agents have difficulty\nin effectively using their network parameters. We leverage prior insights into\nthe advantages of sparse training techniques and demonstrate that gradual\nmagnitude pruning enables value-based agents to maximize parameter\neffectiveness. This results in networks that yield dramatic performance\nimprovements over traditional networks, using only a small fraction of the full\nnetwork parameters.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.12479v3.pdf",
        "similarity": 0.29992326020481086,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-19"
    },
    {
        "new_title": "Machine Learning-Based Completions Sequencing for Well Performance\n  Optimization",
        "new_link": "http://arxiv.org/abs/2402.15608v1",
        "new_summary": "  Establishing accurate field development parameters to optimize long-term oil\nproduction takes time and effort due to the complexity of oil well development,\nand the uncertainty in estimating long-term well production. Traditionally, oil\nand gas companies use simulation software that are inherently computationally\nexpensive to forecast production. Thus, machine learning approaches are\nrecently utilized in literature as an efficient alternative to optimize well\ndevelopments by enhancing completion conditions. The primary goal of this\nproject is to develop effective machine-learning models that can integrate the\neffects of multidimensional predictive variables (i.e., completion conditions)\nto predict 12-Month Cumulative Production accurately.\n  Three predictive regression machine learning models are implemented for\npredicting 12-month cumulative oil production: Random Forest, Gradient\nBoosting, and Long Short-Term Memory Models. All three models yielded\ncumulative production predictions with root mean squared error (RMSE ) values\nranging from 7.35 to 20.01 thousand barrels of oil. Although we hypothesized\nthat all models would yield accurate predictions, the results indicated a\ncrucial need for further refinement to create reliable and rational predictive\ntools in the subsurface. While this study did not produce optimal models for\ncompletion sequencing to maximize long-term production, we established that\nmachine learning models alone are not self-sufficient for problems of this\nnature. Hence, there is potential for significant improvement, including\ncomprehensive feature engineering, and a recommendation of exploring the use of\nhybrid or surrogate models (i.e., coupling physics reduced models and machine\nlearning models), to ascertain significant contribution to the progress of\ncompletion sequencing workflows.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.15608v1.pdf",
        "similarity": 0.29984480331939045,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-23"
    },
    {
        "new_title": "Hybrid Quantum Graph Neural Network for Molecular Property Prediction",
        "new_link": "http://arxiv.org/abs/2405.05205v1",
        "new_summary": "  To accelerate the process of materials design, materials science has\nincreasingly used data driven techniques to extract information from collected\ndata. Specially, machine learning (ML) algorithms, which span the ML\ndiscipline, have demonstrated ability to predict various properties of\nmaterials with the level of accuracy similar to explicit calculation of quantum\nmechanical theories, but with significantly reduced run time and computational\nresources. Within ML, graph neural networks have emerged as an important\nalgorithm within the field of machine learning, since they are capable of\npredicting accurately a wide range of important physical, chemical and\nelectronic properties due to their higher learning ability based on the graph\nrepresentation of material and molecular descriptors through the aggregation of\ninformation embedded within the graph. In parallel with the development of\nstate of the art classical machine learning applications, the fusion of quantum\ncomputing and machine learning have created a new paradigm where classical\nmachine learning model can be augmented with quantum layers which are able to\nencode high dimensional data more efficiently. Leveraging the structure of\nexisting algorithms, we developed a unique and novel gradient free hybrid\nquantum classical convoluted graph neural network (HyQCGNN) to predict\nformation energies of perovskite materials. The performance of our hybrid\nstatistical model is competitive with the results obtained purely from a\nclassical convoluted graph neural network, and other classical machine learning\nalgorithms, such as XGBoost. Consequently, our study suggests a new pathway to\nexplore how quantum feature encoding and parametric quantum circuits can yield\ndrastic improvements of complex ML algorithm like graph neural network.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.05205v1.pdf",
        "similarity": 0.29981675923036116,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-08"
    },
    {
        "new_title": "Searching for new heavy fermions with deep learning",
        "new_link": "http://arxiv.org/abs/2407.17290v1",
        "new_summary": "  Deep learning models were developed and implemented to aid the search for new\nheavy fermion compounds. For the purpose of these calculations a database of\nmore than 200 heavy fermions was compiled from the literature. The deep\nlearning networks trained on the database were then used for regression\ncalculations, and predictions were made about the coherence temperature,\nSommerfeld coefficient and carrier effective mass of potential new heavy\nfermions. Classification calculations were also performed in order to check\nwhether predicted heavy fermions are superconducting and/or antiferromagnetic.\nChemical composition was the only physical predictor used during the learning\nprocess. Suggestions were made for future improvements in terms of expanding\nthe database, as well as for other artificial intelligence calculations.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.17290v1.pdf",
        "similarity": 0.2997123318563744,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-24"
    },
    {
        "new_title": "Calibration of Deep Learning Classification Models in fNIRS",
        "new_link": "http://arxiv.org/abs/2402.15266v2",
        "new_summary": "  Functional near-infrared spectroscopy (fNIRS) is a valuable non-invasive tool\nfor monitoring brain activity. The classification of fNIRS data in relation to\nconscious activity holds significance for advancing our understanding of the\nbrain and facilitating the development of brain-computer interfaces (BCI). Many\nresearchers have turned to deep learning to tackle the classification\nchallenges inherent in fNIRS data due to its strong generalization and\nrobustness. In the application of fNIRS, reliability is really important, and\none mathematical formulation of the reliability of confidence is calibration.\nHowever, many researchers overlook the important issue of calibration. To\naddress this gap, we propose integrating calibration into fNIRS field and\nassess the reliability of existing models. Surprisingly, our results indicate\npoor calibration performance in many proposed models. To advance calibration\ndevelopment in the fNIRS field, we summarize three practical tips. Through this\nletter, we hope to emphasize the critical role of calibration in fNIRS research\nand argue for enhancing the reliability of deep learning-based predictions in\nfNIRS classification tasks. All data from our experimental process are openly\navailable on GitHub.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.15266v2.pdf",
        "similarity": 0.2996927060928325,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-23"
    },
    {
        "new_title": "Pi-fusion: Physics-informed diffusion model for learning fluid dynamics",
        "new_link": "http://arxiv.org/abs/2406.03711v1",
        "new_summary": "  Physics-informed deep learning has been developed as a novel paradigm for\nlearning physical dynamics recently. While general physics-informed deep\nlearning methods have shown early promise in learning fluid dynamics, they are\ndifficult to generalize in arbitrary time instants in real-world scenario,\nwhere the fluid motion can be considered as a time-variant trajectory involved\nlarge-scale particles. Inspired by the advantage of diffusion model in learning\nthe distribution of data, we first propose Pi-fusion, a physics-informed\ndiffusion model for predicting the temporal evolution of velocity and pressure\nfield in fluid dynamics. Physics-informed guidance sampling is proposed in the\ninference procedure of Pi-fusion to improve the accuracy and interpretability\nof learning fluid dynamics. Furthermore, we introduce a training strategy based\non reciprocal learning to learn the quasiperiodical pattern of fluid motion and\nthus improve the generalizability of the model. The proposed approach are then\nevaluated on both synthetic and real-world dataset, by comparing it with\nstate-of-the-art physics-informed deep learning methods. Experimental results\nshow that the proposed approach significantly outperforms existing methods for\npredicting temporal evolution of velocity and pressure field, confirming its\nstrong generalization by drawing probabilistic inference of forward process and\nphysics-informed guidance sampling. The proposed Pi-fusion can also be\ngeneralized in learning other physical dynamics governed by partial\ndifferential equations.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.03711v1.pdf",
        "similarity": 0.29957212790356347,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-06"
    },
    {
        "new_title": "Lessons on Datasets and Paradigms in Machine Learning for Symbolic\n  Computation: A Case Study on CAD",
        "new_link": "http://arxiv.org/abs/2401.13343v2",
        "new_summary": "  Symbolic Computation algorithms and their implementation in computer algebra\nsystems often contain choices which do not affect the correctness of the output\nbut can significantly impact the resources required: such choices can benefit\nfrom having them made separately for each problem via a machine learning model.\nThis study reports lessons on such use of machine learning in symbolic\ncomputation, in particular on the importance of analysing datasets prior to\nmachine learning and on the different machine learning paradigms that may be\nutilised. We present results for a particular case study, the selection of\nvariable ordering for cylindrical algebraic decomposition, but expect that the\nlessons learned are applicable to other decisions in symbolic computation.\n  We utilise an existing dataset of examples derived from applications which\nwas found to be imbalanced with respect to the variable ordering decision. We\nintroduce an augmentation technique for polynomial systems problems that allows\nus to balance and further augment the dataset, improving the machine learning\nresults by 28\\% and 38\\% on average, respectively. We then demonstrate how the\nexisting machine learning methodology used for the problem $-$ classification\n$-$ might be recast into the regression paradigm. While this does not have a\nradical change on the performance, it does widen the scope in which the\nmethodology can be applied to make choices.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.13343v2.pdf",
        "similarity": 0.29955974205470337,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-24"
    },
    {
        "new_title": "Graphs Unveiled: Graph Neural Networks and Graph Generation",
        "new_link": "http://arxiv.org/abs/2403.13849v1",
        "new_summary": "  One of the hot topics in machine learning is the field of GNN. The complexity\nof graph data has imposed significant challenges on existing machine learning\nalgorithms. Recently, many studies on extending deep learning approaches for\ngraph data have emerged. This paper represents a survey, providing a\ncomprehensive overview of Graph Neural Networks (GNNs). We discuss the\napplications of graph neural networks across various domains. Finally, we\npresent an advanced field in GNNs: graph generation.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.13849v1.pdf",
        "similarity": 0.2995527649294426,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-18"
    },
    {
        "new_title": "Learning More Generalized Experts by Merging Experts in\n  Mixture-of-Experts",
        "new_link": "http://arxiv.org/abs/2405.11530v1",
        "new_summary": "  We observe that incorporating a shared layer in a mixture-of-experts can lead\nto performance degradation. This leads us to hypothesize that learning shared\nfeatures poses challenges in deep learning, potentially caused by the same\nfeature being learned as various different features. To address this issue, we\ntrack each expert's usage frequency and merge the two most frequently selected\nexperts. We then update the least frequently selected expert using the\ncombination of experts. This approach, combined with the subsequent learning of\nthe router's expert selection, allows the model to determine if the most\nfrequently selected experts have learned the same feature differently. If they\nhave, the combined expert can be further trained to learn a more general\nfeature. Consequently, our algorithm enhances transfer learning and mitigates\ncatastrophic forgetting when applied to multi-domain task incremental learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.11530v1.pdf",
        "similarity": 0.2995024243627174,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-19"
    },
    {
        "new_title": "Continual Learning for Smart City: A Survey",
        "new_link": "http://arxiv.org/abs/2404.00983v1",
        "new_summary": "  With the digitization of modern cities, large data volumes and powerful\ncomputational resources facilitate the rapid update of intelligent models\ndeployed in smart cities. Continual learning (CL) is a novel machine learning\nparadigm that constantly updates models to adapt to changing environments,\nwhere the learning tasks, data, and distributions can vary over time. Our\nsurvey provides a comprehensive review of continual learning methods that are\nwidely used in smart city development. The content consists of three parts: 1)\nMethodology-wise. We categorize a large number of basic CL methods and advanced\nCL frameworks in combination with other learning paradigms including graph\nlearning, spatial-temporal learning, multi-modal learning, and federated\nlearning. 2) Application-wise. We present numerous CL applications covering\ntransportation, environment, public health, safety, networks, and associated\ndatasets related to urban computing. 3) Challenges. We discuss current problems\nand challenges and envision several promising research directions. We believe\nthis survey can help relevant researchers quickly familiarize themselves with\nthe current state of continual learning research used in smart city development\nand direct them to future research trends.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.00983v1.pdf",
        "similarity": 0.2993147345343835,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-01"
    },
    {
        "new_title": "HLOB -- Information Persistence and Structure in Limit Order Books",
        "new_link": "http://arxiv.org/abs/2405.18938v3",
        "new_summary": "  We introduce a novel large-scale deep learning model for Limit Order Book\nmid-price changes forecasting, and we name it `HLOB'. This architecture (i)\nexploits the information encoded by an Information Filtering Network, namely\nthe Triangulated Maximally Filtered Graph, to unveil deeper and non-trivial\ndependency structures among volume levels; and (ii) guarantees deterministic\ndesign choices to handle the complexity of the underlying system by drawing\ninspiration from the groundbreaking class of Homological Convolutional Neural\nNetworks. We test our model against 9 state-of-the-art deep learning\nalternatives on 3 real-world Limit Order Book datasets, each including 15\nstocks traded on the NASDAQ exchange, and we systematically characterize the\nscenarios where HLOB outperforms state-of-the-art architectures. Our approach\nsheds new light on the spatial distribution of information in Limit Order Books\nand on its degradation over increasing prediction horizons, narrowing the gap\nbetween microstructural modeling and deep learning-based forecasting in\nhigh-frequency financial markets.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.18938v3.pdf",
        "similarity": 0.299244515201393,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-29"
    },
    {
        "new_title": "Learning from the Best: Active Learning for Wireless Communications",
        "new_link": "http://arxiv.org/abs/2402.04896v1",
        "new_summary": "  Collecting an over-the-air wireless communications training dataset for deep\nlearning-based communication tasks is relatively simple. However, labeling the\ndataset requires expert involvement and domain knowledge, may involve private\nintellectual properties, and is often computationally and financially\nexpensive. Active learning is an emerging area of research in machine learning\nthat aims to reduce the labeling overhead without accuracy degradation. Active\nlearning algorithms identify the most critical and informative samples in an\nunlabeled dataset and label only those samples, instead of the complete set. In\nthis paper, we introduce active learning for deep learning applications in\nwireless communications, and present its different categories. We present a\ncase study of deep learning-based mmWave beam selection, where labeling is\nperformed by a compute-intensive algorithm based on exhaustive search. We\nevaluate the performance of different active learning algorithms on a publicly\navailable multi-modal dataset with different modalities including image and\nLiDAR. Our results show that using an active learning algorithm for\nclass-imbalanced datasets can reduce labeling overhead by up to 50% for this\ndataset while maintaining the same accuracy as classical training.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.04896v1.pdf",
        "similarity": 0.29918942657004066,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-23"
    },
    {
        "new_title": "IDA-UIE: An Iterative Framework for Deep Network-based Degradation Aware\n  Underwater Image Enhancement",
        "new_link": "http://arxiv.org/abs/2406.18628v1",
        "new_summary": "  Underwater image quality is affected by fluorescence, low illumination,\nabsorption, and scattering. Recent works in underwater image enhancement have\nproposed different deep network architectures to handle these problems. Most of\nthese works have proposed a single network to handle all the challenges. We\nbelieve that deep networks trained for specific conditions deliver better\nperformance than a single network learned from all degradation cases.\nAccordingly, the first contribution of this work lies in the proposal of an\niterative framework where a single dominant degradation condition is identified\nand resolved. This proposal considers the following eight degradation\nconditions -- low illumination, low contrast, haziness, blurred image, presence\nof noise and color imbalance in three different channels. A deep network is\ndesigned to identify the dominant degradation condition. Accordingly, an\nappropriate deep network is selected for degradation condition-specific\nenhancement. The second contribution of this work is the construction of\ndegradation condition specific datasets from good quality images of two\nstandard datasets (UIEB and EUVP). This dataset is used to learn the condition\nspecific enhancement networks. The proposed approach is found to outperform\nnine baseline methods on UIEB and EUVP datasets.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.18628v1.pdf",
        "similarity": 0.2991722830200323,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-26"
    },
    {
        "new_title": "Deep Horseshoe Gaussian Processes",
        "new_link": "http://arxiv.org/abs/2403.01737v1",
        "new_summary": "  Deep Gaussian processes have recently been proposed as natural objects to\nfit, similarly to deep neural networks, possibly complex features present in\nmodern data samples, such as compositional structures. Adopting a Bayesian\nnonparametric approach, it is natural to use deep Gaussian processes as prior\ndistributions, and use the corresponding posterior distributions for\nstatistical inference. We introduce the deep Horseshoe Gaussian process\nDeep-HGP, a new simple prior based on deep Gaussian processes with a\nsquared-exponential kernel, that in particular enables data-driven choices of\nthe key lengthscale parameters. For nonparametric regression with random\ndesign, we show that the associated tempered posterior distribution recovers\nthe unknown true regression curve optimally in terms of quadratic loss, up to a\nlogarithmic factor, in an adaptive way. The convergence rates are\nsimultaneously adaptive to both the smoothness of the regression function and\nto its structure in terms of compositions. The dependence of the rates in terms\nof dimension are explicit, allowing in particular for input spaces of dimension\nincreasing with the number of observations.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.01737v1.pdf",
        "similarity": 0.29906684000761463,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-04"
    },
    {
        "new_title": "Neural Policy Style Transfer",
        "new_link": "http://arxiv.org/abs/2402.00677v1",
        "new_summary": "  Style Transfer has been proposed in a number of fields: fine arts, natural\nlanguage processing, and fixed trajectories. We scale this concept up to\ncontrol policies within a Deep Reinforcement Learning infrastructure. Each\nnetwork is trained to maximize the expected reward, which typically encodes the\ngoal of an action, and can be described as the content. The expressive power of\ndeep neural networks enables encoding a secondary task, which can be described\nas the style. The Neural Policy Style Transfer (NPST) algorithm is proposed to\ntransfer the style of one policy to another, while maintaining the content of\nthe latter. Different policies are defined via Deep Q-Network architectures.\nThese models are trained using demonstrations through Inverse Reinforcement\nLearning. Two different sets of user demonstrations are performed, one for\ncontent and other for style. Different styles are encoded as defined by user\ndemonstrations. The generated policy is the result of feeding a content policy\nand a style policy to the NPST algorithm. Experiments are performed in a\ncatch-ball game inspired by the Deep Reinforcement Learning classical Atari\ngames; and a real-world painting scenario with a full-sized humanoid robot,\nbased on previous works of the authors. The implementation of three different\nQ-Network architectures (Shallow, Deep and Deep Recurrent Q-Network) to encode\nthe policies within the NPST framework is proposed and the results obtained in\nthe experiments with each of these architectures compared.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.00677v1.pdf",
        "similarity": 0.2987818871291837,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-01"
    },
    {
        "new_title": "Learning Decision Policies with Instrumental Variables through Double\n  Machine Learning",
        "new_link": "http://arxiv.org/abs/2405.08498v3",
        "new_summary": "  A common issue in learning decision-making policies in data-rich settings is\nspurious correlations in the offline dataset, which can be caused by hidden\nconfounders. Instrumental variable (IV) regression, which utilises a key\nunconfounded variable known as the instrument, is a standard technique for\nlearning causal relationships between confounded action, outcome, and context\nvariables. Most recent IV regression algorithms use a two-stage approach, where\na deep neural network (DNN) estimator learnt in the first stage is directly\nplugged into the second stage, in which another DNN is used to estimate the\ncausal effect. Naively plugging the estimator can cause heavy bias in the\nsecond stage, especially when regularisation bias is present in the first stage\nestimator. We propose DML-IV, a non-linear IV regression method that reduces\nthe bias in two-stage IV regressions and effectively learns high-performing\npolicies. We derive a novel learning objective to reduce bias and design the\nDML-IV algorithm following the double/debiased machine learning (DML)\nframework. The learnt DML-IV estimator has strong convergence rate and\n$O(N^{-1/2})$ suboptimality guarantees that match those when the dataset is\nunconfounded. DML-IV outperforms state-of-the-art IV regression methods on IV\nregression benchmarks and learns high-performing policies in the presence of\ninstruments.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.08498v3.pdf",
        "similarity": 0.2987539791744531,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-14"
    },
    {
        "new_title": "Learning equivariant tensor functions with applications to sparse vector\n  recovery",
        "new_link": "http://arxiv.org/abs/2406.01552v1",
        "new_summary": "  This work characterizes equivariant polynomial functions from tuples of\ntensor inputs to tensor outputs. Loosely motivated by physics, we focus on\nequivariant functions with respect to the diagonal action of the orthogonal\ngroup on tensors. We show how to extend this characterization to other linear\nalgebraic groups, including the Lorentz and symplectic groups.\n  Our goal behind these characterizations is to define equivariant machine\nlearning models. In particular, we focus on the sparse vector estimation\nproblem. This problem has been broadly studied in the theoretical computer\nscience literature, and explicit spectral methods, derived by techniques from\nsum-of-squares, can be shown to recover sparse vectors under certain\nassumptions. Our numerical results show that the proposed equivariant machine\nlearning models can learn spectral methods that outperform the best\ntheoretically known spectral methods in some regimes. The experiments also\nsuggest that learned spectral methods can solve the problem in settings that\nhave not yet been theoretically analyzed.\n  This is an example of a promising direction in which theory can inform\nmachine learning models and machine learning models could inform theory.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.01552v1.pdf",
        "similarity": 0.2982589683687302,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-03"
    },
    {
        "new_title": "How to integrate cloud service, data analytic and machine learning\n  technique to reduce cyber risks associated with the modern cloud based\n  infrastructure",
        "new_link": "http://arxiv.org/abs/2405.11601v1",
        "new_summary": "  The combination of cloud technology, machine learning, and data visualization\ntechniques allows hybrid enterprise networks to hold massive volumes of data\nand provide employees and customers easy access to these cloud data. These\nmassive collections of complex data sets are facing security challenges. While\ncloud platforms are more vulnerable to security threats and traditional\nsecurity technologies are unable to cope with the rapid data explosion in cloud\nplatforms, machine learning powered security solutions and data visualization\ntechniques are playing instrumental roles in detecting security threat, data\nbreaches, and automatic finding software vulnerabilities. The purpose of this\npaper is to present some of the widely used cloud services, machine learning\ntechniques and data visualization approach and demonstrate how to integrate\ncloud service, data analytic and machine learning techniques that can be used\nto detect and reduce cyber risks associated with the modern cloud based\ninfrastructure. In this paper I applied the machine learning supervised\nclassifier to design a model based on well-known UNSW-NB15 dataset to predict\nthe network behavior metrics and demonstrated how data analytics techniques can\nbe integrated to visualize network traffics.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.11601v1.pdf",
        "similarity": 0.29815509822834135,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-19"
    },
    {
        "new_title": "Understanding and Diagnosing Deep Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2406.16979v1",
        "new_summary": "  Deep neural policies have recently been installed in a diverse range of\nsettings, from biotechnology to automated financial systems. However, the\nutilization of deep neural networks to approximate the value function leads to\nconcerns on the decision boundary stability, in particular, with regard to the\nsensitivity of policy decision making to indiscernible, non-robust features due\nto highly non-convex and complex deep neural manifolds. These concerns\nconstitute an obstruction to understanding the reasoning made by deep neural\npolicies, and their foundational limitations. Hence, it is crucial to develop\ntechniques that aim to understand the sensitivities in the learnt\nrepresentations of neural network policies. To achieve this we introduce a\ntheoretically founded method that provides a systematic analysis of the\nunstable directions in the deep neural policy decision boundary across both\ntime and space. Through experiments in the Arcade Learning Environment (ALE),\nwe demonstrate the effectiveness of our technique for identifying correlated\ndirections of instability, and for measuring how sample shifts remold the set\nof sensitive directions in the neural policy landscape. Most importantly, we\ndemonstrate that state-of-the-art robust training techniques yield learning of\ndisjoint unstable directions, with dramatically larger oscillations over time,\nwhen compared to standard training. We believe our results reveal the\nfundamental properties of the decision process made by reinforcement learning\npolicies, and can help in constructing reliable and robust deep neural\npolicies.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.16979v1.pdf",
        "similarity": 0.2975755594472834,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-23"
    },
    {
        "new_title": "SpiNNaker2: A Large-Scale Neuromorphic System for Event-Based and\n  Asynchronous Machine Learning",
        "new_link": "http://arxiv.org/abs/2401.04491v1",
        "new_summary": "  The joint progress of artificial neural networks (ANNs) and domain specific\nhardware accelerators such as GPUs and TPUs took over many domains of machine\nlearning research. This development is accompanied by a rapid growth of the\nrequired computational demands for larger models and more data. Concurrently,\nemerging properties of foundation models such as in-context learning drive new\nopportunities for machine learning applications. However, the computational\ncost of such applications is a limiting factor of the technology in data\ncenters, and more importantly in mobile devices and edge systems. To mediate\nthe energy footprint and non-trivial latency of contemporary systems,\nneuromorphic computing systems deeply integrate computational principles of\nneurobiological systems by leveraging low-power analog and digital\ntechnologies. SpiNNaker2 is a digital neuromorphic chip developed for scalable\nmachine learning. The event-based and asynchronous design of SpiNNaker2 allows\nthe composition of large-scale systems involving thousands of chips. This work\nfeatures the operating principles of SpiNNaker2 systems, outlining the\nprototype of novel machine learning applications. These applications range from\nANNs over bio-inspired spiking neural networks to generalized event-based\nneural networks. With the successful development and deployment of SpiNNaker2,\nwe aim to facilitate the advancement of event-based and asynchronous algorithms\nfor future generations of machine learning systems.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.04491v1.pdf",
        "similarity": 0.29757102057016854,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-09"
    },
    {
        "new_title": "Synthetic Information towards Maximum Posterior Ratio for deep learning\n  on Imbalanced Data",
        "new_link": "http://arxiv.org/abs/2401.02591v1",
        "new_summary": "  This study examines the impact of class-imbalanced data on deep learning\nmodels and proposes a technique for data balancing by generating synthetic data\nfor the minority class. Unlike random-based oversampling, our method\nprioritizes balancing the informative regions by identifying high entropy\nsamples. Generating well-placed synthetic data can enhance machine learning\nalgorithms accuracy and efficiency, whereas poorly-placed ones may lead to\nhigher misclassification rates. We introduce an algorithm that maximizes the\nprobability of generating a synthetic sample in the correct region of its class\nby optimizing the class posterior ratio. Additionally, to maintain data\ntopology, synthetic data are generated within each minority sample's\nneighborhood. Our experimental results on forty-one datasets demonstrate the\nsuperior performance of our technique in enhancing deep-learning models.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.02591v1.pdf",
        "similarity": 0.297157190368325,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-05"
    },
    {
        "new_title": "Five reasons against assuming a data-generating distribution in Machine\n  Learning",
        "new_link": "http://arxiv.org/abs/2407.17395v1",
        "new_summary": "  Machine Learning research, as most of Statistics, heavily relies on the\nconcept of a data-generating probability distribution. As data points are\nthought to be sampled from such a distribution, we can learn from observed data\nabout this distribution and, thus, predict future data points drawn from it\n(with some probability of success). Drawing on scholarship across disciplines,\nwe here argue that this framework is not always a good model. Not only do such\ntrue probability distributions not exist; the framework can also be misleading\nand obscure both the choices made and the goals pursued in machine learning\npractice. We suggest an alternative framework that focuses on finite\npopulations rather than abstract distributions; while classical learning theory\ncan be left almost unchanged, it opens new opportunities, especially to model\nsampling. We compile these considerations into five reasons for modelling\nmachine learning -- in some settings -- with finite distributions rather than\ngenerative distributions, both to be more faithful to practice and to provide\nnovel theoretical insights.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.17395v1.pdf",
        "similarity": 0.29702637817716265,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-24"
    },
    {
        "new_title": "Deep Limit Order Book Forecasting",
        "new_link": "http://arxiv.org/abs/2403.09267v4",
        "new_summary": "  We exploit cutting-edge deep learning methodologies to explore the\npredictability of high-frequency Limit Order Book mid-price changes for a\nheterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we\nrelease `LOBFrame', an open-source code base to efficiently process large-scale\nLimit Order Book data and quantitatively assess state-of-the-art deep learning\nmodels' forecasting capabilities. Our results are twofold. We demonstrate that\nthe stocks' microstructural characteristics influence the efficacy of deep\nlearning methods and that their high forecasting power does not necessarily\ncorrespond to actionable trading signals. We argue that traditional machine\nlearning metrics fail to adequately assess the quality of forecasts in the\nLimit Order Book context. As an alternative, we propose an innovative\noperational framework that evaluates predictions' practicality by focusing on\nthe probability of accurately forecasting complete transactions. This work\noffers academics and practitioners an avenue to make informed and robust\ndecisions on the application of deep learning techniques, their scope and\nlimitations, effectively exploiting emergent statistical properties of the\nLimit Order Book.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.09267v4.pdf",
        "similarity": 0.29702064472978157,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-14"
    },
    {
        "new_title": "How to deal with glare for improved perception of Autonomous Vehicles",
        "new_link": "http://arxiv.org/abs/2404.10992v1",
        "new_summary": "  Vision sensors are versatile and can capture a wide range of visual cues,\nsuch as color, texture, shape, and depth. This versatility, along with the\nrelatively inexpensive availability of machine vision cameras, played an\nimportant role in adopting vision-based environment perception systems in\nautonomous vehicles (AVs). However, vision-based perception systems can be\neasily affected by glare in the presence of a bright source of light, such as\nthe sun or the headlights of the oncoming vehicle at night or simply by light\nreflecting off snow or ice-covered surfaces; scenarios encountered frequently\nduring driving. In this paper, we investigate various glare reduction\ntechniques, including the proposed saturated pixel-aware glare reduction\ntechnique for improved performance of the computer vision (CV) tasks employed\nby the perception layer of AVs. We evaluate these glare reduction methods based\non various performance metrics of the CV algorithms used by the perception\nlayer. Specifically, we considered object detection, object recognition, object\ntracking, depth estimation, and lane detection which are crucial for autonomous\ndriving. The experimental findings validate the efficacy of the proposed glare\nreduction approach, showcasing enhanced performance across diverse perception\ntasks and remarkable resilience against varying levels of glare.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.10992v1.pdf",
        "similarity": 0.2970197080903706,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-17"
    },
    {
        "new_title": "Advancing Forest Fire Prevention: Deep Reinforcement Learning for\n  Effective Firebreak Placement",
        "new_link": "http://arxiv.org/abs/2404.08523v1",
        "new_summary": "  Over the past decades, the increase in both frequency and intensity of\nlarge-scale wildfires due to climate change has emerged as a significant\nnatural threat. The pressing need to design resilient landscapes capable of\nwithstanding such disasters has become paramount, requiring the development of\nadvanced decision-support tools. Existing methodologies, including Mixed\nInteger Programming, Stochastic Optimization, and Network Theory, have proven\neffective but are hindered by computational demands, limiting their\napplicability.\n  In response to this challenge, we propose using artificial intelligence\ntechniques, specifically Deep Reinforcement Learning, to address the complex\nproblem of firebreak placement in the landscape. We employ value-function based\napproaches like Deep Q-Learning, Double Deep Q-Learning, and Dueling Double\nDeep Q-Learning. Utilizing the Cell2Fire fire spread simulator combined with\nConvolutional Neural Networks, we have successfully implemented a computational\nagent capable of learning firebreak locations within a forest environment,\nachieving good results.\n  Furthermore, we incorporate a pre-training loop, initially teaching our agent\nto mimic a heuristic-based algorithm and observe that it consistently exceeds\nthe performance of these solutions. Our findings underscore the immense\npotential of Deep Reinforcement Learning for operational research challenges,\nespecially in fire prevention. Our approach demonstrates convergence with\nhighly favorable results in problem instances as large as 40 x 40 cells,\nmarking a significant milestone in applying Reinforcement Learning to this\ncritical issue.\n  To the best of our knowledge, this study represents a pioneering effort in\nusing Reinforcement Learning to address the aforementioned problem, offering\npromising perspectives in fire prevention and landscape management\n",
        "pdf_link": "https://arxiv.org/pdf/2404.08523v1.pdf",
        "similarity": 0.2967994402925893,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-12"
    },
    {
        "new_title": "Quantum Curriculum Learning",
        "new_link": "http://arxiv.org/abs/2407.02419v2",
        "new_summary": "  Quantum machine learning (QML) requires significant quantum resources to\nachieve quantum advantage. Research should prioritize both the efficient design\nof quantum architectures and the development of learning strategies to optimize\nresource usage. We propose a framework called quantum curriculum learning\n(Q-CurL) for quantum data, where the curriculum introduces simpler tasks or\ndata to the learning model before progressing to more challenging ones. We\ndefine the curriculum criteria based on the data density ratio between tasks to\ndetermine the curriculum order. We also implement a dynamic learning schedule\nto emphasize the significance of quantum data in optimizing the loss function.\nEmpirical evidence shows that Q-CurL significantly enhances the training\nconvergence and the generalization for unitary learning tasks and improves the\nrobustness of quantum phase recognition tasks. Our framework provides a general\nlearning strategy, bringing QML closer to realizing practical advantages.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.02419v2.pdf",
        "similarity": 0.2966534037889975,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-02"
    },
    {
        "new_title": "Overfitting In Contrastive Learning?",
        "new_link": "http://arxiv.org/abs/2407.15863v1",
        "new_summary": "  Overfitting describes a machine learning phenomenon where the model fits too\nclosely to the training data, resulting in poor generalization. While this\noccurrence is thoroughly documented for many forms of supervised learning, it\nis not well examined in the context of \\underline{un}supervised learning. In\nthis work we examine the nature of overfitting in unsupervised contrastive\nlearning. We show that overfitting can indeed occur and the mechanism behind\noverfitting.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.15863v1.pdf",
        "similarity": 0.2963688752038114,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-16"
    },
    {
        "new_title": "Three-layer deep learning network random trees for fault detection in\n  chemical production process",
        "new_link": "http://arxiv.org/abs/2405.00311v2",
        "new_summary": "  With the development of technology, the chemical production process is\nbecoming increasingly complex and large-scale, making fault detection\nparticularly important. However, current detective methods struggle to address\nthe complexities of large-scale production processes. In this paper, we\nintegrate the strengths of deep learning and machine learning technologies,\ncombining the advantages of bidirectional long and short-term memory neural\nnetworks, fully connected neural networks, and the extra trees algorithm to\npropose a novel fault detection model named three-layer deep learning network\nrandom trees (TDLN-trees). First, the deep learning component extracts temporal\nfeatures from industrial data, combining and transforming them into a\nhigher-level data representation. Second, the machine learning component\nprocesses and classifies the features extracted in the first step. An\nexperimental analysis based on the Tennessee Eastman process verifies the\nsuperiority of the proposed method.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.00311v2.pdf",
        "similarity": 0.2962548621080289,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-01"
    },
    {
        "new_title": "On Measuring Calibration of Discrete Probabilistic Neural Networks",
        "new_link": "http://arxiv.org/abs/2405.12412v1",
        "new_summary": "  As machine learning systems become increasingly integrated into real-world\napplications, accurately representing uncertainty is crucial for enhancing\ntheir safety, robustness, and reliability. Training neural networks to fit\nhigh-dimensional probability distributions via maximum likelihood has become an\neffective method for uncertainty quantification. However, such models often\nexhibit poor calibration, leading to overconfident predictions. Traditional\nmetrics like Expected Calibration Error (ECE) and Negative Log Likelihood (NLL)\nhave limitations, including biases and parametric assumptions. This paper\nproposes a new approach using conditional kernel mean embeddings to measure\ncalibration discrepancies without these biases and assumptions. Preliminary\nexperiments on synthetic data demonstrate the method's potential, with future\nwork planned for more complex applications.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.12412v1.pdf",
        "similarity": 0.29618328765839735,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-20"
    },
    {
        "new_title": "Neural-network density functional theory based on variational energy\n  minimization",
        "new_link": "http://arxiv.org/abs/2403.11287v2",
        "new_summary": "  Deep-learning density functional theory (DFT) shows great promise to\nsignificantly accelerate material discovery and potentially revolutionize\nmaterials research. However, current research in this field primarily relies on\ndata-driven supervised learning, making the developments of neural networks and\nDFT isolated from each other. In this work, we present a theoretical framework\nof neural-network DFT, which unifies the optimization of neural networks with\nthe variational computation of DFT, enabling physics-informed unsupervised\nlearning. Moreover, we develop a differential DFT code incorporated with\ndeep-learning DFT Hamiltonian, and introduce algorithms of automatic\ndifferentiation and backpropagation into DFT, demonstrating the capability of\nneural-network DFT. The physics-informed neural-network architecture not only\nsurpasses conventional approaches in accuracy and efficiency, but also offers a\nnew paradigm for developing deep-learning DFT methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.11287v2.pdf",
        "similarity": 0.2957413189955738,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-17"
    },
    {
        "new_title": "Variational Learning ISTA",
        "new_link": "http://arxiv.org/abs/2407.06646v1",
        "new_summary": "  Compressed sensing combines the power of convex optimization techniques with\na sparsity-inducing prior on the signal space to solve an underdetermined\nsystem of equations. For many problems, the sparsifying dictionary is not\ndirectly given, nor its existence can be assumed. Besides, the sensing matrix\ncan change across different scenarios. Addressing these issues requires solving\na sparse representation learning problem, namely dictionary learning, taking\ninto account the epistemic uncertainty of the learned dictionaries and,\nfinally, jointly learning sparse representations and reconstructions under\nvarying sensing matrix conditions. We address both concerns by proposing a\nvariant of the LISTA architecture. First, we introduce Augmented Dictionary\nLearning ISTA (A-DLISTA), which incorporates an augmentation module to adapt\nparameters to the current measurement setup. Then, we propose to learn a\ndistribution over dictionaries via a variational approach, dubbed Variational\nLearning ISTA (VLISTA). VLISTA exploits A-DLISTA as the likelihood model and\napproximates a posterior distribution over the dictionaries as part of an\nunfolded LISTA-based recovery algorithm. As a result, VLISTA provides a\nprobabilistic way to jointly learn the dictionary distribution and the\nreconstruction algorithm with varying sensing matrices. We provide theoretical\nand experimental support for our architecture and show that our model learns\ncalibrated uncertainties.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.06646v1.pdf",
        "similarity": 0.29568568132668793,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-09"
    },
    {
        "new_title": "Feature-based Federated Transfer Learning: Communication Efficiency,\n  Robustness and Privacy",
        "new_link": "http://arxiv.org/abs/2405.09014v1",
        "new_summary": "  In this paper, we propose feature-based federated transfer learning as a\nnovel approach to improve communication efficiency by reducing the uplink\npayload by multiple orders of magnitude compared to that of existing approaches\nin federated learning and federated transfer learning. Specifically, in the\nproposed feature-based federated learning, we design the extracted features and\noutputs to be uploaded instead of parameter updates. For this distributed\nlearning model, we determine the required payload and provide comparisons with\nthe existing schemes. Subsequently, we analyze the robustness of feature-based\nfederated transfer learning against packet loss, data insufficiency, and\nquantization. Finally, we address privacy considerations by defining and\nanalyzing label privacy leakage and feature privacy leakage, and investigating\nmitigating approaches. For all aforementioned analyses, we evaluate the\nperformance of the proposed learning scheme via experiments on an image\nclassification task and a natural language processing task to demonstrate its\neffectiveness.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.09014v1.pdf",
        "similarity": 0.29541491158394145,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-15"
    },
    {
        "new_title": "Machine learning based state observer for discrete time systems evolving\n  on Lie groups",
        "new_link": "http://arxiv.org/abs/2401.11196v1",
        "new_summary": "  In this paper, a machine learning based observer for systems evolving on\nmanifolds is designed such that the state of the observer is restricted to the\nLie group on which the system evolves. Conventional techniques involving\nmachine learning based observers on systems evolving on Lie groups involve\ndesigning charts for the Lie group, training a machine learning based observer\nfor each chart, and switching between the trained models based on the state of\nthe system. We propose a novel deep learning based technique whose predictions\nare restricted to a measure 0 subset of Euclidean space without using charts.\nUsing this network, we design an observer ensuring that the state of the\nobserver is restricted to the Lie group, and predicting the state using only\none trained algorithm. The deep learning network predicts an ``error term'' on\nthe Lie algebra of the Lie group, uses the map from the Lie algebra to the\ngroup, and uses the group action and the present state to estimate the state at\nthe next epoch. This model being purely data driven does not require the model\nof the system. The proposed algorithm provides a novel framework for\nconstraining the output of machine learning networks to a measure 0 subset of a\nEuclidean space without chart specific training and without requiring\nswitching. We show the validity of this method using Monte Carlo simulations\nperformed of the rigid body rotation and translation system.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.11196v1.pdf",
        "similarity": 0.29524204078049915,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-20"
    },
    {
        "new_title": "Exploring galactic properties with machine learning Predicting star\n  formation, stellar mass, and metallicity from photometric data",
        "new_link": "http://arxiv.org/abs/2405.15566v1",
        "new_summary": "  Aims. We explore machine learning techniques to forecast star formation rate,\nstellar mass, and metallicity across galaxies with redshifts ranging from 0.01\nto 0.3.\n  Methods. Leveraging CatBoost and deep learning architectures, we utilize\nmultiband optical and infrared photometric data from SDSS and AllWISE, trained\non the SDSS MPA-JHU DR8 catalogue.\n  Results. Our study demonstrates the potential of machine learning in\naccurately predicting galaxy properties solely from photometric data. We\nachieve minimised root mean square errors, specifically employing the CatBoost\nmodel. For star formation rate prediction, we attain a value of RMSESFR = 0.336\ndex, while for stellar mass prediction, the error is reduced to RMSESM = 0.206\ndex. Additionally, our model yields a metallicity prediction of RMSEmetallicity\n= 0.097 dex.\n  Conclusions. These findings underscore the significance of automated\nmethodologies in efficiently estimating critical galaxy properties, amid the\nexponential growth of multi-wavelength astronomy data. Future research may\nfocus on refining machine learning models and expanding datasets for even more\naccurate predictions.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.15566v1.pdf",
        "similarity": 0.29495844925834286,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-24"
    },
    {
        "new_title": "Automated Machine Learning for Positive-Unlabelled Learning",
        "new_link": "http://arxiv.org/abs/2401.06452v1",
        "new_summary": "  Positive-Unlabelled (PU) learning is a growing field of machine learning that\naims to learn classifiers from data consisting of labelled positive and\nunlabelled instances, which can be in reality positive or negative, but whose\nlabel is unknown. An extensive number of methods have been proposed to address\nPU learning over the last two decades, so many so that selecting an optimal\nmethod for a given PU learning task presents a challenge. Our previous work has\naddressed this by proposing GA-Auto-PU, the first Automated Machine Learning\n(Auto-ML) system for PU learning. In this work, we propose two new Auto-ML\nsystems for PU learning: BO-Auto-PU, based on a Bayesian Optimisation approach,\nand EBO-Auto-PU, based on a novel evolutionary/Bayesian optimisation approach.\nWe also present an extensive evaluation of the three Auto-ML systems, comparing\nthem to each other and to well-established PU learning methods across 60\ndatasets (20 real-world datasets, each with 3 versions in terms of PU learning\ncharacteristics).\n",
        "pdf_link": "https://arxiv.org/pdf/2401.06452v1.pdf",
        "similarity": 0.29490746239547994,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-12"
    },
    {
        "new_title": "Bridging Domains with Approximately Shared Features",
        "new_link": "http://arxiv.org/abs/2403.06424v1",
        "new_summary": "  Multi-source domain adaptation aims to reduce performance degradation when\napplying machine learning models to unseen domains. A fundamental challenge is\ndevising the optimal strategy for feature selection. Existing literature is\nsomewhat paradoxical: some advocate for learning invariant features from source\ndomains, while others favor more diverse features. To address the challenge, we\npropose a statistical framework that distinguishes the utilities of features\nbased on the variance of their correlation to label $y$ across domains. Under\nour framework, we design and analyze a learning procedure consisting of\nlearning approximately shared feature representation from source tasks and\nfine-tuning it on the target task. Our theoretical analysis necessitates the\nimportance of learning approximately shared features instead of only the\nstrictly invariant features and yields an improved population risk compared to\nprevious results on both source and target tasks, thus partly resolving the\nparadox mentioned above. Inspired by our theory, we proposed a more practical\nway to isolate the content (invariant+approximately shared) from environmental\nfeatures and further consolidate our theoretical findings.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.06424v1.pdf",
        "similarity": 0.2947465698730637,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-11"
    },
    {
        "new_title": "A Metric-based Principal Curve Approach for Learning One-dimensional\n  Manifold",
        "new_link": "http://arxiv.org/abs/2405.12390v1",
        "new_summary": "  Principal curve is a well-known statistical method oriented in manifold\nlearning using concepts from differential geometry. In this paper, we propose a\nnovel metric-based principal curve (MPC) method that learns one-dimensional\nmanifold of spatial data. Synthetic datasets Real applications using MNIST\ndataset show that our method can learn the one-dimensional manifold well in\nterms of the shape.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.12390v1.pdf",
        "similarity": 0.2945957826193651,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-20"
    },
    {
        "new_title": "Review of deep learning models for crypto price prediction:\n  implementation and evaluation",
        "new_link": "http://arxiv.org/abs/2405.11431v2",
        "new_summary": "  There has been much interest in accurate cryptocurrency price forecast models\nby investors and researchers. Deep Learning models are prominent machine\nlearning techniques that have transformed various fields and have shown\npotential for finance and economics. Although various deep learning models have\nbeen explored for cryptocurrency price forecasting, it is not clear which\nmodels are suitable due to high market volatility. In this study, we review the\nliterature about deep learning for cryptocurrency price forecasting and\nevaluate novel deep learning models for cryptocurrency stock price prediction.\nOur deep learning models include variants of long short-term memory (LSTM)\nrecurrent neural networks, variants of convolutional neural networks (CNNs),\nand the Transformer model. We evaluate univariate and multivariate approaches\nfor multi-step ahead predicting of cryptocurrencies close-price. We also carry\nout volatility analysis on the four cryptocurrencies which reveals significant\nfluctuations in their prices throughout the COVID-19 pandemic. Additionally, we\ninvestigate the prediction accuracy of two scenarios identified by different\ntraining sets for the models. First, we use the pre-COVID-19 datasets to model\ncryptocurrency close-price forecasting during the early period of COVID-19.\nSecondly, we utilise data from the COVID-19 period to predict prices for 2023\nto 2024. Our results show that the convolutional LSTM with a multivariate\napproach provides the best prediction accuracy in two major experimental\nsettings.\n  Our results also indicate that the multivariate deep learning models exhibit\nbetter performance in forecasting four different cryptocurrencies when compared\nto the univariate models.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.11431v2.pdf",
        "similarity": 0.29450168881368116,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-19"
    },
    {
        "new_title": "Offline Imitation Learning from Multiple Baselines with Applications to\n  Compiler Optimization",
        "new_link": "http://arxiv.org/abs/2403.19462v1",
        "new_summary": "  This work studies a Reinforcement Learning (RL) problem in which we are given\na set of trajectories collected with K baseline policies. Each of these\npolicies can be quite suboptimal in isolation, and have strong performance in\ncomplementary parts of the state space. The goal is to learn a policy which\nperforms as well as the best combination of baselines on the entire state\nspace. We propose a simple imitation learning based algorithm, show a sample\ncomplexity bound on its accuracy and prove that the the algorithm is minimax\noptimal by showing a matching lower bound. Further, we apply the algorithm in\nthe setting of machine learning guided compiler optimization to learn policies\nfor inlining programs with the objective of creating a small binary. We\ndemonstrate that we can learn a policy that outperforms an initial policy\nlearned via standard RL through a few iterations of our approach.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.19462v1.pdf",
        "similarity": 0.29446655297964475,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-28"
    },
    {
        "new_title": "Machine Learning for Prediction of Unitarity and Bounded from Below\n  Constraints",
        "new_link": "http://arxiv.org/abs/2401.09130v1",
        "new_summary": "  The machine learning (ML) techniques to predict unitarity (UNI) and bounded\nfrom below (BFB) constraints in multi-scalar models is employed. The\neffectiveness of this approach is demonstrated by applying it to the two and\nthree Higgs doublet models, as well as the left-right model. By employing\nsuitable neural network architectures, learning algorithms, and carefully\ncurated training datasets, a significantly high level of predictivity is\nachieved. Machine learning offers a distinct advantage by enabling faster\ncalculations compared to alternative numerical methods, such as scalar\npotential minimization. This research investigates the feasibility of utilizing\nmachine learning techniques as an alternative for predicting these constraints,\noffering potential improvements over traditional numerical calculations.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.09130v1.pdf",
        "similarity": 0.29416550499360006,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-17"
    },
    {
        "new_title": "Nature-Inspired Local Propagation",
        "new_link": "http://arxiv.org/abs/2402.05959v1",
        "new_summary": "  The spectacular results achieved in machine learning, including the recent\nadvances in generative AI, rely on large data collections. On the opposite,\nintelligent processes in nature arises without the need for such collections,\nbut simply by online processing of the environmental information. In\nparticular, natural learning processes rely on mechanisms where data\nrepresentation and learning are intertwined in such a way to respect\nspatiotemporal locality. This paper shows that such a feature arises from a\npre-algorithmic view of learning that is inspired by related studies in\nTheoretical Physics. We show that the algorithmic interpretation of the derived\n\"laws of learning\", which takes the structure of Hamiltonian equations, reduces\nto Backpropagation when the speed of propagation goes to infinity. This opens\nthe doors to machine learning studies based on full on-line information\nprocessing that are based the replacement of Backpropagation with the proposed\nspatiotemporal local algorithm.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.05959v1.pdf",
        "similarity": 0.294145440474393,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-04"
    },
    {
        "new_title": "Data-driven nonlinear turbulent flow scaling with Buckingham Pi\n  variables",
        "new_link": "http://arxiv.org/abs/2402.17990v2",
        "new_summary": "  Nonlinear machine learning for turbulent flows can exhibit robust performance\neven outside the range of training data. This is achieved when machine-learning\nmodels can accommodate scale-invariant characteristics of turbulent flow\nstructures. This study presents a data-driven approach to reveal\nscale-invariant vortical structures across Reynolds numbers that provide\ninsights for supporting nonlinear machine-learning-based studies of turbulent\nflows. To uncover conditions for which nonlinear models are likely to perform\nwell, we use a Buckingham Pi-based sparse nonlinear scaling to find the\ninfluence of the Pi groups on the turbulent flow data. We consider nonlinear\nscalings of the invariants of the velocity gradient tensor for an example of\nthree-dimensional decaying isotropic turbulence. The present scaling not only\nenables the identification of vortical structures that are interpolatory and\nextrapolatory for the given flow field data but also captures non-equilibrium\neffects of the energy cascade. As a demonstration, the present findings are\napplied to machine-learning-based super-resolution analysis of\nthree-dimensional isotropic turbulence. We show that machine-learning models\nreconstruct vortical structures well in the interpolatory space with reduced\nperformance in the extrapolatory space revealed by the nonlinearly scaled\ninvariants. The present approach enables us to depart from labeling turbulent\nflow data with a single parameter of Reynolds number and comprehensively\nexamine the flow field to support training and testing of nonlinear\nmachine-learning techniques.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.17990v2.pdf",
        "similarity": 0.2935368676503762,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-28"
    },
    {
        "new_title": "Exploring Challenges in Deep Learning of Single-Station Ground Motion\n  Records",
        "new_link": "http://arxiv.org/abs/2403.07569v1",
        "new_summary": "  Contemporary deep learning models have demonstrated promising results across\nvarious applications within seismology and earthquake engineering. These models\nrely primarily on utilizing ground motion records for tasks such as earthquake\nevent classification, localization, earthquake early warning systems, and\nstructural health monitoring. However, the extent to which these models\neffectively learn from these complex time-series signals has not been\nthoroughly analyzed. In this study, our objective is to evaluate the degree to\nwhich auxiliary information, such as seismic phase arrival times or seismic\nstation distribution within a network, dominates the process of deep learning\nfrom ground motion records, potentially hindering its effectiveness. We perform\na hyperparameter search on two deep learning models to assess their\neffectiveness in deep learning from ground motion records while also examining\nthe impact of auxiliary information on model performance. Experimental results\nreveal a strong reliance on the highly correlated P and S phase arrival\ninformation. Our observations highlight a potential gap in the field,\nindicating an absence of robust methodologies for deep learning of\nsingle-station ground motion recordings independent of any auxiliary\ninformation.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.07569v1.pdf",
        "similarity": 0.2933882266640885,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-12"
    },
    {
        "new_title": "A Hybrid Deep Learning Classification of Perimetric Glaucoma Using\n  Peripapillary Nerve Fiber Layer Reflectance and Other OCT Parameters from\n  Three Anatomy Regions",
        "new_link": "http://arxiv.org/abs/2406.03663v1",
        "new_summary": "  Precis: A hybrid deep-learning model combines NFL reflectance and other OCT\nparameters to improve glaucoma diagnosis. Objective: To investigate if a deep\nlearning model could be used to combine nerve fiber layer (NFL) reflectance and\nother OCT parameters for glaucoma diagnosis. Patients and Methods: This is a\nprospective observational study where of 106 normal subjects and 164 perimetric\nglaucoma (PG) patients. Peripapillary NFL reflectance map, NFL thickness map,\noptic head analysis of disc, and macular ganglion cell complex thickness were\nobtained using spectral domain OCT. A hybrid deep learning model combined a\nfully connected network (FCN) and a convolution neural network (CNN) to develop\nand combine those OCT maps and parameters to distinguish normal and PG eyes.\nTwo deep learning models were compared based on whether the NFL reflectance map\nwas used as part of the input or not. Results: The hybrid deep learning model\nwith reflectance achieved 0.909 sensitivity at 99% specificity and 0.926 at\n95%. The overall accuracy was 0.948 with 0.893 sensitivity and 1.000\nspecificity, and the AROC was 0.979, which is significantly better than the\nlogistic regression models (p < 0.001). The second best model is the hybrid\ndeep learning model w/o reflectance, which also had significantly higher AROC\nthan logistic regression models (p < 0.001). Logistic regression with\nreflectance model had slightly higher AROC or sensitivity than the other\nlogistic regression model without reflectance (p = 0.024). Conclusions: Hybrid\ndeep learning model significantly improved the diagnostic accuracy, without or\nwithout NFL reflectance. Hybrid deep learning model, combining reflectance/NFL\nthickness/GCC thickness/ONH parameter, may be a practical model for glaucoma\nscreen purposes.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.03663v1.pdf",
        "similarity": 0.29333279732462986,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-06"
    },
    {
        "new_title": "Language processing in humans and computers",
        "new_link": "http://arxiv.org/abs/2405.14233v1",
        "new_summary": "  Machine-learned language models have transformed everyday life: they steer us\nwhen we study, drive, manage money. They have the potential to transform our\ncivilization. But they hallucinate. Their realities are virtual. This note\nprovides a high-level overview of language models and outlines a low-level\nmodel of learning machines. It turns out that, after they become capable of\nrecognizing hallucinations and dreaming safely, as humans tend to be, the\nlanguage-learning machines proceed to generate broader systems of false beliefs\nand self-confirming theories, as humans tend to do.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.14233v1.pdf",
        "similarity": 0.2931872641519175,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-23"
    },
    {
        "new_title": "Causal hybrid modeling with double machine learning",
        "new_link": "http://arxiv.org/abs/2402.13332v2",
        "new_summary": "  Hybrid modeling integrates machine learning with scientific knowledge to\nenhance interpretability, generalization, and adherence to natural laws.\nNevertheless, equifinality and regularization biases pose challenges in hybrid\nmodeling to achieve these purposes. This paper introduces a novel approach to\nestimating hybrid models via a causal inference framework, specifically\nemploying Double Machine Learning (DML) to estimate causal effects. We showcase\nits use for the Earth sciences on two problems related to carbon dioxide\nfluxes. In the $Q_{10}$ model, we demonstrate that DML-based hybrid modeling is\nsuperior in estimating causal parameters over end-to-end deep neural network\n(DNN) approaches, proving efficiency, robustness to bias from regularization\nmethods, and circumventing equifinality. Our approach, applied to carbon flux\npartitioning, exhibits flexibility in accommodating heterogeneous causal\neffects. The study emphasizes the necessity of explicitly defining causal\ngraphs and relationships, advocating for this as a general best practice. We\nencourage the continued exploration of causality in hybrid models for more\ninterpretable and trustworthy results in knowledge-guided machine learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.13332v2.pdf",
        "similarity": 0.2929945760763402,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-20"
    },
    {
        "new_title": "Introduction to speech recognition",
        "new_link": "http://arxiv.org/abs/2402.01778v1",
        "new_summary": "  This document contains lectures and practical experimentations using Matlab\nand implementing a system which is actually correctly classifying three words\n(one, two and three) with the help of a very small database. To achieve this\nperformance, it uses speech modeling specificities, powerful computer\nalgorithms (dynamic time warping and Dijktra's algorithm) and machine learning\n(nearest neighbor). This document introduces also some machine learning\nevaluation metrics.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.01778v1.pdf",
        "similarity": 0.2929897250947704,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-01"
    },
    {
        "new_title": "Insights into Deep Learning Refactoring: Bridging the Gap Between\n  Practices and Expectations",
        "new_link": "http://arxiv.org/abs/2405.04861v1",
        "new_summary": "  With the rapid development of deep learning, the implementation of intricate\nalgorithms and substantial data processing have become standard elements of\ndeep learning projects. As a result, the code has become progressively complex\nas the software evolves, which is difficult to maintain and understand.\nExisting studies have investigated the impact of refactoring on software\nquality within traditional software. However, the insight of code refactoring\nin the context of deep learning is still unclear. This study endeavors to fill\nthis knowledge gap by empirically examining the current state of code\nrefactoring in deep learning realm, and practitioners' views on refactoring. We\nfirst manually analyzed the commit history of five popular and well-maintained\ndeep learning projects (e.g., PyTorch). We mined 4,921 refactoring practices in\nhistorical commits and measured how different types and elements of refactoring\noperations are distributed and found that refactoring operation types'\ndistribution in deep learning projects is different from it in traditional Java\nsoftware. We then surveyed 159 practitioners about their views of code\nrefactoring in deep learning projects and their expectations of current\nrefactoring tools. The result of the survey showed that refactoring research\nand the development of related tools in the field of deep learning are crucial\nfor improving project maintainability and code quality, and that current\nrefactoring tools do not adequately meet the needs of practitioners. Lastly, we\nprovided our perspective on the future advancement of refactoring tools and\noffered suggestions for developers' development practices.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.04861v1.pdf",
        "similarity": 0.29287511814865663,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-08"
    },
    {
        "new_title": "On the impact of measure pre-conditionings on general parametric ML\n  models and transfer learning via domain adaptation",
        "new_link": "http://arxiv.org/abs/2403.02432v1",
        "new_summary": "  We study a new technique for understanding convergence of learning agents\nunder small modifications of data. We show that such convergence can be\nunderstood via an analogue of Fatou's lemma which yields gamma-convergence. We\nshow it's relevance and applications in general machine learning tasks and\ndomain adaptation transfer learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.02432v1.pdf",
        "similarity": 0.2927830989311796,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-04"
    },
    {
        "new_title": "Deep Evidential Learning for Dose Prediction",
        "new_link": "http://arxiv.org/abs/2404.17126v1",
        "new_summary": "  In this work, we present a novel application of an uncertainty-quantification\nframework called Deep Evidential Learning in the domain of radiotherapy dose\nprediction. Using medical images of the Open Knowledge-Based Planning Challenge\ndataset, we found that this model can be effectively harnessed to yield\nuncertainty estimates that inherited correlations with prediction errors upon\ncompletion of network training. This was achieved only after reformulating the\noriginal loss function for a stable implementation. We found that (i)epistemic\nuncertainty was highly correlated with prediction errors, with various\nassociation indices comparable or stronger than those for Monte-Carlo Dropout\nand Deep Ensemble methods, (ii)the median error varied with uncertainty\nthreshold much more linearly for epistemic uncertainty in Deep Evidential\nLearning relative to these other two conventional frameworks, indicative of a\nmore uniformly calibrated sensitivity to model errors, (iii)relative to\nepistemic uncertainty, aleatoric uncertainty demonstrated a more significant\nshift in its distribution in response to Gaussian noise added to CT intensity,\ncompatible with its interpretation as reflecting data noise. Collectively, our\nresults suggest that Deep Evidential Learning is a promising approach that can\nendow deep-learning models in radiotherapy dose prediction with statistical\nrobustness. Towards enhancing its clinical relevance, we demonstrate how we can\nuse such a model to construct the predicted Dose-Volume-Histograms' confidence\nintervals.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.17126v1.pdf",
        "similarity": 0.29272198603974997,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-26"
    },
    {
        "new_title": "Multi-model learning by sequential reading of untrimmed videos for\n  action recognition",
        "new_link": "http://arxiv.org/abs/2401.14675v1",
        "new_summary": "  We propose a new method for learning videos by aggregating multiple models by\nsequentially extracting video clips from untrimmed video. The proposed method\nreduces the correlation between clips by feeding clips to multiple models in\nturn and synchronizes these models through federated learning. Experimental\nresults show that the proposed method improves the performance compared to the\nno synchronization.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.14675v1.pdf",
        "similarity": 0.29261701148179114,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-26"
    },
    {
        "new_title": "Machine learning-enabled exploration of mesoscale architectures in\n  amphiphilic-molecule self-assembly",
        "new_link": "http://arxiv.org/abs/2402.19019v2",
        "new_summary": "  Amphiphilic molecules spontaneously form self-assembled structures of various\nshapes depending on their molecular structures, the temperature, and other\nphysical conditions. The functionalities of these structures are dictated by\ntheir formations and their properties must be evaluated for reproduction using\nmolecular simulations. However, the assessment of such intricate structures\ninvolves many procedural steps. This study investigates the potential of\nmachine-learning models to extract structural features from mesoscale\nnon-ordered self-assembled structures, and suggests a methodology in which\nmachine-learning models for the structural analysis of self-assembled\nstructures are trained on particle types and coordinate data. In the proposed\napproach, graph neural networks are utilised to extract local structural data\nfor analysis. In simulations using several hundred self-assembled structures of\nup to 4050 coarse-grained particles, local structures are successfully\nextracted and classified with up to 78.35 % accuracy. As the machine-learning\nmodels learn structural characteristics without the need for human-made feature\nengineering, the proposed method has important potential applications in the\nfield of materials science.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.19019v2.pdf",
        "similarity": 0.292471691176869,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-29"
    },
    {
        "new_title": "Tabular and Deep Reinforcement Learning for Gittins Index",
        "new_link": "http://arxiv.org/abs/2405.01157v1",
        "new_summary": "  In the realm of multi-arm bandit problems, the Gittins index policy is known\nto be optimal in maximizing the expected total discounted reward obtained from\npulling the Markovian arms. In most realistic scenarios however, the Markovian\nstate transition probabilities are unknown and therefore the Gittins indices\ncannot be computed. One can then resort to reinforcement learning (RL)\nalgorithms that explore the state space to learn these indices while exploiting\nto maximize the reward collected. In this work, we propose tabular (QGI) and\nDeep RL (DGN) algorithms for learning the Gittins index that are based on the\nretirement formulation for the multi-arm bandit problem. When compared with\nexisting RL algorithms that learn the Gittins index, our algorithms have a\nlower run time, require less storage space (small Q-table size in QGI and\nsmaller replay buffer in DGN), and illustrate better empirical convergence to\nthe Gittins index. This makes our algorithm well suited for problems with large\nstate spaces and is a viable alternative to existing methods. As a key\napplication, we demonstrate the use of our algorithms in minimizing the mean\nflowtime in a job scheduling problem when jobs are available in batches and\nhave an unknown service time distribution. \\\n",
        "pdf_link": "https://arxiv.org/pdf/2405.01157v1.pdf",
        "similarity": 0.2923535502177745,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-02"
    },
    {
        "new_title": "A Two-Scale Complexity Measure for Deep Learning Models",
        "new_link": "http://arxiv.org/abs/2401.09184v1",
        "new_summary": "  We introduce a novel capacity measure 2sED for statistical models based on\nthe effective dimension. The new quantity provably bounds the generalization\nerror under mild assumptions on the model. Furthermore, simulations on standard\ndata sets and popular model architectures show that 2sED correlates well with\nthe training error. For Markovian models, we show how to efficiently\napproximate 2sED from below through a layerwise iterative approach, which\nallows us to tackle deep learning models with a large number of parameters.\nSimulation results suggest that the approximation is good for different\nprominent models and data sets.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.09184v1.pdf",
        "similarity": 0.29229782441042323,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-17"
    },
    {
        "new_title": "A Survey Analyzing Generalization in Deep Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2401.02349v1",
        "new_summary": "  Reinforcement learning research obtained significant success and attention\nwith the utilization of deep neural networks to solve problems in high\ndimensional state or action spaces. While deep reinforcement learning policies\nare currently being deployed in many different fields from medical applications\nto self driving vehicles, there are still ongoing questions the field is trying\nto answer on the generalization capabilities of deep reinforcement learning\npolicies. In this paper, we will outline the fundamental reasons why deep\nreinforcement learning policies encounter overfitting problems that limit their\nrobustness and generalization capabilities. Furthermore, we will formalize and\nunify the diverse solution approaches to increase generalization, and overcome\noverfitting in state-action value functions. We believe our study can provide a\ncompact systematic unified analysis for the current advancements in deep\nreinforcement learning, and help to construct robust deep neural policies with\nimproved generalization abilities.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.02349v1.pdf",
        "similarity": 0.2922380217229589,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-04"
    },
    {
        "new_title": "Stochastic weight matrix dynamics during learning and Dyson Brownian\n  motion",
        "new_link": "http://arxiv.org/abs/2407.16427v1",
        "new_summary": "  We demonstrate that the update of weight matrices in learning algorithms can\nbe described in the framework of Dyson Brownian motion, thereby inheriting many\nfeatures of random matrix theory. We relate the level of stochasticity to the\nratio of the learning rate and the mini-batch size, providing more robust\nevidence to a previously conjectured scaling relationship. We discuss universal\nand non-universal features in the resulting Coulomb gas distribution and\nidentify the Wigner surmise and Wigner semicircle explicitly in a\nteacher-student model and in the (near-)solvable case of the Gaussian\nrestricted Boltzmann machine.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.16427v1.pdf",
        "similarity": 0.29204195148621964,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-23"
    },
    {
        "new_title": "Practical multi-fidelity machine learning: fusion of deterministic and\n  Bayesian models",
        "new_link": "http://arxiv.org/abs/2407.15110v1",
        "new_summary": "  Multi-fidelity machine learning methods address the accuracy-efficiency\ntrade-off by integrating scarce, resource-intensive high-fidelity data with\nabundant but less accurate low-fidelity data. We propose a practical\nmulti-fidelity strategy for problems spanning low- and high-dimensional\ndomains, integrating a non-probabilistic regression model for the low-fidelity\nwith a Bayesian model for the high-fidelity. The models are trained in a\nstaggered scheme, where the low-fidelity model is transfer-learned to the\nhigh-fidelity data and a Bayesian model is trained for the residual. This\nthree-model strategy -- deterministic low-fidelity, transfer learning, and\nBayesian residual -- leads to a prediction that includes uncertainty\nquantification both for noisy and noiseless multi-fidelity data. The strategy\nis general and unifies the topic, highlighting the expressivity trade-off\nbetween the transfer-learning and Bayesian models (a complex transfer-learning\nmodel leads to a simpler Bayesian model, and vice versa). We propose modeling\nchoices for two scenarios, and argue in favor of using a linear\ntransfer-learning model that fuses 1) kernel ridge regression for low-fidelity\nwith Gaussian processes for high-fidelity; or 2) deep neural network for\nlow-fidelity with a Bayesian neural network for high-fidelity. We demonstrate\nthe effectiveness and efficiency of the proposed strategies and contrast them\nwith the state-of-the-art based on various numerical examples. The simplicity\nof these formulations makes them practical for a broad scope of future\nengineering applications.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.15110v1.pdf",
        "similarity": 0.29198334986277696,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-21"
    },
    {
        "new_title": "Scheduling Drone and Mobile Charger via Hybrid-Action Deep Reinforcement\n  Learning",
        "new_link": "http://arxiv.org/abs/2403.10761v1",
        "new_summary": "  Recently there has been a growing interest in industry and academia,\nregarding the use of wireless chargers to prolong the operational longevity of\nunmanned aerial vehicles (commonly knowns as drones). In this paper we consider\na charger-assisted drone application: a drone is deployed to observe a set\npoints of interest, while a charger can move to recharge the drone's battery.\nWe focus on the route and charging schedule of the drone and the mobile\ncharger, to obtain high observation utility with the shortest possible time,\nwhile ensuring the drone remains operational during task execution.\nEssentially, this proposed drone-charger scheduling problem is a multi-stage\ndecision-making process, in which the drone and the mobile charger act as two\nagents who cooperate to finish a task. The discrete-continuous hybrid action\nspace of the two agents poses a significant challenge in our problem. To\naddress this issue, we present a hybrid-action deep reinforcement learning\nframework, called HaDMC, which uses a standard policy learning algorithm to\ngenerate latent continuous actions. Motivated by representation learning, we\nspecifically design and train an action decoder. It involves two pipelines to\nconvert the latent continuous actions into original discrete and continuous\nactions, by which the drone and the charger can directly interact with\nenvironment. We embed a mutual learning scheme in model training, emphasizing\nthe collaborative rather than individual actions. We conduct extensive\nnumerical experiments to evaluate HaDMC and compare it with state-of-the-art\ndeep reinforcement learning approaches. The experimental results show the\neffectiveness and efficiency of our solution.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.10761v1.pdf",
        "similarity": 0.29195613744809845,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-16"
    },
    {
        "new_title": "Feature learning in finite-width Bayesian deep linear networks with\n  multiple outputs and convolutional layers",
        "new_link": "http://arxiv.org/abs/2406.03260v1",
        "new_summary": "  Deep linear networks have been extensively studied, as they provide\nsimplified models of deep learning. However, little is known in the case of\nfinite-width architectures with multiple outputs and convolutional layers. In\nthis manuscript, we provide rigorous results for the statistics of functions\nimplemented by the aforementioned class of networks, thus moving closer to a\ncomplete characterization of feature learning in the Bayesian setting. Our\nresults include: (i) an exact and elementary non-asymptotic integral\nrepresentation for the joint prior distribution over the outputs, given in\nterms of a mixture of Gaussians; (ii) an analytical formula for the posterior\ndistribution in the case of squared error loss function (Gaussian likelihood);\n(iii) a quantitative description of the feature learning infinite-width regime,\nusing large deviation theory. From a physical perspective, deep architectures\nwith multiple outputs or convolutional layers represent different\nmanifestations of kernel shape renormalization, and our work provides a\ndictionary that translates this physics intuition and terminology into rigorous\nBayesian statistics.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.03260v1.pdf",
        "similarity": 0.2918671092403577,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-05"
    },
    {
        "new_title": "Self-Improving Interference Management Based on Deep Learning With\n  Uncertainty Quantification",
        "new_link": "http://arxiv.org/abs/2401.13206v1",
        "new_summary": "  This paper presents a groundbreaking self-improving interference management\nframework tailored for wireless communications, integrating deep learning with\nuncertainty quantification to enhance overall system performance. Our approach\naddresses the computational challenges inherent in traditional\noptimization-based algorithms by harnessing deep learning models to predict\noptimal interference management solutions. A significant breakthrough of our\nframework is its acknowledgment of the limitations inherent in data-driven\nmodels, particularly in scenarios not adequately represented by the training\ndataset. To overcome these challenges, we propose a method for uncertainty\nquantification, accompanied by a qualifying criterion, to assess the\ntrustworthiness of model predictions. This framework strategically alternates\nbetween model-generated solutions and traditional algorithms, guided by a\ncriterion that assesses the prediction credibility based on quantified\nuncertainties. Experimental results validate the framework's efficacy,\ndemonstrating its superiority over traditional deep learning models, notably in\nscenarios underrepresented in the training dataset. This work marks a\npioneering endeavor in harnessing self-improving deep learning for interference\nmanagement, through the lens of uncertainty quantification.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.13206v1.pdf",
        "similarity": 0.2918549366649836,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-24"
    },
    {
        "new_title": "Continual Learning by Three-Phase Consolidation",
        "new_link": "http://arxiv.org/abs/2403.14679v1",
        "new_summary": "  TPC (Three-Phase Consolidation) is here introduced as a simple but effective\napproach to continually learn new classes (and/or instances of known classes)\nwhile controlling forgetting of previous knowledge. Each experience (a.k.a.\ntask) is learned in three phases characterized by different rules and learning\ndynamics, aimed at removing the class-bias problem (due to class unbalancing)\nand limiting gradient-based corrections to prevent forgetting of\nunderrepresented classes. Several experiments on complex datasets demonstrate\nits accuracy and efficiency advantages over competitive existing approaches.\nThe algorithm and all the results presented in this paper are fully\nreproducible thanks to its publication on the Avalanche open framework for\ncontinual learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.14679v1.pdf",
        "similarity": 0.2918458155843074,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-12"
    },
    {
        "new_title": "ALICE: Combining Feature Selection and Inter-Rater Agreeability for\n  Machine Learning Insights",
        "new_link": "http://arxiv.org/abs/2404.09053v1",
        "new_summary": "  This paper presents a new Python library called Automated Learning for\nInsightful Comparison and Evaluation (ALICE), which merges conventional feature\nselection and the concept of inter-rater agreeability in a simple,\nuser-friendly manner to seek insights into black box Machine Learning models.\nThe framework is proposed following an overview of the key concepts of\ninterpretability in ML. The entire architecture and intuition of the main\nmethods of the framework are also thoroughly discussed and results from initial\nexperiments on a customer churn predictive modeling task are presented,\nalongside ideas for possible avenues to explore for the future. The full source\ncode for the framework and the experiment notebooks can be found at:\nhttps://github.com/anasashb/aliceHU\n",
        "pdf_link": "https://arxiv.org/pdf/2404.09053v1.pdf",
        "similarity": 0.29183742808210844,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-13"
    },
    {
        "new_title": "FedClust: Tackling Data Heterogeneity in Federated Learning through\n  Weight-Driven Client Clustering",
        "new_link": "http://arxiv.org/abs/2407.07124v1",
        "new_summary": "  Federated learning (FL) is an emerging distributed machine learning paradigm\nthat enables collaborative training of machine learning models over\ndecentralized devices without exposing their local data. One of the major\nchallenges in FL is the presence of uneven data distributions across client\ndevices, violating the well-known assumption of\nindependent-and-identically-distributed (IID) training samples in conventional\nmachine learning. To address the performance degradation issue incurred by such\ndata heterogeneity, clustered federated learning (CFL) shows its promise by\ngrouping clients into separate learning clusters based on the similarity of\ntheir local data distributions. However, state-of-the-art CFL approaches\nrequire a large number of communication rounds to learn the distribution\nsimilarities during training until the formation of clusters is stabilized.\nMoreover, some of these algorithms heavily rely on a predefined number of\nclusters, thus limiting their flexibility and adaptability. In this paper, we\npropose {\\em FedClust}, a novel approach for CFL that leverages the correlation\nbetween local model weights and the data distribution of clients. {\\em\nFedClust} groups clients into clusters in a one-shot manner by measuring the\nsimilarity degrees among clients based on the strategically selected partial\nweights of locally trained models. We conduct extensive experiments on four\nbenchmark datasets with different non-IID data settings. Experimental results\ndemonstrate that {\\em FedClust} achieves higher model accuracy up to $\\sim$45\\%\nas well as faster convergence with a significantly reduced communication cost\nup to 2.7$\\times$ compared to its state-of-the-art counterparts.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.07124v1.pdf",
        "similarity": 0.29165251302649203,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-09"
    },
    {
        "new_title": "A Systematic Literature Review on the Use of Machine Learning in\n  Software Engineering",
        "new_link": "http://arxiv.org/abs/2406.13877v1",
        "new_summary": "  Software engineering (SE) is a dynamic field that involves multiple phases\nall of which are necessary to develop sustainable software systems. Machine\nlearning (ML), a branch of artificial intelligence (AI), has drawn a lot of\nattention in recent years thanks to its ability to analyze massive volumes of\ndata and extract useful patterns from data. Several studies have focused on\nexamining, categorising, and assessing the application of ML in SE processes.\nWe conducted a literature review on primary studies to address this gap. The\nstudy was carried out following the objective and the research questions to\nexplore the current state of the art in applying machine learning techniques in\nsoftware engineering processes. The review identifies the key areas within\nsoftware engineering where ML has been applied, including software quality\nassurance, software maintenance, software comprehension, and software\ndocumentation. It also highlights the specific ML techniques that have been\nleveraged in these domains, such as supervised learning, unsupervised learning,\nand deep learning.\n  Keywords: machine learning, deep learning, software engineering, natural\nlanguage processing, source code\n",
        "pdf_link": "https://arxiv.org/pdf/2406.13877v1.pdf",
        "similarity": 0.2916448427239561,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-19"
    },
    {
        "new_title": "On the Calibration of Epistemic Uncertainty: Principles, Paradoxes and\n  Conflictual Loss",
        "new_link": "http://arxiv.org/abs/2407.12211v1",
        "new_summary": "  The calibration of predictive distributions has been widely studied in deep\nlearning, but the same cannot be said about the more specific epistemic\nuncertainty as produced by Deep Ensembles, Bayesian Deep Networks, or\nEvidential Deep Networks. Although measurable, this form of uncertainty is\ndifficult to calibrate on an objective basis as it depends on the prior for\nwhich a variety of choices exist. Nevertheless, epistemic uncertainty must in\nall cases satisfy two formal requirements: first, it must decrease when the\ntraining dataset gets larger and, second, it must increase when the model\nexpressiveness grows. Despite these expectations, our experimental study shows\nthat on several reference datasets and models, measures of epistemic\nuncertainty violate these requirements, sometimes presenting trends completely\nopposite to those expected. These paradoxes between expectation and reality\nraise the question of the true utility of epistemic uncertainty as estimated by\nthese models. A formal argument suggests that this disagreement is due to a\npoor approximation of the posterior distribution rather than to a flaw in the\nmeasure itself. Based on this observation, we propose a regularization function\nfor deep ensembles, called conflictual loss in line with the above\nrequirements. We emphasize its strengths by showing experimentally that it\nrestores both requirements of epistemic uncertainty, without sacrificing either\nthe performance or the calibration of the deep ensembles.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.12211v1.pdf",
        "similarity": 0.2914751117010492,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-16"
    },
    {
        "new_title": "Programmable Photonic Extreme Learning Machines",
        "new_link": "http://arxiv.org/abs/2407.03218v1",
        "new_summary": "  Photonic neural networks offer a promising alternative to traditional\nelectronic systems for machine learning accelerators due to their low latency\nand energy efficiency. However, the challenge of implementing the\nbackpropagation algorithm during training has limited their development. To\naddress this, alternative machine learning schemes, such as extreme learning\nmachines (ELMs), have been proposed. ELMs use a random hidden layer to increase\nthe feature space dimensionality, requiring only the output layer to be trained\nthrough linear regression, thus reducing training complexity. Here, we\nexperimentally demonstrate a programmable photonic extreme learning machine\n(PPELM) using a hexagonal waveguide mesh, and which enables to program directly\non chip the input feature vector and the random hidden layer. Our system also\npermits to apply the nonlinearity directly on-chip by using the systems\nintegrated photodetecting elements. Using the PPELM we solved successfully\nthree different complex classification tasks. Additioanlly, we also propose and\ndemonstrate two techniques to increase the accuracy of the models and reduce\ntheir variability using an evolutionary algorithm and a wavelength division\nmultiplexing approach, obtaining excellent performance. Our results show that\nprogrammable photonic processors may become a feasible way to train competitive\nmachine learning models on a versatile and compact platform.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.03218v1.pdf",
        "similarity": 0.29137281949213945,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-03"
    },
    {
        "new_title": "From Learning to Optimize to Learning Optimization Algorithms",
        "new_link": "http://arxiv.org/abs/2405.18222v1",
        "new_summary": "  Towards designing learned optimization algorithms that are usable beyond\ntheir training setting, we identify key principles that classical algorithms\nobey, but have up to now, not been used for Learning to Optimize (L2O).\nFollowing these principles, we provide a general design pipeline, taking into\naccount data, architecture and learning strategy, and thereby enabling a\nsynergy between classical optimization and L2O, resulting in a philosophy of\nLearning Optimization Algorithms. As a consequence our learned algorithms\nperform well far beyond problems from the training distribution. We demonstrate\nthe success of these novel principles by designing a new learning-enhanced BFGS\nalgorithm and provide numerical experiments evidencing its adaptation to many\nsettings at test time.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.18222v1.pdf",
        "similarity": 0.2913443204266379,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-28"
    },
    {
        "new_title": "Near-field Beamforming for Extremely Large-scale MIMO Based on\n  Unsupervised Deep Learning",
        "new_link": "http://arxiv.org/abs/2406.03249v1",
        "new_summary": "  Extremely Large-scale Array (ELAA) is considered a frontier technology for\nfuture communication systems, pivotal in improving wireless systems' rate and\nspectral efficiency. However, as ELAA employs a multitude of antennas operating\nat higher frequencies, users are typically situated in the near-field region\nwhere the spherical wavefront propagates. This inevitably leads to a\nsignificant increase in the overhead of beam training, requiring complex\ntwo-dimensional beam searching in both the angle domain and the distance\ndomain. To address this problem, we propose a near-field beamforming method\nbased on unsupervised deep learning. Our convolutional neural network\nefficiently extracts complex channel state information features by\nstrategically selecting padding and kernel size. We optimize the beamformers to\nmaximize achievable rates in a multi-user network without relying on predefined\ncustom codebooks. Upon deployment, the model requires solely the input of\npre-estimated channel state information to derive the optimal beamforming\nvector. Simulation results show that our proposed scheme can obtain stable\nbeamforming gain compared with the baseline scheme. Furthermore, owing to the\ninherent traits of deep learning methodologies, this approach substantially\ndiminishes the beam training costs in near-field regions.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.03249v1.pdf",
        "similarity": 0.2912946885990312,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-05"
    },
    {
        "new_title": "Towards Understanding the Challenges of Bug Localization in Deep\n  Learning Systems",
        "new_link": "http://arxiv.org/abs/2402.01021v1",
        "new_summary": "  Software bugs cost the global economy billions of dollars annually and claim\n~50\\% of the programming time from software developers. Locating these bugs is\ncrucial for their resolution but challenging. It is even more challenging in\ndeep-learning systems due to their black-box nature. Bugs in these systems are\nalso hidden not only in the code but also in the models and training data,\nwhich might make traditional debugging methods less effective. In this article,\nwe conduct a large-scale empirical study to better understand the challenges of\nlocalizing bugs in deep-learning systems. First, we determine the bug\nlocalization performance of four existing techniques using 2,365 bugs from\ndeep-learning systems and 2,913 from traditional software. We found these\ntechniques significantly underperform in localizing deep-learning system bugs.\nSecond, we evaluate how different bug types in deep learning systems impact bug\nlocalization. We found that the effectiveness of localization techniques varies\nwith bug type due to their unique challenges. For example, tensor bugs were\nmore accessible to locate due to their structural nature, while all techniques\nstruggled with GPU bugs due to their external dependencies. Third, we\ninvestigate the impact of bugs' extrinsic nature on localization in\ndeep-learning systems. We found that deep learning bugs are often extrinsic and\nthus connected to artifacts other than source code (e.g., GPU, training data),\ncontributing to the poor performance of existing localization methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.01021v1.pdf",
        "similarity": 0.291216102589539,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-01"
    },
    {
        "new_title": "Seasonal Fire Prediction using Spatio-Temporal Deep Neural Networks",
        "new_link": "http://arxiv.org/abs/2404.06437v1",
        "new_summary": "  With climate change expected to exacerbate fire weather conditions, the\naccurate anticipation of wildfires on a global scale becomes increasingly\ncrucial for disaster mitigation. In this study, we utilize SeasFire, a\ncomprehensive global wildfire dataset with climate, vegetation, oceanic\nindices, and human-related variables, to enable seasonal wildfire forecasting\nwith machine learning. For the predictive analysis, we train deep learning\nmodels with different architectures that capture the spatio-temporal context\nleading to wildfires. Our investigation focuses on assessing the effectiveness\nof these models in predicting the presence of burned areas at varying\nforecasting time horizons globally, extending up to six months into the future,\nand on how different spatial or/and temporal context affects the performance of\nthe models. Our findings demonstrate the great potential of deep learning\nmodels in seasonal fire forecasting; longer input time-series leads to more\nrobust predictions across varying forecasting horizons, while integrating\nspatial information to capture wildfire spatio-temporal dynamics boosts\nperformance. Finally, our results hint that in order to enhance performance at\nlonger forecasting horizons, a larger receptive field spatially needs to be\nconsidered.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.06437v1.pdf",
        "similarity": 0.2911680936665328,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-09"
    },
    {
        "new_title": "Deep learning density functional theory Hamiltonian in real space",
        "new_link": "http://arxiv.org/abs/2407.14379v1",
        "new_summary": "  Deep learning electronic structures from ab initio calculations holds great\npotential to revolutionize computational materials studies. While existing\nmethods proved success in deep-learning density functional theory (DFT)\nHamiltonian matrices, they are limited to DFT programs using localized\natomic-like bases and heavily depend on the form of the bases. Here, we propose\nthe DeepH-r method for deep-learning DFT Hamiltonians in real space,\nfacilitating the prediction of DFT Hamiltonian in a basis-independent manner.\nAn equivariant neural network architecture for modeling the real-space DFT\npotential is developed, targeting a more fundamental quantity in DFT. The\nreal-space potential exhibits simplified principles of equivariance and\nenhanced nearsightedness, further boosting the performance of deep learning.\nWhen applied to evaluate the Hamiltonian matrix, this method significantly\nimproved in accuracy, as exemplified in multiple case studies. Given the\nabundance of data in the real-space potential, this work may pave a novel\npathway for establishing a ``large materials model\" with increased accuracy.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.14379v1.pdf",
        "similarity": 0.2903557764938201,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-19"
    },
    {
        "new_title": "Is K-fold cross validation the best model selection method for Machine\n  Learning?",
        "new_link": "http://arxiv.org/abs/2401.16407v1",
        "new_summary": "  As a technique that can compactly represent complex patterns, machine\nlearning has significant potential for predictive inference. K-fold\ncross-validation (CV) is the most common approach to ascertaining the\nlikelihood that a machine learning outcome is generated by chance and\nfrequently outperforms conventional hypothesis testing. This improvement uses\nmeasures directly obtained from machine learning classifications, such as\naccuracy, that do not have a parametric description. To approach a frequentist\nanalysis within machine learning pipelines, a permutation test or simple\nstatistics from data partitions (i.e. folds) can be added to estimate\nconfidence intervals. Unfortunately, neither parametric nor non-parametric\ntests solve the inherent problems around partitioning small sample-size\ndatasets and learning from heterogeneous data sources. The fact that machine\nlearning strongly depends on the learning parameters and the distribution of\ndata across folds recapitulates familiar difficulties around excess false\npositives and replication. The origins of this problem are demonstrated by\nsimulating common experimental circumstances, including small sample sizes, low\nnumbers of predictors, and heterogeneous data sources. A novel statistical test\nbased on K-fold CV and the Upper Bound of the actual error (K-fold CUBV) is\ncomposed, where uncertain predictions of machine learning with CV are bounded\nby the \\emph{worst case} through the evaluation of concentration inequalities.\nProbably Approximately Correct-Bayesian upper bounds for linear classifiers in\ncombination with K-fold CV is used to estimate the empirical error. The\nperformance with neuroimaging datasets suggests this is a robust criterion for\ndetecting effects, validating accuracy values obtained from machine learning\nwhilst avoiding excess false positives.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.16407v1.pdf",
        "similarity": 0.2903537370789878,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-29"
    },
    {
        "new_title": "VC Theory for Inventory Policies",
        "new_link": "http://arxiv.org/abs/2404.11509v2",
        "new_summary": "  Advances in computational power and AI have increased interest in\nreinforcement learning approaches to inventory management. This paper provides\na theoretical foundation for these approaches and investigates the benefits of\nrestricting to policy structures that are well-established by inventory theory.\nIn particular, we prove generalization guarantees for learning several\nwell-known classes of inventory policies, including base-stock and (s, S)\npolicies, by leveraging the celebrated Vapnik-Chervonenkis (VC) theory. We\napply the Pseudo-dimension and Fat-shattering dimension from VC theory to\ndetermine the generalization error of inventory policies, that is, the\ndifference between an inventory policy's performance on training data and its\nexpected performance on unseen data. We focus on a classical setting without\ncontexts, but allow for an arbitrary distribution over demand sequences and do\nnot make any assumptions such as independence over time. We corroborate our\nsupervised learning results using numerical simulations.\n  Managerially, our theory and simulations translate to the following insights.\nFirst, there is a principle of ``learning less is more'' in inventory\nmanagement: depending on the amount of data available, it may be beneficial to\nrestrict oneself to a simpler, albeit suboptimal, class of inventory policies\nto minimize overfitting errors. Second, the number of parameters in a policy\nclass may not be the correct measure of overfitting error: in fact, the class\nof policies defined by T time-varying base-stock levels exhibits a\ngeneralization error an order of magnitude lower than that of the two-parameter\n(s, S) policy class. Finally, our research suggests situations in which it\ncould be beneficial to incorporate the concepts of base-stock and inventory\nposition into black-box learning machines, instead of having these machines\ndirectly learn the order quantity actions.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.11509v2.pdf",
        "similarity": 0.28995602529451975,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-17"
    },
    {
        "new_title": "Quantum Embedding with Transformer for High-dimensional Data",
        "new_link": "http://arxiv.org/abs/2402.12704v1",
        "new_summary": "  Quantum embedding with transformers is a novel and promising architecture for\nquantum machine learning to deliver exceptional capability on near-term devices\nor simulators. The research incorporated a vision transformer (ViT) to advance\nquantum significantly embedding ability and results for a single qubit\nclassifier with around 3 percent in the median F1 score on the BirdCLEF-2021, a\nchallenging high-dimensional dataset. The study showcases and analyzes\nempirical evidence that our transformer-based architecture is a highly\nversatile and practical approach to modern quantum machine learning problems.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.12704v1.pdf",
        "similarity": 0.28990290474688957,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-20"
    },
    {
        "new_title": "VarteX: Enhancing Weather Forecast through Distributed Variable\n  Representation",
        "new_link": "http://arxiv.org/abs/2406.19615v1",
        "new_summary": "  Weather forecasting is essential for various human activities. Recent\ndata-driven models have outperformed numerical weather prediction by utilizing\ndeep learning in forecasting performance. However, challenges remain in\nefficiently handling multiple meteorological variables. This study proposes a\nnew variable aggregation scheme and an efficient learning framework for that\nchallenge. Experiments show that VarteX outperforms the conventional model in\nforecast performance, requiring significantly fewer parameters and resources.\nThe effectiveness of learning through multiple aggregations and regional split\ntraining is demonstrated, enabling more efficient and accurate deep\nlearning-based weather forecasting.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.19615v1.pdf",
        "similarity": 0.28943235548979296,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-28"
    },
    {
        "new_title": "Accelerating Convergence of Stein Variational Gradient Descent via Deep\n  Unfolding",
        "new_link": "http://arxiv.org/abs/2402.15125v1",
        "new_summary": "  Stein variational gradient descent (SVGD) is a prominent particle-based\nvariational inference method used for sampling a target distribution. SVGD has\nattracted interest for application in machine-learning techniques such as\nBayesian inference. In this paper, we propose novel trainable algorithms that\nincorporate a deep-learning technique called deep unfolding,into SVGD. This\napproach facilitates the learning of the internal parameters of SVGD, thereby\naccelerating its convergence speed. To evaluate the proposed trainable SVGD\nalgorithms, we conducted numerical simulations of three tasks: sampling a\none-dimensional Gaussian mixture, performing Bayesian logistic regression, and\nlearning Bayesian neural networks. The results show that our proposed\nalgorithms exhibit faster convergence than the conventional variants of SVGD.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.15125v1.pdf",
        "similarity": 0.2886642907610931,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-23"
    },
    {
        "new_title": "An Overview of Machine Learning-Enabled Optimization for Reconfigurable\n  Intelligent Surfaces-Aided 6G Networks: From Reinforcement Learning to Large\n  Language Models",
        "new_link": "http://arxiv.org/abs/2405.17439v1",
        "new_summary": "  Reconfigurable intelligent surface (RIS) becomes a promising technique for 6G\nnetworks by reshaping signal propagation in smart radio environments. However,\nit also leads to significant complexity for network management due to the large\nnumber of elements and dedicated phase-shift optimization. In this work, we\nprovide an overview of machine learning (ML)-enabled optimization for RIS-aided\n6G networks. In particular, we focus on various reinforcement learning (RL)\ntechniques, e.g., deep Q-learning, multi-agent reinforcement learning, transfer\nreinforcement learning, hierarchical reinforcement learning, and offline\nreinforcement learning. Different from existing studies, this work further\ndiscusses how large language models (LLMs) can be combined with RL to handle\nnetwork optimization problems. It shows that LLM offers new opportunities to\nenhance the capabilities of RL algorithms in terms of generalization, reward\nfunction design, multi-modal information processing, etc. Finally, we identify\nthe future challenges and directions of ML-enabled optimization for RIS-aided\n6G networks.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.17439v1.pdf",
        "similarity": 0.28866167020631534,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-09"
    },
    {
        "new_title": "The Performance of Sequential Deep Learning Models in Detecting Phishing\n  Websites Using Contextual Features of URLs",
        "new_link": "http://arxiv.org/abs/2404.09802v1",
        "new_summary": "  Cyber attacks continue to pose significant threats to individuals and\norganizations, stealing sensitive data such as personally identifiable\ninformation, financial information, and login credentials. Hence, detecting\nmalicious websites before they cause any harm is critical to preventing fraud\nand monetary loss. To address the increasing number of phishing attacks,\nprotective mechanisms must be highly responsive, adaptive, and scalable.\nFortunately, advances in the field of machine learning, coupled with access to\nvast amounts of data, have led to the adoption of various deep learning models\nfor timely detection of these cyber crimes. This study focuses on the detection\nof phishing websites using deep learning models such as Multi-Head Attention,\nTemporal Convolutional Network (TCN), BI-LSTM, and LSTM where URLs of the\nphishing websites are treated as a sequence. The results demonstrate that\nMulti-Head Attention and BI-LSTM model outperform some other deep\nlearning-based algorithms such as TCN and LSTM in producing better precision,\nrecall, and F1-scores.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.09802v1.pdf",
        "similarity": 0.28826677294983366,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-15"
    },
    {
        "new_title": "Computationally Efficient Unsupervised Deep Learning for Robust Joint AP\n  Clustering and Beamforming Design in Cell-Free Systems",
        "new_link": "http://arxiv.org/abs/2404.02531v1",
        "new_summary": "  In this paper, we consider robust joint access point (AP) clustering and\nbeamforming design with imperfect channel state information (CSI) in cell-free\nsystems. Specifically, we jointly optimize AP clustering and beamforming with\nimperfect CSI to simultaneously maximize the worst-case sum rate and minimize\nthe number of AP clustering under power constraint and the sparsity constraint\nof AP clustering. By transformations, the semi-infinite constraints caused by\nthe imperfect CSI are converted into more tractable forms for facilitating a\ncomputationally efficient unsupervised deep learning algorithm. In addition, to\nfurther reduce the computational complexity, a computationally effective\nunsupervised deep learning algorithm is proposed to implement robust joint AP\nclustering and beamforming design with imperfect CSI in cell-free systems.\nNumerical results demonstrate that the proposed unsupervised deep learning\nalgorithm achieves a higher worst-case sum rate under a smaller number of AP\nclustering with computational efficiency.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.02531v1.pdf",
        "similarity": 0.2881781605069882,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-03"
    },
    {
        "new_title": "A Survey of Deep Learning Based Software Refactoring",
        "new_link": "http://arxiv.org/abs/2404.19226v1",
        "new_summary": "  Refactoring is one of the most important activities in software engineering\nwhich is used to improve the quality of a software system. With the advancement\nof deep learning techniques, researchers are attempting to apply deep learning\ntechniques to software refactoring. Consequently, dozens of deep learning-based\nrefactoring approaches have been proposed. However, there is a lack of\ncomprehensive reviews on such works as well as a taxonomy for deep\nlearning-based refactoring. To this end, in this paper, we present a survey on\ndeep learning-based software refactoring. We classify related works into five\ncategories according to the major tasks they cover. Among these categories, we\nfurther present key aspects (i.e., code smell types, refactoring types,\ntraining strategies, and evaluation) to give insight into the details of the\ntechnologies that have supported refactoring through deep learning. The\nclassification indicates that there is an imbalance in the adoption of deep\nlearning techniques for the process of refactoring. Most of the deep learning\ntechniques have been used for the detection of code smells and the\nrecommendation of refactoring solutions as found in 56.25\\% and 33.33\\% of the\nliterature respectively. In contrast, only 6.25\\% and 4.17\\% were towards the\nend-to-end code transformation as refactoring and the mining of refactorings,\nrespectively. Notably, we found no literature representation for the quality\nassurance for refactoring. We also observe that most of the deep learning\ntechniques have been used to support refactoring processes occurring at the\nmethod level whereas classes and variables attracted minimal attention.\nFinally, we discuss the challenges and limitations associated with the\nemployment of deep learning-based refactorings and present some potential\nresearch opportunities for future work.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.19226v1.pdf",
        "similarity": 0.28802517989814325,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-30"
    },
    {
        "new_title": "A backward differential deep learning-based algorithm for solving\n  high-dimensional nonlinear backward stochastic differential equations",
        "new_link": "http://arxiv.org/abs/2404.08456v1",
        "new_summary": "  In this work, we propose a novel backward differential deep learning-based\nalgorithm for solving high-dimensional nonlinear backward stochastic\ndifferential equations (BSDEs), where the deep neural network (DNN) models are\ntrained not only on the inputs and labels but also the differentials of the\ncorresponding labels. This is motivated by the fact that differential deep\nlearning can provide an efficient approximation of the labels and their\nderivatives with respect to inputs. The BSDEs are reformulated as differential\ndeep learning problems by using Malliavin calculus. The Malliavin derivatives\nof solution to a BSDE satisfy themselves another BSDE, resulting thus in a\nsystem of BSDEs. Such formulation requires the estimation of the solution, its\ngradient, and the Hessian matrix, represented by the triple of processes\n$\\left(Y, Z, \\Gamma\\right).$ All the integrals within this system are\ndiscretized by using the Euler-Maruyama method. Subsequently, DNNs are employed\nto approximate the triple of these unknown processes. The DNN parameters are\nbackwardly optimized at each time step by minimizing a differential learning\ntype loss function, which is defined as a weighted sum of the dynamics of the\ndiscretized BSDE system, with the first term providing the dynamics of the\nprocess $Y$ and the other the process $Z$. An error analysis is carried out to\nshow the convergence of the proposed algorithm. Various numerical experiments\nup to $50$ dimensions are provided to demonstrate the high efficiency. Both\ntheoretically and numerically, it is demonstrated that our proposed scheme is\nmore efficient compared to other contemporary deep learning-based\nmethodologies, especially in the computation of the process $\\Gamma$.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.08456v1.pdf",
        "similarity": 0.2880130641791818,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-12"
    },
    {
        "new_title": "Seismic Image Denoising With A Physics-Constrained Deep Image Prior",
        "new_link": "http://arxiv.org/abs/2405.17597v1",
        "new_summary": "  Seismic images often contain both coherent and random artifacts which\ncomplicate their interpretation. To mitigate these artifacts, we introduce a\nnovel unsupervised deep-learning method based on Deep Image Prior (DIP) which\nuses convolutional neural networks. Our approach optimizes the network weights\nto refine the migration velocity model, rather than the seismic image,\neffectively isolating meaningful image features from noise and artifacts. We\napply this method to synthetic and real seismic data, demonstrating significant\nimprovements over standard DIP techniques with minimal computational overhead.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.17597v1.pdf",
        "similarity": 0.2879556496840129,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-27"
    },
    {
        "new_title": "Mechanistic Neural Networks for Scientific Machine Learning",
        "new_link": "http://arxiv.org/abs/2402.13077v1",
        "new_summary": "  This paper presents Mechanistic Neural Networks, a neural network design for\nmachine learning applications in the sciences. It incorporates a new\nMechanistic Block in standard architectures to explicitly learn governing\ndifferential equations as representations, revealing the underlying dynamics of\ndata and enhancing interpretability and efficiency in data modeling. Central to\nour approach is a novel Relaxed Linear Programming Solver (NeuRLP) inspired by\na technique that reduces solving linear ODEs to solving linear programs. This\nintegrates well with neural networks and surpasses the limitations of\ntraditional ODE solvers enabling scalable GPU parallel processing. Overall,\nMechanistic Neural Networks demonstrate their versatility for scientific\nmachine learning applications, adeptly managing tasks from equation discovery\nto dynamic systems modeling. We prove their comprehensive capabilities in\nanalyzing and interpreting complex scientific data across various applications,\nshowing significant performance against specialized state-of-the-art methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.13077v1.pdf",
        "similarity": 0.2879443182355798,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-20"
    },
    {
        "new_title": "Clinical translation of machine learning algorithms for seizure\n  detection in scalp electroencephalography: a systematic review",
        "new_link": "http://arxiv.org/abs/2404.15332v1",
        "new_summary": "  Machine learning algorithms for seizure detection have shown great diagnostic\npotential, with recent reported accuracies reaching 100%. However, few\npublished algorithms have fully addressed the requirements for successful\nclinical translation. For example, the properties of training data may\ncritically limit the generalisability of algorithms, algorithms may be\nsensitive to variability across EEG acquisition hardware, and run-time\nprocessing costs may render them unfeasible for real-time clinical use cases.\nHere, we systematically review machine learning seizure detection algorithms\nwith a focus on clinical translatability, assessed by criteria including\ngeneralisability, run-time costs, explainability, and clinically-relevant\nperformance metrics. For non-specialists, we provide domain-specific knowledge\nnecessary to contextualise model development and evaluation. Our critical\nevaluation of machine learning algorithms with respect to their potential\nreal-world effectiveness can help accelerate clinical translation and identify\ngaps in the current seizure detection literature.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.15332v1.pdf",
        "similarity": 0.28776417147409233,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-08"
    },
    {
        "new_title": "Self-consistent Validation for Machine Learning Electronic Structure",
        "new_link": "http://arxiv.org/abs/2402.10186v1",
        "new_summary": "  Machine learning has emerged as a significant approach to efficiently tackle\nelectronic structure problems. Despite its potential, there is less guarantee\nfor the model to generalize to unseen data that hinders its application in\nreal-world scenarios. To address this issue, a technique has been proposed to\nestimate the accuracy of the predictions. This method integrates machine\nlearning with self-consistent field methods to achieve both low validation cost\nand interpret-ability. This, in turn, enables exploration of the model's\nability with active learning and instills confidence in its integration into\nreal-world studies.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.10186v1.pdf",
        "similarity": 0.28775002690544227,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-15"
    },
    {
        "new_title": "Solving Deep Reinforcement Learning Tasks with Evolution Strategies and\n  Linear Policy Networks",
        "new_link": "http://arxiv.org/abs/2402.06912v2",
        "new_summary": "  Although deep reinforcement learning methods can learn effective policies for\nchallenging problems such as Atari games and robotics tasks, algorithms are\ncomplex, and training times are often long. This study investigates how\nEvolution Strategies perform compared to gradient-based deep reinforcement\nlearning methods. We use Evolution Strategies to optimize the weights of a\nneural network via neuroevolution, performing direct policy search. We\nbenchmark both deep policy networks and networks consisting of a single linear\nlayer from observations to actions for three gradient-based methods, such as\nProximal Policy Optimization. These methods are evaluated against three\nclassical Evolution Strategies and Augmented Random Search, which all use\nlinear policy networks. Our results reveal that Evolution Strategies can find\neffective linear policies for many reinforcement learning benchmark tasks,\nunlike deep reinforcement learning methods that can only find successful\npolicies using much larger networks, suggesting that current benchmarks are\neasier to solve than previously assumed. Interestingly, Evolution Strategies\nalso achieve results comparable to gradient-based deep reinforcement learning\nalgorithms for higher-complexity tasks. Furthermore, we find that by directly\naccessing the memory state of the game, Evolution Strategies can find\nsuccessful policies in Atari that outperform the policies found by Deep\nQ-Learning. Evolution Strategies also outperform Augmented Random Search in\nmost benchmarks, demonstrating superior sample efficiency and robustness in\ntraining linear policy networks.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.06912v2.pdf",
        "similarity": 0.28763609040917276,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-10"
    },
    {
        "new_title": "Enhancing Multistep Brent Oil Price Forecasting with a Multi-Aspect\n  Metaheuristic Optimization Approach and Ensemble Deep Learning Models",
        "new_link": "http://arxiv.org/abs/2407.12062v1",
        "new_summary": "  Accurate crude oil price forecasting is crucial for various economic\nactivities, including energy trading, risk management, and investment planning.\nAlthough deep learning models have emerged as powerful tools for crude oil\nprice forecasting, achieving accurate forecasts remains challenging. Deep\nlearning models' performance is heavily influenced by hyperparameters tuning,\nand they are expected to perform differently under various circumstances.\nFurthermore, price volatility is also sensitive to external factors such as\nworld events. To address these limitations, we propose a hybrid approach\ncombining metaheuristic optimisation and an ensemble of five popular neural\nnetwork architectures used in time series forecasting. Unlike existing methods\nthat apply metaheuristics to optimise hyperparameters within the neural network\narchitecture, we exploit the GWO metaheuristic optimiser at four levels:\nfeature selection, data preparation, model training, and forecast blending. The\nproposed approach has been evaluated for forecasting three-ahead days using\nreal-world Brent crude oil price data, and the obtained results demonstrate\nthat the proposed approach improves the forecasting performance measured using\nvarious benchmarks, achieving 0.000127 of MSE.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.12062v1.pdf",
        "similarity": 0.2876273997106033,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-15"
    },
    {
        "new_title": "TIMIT Speaker Profiling: A Comparison of Multi-task learning and\n  Single-task learning Approaches",
        "new_link": "http://arxiv.org/abs/2404.12077v1",
        "new_summary": "  This study employs deep learning techniques to explore four speaker profiling\ntasks on the TIMIT dataset, namely gender classification, accent\nclassification, age estimation, and speaker identification, highlighting the\npotential and challenges of multi-task learning versus single-task models. The\nmotivation for this research is twofold: firstly, to empirically assess the\nadvantages and drawbacks of multi-task learning over single-task models in the\ncontext of speaker profiling; secondly, to emphasize the undiminished\nsignificance of skillful feature engineering for speaker recognition tasks. The\nfindings reveal challenges in accent classification, and multi-task learning is\nfound advantageous for tasks of similar complexity. Non-sequential features are\nfavored for speaker recognition, but sequential ones can serve as starting\npoints for complex models. The study underscores the necessity of meticulous\nexperimentation and parameter tuning for deep learning models.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.12077v1.pdf",
        "similarity": 0.2875023917414498,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-18"
    },
    {
        "new_title": "Towards an Adaptable and Generalizable Optimization Engine in Decision\n  and Control: A Meta Reinforcement Learning Approach",
        "new_link": "http://arxiv.org/abs/2401.02508v1",
        "new_summary": "  Sampling-based model predictive control (MPC) has found significant success\nin optimal control problems with non-smooth system dynamics and cost function.\nMany machine learning-based works proposed to improve MPC by a) learning or\nfine-tuning the dynamics/ cost function, or b) learning to optimize for the\nupdate of the MPC controllers. For the latter, imitation learning-based\noptimizers are trained to update the MPC controller by mimicking the expert\ndemonstrations, which, however, are expensive or even unavailable. More\nsignificantly, many sequential decision-making problems are in non-stationary\nenvironments, requiring that an optimizer should be adaptable and generalizable\nto update the MPC controller for solving different tasks. To address those\nissues, we propose to learn an optimizer based on meta-reinforcement learning\n(RL) to update the controllers. This optimizer does not need expert\ndemonstration and can enable fast adaptation (e.g., few-shots) when it is\ndeployed in unseen control tasks. Experimental results validate the\neffectiveness of the learned optimizer regarding fast adaptation.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.02508v1.pdf",
        "similarity": 0.2871382083438389,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-04"
    },
    {
        "new_title": "A Review of Pulse-Coupled Neural Network Applications in Computer Vision\n  and Image Processing",
        "new_link": "http://arxiv.org/abs/2406.00239v1",
        "new_summary": "  Research in neural models inspired by mammal's visual cortex has led to many\nspiking neural networks such as pulse-coupled neural networks (PCNNs). These\nmodels are oscillating, spatio-temporal models stimulated with images to\nproduce several time-based responses. This paper reviews PCNN's state of the\nart, covering its mathematical formulation, variants, and other simplifications\nfound in the literature. We present several applications in which PCNN\narchitectures have successfully addressed some fundamental image processing and\ncomputer vision challenges, including image segmentation, edge detection,\nmedical imaging, image fusion, image compression, object recognition, and\nremote sensing. Results achieved in these applications suggest that the PCNN\narchitecture generates useful perceptual information relevant to a wide variety\nof computer vision tasks.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.00239v1.pdf",
        "similarity": 0.2870942298759838,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-01"
    },
    {
        "new_title": "A survey of air combat behavior modeling using machine learning",
        "new_link": "http://arxiv.org/abs/2404.13954v1",
        "new_summary": "  With the recent advances in machine learning, creating agents that behave\nrealistically in simulated air combat has become a growing field of interest.\nThis survey explores the application of machine learning techniques for\nmodeling air combat behavior, motivated by the potential to enhance\nsimulation-based pilot training. Current simulated entities tend to lack\nrealistic behavior, and traditional behavior modeling is labor-intensive and\nprone to loss of essential domain knowledge between development steps.\nAdvancements in reinforcement learning and imitation learning algorithms have\ndemonstrated that agents may learn complex behavior from data, which could be\nfaster and more scalable than manual methods. Yet, making adaptive agents\ncapable of performing tactical maneuvers and operating weapons and sensors\nstill poses a significant challenge. The survey examines applications, behavior\nmodel types, prevalent machine learning methods, and the technical and human\nchallenges in developing adaptive and realistically behaving agents. Another\nchallenge is the transfer of agents from learning environments to military\nsimulation systems and the consequent demand for standardization. Four primary\nrecommendations are presented regarding increased emphasis on\nbeyond-visual-range scenarios, multi-agent machine learning and cooperation,\nutilization of hierarchical behavior models, and initiatives for\nstandardization and research collaboration. These recommendations aim to\naddress current issues and guide the development of more comprehensive,\nadaptable, and realistic machine learning-based behavior models for air combat\napplications.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.13954v1.pdf",
        "similarity": 0.28697661392362656,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-22"
    },
    {
        "new_title": "Application-Driven Innovation in Machine Learning",
        "new_link": "http://arxiv.org/abs/2403.17381v1",
        "new_summary": "  As applications of machine learning proliferate, innovative algorithms\ninspired by specific real-world challenges have become increasingly important.\nSuch work offers the potential for significant impact not merely in domains of\napplication but also in machine learning itself. In this paper, we describe the\nparadigm of application-driven research in machine learning, contrasting it\nwith the more standard paradigm of methods-driven research. We illustrate the\nbenefits of application-driven machine learning and how this approach can\nproductively synergize with methods-driven work. Despite these benefits, we\nfind that reviewing, hiring, and teaching practices in machine learning often\nhold back application-driven innovation. We outline how these processes may be\nimproved.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.17381v1.pdf",
        "similarity": 0.28676872266742653,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-26"
    },
    {
        "new_title": "Use of Boosting Algorithms in Household-Level Poverty Measurement: A\n  Machine Learning Approach to Predict and Classify Household Wealth Quintiles\n  in the Philippines",
        "new_link": "http://arxiv.org/abs/2407.13061v1",
        "new_summary": "  This study assessed the effectiveness of machine learning models in\npredicting poverty levels in the Philippines using five boosting algorithms:\nAdaptive Boosting (AdaBoost), CatBoosting (CatBoost), Gradient Boosting Machine\n(GBM), Light Gradient Boosting Machine (LightGBM), and Extreme Gradient\nBoosting (XGBoost). CatBoost emerged as the superior model and achieved the\nhighest scores across accuracy, precision, recall, and F1-score at 91 percent,\nwhile XGBoost and GBM followed closely with 89 percent and 88 percent\nrespectively. Additionally, the research examined the computational efficiency\nof these models to analyze the balance between training time, testing speed,\nand model size factors crucial for real-world applications. Despite its longer\ntraining duration, CatBoost demonstrated high testing efficiency. These results\nindicate that machine learning can aid in poverty prediction and in the\ndevelopment of targeted policy interventions. Future studies should focus on\nincorporating a wider variety of data to enhance the predictive accuracy and\npolicy utility of these models.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.13061v1.pdf",
        "similarity": 0.28666367473273335,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-28"
    },
    {
        "new_title": "Step-by-Step Diffusion: An Elementary Tutorial",
        "new_link": "http://arxiv.org/abs/2406.08929v2",
        "new_summary": "  We present an accessible first course on diffusion models and flow matching\nfor machine learning, aimed at a technical audience with no diffusion\nexperience. We try to simplify the mathematical details as much as possible\n(sometimes heuristically), while retaining enough precision to derive correct\nalgorithms.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.08929v2.pdf",
        "similarity": 0.28633870951772694,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-13"
    },
    {
        "new_title": "Deep conditional distribution learning via conditional F\u00f6llmer flow",
        "new_link": "http://arxiv.org/abs/2402.01460v2",
        "new_summary": "  We introduce an ordinary differential equation (ODE) based deep generative\nmethod for learning conditional distributions, named Conditional F\\\"ollmer\nFlow. Starting from a standard Gaussian distribution, the proposed flow could\napproximate the target conditional distribution very well when the time is\nclose to 1. For effective implementation, we discretize the flow with Euler's\nmethod where we estimate the velocity field nonparametrically using a deep\nneural network. Furthermore, we also establish the convergence result for the\nWasserstein-2 distance between the distribution of the learned samples and the\ntarget conditional distribution, providing the first comprehensive end-to-end\nerror analysis for conditional distribution learning via ODE flow. Our\nnumerical experiments showcase its effectiveness across a range of scenarios,\nfrom standard nonparametric conditional density estimation problems to more\nintricate challenges involving image data, illustrating its superiority over\nvarious existing conditional density estimation methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.01460v2.pdf",
        "similarity": 0.2862142161916178,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-02"
    },
    {
        "new_title": "Mesh motion in fluid-structure interaction with deep operator networks",
        "new_link": "http://arxiv.org/abs/2402.00774v1",
        "new_summary": "  A mesh motion model based on deep operator networks is presented. The model\nis trained on and evaluated against a biharmonic mesh motion model on a\nfluid-structure interaction benchmark problem and further evaluated in a\nsetting where biharmonic mesh motion fails. The performance of the proposed\nmesh motion model is comparable to the biharmonic mesh motion on the test\nproblems.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.00774v1.pdf",
        "similarity": 0.2861512767538223,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-01"
    },
    {
        "new_title": "Learning Style Identification Using Semi-Supervised Self-Taught Labeling",
        "new_link": "http://arxiv.org/abs/2402.14597v1",
        "new_summary": "  Education is a dynamic field that must be adaptable to sudden changes and\ndisruptions caused by events like pandemics, war, and natural disasters related\nto climate change. When these events occur, traditional classrooms with\ntraditional or blended delivery can shift to fully online learning, which\nrequires an efficient learning environment that meets students' needs. While\nlearning management systems support teachers' productivity and creativity, they\ntypically provide the same content to all learners in a course, ignoring their\nunique learning styles. To address this issue, we propose a semi-supervised\nmachine learning approach that detects students' learning styles using a data\nmining technique. We use the commonly used Felder Silverman learning style\nmodel and demonstrate that our semi-supervised method can produce reliable\nclassification models with few labeled data. We evaluate our approach on two\ndifferent courses and achieve an accuracy of 88.83% and 77.35%, respectively.\nOur work shows that educational data mining and semi-supervised machine\nlearning techniques can identify different learning styles and create a\npersonalized learning environment.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.14597v1.pdf",
        "similarity": 0.2858965434090904,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-04"
    },
    {
        "new_title": "Exploring Machine Learning Algorithms for Infection Detection Using\n  GC-IMS Data: A Preliminary Study",
        "new_link": "http://arxiv.org/abs/2404.15757v1",
        "new_summary": "  The developing field of enhanced diagnostic techniques in the diagnosis of\ninfectious diseases, constitutes a crucial domain in modern healthcare. By\nutilizing Gas Chromatography-Ion Mobility Spectrometry (GC-IMS) data and\nincorporating machine learning algorithms into one platform, our research aims\nto tackle the ongoing issue of precise infection identification. Inspired by\nthese difficulties, our goals consist of creating a strong data analytics\nprocess, enhancing machine learning (ML) models, and performing thorough\nvalidation for clinical applications. Our research contributes to the emerging\nfield of advanced diagnostic technologies by integrating Gas Chromatography-Ion\nMobility Spectrometry (GC-IMS) data and machine learning algorithms within a\nunified Laboratory Information Management System (LIMS) platform. Preliminary\ntrials demonstrate encouraging levels of accuracy when employing various ML\nalgorithms to differentiate between infected and non-infected samples.\nContinuing endeavors are currently concentrated on enhancing the effectiveness\nof the model, investigating techniques to clarify its functioning, and\nincorporating many types of data to further support the early detection of\ndiseases.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.15757v1.pdf",
        "similarity": 0.28572410534319037,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-24"
    },
    {
        "new_title": "Learning to optimize: A tutorial for continuous and mixed-integer\n  optimization",
        "new_link": "http://arxiv.org/abs/2405.15251v1",
        "new_summary": "  Learning to Optimize (L2O) stands at the intersection of traditional\noptimization and machine learning, utilizing the capabilities of machine\nlearning to enhance conventional optimization techniques. As real-world\noptimization problems frequently share common structures, L2O provides a tool\nto exploit these structures for better or faster solutions. This tutorial dives\ndeep into L2O techniques, introducing how to accelerate optimization\nalgorithms, promptly estimate the solutions, or even reshape the optimization\nproblem itself, making it more adaptive to real-world applications. By\nconsidering the prerequisites for successful applications of L2O and the\nstructure of the optimization problems at hand, this tutorial provides a\ncomprehensive guide for practitioners and researchers alike.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.15251v1.pdf",
        "similarity": 0.28553557700963184,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-24"
    },
    {
        "new_title": "Deep Learning for Optical Tweezers",
        "new_link": "http://arxiv.org/abs/2401.02321v1",
        "new_summary": "  Optical tweezers exploit light--matter interactions to trap particles ranging\nfrom single atoms to micrometer-sized eukaryotic cells. For this reason,\noptical tweezers are a ubiquitous tool in physics, biology, and nanotechnology.\nRecently, the use of deep learning has started to enhance optical tweezers by\nimproving their design, calibration, and real-time control as well as the\ntracking and analysis of the trapped objects, often outperforming classical\nmethods thanks to the higher computational speed and versatility of deep\nlearning. Here, we review how deep learning has already remarkably improved\noptical tweezers, while exploring the exciting, new future possibilities\nenabled by this dynamic synergy. Furthermore, we offer guidelines on\nintegrating deep learning with optical trapping and optical manipulation in a\nreliable and trustworthy way.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.02321v1.pdf",
        "similarity": 0.2851585964684073,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-04"
    },
    {
        "new_title": "Optimistic Rates for Learning from Label Proportions",
        "new_link": "http://arxiv.org/abs/2406.00487v1",
        "new_summary": "  We consider a weakly supervised learning problem called Learning from Label\nProportions (LLP), where examples are grouped into ``bags'' and only the\naverage label within each bag is revealed to the learner. We study various\nlearning rules for LLP that achieve PAC learning guarantees for classification\nloss. We establish that the classical Empirical Proportional Risk Minimization\n(EPRM) learning rule (Yu et al., 2014) achieves fast rates under realizability,\nbut EPRM and similar proportion matching learning rules can fail in the\nagnostic setting. We also show that (1) a debiased proportional square loss, as\nwell as (2) a recently proposed EasyLLP learning rule (Busa-Fekete et al.,\n2023) both achieve ``optimistic rates'' (Panchenko, 2002); in both the\nrealizable and agnostic settings, their sample complexity is optimal (up to log\nfactors) in terms of $\\epsilon, \\delta$, and VC dimension.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.00487v1.pdf",
        "similarity": 0.285027016323272,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-01"
    },
    {
        "new_title": "Binary Linear Tree Commitment-based Ownership Protection for Distributed\n  Machine Learning",
        "new_link": "http://arxiv.org/abs/2401.05895v1",
        "new_summary": "  Distributed machine learning enables parallel training of extensive datasets\nby delegating computing tasks across multiple workers. Despite the cost\nreduction benefits of distributed machine learning, the dissemination of final\nmodel weights often leads to potential conflicts over model ownership as\nworkers struggle to substantiate their involvement in the training computation.\nTo address the above ownership issues and prevent accidental failures and\nmalicious attacks, verifying the computational integrity and effectiveness of\nworkers becomes particularly crucial in distributed machine learning. In this\npaper, we proposed a novel binary linear tree commitment-based ownership\nprotection model to ensure computational integrity with limited overhead and\nconcise proof. Due to the frequent updates of parameters during training, our\ncommitment scheme introduces a maintainable tree structure to reduce the costs\nof updating proofs. Distinguished from SNARK-based verifiable computation, our\nmodel achieves efficient proof aggregation by leveraging inner product\narguments. Furthermore, proofs of model weights are watermarked by worker\nidentity keys to prevent commitments from being forged or duplicated. The\nperformance analysis and comparison with SNARK-based hash commitments validate\nthe efficacy of our model in preserving computational integrity within\ndistributed machine learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.05895v1.pdf",
        "similarity": 0.2848482617023169,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-11"
    },
    {
        "new_title": "Deep Learning Approaches for Network Traffic Classification in the\n  Internet of Things (IoT): A Survey",
        "new_link": "http://arxiv.org/abs/2402.00920v1",
        "new_summary": "  The Internet of Things (IoT) has witnessed unprecedented growth, resulting in\na massive influx of diverse network traffic from interconnected devices.\nEffectively classifying this network traffic is crucial for optimizing resource\nallocation, enhancing security measures, and ensuring efficient network\nmanagement in IoT systems. Deep learning has emerged as a powerful technique\nfor network traffic classification due to its ability to automatically learn\ncomplex patterns and representations from raw data. This survey paper aims to\nprovide a comprehensive overview of the existing deep learning approaches\nemployed in network traffic classification specifically tailored for IoT\nenvironments. By systematically analyzing and categorizing the latest research\ncontributions in this domain, we explore the strengths and limitations of\nvarious deep learning models in handling the unique challenges posed by IoT\nnetwork traffic. Through this survey, we aim to offer researchers and\npractitioners valuable insights, identify research gaps, and provide directions\nfor future research to further enhance the effectiveness and efficiency of deep\nlearning-based network traffic classification in IoT.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.00920v1.pdf",
        "similarity": 0.28468344279793767,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-01"
    },
    {
        "new_title": "Learning accurate and interpretable decision trees",
        "new_link": "http://arxiv.org/abs/2405.15911v1",
        "new_summary": "  Decision trees are a popular tool in machine learning and yield\neasy-to-understand models. Several techniques have been proposed in the\nliterature for learning a decision tree classifier, with different techniques\nworking well for data from different domains. In this work, we develop\napproaches to design decision tree learning algorithms given repeated access to\ndata from the same domain. We propose novel parameterized classes of node\nsplitting criteria in top-down algorithms, which interpolate between popularly\nused entropy and Gini impurity based criteria, and provide theoretical bounds\non the number of samples needed to learn the splitting function appropriate for\nthe data at hand. We also study the sample complexity of tuning prior\nparameters in Bayesian decision tree learning, and extend our results to\ndecision tree regression. We further consider the problem of tuning\nhyperparameters in pruning the decision tree for classical pruning algorithms\nincluding min-cost complexity pruning. We also study the interpretability of\nthe learned decision trees and introduce a data-driven approach for optimizing\nthe explainability versus accuracy trade-off using decision trees. Finally, we\ndemonstrate the significance of our approach on real world datasets by learning\ndata-specific decision trees which are simultaneously more accurate and\ninterpretable.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.15911v1.pdf",
        "similarity": 0.28379240780260473,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-24"
    },
    {
        "new_title": "Comprehensive Study Of Predictive Maintenance In Industries Using\n  Classification Models And LSTM Model",
        "new_link": "http://arxiv.org/abs/2403.10259v1",
        "new_summary": "  In today's technology-driven era, the imperative for predictive maintenance\nand advanced diagnostics extends beyond aviation to encompass the\nidentification of damages, failures, and operational defects in rotating and\nmoving machines. Implementing such services not only curtails maintenance costs\nbut also extends machine lifespan, ensuring heightened operational efficiency.\nMoreover, it serves as a preventive measure against potential accidents or\ncatastrophic events. The advent of Artificial Intelligence (AI) has\nrevolutionized maintenance across industries, enabling more accurate and\nefficient prediction and analysis of machine failures, thereby conserving time\nand resources. Our proposed study aims to delve into various machine learning\nclassification techniques, including Support Vector Machine (SVM), Random\nForest, Logistic Regression, and Convolutional Neural Network LSTM-Based, for\npredicting and analyzing machine performance. SVM classifies data into\ndifferent categories based on their positions in a multidimensional space,\nwhile Random Forest employs ensemble learning to create multiple decision trees\nfor classification. Logistic Regression predicts the probability of binary\noutcomes using input data. The primary objective of the study is to assess\nthese algorithms' performance in predicting and analyzing machine performance,\nconsidering factors such as accuracy, precision, recall, and F1 score. The\nfindings will aid maintenance experts in selecting the most suitable machine\nlearning algorithm for effective prediction and analysis of machine\nperformance.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.10259v1.pdf",
        "similarity": 0.2837259480570837,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-15"
    },
    {
        "new_title": "A review on machine learning for arterial extraction and quantitative\n  assessment on invasive coronary angiograms",
        "new_link": "http://arxiv.org/abs/2405.08474v1",
        "new_summary": "  Purpose of Review Recently, machine learning has developed rapidly in the\nfield of medicine, playing an important role in disease diagnosis. Our aim of\nthis paper is to provide an overview of the advancements in machine learning\ntechniques applied to invasive coronary angiography (ICA) for segmentation of\ncoronary arteries and quantitative evaluation like fractional flow reserve\n(FFR) and stenosis assessment.\n  Recent Findings ICA are used extensively along with machine learning\ntechniques for the segmentation of arteries and quantitative evaluation of\nstenosis, coronary artery disease and measurement of fractional flow reserve,\nrepresenting a trend towards using computational methods for enhanced\ndiagnostic precision in cardiovascular medicine.\n  Summary Various research studies have been conducted in this field, each\nusing different algorithms and datasets. The performance of these studies\nlargely depends on the algorithms employed and the datasets used for training\nand evaluation. However, despite the progress made, there remains a need for\nmachine learning (ML) algorithms that can be easily integrated into clinical\npractice.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.08474v1.pdf",
        "similarity": 0.28371098553085955,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-14"
    },
    {
        "new_title": "Explainable concept mappings of MRI: Revealing the mechanisms underlying\n  deep learning-based brain disease classification",
        "new_link": "http://arxiv.org/abs/2404.10433v1",
        "new_summary": "  Motivation. While recent studies show high accuracy in the classification of\nAlzheimer's disease using deep neural networks, the underlying learned concepts\nhave not been investigated.\n  Goals. To systematically identify changes in brain regions through concepts\nlearned by the deep neural network for model validation.\n  Approach. Using quantitative R2* maps we separated Alzheimer's patients\n(n=117) from normal controls (n=219) by using a convolutional neural network\nand systematically investigated the learned concepts using Concept Relevance\nPropagation and compared these results to a conventional region of\ninterest-based analysis.\n  Results. In line with established histological findings and the region of\ninterest-based analyses, highly relevant concepts were primarily found in and\nadjacent to the basal ganglia.\n  Impact. The identification of concepts learned by deep neural networks for\ndisease classification enables validation of the models and could potentially\nimprove reliability.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.10433v1.pdf",
        "similarity": 0.28368866436623197,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-16"
    },
    {
        "new_title": "Coupling Machine Learning with Ontology for Robotics Applications",
        "new_link": "http://arxiv.org/abs/2407.02500v1",
        "new_summary": "  In this paper I present a practical approach for coupling machine learning\n(ML) algorithms with knowledge bases (KB) ontology formalism. The lack of\navailability of prior knowledge in dynamic scenarios is without doubt a major\nbarrier for scalable machine intelligence. My view of the interaction between\nthe two tiers intelligence is based on the idea that when knowledge is not\nreadily available at the knowledge base tier, more knowledge can be extracted\nfrom the other tier, which has access to trained models from machine learning\nalgorithms. To analyse this hypothesis, I create two experiments based on\ndifferent datasets, which are related directly to risk-awareness of autonomous\nsystems, analysed by different machine learning algorithms (namely; multi-layer\nfeedforward backpropagation, Naive Bayes, and J48 decision tree). My analysis\nshows that the two-tiers intelligence approach for coupling ML and KB is\ncomputationally valid and the time complexity of the algorithms during the\nrobot mission is linear with the size of the data and knowledge.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.02500v1.pdf",
        "similarity": 0.2835577386650519,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-08"
    },
    {
        "new_title": "Deep learning methods for Hamiltonian parameter estimation and magnetic\n  domain image generation in twisted van der Waals magnets",
        "new_link": "http://arxiv.org/abs/2402.11434v2",
        "new_summary": "  The application of twist engineering in van der Waals magnets has opened new\nfrontiers in the field of two-dimensional magnetism, yielding distinctive\nmagnetic domain structures. Despite the introduction of numerous theoretical\nmethods, limitations persist in terms of accuracy or efficiency due to the\ncomplex nature of the magnetic Hamiltonians pertinent to these systems. In this\nstudy, we introduce a deep-learning approach to tackle these challenges.\nUtilizing customized, fully connected networks, we develop two\ndeep-neural-network kernels that facilitate efficient and reliable analysis of\ntwisted van der Waals magnets. Our regression model is adept at estimating the\nmagnetic Hamiltonian parameters of twisted bilayer CrI3 from its magnetic\ndomain images generated through atomistic spin simulations. The generative\nmodel excels in producing precise magnetic domain images from the provided\nmagnetic parameters. The trained networks for these models undergo thorough\nvalidation, including statistical error analysis and assessment of robustness\nagainst noisy injections. These advancements not only extend the applicability\nof deep-learning methods to twisted van der Waals magnets but also streamline\nfuture investigations into these captivating yet poorly understood systems.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.11434v2.pdf",
        "similarity": 0.28344024423019143,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-18"
    },
    {
        "new_title": "An Imitative Reinforcement Learning Framework for Autonomous Dogfight",
        "new_link": "http://arxiv.org/abs/2406.11562v1",
        "new_summary": "  Unmanned Combat Aerial Vehicle (UCAV) dogfight, which refers to a fight\nbetween two or more UCAVs usually at close quarters, plays a decisive role on\nthe aerial battlefields. With the evolution of artificial intelligence,\ndogfight progressively transits towards intelligent and autonomous modes.\nHowever, the development of autonomous dogfight policy learning is hindered by\nchallenges such as weak exploration capabilities, low learning efficiency, and\nunrealistic simulated environments. To overcome these challenges, this paper\nproposes a novel imitative reinforcement learning framework, which efficiently\nleverages expert data while enabling autonomous exploration. The proposed\nframework not only enhances learning efficiency through expert imitation, but\nalso ensures adaptability to dynamic environments via autonomous exploration\nwith reinforcement learning. Therefore, the proposed framework can learn a\nsuccessful dogfight policy of 'pursuit-lock-launch' for UCAVs. To support\ndata-driven learning, we establish a dogfight environment based on the\nHarfang3D sandbox, where we conduct extensive experiments. The results indicate\nthat the proposed framework excels in multistage dogfight, significantly\noutperforms state-of-the-art reinforcement learning and imitation learning\nmethods. Thanks to the ability of imitating experts and autonomous exploration,\nour framework can quickly learn the critical knowledge in complex aerial combat\ntasks, achieving up to a 100% success rate and demonstrating excellent\nrobustness.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.11562v1.pdf",
        "similarity": 0.2833287344821086,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-17"
    },
    {
        "new_title": "Training Machine Learning models at the Edge: A Survey",
        "new_link": "http://arxiv.org/abs/2403.02619v2",
        "new_summary": "  Edge Computing (EC) has gained significant traction in recent years,\npromising enhanced efficiency by integrating Artificial Intelligence (AI)\ncapabilities at the edge. While the focus has primarily been on the deployment\nand inference of Machine Learning (ML) models at the edge, the training aspect\nremains less explored. This survey delves into Edge Learning (EL), specifically\nthe optimization of ML model training at the edge. The objective is to\ncomprehensively explore diverse approaches and methodologies in EL, synthesize\nexisting knowledge, identify challenges, and highlight future trends. Utilizing\nScopus' advanced search, relevant literature on EL was identified, revealing a\nconcentration of research efforts in distributed learning methods, particularly\nFederated Learning (FL). This survey further provides a guideline for comparing\ntechniques used to optimize ML for edge learning, along with an exploration of\ndifferent frameworks, libraries, and simulation tools available for EL. In\ndoing so, the paper contributes to a holistic understanding of the current\nlandscape and future directions in the intersection of edge computing and\nmachine learning, paving the way for informed comparisons between optimization\nmethods and techniques designed for edge learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.02619v2.pdf",
        "similarity": 0.2829221567413578,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-05"
    },
    {
        "new_title": "RIS-empowered Topology Control for Distributed Learning in Urban Air\n  Mobility",
        "new_link": "http://arxiv.org/abs/2403.05133v1",
        "new_summary": "  Urban Air Mobility (UAM) expands vehicles from the ground to the near-ground\nspace, envisioned as a revolution for transportation systems. Comprehensive\nscene perception is the foundation for autonomous aerial driving. However, UAM\nencounters the intelligent perception challenge: high perception learning\nrequirements conflict with the limited sensors and computing chips of flying\ncars. To overcome the challenge, federated learning (FL) and other\ncollaborative learning have been proposed to enable resource-limited devices to\nconduct onboard deep learning (DL) collaboratively. But traditional\ncollaborative learning like FL relies on a central integrator for DL model\naggregation, which is difficult to deploy in dynamic environments. The fully\ndecentralized learning schemes may be the intuitive solution while the\nconvergence of distributed learning cannot be guaranteed. Accordingly, this\npaper explores reconfigurable intelligent surfaces (RIS) empowered distributed\nlearning, taking account of topological attributes to facilitate the learning\nperformance with convergence guarantee. We propose several FL topological\ncriteria for optimizing the transmission delay and convergence rate by\nexploiting the Laplacian matrix eigenvalues of the communication network.\nSubsequently, we innovatively leverage the RIS link modification ability to\nremold the current network according to the proposed topological criteria. This\npaper rethinks the functions of RIS from the perspective of the network layer.\nFurthermore, a deep deterministic policy gradient-based RIS phase shift control\nalgorithm is developed to construct or deconstruct the network links\nsimultaneously to reshape the communication network. Simulation experiments are\nconducted over MobileNet-based multi-view learning to verify the efficiency of\nthe distributed FL framework.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.05133v1.pdf",
        "similarity": 0.2828816967705093,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-08"
    },
    {
        "new_title": "An Organic Weed Control Prototype using Directed Energy and Deep\n  Learning",
        "new_link": "http://arxiv.org/abs/2405.21056v1",
        "new_summary": "  Organic weed control is a vital to improve crop yield with a sustainable\napproach. In this work, a directed energy weed control robot prototype\nspecifically designed for organic farms is proposed. The robot uses a novel\ndistributed array robot (DAR) unit for weed treatment. Soybean and corn\ndatabases are built to train deep learning neural nets to perform weed\nrecognition. The initial deep learning neural nets show a high performance in\nclassifying crops. The robot uses a patented directed energy plant eradication\nrecipe that is completely organic and UV-C free, with no chemical damage or\nphysical disturbance to the soil. The deep learning can classify 8 common weed\nspecies in a soybean field under natural environment with up to 98% accuracy.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.21056v1.pdf",
        "similarity": 0.2825454915476955,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-31"
    },
    {
        "new_title": "Logistic Variational Bayes Revisited",
        "new_link": "http://arxiv.org/abs/2406.00713v1",
        "new_summary": "  Variational logistic regression is a popular method for approximate Bayesian\ninference seeing wide-spread use in many areas of machine learning including:\nBayesian optimization, reinforcement learning and multi-instance learning to\nname a few. However, due to the intractability of the Evidence Lower Bound,\nauthors have turned to the use of Monte Carlo, quadrature or bounds to perform\ninference, methods which are costly or give poor approximations to the true\nposterior.\n  In this paper we introduce a new bound for the expectation of softplus\nfunction and subsequently show how this can be applied to variational logistic\nregression and Gaussian process classification. Unlike other bounds, our\nproposal does not rely on extending the variational family, or introducing\nadditional parameters to ensure the bound is tight. In fact, we show that this\nbound is tighter than the state-of-the-art, and that the resulting variational\nposterior achieves state-of-the-art performance, whilst being significantly\nfaster to compute than Monte-Carlo methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.00713v1.pdf",
        "similarity": 0.2825151873156451,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-02"
    },
    {
        "new_title": "A Simple and Adaptive Learning Rate for FTRL in Online Learning with\n  Minimax Regret of $\u0398(T^{2/3})$ and its Application to\n  Best-of-Both-Worlds",
        "new_link": "http://arxiv.org/abs/2405.20028v1",
        "new_summary": "  Follow-the-Regularized-Leader (FTRL) is a powerful framework for various\nonline learning problems. By designing its regularizer and learning rate to be\nadaptive to past observations, FTRL is known to work adaptively to various\nproperties of an underlying environment. However, most existing adaptive\nlearning rates are for online learning problems with a minimax regret of\n$\\Theta(\\sqrt{T})$ for the number of rounds $T$, and there are only a few\nstudies on adaptive learning rates for problems with a minimax regret of\n$\\Theta(T^{2/3})$, which include several important problems dealing with\nindirect feedback. To address this limitation, we establish a new adaptive\nlearning rate framework for problems with a minimax regret of\n$\\Theta(T^{2/3})$. Our learning rate is designed by matching the stability,\npenalty, and bias terms that naturally appear in regret upper bounds for\nproblems with a minimax regret of $\\Theta(T^{2/3})$. As applications of this\nframework, we consider two major problems dealing with indirect feedback:\npartial monitoring and graph bandits. We show that FTRL with our learning rate\nand the Tsallis entropy regularizer improves existing Best-of-Both-Worlds\n(BOBW) regret upper bounds, which achieve simultaneous optimality in the\nstochastic and adversarial regimes. The resulting learning rate is surprisingly\nsimple compared to the existing learning rates for BOBW algorithms for problems\nwith a minimax regret of $\\Theta(T^{2/3})$.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.20028v1.pdf",
        "similarity": 0.2821891975838579,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-30"
    },
    {
        "new_title": "Multiple Yield Curve Modeling and Forecasting using Deep Learning",
        "new_link": "http://arxiv.org/abs/2401.16985v1",
        "new_summary": "  This manuscript introduces deep learning models that simultaneously describe\nthe dynamics of several yield curves. We aim to learn the dependence structure\namong the different yield curves induced by the globalization of financial\nmarkets and exploit it to produce more accurate forecasts. By combining the\nself-attention mechanism and nonparametric quantile regression, our model\ngenerates both point and interval forecasts of future yields. The architecture\nis designed to avoid quantile crossing issues affecting multiple quantile\nregression models. Numerical experiments conducted on two different datasets\nconfirm the effectiveness of our approach. Finally, we explore potential\nextensions and enhancements by incorporating deep ensemble methods and transfer\nlearning mechanisms.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.16985v1.pdf",
        "similarity": 0.2821125932022955,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-30"
    },
    {
        "new_title": "Similarity-Based Analysis of Atmospheric Organic Compounds for Machine\n  Learning Applications",
        "new_link": "http://arxiv.org/abs/2406.18171v1",
        "new_summary": "  The formation of aerosol particles in the atmosphere impacts air quality and\nclimate change, but many of the organic molecules involved remain unknown.\nMachine learning could aid in identifying these compounds through accelerated\nanalysis of molecular properties and detection characteristics. However, such\nprogress is hindered by the current lack of curated datasets for atmospheric\nmolecules and their associated properties. To tackle this challenge, we propose\na similarity analysis that connects atmospheric compounds to existing large\nmolecular datasets used for machine learning development. We find a small\noverlap between atmospheric and non-atmospheric molecules using standard\nmolecular representations in machine learning applications. The identified\nout-of-domain character of atmospheric compounds is related to their distinct\nfunctional groups and atomic composition. Our investigation underscores the\nneed for collaborative efforts to gather and share more molecular-level\natmospheric chemistry data. The presented similarity based analysis can be used\nfor future dataset curation for machine learning development in the atmospheric\nsciences.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.18171v1.pdf",
        "similarity": 0.2820186304919218,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-26"
    },
    {
        "new_title": "Hyperparameter Tuning for Causal Inference with Double Machine Learning:\n  A Simulation Study",
        "new_link": "http://arxiv.org/abs/2402.04674v1",
        "new_summary": "  Proper hyperparameter tuning is essential for achieving optimal performance\nof modern machine learning (ML) methods in predictive tasks. While there is an\nextensive literature on tuning ML learners for prediction, there is only little\nguidance available on tuning ML learners for causal machine learning and how to\nselect among different ML learners. In this paper, we empirically assess the\nrelationship between the predictive performance of ML methods and the resulting\ncausal estimation based on the Double Machine Learning (DML) approach by\nChernozhukov et al. (2018). DML relies on estimating so-called nuisance\nparameters by treating them as supervised learning problems and using them as\nplug-in estimates to solve for the (causal) parameter. We conduct an extensive\nsimulation study using data from the 2019 Atlantic Causal Inference Conference\nData Challenge. We provide empirical insights on the role of hyperparameter\ntuning and other practical decisions for causal estimation with DML. First, we\nassess the importance of data splitting schemes for tuning ML learners within\nDouble Machine Learning. Second, we investigate how the choice of ML methods\nand hyperparameters, including recent AutoML frameworks, impacts the estimation\nperformance for a causal parameter of interest. Third, we assess to what extent\nthe choice of a particular causal model, as characterized by incorporated\nparametric assumptions, can be based on predictive performance metrics.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.04674v1.pdf",
        "similarity": 0.281749756739472,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-07"
    },
    {
        "new_title": "Deep Learning without Global Optimization by Random Fourier Neural\n  Networks",
        "new_link": "http://arxiv.org/abs/2407.11894v1",
        "new_summary": "  We introduce a new training algorithm for variety of deep neural networks\nthat utilize random complex exponential activation functions. Our approach\nemploys a Markov Chain Monte Carlo sampling procedure to iteratively train\nnetwork layers, avoiding global and gradient-based optimization while\nmaintaining error control. It consistently attains the theoretical\napproximation rate for residual networks with complex exponential activation\nfunctions, determined by network complexity. Additionally, it enables efficient\nlearning of multiscale and high-frequency features, producing interpretable\nparameter distributions. Despite using sinusoidal basis functions, we do not\nobserve Gibbs phenomena in approximating discontinuous target functions.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.11894v1.pdf",
        "similarity": 0.2812274602268584,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-16"
    },
    {
        "new_title": "Restricted Boltzmann Machine for Modeling Complex Physical Systems: A\n  Case Study in Artificial Spin Ice",
        "new_link": "http://arxiv.org/abs/2407.11165v1",
        "new_summary": "  Restricted Boltzmann machines are powerful tools in the field of generative,\nprobabilistic learning, capable of capturing complex dependencies in data. By\nunderstanding their architecture and operational principles, one can employ\nthem for diverse purposes such as dimensionality reduction, feature learning\nand even representing and analyzing various physical systems. This work aims to\nprovide insights into the capabilities and limitations of restricted Boltzmann\nmachines in modelling complex physical systems in the context of artificial\nspin ice. Geometrical frustration in artificial spin ice systems creates\ndegeneracies leading to complex states and collective dynamics. From\nreconfigurable magnonics to neuromorphic computing, artificial spin systems are\nemerging as versatile functional platforms that go beyond simply imitating\nnaturally occurring materials. Using out of equilibrium data from Monte Carlo\nsimulations of artificial spin ice geometries, this work demonstrates the\nsensitivity of learning artificial spin ice state distributions with restricted\nBoltzmann machines. Results indicate that careful application of the restricted\nBoltzmann machine algorithm can reduce the training data required for feature\nextraction, which can be used for faster sample generation. Additionally, we\ndemonstrate how the restricted Boltzmann machine can distinguish different\nartificial spin ice geometries by identifying their respective state\ndistribution features.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.11165v1.pdf",
        "similarity": 0.2808091865457738,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-15"
    },
    {
        "new_title": "Distributed Deep Reinforcement Learning Based Gradient Quantization for\n  Federated Learning Enabled Vehicle Edge Computing",
        "new_link": "http://arxiv.org/abs/2407.08462v1",
        "new_summary": "  Federated Learning (FL) can protect the privacy of the vehicles in vehicle\nedge computing (VEC) to a certain extent through sharing the gradients of\nvehicles' local models instead of local data. The gradients of vehicles' local\nmodels are usually large for the vehicular artificial intelligence (AI)\napplications, thus transmitting such large gradients would cause large\nper-round latency. Gradient quantization has been proposed as one effective\napproach to reduce the per-round latency in FL enabled VEC through compressing\ngradients and reducing the number of bits, i.e., the quantization level, to\ntransmit gradients. The selection of quantization level and thresholds\ndetermines the quantization error, which further affects the model accuracy and\ntraining time. To do so, the total training time and quantization error (QE)\nbecome two key metrics for the FL enabled VEC. It is critical to jointly\noptimize the total training time and QE for the FL enabled VEC. However, the\ntime-varying channel condition causes more challenges to solve this problem. In\nthis paper, we propose a distributed deep reinforcement learning (DRL)-based\nquantization level allocation scheme to optimize the long-term reward in terms\nof the total training time and QE. Extensive simulations identify the optimal\nweighted factors between the total training time and QE, and demonstrate the\nfeasibility and effectiveness of the proposed scheme.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.08462v1.pdf",
        "similarity": 0.2807930348723655,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-11"
    },
    {
        "new_title": "End-to-End Quantum Vision Transformer: Towards Practical Quantum Speedup\n  in Large-Scale Models",
        "new_link": "http://arxiv.org/abs/2402.18940v2",
        "new_summary": "  The field of quantum deep learning presents significant opportunities for\nadvancing computational capabilities, yet it faces a major obstacle in the form\nof the \"information loss problem\" due to the inherent limitations of the\nnecessary quantum tomography in scaling quantum deep neural networks. This\npaper introduces an end-to-end Quantum Vision Transformer (QViT), which\nincorporates an innovative quantum residual connection technique, to overcome\nthese challenges and therefore optimize quantum computing processes in deep\nlearning. Our thorough complexity analysis of the QViT reveals a theoretically\nexponential and empirically polynomial speedup, showcasing the model's\nefficiency and potential in quantum computing applications. We conducted\nextensive numerical tests on modern, large-scale transformers and datasets,\nestablishing the QViT as a pioneering advancement in applying quantum deep\nneural networks in practical scenarios. Our work provides a comprehensive\nquantum deep learning paradigm, which not only demonstrates the versatility of\ncurrent quantum linear algebra algorithms but also promises to enhance future\nresearch and development in quantum deep learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.18940v2.pdf",
        "similarity": 0.28055803952760155,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-29"
    },
    {
        "new_title": "Representation learning with CGAN for casual inference",
        "new_link": "http://arxiv.org/abs/2407.02825v1",
        "new_summary": "  Conditional Generative Adversarial Nets (CGAN) is often used to improve\nconditional image generation performance. However, there is little research on\nRepresentation learning with CGAN for causal inference. This paper proposes a\nnew method for finding representation learning functions by adopting the\nadversarial idea. We apply the pattern of CGAN and theoretically emonstrate the\nfeasibility of finding a suitable representation function in the context of two\ndistributions being balanced. The theoretical result shows that when two\ndistributions are balanced, the ideal representation function can be found and\nthus can be used to further research.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.02825v1.pdf",
        "similarity": 0.28044606872207023,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-03"
    },
    {
        "new_title": "Resolution invariant deep operator network for PDEs with complex\n  geometries",
        "new_link": "http://arxiv.org/abs/2402.00825v1",
        "new_summary": "  Neural operators (NO) are discretization invariant deep learning methods with\nfunctional output and can approximate any continuous operator. NO have\ndemonstrated the superiority of solving partial differential equations (PDEs)\nover other deep learning methods. However, the spatial domain of its input\nfunction needs to be identical to its output, which limits its applicability.\nFor instance, the widely used Fourier neural operator (FNO) fails to\napproximate the operator that maps the boundary condition to the PDE solution.\nTo address this issue, we propose a novel framework called resolution-invariant\ndeep operator (RDO) that decouples the spatial domain of the input and output.\nRDO is motivated by the Deep operator network (DeepONet) and it does not\nrequire retraining the network when the input/output is changed compared with\nDeepONet. RDO takes functional input and its output is also functional so that\nit keeps the resolution invariant property of NO. It can also resolve PDEs with\ncomplex geometries whereas NO fail. Various numerical experiments demonstrate\nthe advantage of our method over DeepONet and FNO.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.00825v1.pdf",
        "similarity": 0.2802786996174851,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-01"
    },
    {
        "new_title": "Learning Structural Causal Models through Deep Generative Models:\n  Methods, Guarantees, and Challenges",
        "new_link": "http://arxiv.org/abs/2405.05025v1",
        "new_summary": "  This paper provides a comprehensive review of deep structural causal models\n(DSCMs), particularly focusing on their ability to answer counterfactual\nqueries using observational data within known causal structures. It delves into\nthe characteristics of DSCMs by analyzing the hypotheses, guarantees, and\napplications inherent to the underlying deep learning components and structural\ncausal models, fostering a finer understanding of their capabilities and\nlimitations in addressing different counterfactual queries. Furthermore, it\nhighlights the challenges and open questions in the field of deep structural\ncausal modeling. It sets the stages for researchers to identify future work\ndirections and for practitioners to get an overview in order to find out the\nmost appropriate methods for their needs.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.05025v1.pdf",
        "similarity": 0.28010669473028027,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-08"
    },
    {
        "new_title": "MERGE -- A Bimodal Dataset for Static Music Emotion Recognition",
        "new_link": "http://arxiv.org/abs/2407.06060v1",
        "new_summary": "  The Music Emotion Recognition (MER) field has seen steady developments in\nrecent years, with contributions from feature engineering, machine learning,\nand deep learning. The landscape has also shifted from audio-centric systems to\nbimodal ensembles that combine audio and lyrics. However, a severe lack of\npublic and sizeable bimodal databases has hampered the development and\nimprovement of bimodal audio-lyrics systems. This article proposes three new\naudio, lyrics, and bimodal MER research datasets, collectively called MERGE,\ncreated using a semi-automatic approach. To comprehensively assess the proposed\ndatasets and establish a baseline for benchmarking, we conducted several\nexperiments for each modality, using feature engineering, machine learning, and\ndeep learning methodologies. In addition, we propose and validate fixed\ntrain-validate-test splits. The obtained results confirm the viability of the\nproposed datasets, achieving the best overall result of 79.21% F1-score for\nbimodal classification using a deep neural network.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.06060v1.pdf",
        "similarity": 0.27963853270427025,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-08"
    },
    {
        "new_title": "Benchmarking Machine Learning Applications on Heterogeneous Architecture\n  using Reframe",
        "new_link": "http://arxiv.org/abs/2404.10536v2",
        "new_summary": "  With the rapid increase in machine learning workloads performed on HPC\nsystems, it is beneficial to regularly perform machine learning specific\nbenchmarks to monitor performance and identify issues. Furthermore, as part of\nthe Edinburgh International Data Facility, EPCC currently hosts a wide range of\nmachine learning accelerators including Nvidia GPUs, the Graphcore Bow Pod64\nand Cerebras CS-2, which are managed via Kubernetes and Slurm. We extended the\nReframe framework to support the Kubernetes scheduler backend, and utilise\nReframe to perform machine learning benchmarks, and we discuss the preliminary\nresults collected and challenges involved in integrating Reframe across\nmultiple platforms and architectures.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.10536v2.pdf",
        "similarity": 0.27941277739497067,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-16"
    },
    {
        "new_title": "Global Safe Sequential Learning via Efficient Knowledge Transfer",
        "new_link": "http://arxiv.org/abs/2402.14402v2",
        "new_summary": "  Sequential learning methods such as active learning and Bayesian optimization\nselect the most informative data to learn about a task. In many medical or\nengineering applications, the data selection is constrained by a priori unknown\nsafety conditions. A promissing line of safe learning methods utilize Gaussian\nprocesses (GPs) to model the safety probability and perform data selection in\nareas with high safety confidence. However, accurate safety modeling requires\nprior knowledge or consumes data. In addition, the safety confidence centers\naround the given observations which leads to local exploration. As transferable\nsource knowledge is often available in safety critical experiments, we propose\nto consider transfer safe sequential learning to accelerate the learning of\nsafety. We further consider a pre-computation of source components to reduce\nthe additional computational load that is introduced by incorporating source\ndata. In this paper, we theoretically analyze the maximum explorable safe\nregions of conventional safe learning methods. Furthermore, we empirically\ndemonstrate that our approach 1) learns a task with lower data consumption, 2)\nglobally explores multiple disjoint safe regions under guidance of the source\nknowledge, and 3) operates with computation comparable to conventional safe\nlearning methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.14402v2.pdf",
        "similarity": 0.2793593646854431,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-22"
    },
    {
        "new_title": "Interpretable Machine Learning for Survival Analysis",
        "new_link": "http://arxiv.org/abs/2403.10250v1",
        "new_summary": "  With the spread and rapid advancement of black box machine learning models,\nthe field of interpretable machine learning (IML) or explainable artificial\nintelligence (XAI) has become increasingly important over the last decade. This\nis particularly relevant for survival analysis, where the adoption of IML\ntechniques promotes transparency, accountability and fairness in sensitive\nareas, such as clinical decision making processes, the development of targeted\ntherapies, interventions or in other medical or healthcare related contexts.\nMore specifically, explainability can uncover a survival model's potential\nbiases and limitations and provide more mathematically sound ways to understand\nhow and which features are influential for prediction or constitute risk\nfactors. However, the lack of readily available IML methods may have deterred\nmedical practitioners and policy makers in public health from leveraging the\nfull potential of machine learning for predicting time-to-event data. We\npresent a comprehensive review of the limited existing amount of work on IML\nmethods for survival analysis within the context of the general IML taxonomy.\nIn addition, we formally detail how commonly used IML methods, such as such as\nindividual conditional expectation (ICE), partial dependence plots (PDP),\naccumulated local effects (ALE), different feature importance measures or\nFriedman's H-interaction statistics can be adapted to survival outcomes. An\napplication of several IML methods to real data on data on under-5 year\nmortality of Ghanaian children from the Demographic and Health Surveys (DHS)\nProgram serves as a tutorial or guide for researchers, on how to utilize the\ntechniques in practice to facilitate understanding of model decisions or\npredictions.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.10250v1.pdf",
        "similarity": 0.2788815091947802,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-15"
    },
    {
        "new_title": "On-Demand Model and Client Deployment in Federated Learning with Deep\n  Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2405.07175v1",
        "new_summary": "  In Federated Learning (FL), the limited accessibility of data from diverse\nlocations and user types poses a significant challenge due to restricted user\nparticipation. Expanding client access and diversifying data enhance models by\nincorporating diverse perspectives, thereby enhancing adaptability. However,\nchallenges arise in dynamic and mobile environments where certain devices may\nbecome inaccessible as FL clients, impacting data availability and client\nselection methods. To address this, we propose an On-Demand solution, deploying\nnew clients using Docker Containers on-the-fly. Our On-Demand solution,\nemploying Deep Reinforcement Learning (DRL), targets client availability and\nselection, while considering data shifts, and container deployment\ncomplexities. It employs an autonomous end-to-end solution for handling model\ndeployment and client selection. The DRL strategy uses a Markov Decision\nProcess (MDP) framework, with a Master Learner and a Joiner Learner. The\ndesigned cost functions represent the complexity of the dynamic client\ndeployment and selection. Simulated tests show that our architecture can easily\nadjust to changes in the environment and respond to On-Demand requests. This\nunderscores its ability to improve client availability, capability, accuracy,\nand learning efficiency, surpassing heuristic and tabular reinforcement\nlearning solutions.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.07175v1.pdf",
        "similarity": 0.2784964864198732,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-12"
    },
    {
        "new_title": "A Short Review for Ontology Learning: Stride to Large Language Models\n  Trend",
        "new_link": "http://arxiv.org/abs/2404.14991v2",
        "new_summary": "  Ontologies provide formal representation of knowledge shared within Semantic\nWeb applications. Ontology learning involves the construction of ontologies\nfrom a given corpus. In the past years, ontology learning has traversed through\nshallow learning and deep learning methodologies, each offering distinct\nadvantages and limitations in the quest for knowledge extraction and\nrepresentation. A new trend of these approaches is relying on large language\nmodels (LLMs) to enhance ontology learning. This paper gives a review in\napproaches and challenges of ontology learning. It analyzes the methodologies\nand limitations of shallow-learning-based and deep-learning-based techniques\nfor ontology learning, and provides comprehensive knowledge for the frontier\nwork of using LLMs to enhance ontology learning. In addition, it proposes\nseveral noteworthy future directions for further exploration into the\nintegration of LLMs with ontology learning tasks.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.14991v2.pdf",
        "similarity": 0.2783983196169172,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-23"
    },
    {
        "new_title": "Self-Attention and Hybrid Features for Replay and Deep-Fake Audio\n  Detection",
        "new_link": "http://arxiv.org/abs/2401.05614v1",
        "new_summary": "  Due to the successful application of deep learning, audio spoofing detection\nhas made significant progress. Spoofed audio with speech synthesis or voice\nconversion can be well detected by many countermeasures. However, an automatic\nspeaker verification system is still vulnerable to spoofing attacks such as\nreplay or Deep-Fake audio. Deep-Fake audio means that the spoofed utterances\nare generated using text-to-speech (TTS) and voice conversion (VC) algorithms.\nHere, we propose a novel framework based on hybrid features with the\nself-attention mechanism. It is expected that hybrid features can be used to\nget more discrimination capacity. Firstly, instead of only one type of\nconventional feature, deep learning features and Mel-spectrogram features will\nbe extracted by two parallel paths: convolution neural networks and a\nshort-time Fourier transform (STFT) followed by Mel-frequency. Secondly,\nfeatures will be concatenated by a max-pooling layer. Thirdly, there is a\nSelf-attention mechanism for focusing on essential elements. Finally, ResNet\nand a linear layer are built to get the results. Experimental results reveal\nthat the hybrid features, compared with conventional features, can cover more\ndetails of an utterance. We achieve the best Equal Error Rate (EER) of 9.67\\%\nin the physical access (PA) scenario and 8.94\\% in the Deep fake task on the\nASVspoof 2021 dataset. Compared with the best baseline system, the proposed\napproach improves by 74.60\\% and 60.05\\%, respectively.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.05614v1.pdf",
        "similarity": 0.27828011785355944,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-11"
    },
    {
        "new_title": "Compressed Online Learning of Conditional Mean Embedding",
        "new_link": "http://arxiv.org/abs/2405.07432v1",
        "new_summary": "  The conditional mean embedding (CME) encodes Markovian stochastic kernels\nthrough their actions on probability distributions embedded within the\nreproducing kernel Hilbert spaces (RKHS). The CME plays a key role in several\nwell-known machine learning tasks such as reinforcement learning, analysis of\ndynamical systems, etc. We present an algorithm to learn the CME incrementally\nfrom data via an operator-valued stochastic gradient descent. As is well-known,\nfunction learning in RKHS suffers from scalability challenges from large data.\nWe utilize a compression mechanism to counter the scalability challenge. The\ncore contribution of this paper is a finite-sample performance guarantee on the\nlast iterate of the online compressed operator learning algorithm with\nfast-mixing Markovian samples, when the target CME may not be contained in the\nhypothesis space. We illustrate the efficacy of our algorithm by applying it to\nthe analysis of an example dynamical system.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.07432v1.pdf",
        "similarity": 0.2780988202682764,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-13"
    },
    {
        "new_title": "Statistical validation of a deep learning algorithm for dental anomaly\n  detection in intraoral radiographs using paired data",
        "new_link": "http://arxiv.org/abs/2402.14022v1",
        "new_summary": "  This article describes the clinical validation study setup, statistical\nanalysis and results for a deep learning algorithm which detects dental\nanomalies in intraoral radiographic images, more specifically caries, apical\nlesions, root canal treatment defects, marginal defects at crown restorations,\nperiodontal bone loss and calculus. The study compares the detection\nperformance of dentists using the deep learning algorithm to the prior\nperformance of these dentists evaluating the images without algorithmic\nassistance. Calculating the marginal profit and loss of performance from the\nannotated paired image data allows for a quantification of the hypothesized\nchange in sensitivity and specificity. The statistical significance of these\nresults is extensively proven using both McNemar's test and the binomial\nhypothesis test. The average sensitivity increases from $60.7\\%$ to $85.9\\%$,\nwhile the average specificity slightly decreases from $94.5\\%$ to $92.7\\%$. We\nprove that the increase of the area under the localization ROC curve (AUC) is\nsignificant (from $0.60$ to $0.86$ on average), while the average AUC is\nbounded by the $95\\%$ confidence intervals ${[}0.54, 0.65{]}$ and ${[}0.82,\n0.90{]}$. When using the deep learning algorithm for diagnostic guidance, the\ndentist can be $95\\%$ confident that the average true population sensitivity is\nbounded by the range $79.6\\%$ to $91.9\\%$. The proposed paired data setup and\nstatistical analysis can be used as a blueprint to thoroughly test the effect\nof a modality change, like a deep learning based detection and/or segmentation,\non radiographic images.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.14022v1.pdf",
        "similarity": 0.2779047580079882,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-01"
    },
    {
        "new_title": "Towards Safe Load Balancing based on Control Barrier Functions and Deep\n  Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2401.05525v1",
        "new_summary": "  Deep Reinforcement Learning (DRL) algorithms have recently made significant\nstrides in improving network performance. Nonetheless, their practical use is\nstill limited in the absence of safe exploration and safe decision-making. In\nthe context of commercial solutions, reliable and safe-to-operate systems are\nof paramount importance. Taking this problem into account, we propose a safe\nlearning-based load balancing algorithm for Software Defined-Wide Area Network\n(SD-WAN), which is empowered by Deep Reinforcement Learning (DRL) combined with\na Control Barrier Function (CBF). It safely projects unsafe actions into\nfeasible ones during both training and testing, and it guides learning towards\nsafe policies. We successfully implemented the solution on GPU to accelerate\ntraining by approximately 110x times and achieve model updates for on-policy\nmethods within a few seconds, making the solution practical. We show that our\napproach delivers near-optimal Quality-of-Service (QoS performance in terms of\nend-to-end delay while respecting safety requirements related to link capacity\nconstraints. We also demonstrated that on-policy learning based on Proximal\nPolicy Optimization (PPO) performs better than off-policy learning with Deep\nDeterministic Policy Gradient (DDPG) when both are combined with a CBF for safe\nload balancing.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.05525v1.pdf",
        "similarity": 0.27759949558173386,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-10"
    },
    {
        "new_title": "A generic and robust quantum agent inspired by deep meta-reinforcement\n  learning",
        "new_link": "http://arxiv.org/abs/2406.07225v1",
        "new_summary": "  Deep reinforcement learning (deep RL) has enabled human- or superhuman-\nperformances in various applications. Recently, deep RL has also been adopted\nto improve the performance of quantum control. However, a large volume of data\nis typically required to train the neural network in deep RL, making it\ninefficient compared with the traditional optimal quantum control method. Here,\nwe thus develop a new training algorithm inspired by the deep\nmeta-reinforcement learning (deep meta-RL), which requires significantly less\ntraining data. The trained neural network is adaptive and robust. In addition,\nthe algorithm proposed by us has been applied to design the Hadamard gate and\nshow that for a wide range of parameters the infidelity of the obtained gate\ncan be made of the order 0.0001. Our algorithm can also automatically adjust\nthe number of pulses required to generate the target gate, which is different\nfrom the traditional optimal quantum control method which typically fixes the\nnumber of pulses a-priory. The results of this paper can pave the way towards\nconstructing a universally robust quantum agent catering to the different\ndemands in quantum technologies.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.07225v1.pdf",
        "similarity": 0.2775510781641463,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-11"
    },
    {
        "new_title": "Counterfactual Fairness through Transforming Data Orthogonal to Bias",
        "new_link": "http://arxiv.org/abs/2403.17852v2",
        "new_summary": "  Machine learning models have shown exceptional prowess in solving complex\nissues across various domains. However, these models can sometimes exhibit\nbiased decision-making, resulting in unequal treatment of different groups.\nDespite substantial research on counterfactual fairness, methods to reduce the\nimpact of multivariate and continuous sensitive variables on decision-making\noutcomes are still underdeveloped. We propose a novel data pre-processing\nalgorithm, Orthogonal to Bias (OB), which is designed to eliminate the\ninfluence of a group of continuous sensitive variables, thus promoting\ncounterfactual fairness in machine learning applications. Our approach, based\non the assumption of a jointly normal distribution within a structural causal\nmodel (SCM), demonstrates that counterfactual fairness can be achieved by\nensuring the data is orthogonal to the observed sensitive variables. The OB\nalgorithm is model-agnostic, making it applicable to a wide range of machine\nlearning models and tasks. Additionally, it includes a sparse variant to\nimprove numerical stability through regularization. Empirical evaluations on\nboth simulated and real-world datasets, encompassing settings with both\ndiscrete and continuous sensitive variables, show that our methodology\neffectively promotes fairer outcomes without compromising accuracy.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.17852v2.pdf",
        "similarity": 0.27715165104894823,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-26"
    },
    {
        "new_title": "FAIRM: Learning invariant representations for algorithmic fairness and\n  domain generalization with minimax optimality",
        "new_link": "http://arxiv.org/abs/2404.01608v1",
        "new_summary": "  Machine learning methods often assume that the test data have the same\ndistribution as the training data. However, this assumption may not hold due to\nmultiple levels of heterogeneity in applications, raising issues in algorithmic\nfairness and domain generalization. In this work, we address the problem of\nfair and generalizable machine learning by invariant principles. We propose a\ntraining environment-based oracle, FAIRM, which has desirable fairness and\ndomain generalization properties under a diversity-type condition. We then\nprovide an empirical FAIRM with finite-sample theoretical guarantees under weak\ndistributional assumptions. We then develop efficient algorithms to realize\nFAIRM in linear models and demonstrate the nonasymptotic performance with\nminimax optimality. We evaluate our method in numerical experiments with\nsynthetic data and MNIST data and show that it outperforms its counterparts.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.01608v1.pdf",
        "similarity": 0.2770194653098503,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-02"
    },
    {
        "new_title": "A Machine Learning Ensemble Model for the Detection of Cyberbullying",
        "new_link": "http://arxiv.org/abs/2402.12538v1",
        "new_summary": "  The pervasive use of social media platforms, such as Facebook, Instagram, and\nX, has significantly amplified our electronic interconnectedness. Moreover,\nthese platforms are now easily accessible from any location at any given time.\nHowever, the increased popularity of social media has also led to\ncyberbullying.It is imperative to address the need for finding, monitoring, and\nmitigating cyberbullying posts on social media platforms. Motivated by this\nnecessity, we present this paper to contribute to developing an automated\nsystem for detecting binary labels of aggressive tweets.Our study has\ndemonstrated remarkable performance compared to previous experiments on the\nsame dataset. We employed the stacking ensemble machine learning method,\nutilizing four various feature extraction techniques to optimize performance\nwithin the stacking ensemble learning framework. Combining five machine\nlearning algorithms,Decision Trees, Random Forest, Linear Support Vector\nClassification, Logistic Regression, and K-Nearest Neighbors into an ensemble\nmethod, we achieved superior results compared to traditional machine learning\nclassifier models. The stacking classifier achieved a high accuracy rate of\n94.00%, outperforming traditional machine learning models and surpassing the\nresults of prior experiments that utilized the same dataset. The outcomes of\nour experiments showcased an accuracy rate of 0.94% in detection tweets as\naggressive or non-aggressive.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.12538v1.pdf",
        "similarity": 0.2767249306621726,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-19"
    },
    {
        "new_title": "Combining Machine Learning and Ontology: A Systematic Literature Review",
        "new_link": "http://arxiv.org/abs/2401.07744v2",
        "new_summary": "  Motivated by the desire to explore the process of combining inductive and\ndeductive reasoning, we conducted a systematic literature review of articles\nthat investigate the integration of machine learning and ontologies. The\nobjective was to identify diverse techniques that incorporate both inductive\nreasoning (performed by machine learning) and deductive reasoning (performed by\nontologies) into artificial intelligence systems. Our review, which included\nthe analysis of 128 studies, allowed us to identify three main categories of\nhybridization between machine learning and ontologies: learning-enhanced\nontologies, semantic data mining, and learning and reasoning systems. We\nprovide a comprehensive examination of all these categories, emphasizing the\nvarious machine learning algorithms utilized in the studies. Furthermore, we\ncompared our classification with similar recent work in the field of hybrid AI\nand neuro-symbolic approaches.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.07744v2.pdf",
        "similarity": 0.27671073075122915,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-15"
    },
    {
        "new_title": "Hierarchical energy signatures using machine learning for operational\n  visibility and diagnostics in automotive manufacturing",
        "new_link": "http://arxiv.org/abs/2402.15962v1",
        "new_summary": "  Manufacturing energy consumption data contains important process signatures\nrequired for operational visibility and diagnostics. These signatures may be of\ndifferent temporal scales, ranging from monthly to sub-second resolutions. We\nintroduce a hierarchical machine learning approach to identify automotive\nprocess signatures from paint shop electricity consumption data at varying\ntemporal scales (weekly and daily). A Multi-Layer Perceptron (MLP), a\nConvolutional Neural Network (CNN), and Principal Component Analysis (PCA)\ncombined with Logistic Regression (LR) are used for the analysis. We validate\nthe utility of the developed algorithms with subject matter experts for (i)\nbetter operational visibility, and (ii) identifying energy saving\nopportunities.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.15962v1.pdf",
        "similarity": 0.27648576096383415,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-25"
    },
    {
        "new_title": "Machine Learning Based Prediction of Proton Conductivity in\n  Metal-Organic Frameworks",
        "new_link": "http://arxiv.org/abs/2407.09514v2",
        "new_summary": "  Recently, metal-organic frameworks (MOFs) have demonstrated their potential\nas solid-state electrolytes in proton exchange membrane fuel cells. However,\nthe number of MOFs reported to exhibit proton conductivity remains limited, and\nthe mechanisms underlying this phenomenon are not fully elucidated,\ncomplicating the design of proton-conductive MOFs. In response, we developed a\ncomprehensive database of proton-conductive MOFs and applied machine learning\ntechniques to predict their proton conductivity. Our approach included the\nconstruction of both descriptor-based and transformer-based models. Notably,\nthe transformer-based transfer learning (Freeze) model performed the best with\na mean absolute error (MAE) of 0.91, suggesting that the proton conductivity of\nMOFs can be estimated within one order of magnitude using this model.\nAdditionally, we employed feature importance and principal component analysis\nto explore the factors influencing proton conductivity. The insights gained\nfrom our database and machine learning model are expected to facilitate the\ntargeted design of proton-conductive MOFs.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.09514v2.pdf",
        "similarity": 0.2758833084101407,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-18"
    },
    {
        "new_title": "IoT Network Traffic Analysis with Deep Learning",
        "new_link": "http://arxiv.org/abs/2402.04469v1",
        "new_summary": "  As IoT networks become more complex and generate massive amounts of dynamic\ndata, it is difficult to monitor and detect anomalies using traditional\nstatistical methods and machine learning methods. Deep learning algorithms can\nprocess and learn from large amounts of data and can also be trained using\nunsupervised learning techniques, meaning they don't require labelled data to\ndetect anomalies. This makes it possible to detect new and unknown anomalies\nthat may not have been detected before. Also, deep learning algorithms can be\nautomated and highly scalable; thereby, they can run continuously in the\nbackend and make it achievable to monitor large IoT networks instantly. In this\nwork, we conduct a literature review on the most recent works using deep\nlearning techniques and implement a model using ensemble techniques on the KDD\nCup 99 dataset. The experimental results showcase the impressive performance of\nour deep anomaly detection model, achieving an accuracy of over 98\\%.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.04469v1.pdf",
        "similarity": 0.27580930873215387,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-06"
    },
    {
        "new_title": "Learning Linear Utility Functions From Pairwise Comparison Queries",
        "new_link": "http://arxiv.org/abs/2405.02612v3",
        "new_summary": "  We study learnability of linear utility functions from pairwise comparison\nqueries. In particular, we consider two learning objectives. The first\nobjective is to predict out-of-sample responses to pairwise comparisons,\nwhereas the second is to approximately recover the true parameters of the\nutility function. We show that in the passive learning setting, linear\nutilities are efficiently learnable with respect to the first objective, both\nwhen query responses are uncorrupted by noise, and under Tsybakov noise when\nthe distributions are sufficiently \"nice\". In contrast, we show that utility\nparameters are not learnable for a large set of data distributions without\nstrong modeling assumptions, even when query responses are noise-free. Next, we\nproceed to analyze the learning problem in an active learning setting. In this\ncase, we show that even the second objective is efficiently learnable, and\npresent algorithms for both the noise-free and noisy query response settings.\nOur results thus exhibit a qualitative learnability gap between passive and\nactive learning from pairwise preference queries, demonstrating the value of\nthe ability to select pairwise queries for utility learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.02612v3.pdf",
        "similarity": 0.275782813648921,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-04"
    },
    {
        "new_title": "A Benchmark Study of Deep-RL Methods for Maximum Coverage Problems over\n  Graphs",
        "new_link": "http://arxiv.org/abs/2406.14697v2",
        "new_summary": "  Recent years have witnessed a growing trend toward employing deep\nreinforcement learning (Deep-RL) to derive heuristics for combinatorial\noptimization (CO) problems on graphs. Maximum Coverage Problem (MCP) and its\nprobabilistic variant on social networks, Influence Maximization (IM), have\nbeen particularly prominent in this line of research. In this paper, we present\na comprehensive benchmark study that thoroughly investigates the effectiveness\nand efficiency of five recent Deep-RL methods for MCP and IM. These methods\nwere published in top data science venues, namely S2V-DQN, Geometric-QN, GCOMB,\nRL4IM, and LeNSE. Our findings reveal that, across various scenarios, the Lazy\nGreedy algorithm consistently outperforms all Deep-RL methods for MCP. In the\ncase of IM, theoretically sound algorithms like IMM and OPIM demonstrate\nsuperior performance compared to Deep-RL methods in most scenarios. Notably, we\nobserve an abnormal phenomenon in IM problem where Deep-RL methods slightly\noutperform IMM and OPIM when the influence spread nearly does not increase as\nthe budget increases. Furthermore, our experimental results highlight common\nissues when applying Deep-RL methods to MCP and IM in practical settings.\nFinally, we discuss potential avenues for improving Deep-RL methods. Our\nbenchmark study sheds light on potential challenges in current deep\nreinforcement learning research for solving combinatorial optimization\nproblems.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.14697v2.pdf",
        "similarity": 0.2757286671667009,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-20"
    },
    {
        "new_title": "Unification of Symmetries Inside Neural Networks: Transformer,\n  Feedforward and Neural ODE",
        "new_link": "http://arxiv.org/abs/2402.02362v1",
        "new_summary": "  Understanding the inner workings of neural networks, including transformers,\nremains one of the most challenging puzzles in machine learning. This study\nintroduces a novel approach by applying the principles of gauge symmetries, a\nkey concept in physics, to neural network architectures. By regarding model\nfunctions as physical observables, we find that parametric redundancies of\nvarious machine learning models can be interpreted as gauge symmetries. We\nmathematically formulate the parametric redundancies in neural ODEs, and find\nthat their gauge symmetries are given by spacetime diffeomorphisms, which play\na fundamental role in Einstein's theory of gravity. Viewing neural ODEs as a\ncontinuum version of feedforward neural networks, we show that the parametric\nredundancies in feedforward neural networks are indeed lifted to\ndiffeomorphisms in neural ODEs. We further extend our analysis to transformer\nmodels, finding natural correspondences with neural ODEs and their gauge\nsymmetries. The concept of gauge symmetries sheds light on the complex behavior\nof deep learning models through physics and provides us with a unifying\nperspective for analyzing various machine learning architectures.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.02362v1.pdf",
        "similarity": 0.27570854870176875,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-04"
    },
    {
        "new_title": "Validation, Robustness, and Accuracy of Perturbation-Based Sensitivity\n  Analysis Methods for Time-Series Deep Learning Models",
        "new_link": "http://arxiv.org/abs/2401.16521v1",
        "new_summary": "  This work undertakes studies to evaluate Interpretability Methods for\nTime-Series Deep Learning. Sensitivity analysis assesses how input changes\naffect the output, constituting a key component of interpretation. Among the\npost-hoc interpretation methods such as back-propagation, perturbation, and\napproximation, my work will investigate perturbation-based sensitivity Analysis\nmethods on modern Transformer models to benchmark their performances.\nSpecifically, my work answers three research questions: 1) Do different\nsensitivity analysis (SA) methods yield comparable outputs and attribute\nimportance rankings? 2) Using the same sensitivity analysis method, do\ndifferent Deep Learning (DL) models impact the output of the sensitivity\nanalysis? 3) How well do the results from sensitivity analysis methods align\nwith the ground truth?\n",
        "pdf_link": "https://arxiv.org/pdf/2401.16521v1.pdf",
        "similarity": 0.2756196388842107,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-29"
    },
    {
        "new_title": "Quantum Extreme Learning of molecular potential energy surfaces and\n  force fields",
        "new_link": "http://arxiv.org/abs/2406.14607v1",
        "new_summary": "  Quantum machine learning algorithms are expected to play a pivotal role in\nquantum chemistry simulations in the immediate future. One such key application\nis the training of a quantum neural network to learn the potential energy\nsurface and force field of molecular systems. We address this task by using the\nquantum extreme learning machine paradigm. This particular supervised learning\nroutine allows for resource-efficient training, consisting of a simple linear\nregression performed on a classical computer. We have tested a setup that can\nbe used to study molecules of any dimension and is optimized for immediate use\non NISQ devices with a limited number of native gates. We have applied this\nsetup to three case studies: lithium hydride, water, and formamide, carrying\nout both noiseless simulations and actual implementation on IBM quantum\nhardware. Compared to other supervised learning routines, the proposed setup\nrequires minimal quantum resources, making it feasible for direct\nimplementation on quantum platforms, while still achieving a high level of\npredictive accuracy compared to simulations. Our encouraging results pave the\nway towards the future application to more complex molecules, being the\nproposed setup scalable.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.14607v1.pdf",
        "similarity": 0.2752427603735634,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-20"
    },
    {
        "new_title": "Physics-Inspired Deep Learning Anti-Aliasing Framework in Efficient\n  Channel State Feedback",
        "new_link": "http://arxiv.org/abs/2403.08133v1",
        "new_summary": "  Acquiring downlink channel state information (CSI) at the base station is\nvital for optimizing performance in massive Multiple input multiple output\n(MIMO) Frequency-Division Duplexing (FDD) systems. While deep learning\narchitectures have been successful in facilitating UE-side CSI feedback and\ngNB-side recovery, the undersampling issue prior to CSI feedback is often\noverlooked. This issue, which arises from low density pilot placement in\ncurrent standards, results in significant aliasing effects in outdoor channels\nand consequently limits CSI recovery performance. To this end, this work\nintroduces a new CSI upsampling framework at the gNB as a post-processing\nsolution to address the gaps caused by undersampling. Leveraging the physical\nprinciples of discrete Fourier transform shifting theorem and multipath\nreciprocity, our framework effectively uses uplink CSI to mitigate aliasing\neffects. We further develop a learning-based method that integrates the\nproposed algorithm with the Iterative Shrinkage-Thresholding Algorithm Net\n(ISTA-Net) architecture, enhancing our approach for non-uniform sampling\nrecovery. Our numerical results show that both our rule-based and deep learning\nmethods significantly outperform traditional interpolation techniques and\ncurrent state-of-the-art approaches in terms of performance.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.08133v1.pdf",
        "similarity": 0.2747656783778889,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-12"
    },
    {
        "new_title": "Deep learning-based variational autoencoder for classification of\n  quantum and classical states of light",
        "new_link": "http://arxiv.org/abs/2405.05243v1",
        "new_summary": "  Advancements in optical quantum technologies have been enabled by the\ngeneration, manipulation, and characterization of light, with identification\nbased on its photon statistics. However, characterizing light and its sources\nthrough single photon measurements often requires efficient detectors and\nlonger measurement times to obtain high-quality photon statistics. Here we\nintroduce a deep learning-based variational autoencoder (VAE) method for\nclassifying single photon added coherent state (SPACS), single photon added\nthermal state (SPACS), mixed states between coherent/SPACS and thermal/SPATS of\nlight. Our semisupervised learning-based VAE efficiently maps the photon\nstatistics features of light to a lower dimension, enabling quasi-instantaneous\nclassification with low average photon counts. The proposed VAE method is\nrobust and maintains classification accuracy in the presence of losses inherent\nin an experiment, such as finite collection efficiency, non-unity quantum\nefficiency, finite number of detectors, etc. Additionally, leveraging the\ntransfer learning capabilities of VAE enables successful classification of data\nof any quality using a single trained model. We envision that such a deep\nlearning methodology will enable better classification of quantum light and\nlight sources even in the presence of poor detection quality.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.05243v1.pdf",
        "similarity": 0.27456169206707004,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-08"
    },
    {
        "new_title": "MDDD: Manifold-based Domain Adaptation with Dynamic Distribution for\n  Non-Deep Transfer Learning in Cross-subject and Cross-session EEG-based\n  Emotion Recognition",
        "new_link": "http://arxiv.org/abs/2404.15615v1",
        "new_summary": "  Emotion decoding using Electroencephalography (EEG)-based affective\nbrain-computer interfaces represents a significant area within the field of\naffective computing. In the present study, we propose a novel non-deep transfer\nlearning method, termed as Manifold-based Domain adaptation with Dynamic\nDistribution (MDDD). The proposed MDDD includes four main modules: manifold\nfeature transformation, dynamic distribution alignment, classifier learning,\nand ensemble learning. The data undergoes a transformation onto an optimal\nGrassmann manifold space, enabling dynamic alignment of the source and target\ndomains. This process prioritizes both marginal and conditional distributions\naccording to their significance, ensuring enhanced adaptation efficiency across\nvarious types of data. In the classifier learning, the principle of structural\nrisk minimization is integrated to develop robust classification models. This\nis complemented by dynamic distribution alignment, which refines the classifier\niteratively. Additionally, the ensemble learning module aggregates the\nclassifiers obtained at different stages of the optimization process, which\nleverages the diversity of the classifiers to enhance the overall prediction\naccuracy. The experimental results indicate that MDDD outperforms traditional\nnon-deep learning methods, achieving an average improvement of 3.54%, and is\ncomparable to deep learning methods. This suggests that MDDD could be a\npromising method for enhancing the utility and applicability of aBCIs in\nreal-world scenarios.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.15615v1.pdf",
        "similarity": 0.27349125422994436,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-24"
    },
    {
        "new_title": "Lagrangian operator inference enhanced with structure-preserving machine\n  learning for nonintrusive model reduction of mechanical systems",
        "new_link": "http://arxiv.org/abs/2404.05040v1",
        "new_summary": "  Complex mechanical systems often exhibit strongly nonlinear behavior due to\nthe presence of nonlinearities in the energy dissipation mechanisms, material\nconstitutive relationships, or geometric/connectivity mechanics. Numerical\nmodeling of these systems leads to nonlinear full-order models that possess an\nunderlying Lagrangian structure. This work proposes a Lagrangian operator\ninference method enhanced with structure-preserving machine learning to learn\nnonlinear reduced-order models (ROMs) of nonlinear mechanical systems. This\ntwo-step approach first learns the best-fit linear Lagrangian ROM via\nLagrangian operator inference and then presents a structure-preserving machine\nlearning method to learn nonlinearities in the reduced space. The proposed\napproach can learn a structure-preserving nonlinear ROM purely from data,\nunlike the existing operator inference approaches that require knowledge about\nthe mathematical form of nonlinear terms. From a machine learning perspective,\nit accelerates the training of the structure-preserving neural network by\nproviding an informed prior, and it reduces the computational cost of the\nnetwork training by operating on the reduced space. The method is first\ndemonstrated on two simulated examples: a conservative nonlinear rod model and\na two-dimensional nonlinear membrane with nonlinear internal damping. Finally,\nthe method is demonstrated on an experimental dataset consisting of digital\nimage correlation measurements taken from a lap-joint beam structure from which\na predictive model is learned that captures amplitude-dependent frequency and\ndamping characteristics accurately. The numerical results demonstrate that the\nproposed approach yields generalizable nonlinear ROMs that exhibit bounded\nenergy error, capture the nonlinear characteristics reliably, and provide\naccurate long-time predictions outside the training data regime.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.05040v1.pdf",
        "similarity": 0.2733616314301438,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-07"
    },
    {
        "new_title": "Brain Tumor Classification From MRI Images Using Machine Learning",
        "new_link": "http://arxiv.org/abs/2407.10630v1",
        "new_summary": "  Brain tumor is a life-threatening problem and hampers the normal functioning\nof the human body. The average five-year relative survival rate for malignant\nbrain tumors is 35.6 percent. For proper diagnosis and efficient treatment\nplanning, it is necessary to detect the brain tumor in early stages. Due to\nadvancement in medical imaging technology, the brain images are taken in\ndifferent modalities. The ability to extract relevant characteristics from\nmagnetic resonance imaging (MRI) scans is a crucial step for brain tumor\nclassifiers. Several studies have proposed various strategies to extract\nrelevant features from different modalities of MRI to predict the growth of\nabnormal tumors. Most techniques used conventional methods of image processing\nfor feature extraction and machine learning for classification. More recently,\nthe use of deep learning algorithms in medical imaging has resulted in\nsignificant improvements in the classification and diagnosis of brain tumors.\nSince tumors are located at different regions of the brain, localizing the\ntumor and classifying it to a particular category is a challenging task. The\nobjective of this project is to develop a predictive system for brain tumor\ndetection using machine learning(ensembling).\n",
        "pdf_link": "https://arxiv.org/pdf/2407.10630v1.pdf",
        "similarity": 0.27313158227726547,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-15"
    },
    {
        "new_title": "Deep Penalty Methods: A Class of Deep Learning Algorithms for Solving\n  High Dimensional Optimal Stopping Problems",
        "new_link": "http://arxiv.org/abs/2405.11392v1",
        "new_summary": "  We propose a deep learning algorithm for high dimensional optimal stopping\nproblems. Our method is inspired by the penalty method for solving free\nboundary PDEs. Within our approach, the penalized PDE is approximated using the\nDeep BSDE framework proposed by \\cite{weinan2017deep}, which leads us to coin\nthe term \"Deep Penalty Method (DPM)\" to refer to our algorithm. We show that\nthe error of the DPM can be bounded by the loss function and\n$O(\\frac{1}{\\lambda})+O(\\lambda h) +O(\\sqrt{h})$, where $h$ is the step size in\ntime and $\\lambda$ is the penalty parameter. This finding emphasizes the need\nfor careful consideration when selecting the penalization parameter and\nsuggests that the discretization error converges at a rate of order\n$\\frac{1}{2}$. We validate the efficacy of the DPM through numerical tests\nconducted on a high-dimensional optimal stopping model in the area of American\noption pricing. The numerical tests confirm both the accuracy and the\ncomputational efficiency of our proposed algorithm.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.11392v1.pdf",
        "similarity": 0.272923050374358,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-18"
    },
    {
        "new_title": "Enhancing High-Speed Cruising Performance of Autonomous Vehicles through\n  Integrated Deep Reinforcement Learning Framework",
        "new_link": "http://arxiv.org/abs/2404.14713v1",
        "new_summary": "  High-speed cruising scenarios with mixed traffic greatly challenge the road\nsafety of autonomous vehicles (AVs). Unlike existing works that only look at\nfundamental modules in isolation, this work enhances AV safety in mixed-traffic\nhigh-speed cruising scenarios by proposing an integrated framework that\nsynthesizes three fundamental modules, i.e., behavioral decision-making,\npath-planning, and motion-control modules. Considering that the integrated\nframework would increase the system complexity, a bootstrapped deep Q-Network\n(DQN) is employed to enhance the deep exploration of the reinforcement learning\nmethod and achieve adaptive decision making of AVs. Moreover, to make AV\nbehavior understandable by surrounding HDVs to prevent unexpected operations\ncaused by misinterpretations, we derive an inverse reinforcement learning (IRL)\napproach to learn the reward function of skilled drivers for the path planning\nof lane-changing maneuvers. Such a design enables AVs to achieve a human-like\ntradeoff between multi-performance requirements. Simulations demonstrate that\nthe proposed integrated framework can guide AVs to take safe actions while\nguaranteeing high-speed cruising performance.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.14713v1.pdf",
        "similarity": 0.27289068899872365,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-23"
    },
    {
        "new_title": "Validating Deep-Learning Weather Forecast Models on Recent High-Impact\n  Extreme Events",
        "new_link": "http://arxiv.org/abs/2404.17652v1",
        "new_summary": "  The forecast accuracy of deep-learning-based weather prediction models is\nimproving rapidly, leading many to speak of a \"second revolution in weather\nforecasting\". With numerous methods being developed, and limited physical\nguarantees offered by deep-learning models, there is a critical need for\ncomprehensive evaluation of these emerging techniques. While this need has been\npartly fulfilled by benchmark datasets, they provide little information on rare\nand impactful extreme events, or on compound impact metrics, for which model\naccuracy might degrade due to misrepresented dependencies between variables. To\naddress these issues, we compare deep-learning weather prediction models\n(GraphCast, PanguWeather, FourCastNet) and ECMWF's high-resolution forecast\n(HRES) system in three case studies: the 2021 Pacific Northwest heatwave, the\n2023 South Asian humid heatwave, and the North American winter storm in 2021.\nWe find evidence that machine learning (ML) weather prediction models can\nlocally achieve similar accuracy to HRES on record-shattering events such as\nthe 2021 Pacific Northwest heatwave and even forecast the compound 2021 North\nAmerican winter storm substantially better. However, extrapolating to extreme\nconditions may impact machine learning models more severely than HRES, as\nevidenced by the comparable or superior spatially- and temporally-aggregated\nforecast accuracy of HRES for the two heatwaves studied. The ML forecasts also\nlack variables required to assess the health risks of events such as the 2023\nSouth Asian humid heatwave. Generally, case-study-driven, impact-centric\nevaluation can complement existing research, increase public trust, and aid in\ndeveloping reliable ML weather prediction models.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.17652v1.pdf",
        "similarity": 0.2723545396463043,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-26"
    },
    {
        "new_title": "A Theory of Machine Learning",
        "new_link": "http://arxiv.org/abs/2407.05520v1",
        "new_summary": "  We critically review three major theories of machine learning and provide a\nnew theory according to which machines learn a function when the machines\nsuccessfully compute it. We show that this theory challenges common assumptions\nin the statistical and the computational learning theories, for it implies that\nlearning true probabilities is equivalent neither to obtaining a correct\ncalculation of the true probabilities nor to obtaining an almost-sure\nconvergence to them. We also briefly discuss some case studies from natural\nlanguage processing and macroeconomics from the perspective of the new theory.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.05520v1.pdf",
        "similarity": 0.2721556557200961,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-07"
    },
    {
        "new_title": "Employee Turnover Analysis Using Machine Learning Algorithms",
        "new_link": "http://arxiv.org/abs/2402.03905v1",
        "new_summary": "  Employee's knowledge is an organization asset. Turnover may impose apparent\nand hidden costs and irreparable damages. To overcome and mitigate this risk,\nemployee's condition should be monitored. Due to high complexity of analyzing\nwell-being features, employee's turnover predicting can be delegated to machine\nlearning techniques. In this paper, we discuss employee's attrition rate. Three\ndifferent supervised learning algorithms comprising AdaBoost, SVM and\nRandomForest are used to benchmark employee attrition accuracy. Attained models\ncan help out at establishing predictive analytics.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.03905v1.pdf",
        "similarity": 0.2720375188825257,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-06"
    },
    {
        "new_title": "Potential of Domain Adaptation in Machine Learning in Ecology and\n  Hydrology to Improve Model Extrapolability",
        "new_link": "http://arxiv.org/abs/2403.11331v1",
        "new_summary": "  Due to the heterogeneity of the global distribution of ecological and\nhydrological ground-truth observations, machine learning models can have\nlimited adaptability when applied to unknown locations, which is referred to as\nweak extrapolability. Domain adaptation techniques have been widely used in\nmachine learning domains such as image classification, which can improve the\nmodel generalization ability by adjusting the difference or inconsistency of\nthe domain distribution between the training and test sets. However, this\napproach has rarely been used explicitly in machine learning models in ecology\nand hydrology at the global scale, although these models have often been\nquestioned due to geographic extrapolability issues. This paper briefly\ndescribes the shortcomings of current machine learning models of ecology and\nhydrology in terms of the global representativeness of the distribution of\nobservations and the resulting limitations of the lack of extrapolability and\nsuggests that future related modelling efforts should consider the use of\ndomain adaptation techniques to improve extrapolability.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.11331v1.pdf",
        "similarity": 0.27191305292297197,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-17"
    },
    {
        "new_title": "Systematic assessment of various universal machine-learning interatomic\n  potentials",
        "new_link": "http://arxiv.org/abs/2403.05729v3",
        "new_summary": "  Machine-learning interatomic potentials have revolutionized materials\nmodeling at the atomic scale. Thanks to these, it is now indeed possible to\nperform simulations of \\abinitio quality over very large time and length\nscales. More recently, various universal machine-learning models have been\nproposed as an out-of-box approach avoiding the need to train and validate\nspecific potentials for each particular material of interest. In this paper, we\nreview and evaluate five different universal machine-learning interatomic\npotentials (uMLIPs), all based on graph neural network architectures which have\ndemonstrated transferability from one chemical system to another. The\nevaluation procedure relies on data both from a recent verification study of\ndensity-functional-theory implementations and from the Materials Project.\nThrough this comprehensive evaluation, we aim to provide guidance to materials\nscientists in selecting suitable models for their specific research problems,\noffer recommendations for model selection and optimization, and stimulate\ndiscussion on potential areas for improvement in current machine-learning\nmethodologies in materials science.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.05729v3.pdf",
        "similarity": 0.2716518662718628,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-08"
    },
    {
        "new_title": "Harnessing Data and Physics for Deep Learning Phase Recovery",
        "new_link": "http://arxiv.org/abs/2404.01360v1",
        "new_summary": "  Phase recovery, calculating the phase of a light wave from its intensity\nmeasurements, is essential for various applications, such as coherent\ndiffraction imaging, adaptive optics, and biomedical imaging. It enables the\nreconstruction of an object's refractive index distribution or topography as\nwell as the correction of imaging system aberrations. In recent years, deep\nlearning has been proven to be highly effective in addressing phase recovery\nproblems. Two main deep learning phase recovery strategies are data-driven (DD)\nwith supervised learning mode and physics-driven (PD) with self-supervised\nlearning mode. DD and PD achieve the same goal in different ways and lack the\nnecessary study to reveal similarities and differences. Therefore, in this\npaper, we comprehensively compare these two deep learning phase recovery\nstrategies in terms of time consumption, accuracy, generalization ability,\nill-posedness adaptability, and prior capacity. What's more, we propose a\nco-driven (CD) strategy of combining datasets and physics for the balance of\nhigh- and low-frequency information. The codes for DD, PD, and CD are publicly\navailable at https://github.com/kqwang/DLPR.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.01360v1.pdf",
        "similarity": 0.27159589856899463,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-01"
    },
    {
        "new_title": "Dimensional Neuroimaging Endophenotypes: Neurobiological Representations\n  of Disease Heterogeneity Through Machine Learning",
        "new_link": "http://arxiv.org/abs/2401.09517v1",
        "new_summary": "  Machine learning has been increasingly used to obtain individualized\nneuroimaging signatures for disease diagnosis, prognosis, and response to\ntreatment in neuropsychiatric and neurodegenerative disorders. Therefore, it\nhas contributed to a better understanding of disease heterogeneity by\nidentifying disease subtypes that present significant differences in various\nbrain phenotypic measures. In this review, we first present a systematic\nliterature overview of studies using machine learning and multimodal MRI to\nunravel disease heterogeneity in various neuropsychiatric and neurodegenerative\ndisorders, including Alzheimer disease, schizophrenia, major depressive\ndisorder, autism spectrum disorder, multiple sclerosis, as well as their\npotential in transdiagnostic settings. Subsequently, we summarize relevant\nmachine learning methodologies and discuss an emerging paradigm which we call\ndimensional neuroimaging endophenotype (DNE). DNE dissects the neurobiological\nheterogeneity of neuropsychiatric and neurodegenerative disorders into a low\ndimensional yet informative, quantitative brain phenotypic representation,\nserving as a robust intermediate phenotype (i.e., endophenotype) largely\nreflecting underlying genetics and etiology. Finally, we discuss the potential\nclinical implications of the current findings and envision future research\navenues.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.09517v1.pdf",
        "similarity": 0.2715433058468277,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-17"
    },
    {
        "new_title": "Constrained Neural Networks for Interpretable Heuristic Creation to\n  Optimise Computer Algebra Systems",
        "new_link": "http://arxiv.org/abs/2404.17508v1",
        "new_summary": "  We present a new methodology for utilising machine learning technology in\nsymbolic computation research. We explain how a well known human-designed\nheuristic to make the choice of variable ordering in cylindrical algebraic\ndecomposition may be represented as a constrained neural network. This allows\nus to then use machine learning methods to further optimise the heuristic,\nleading to new networks of similar size, representing new heuristics of similar\ncomplexity as the original human-designed one. We present this as a form of\nante-hoc explainability for use in computer algebra development.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.17508v1.pdf",
        "similarity": 0.27147028986743993,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-26"
    },
    {
        "new_title": "Symplectic Methods in Deep Learning",
        "new_link": "http://arxiv.org/abs/2406.04104v1",
        "new_summary": "  Deep learning is widely used in tasks including image recognition and\ngeneration, in learning dynamical systems from data and many more. It is\nimportant to construct learning architectures with theoretical guarantees to\npermit safety in the applications. There has been considerable progress in this\ndirection lately. In particular, symplectic networks were shown to have the non\nvanishing gradient property, essential for numerical stability. On the other\nhand, architectures based on higher order numerical methods were shown to be\nefficient in many tasks where the learned function has an underlying dynamical\nstructure. In this work we construct symplectic networks based on higher order\nexplicit methods with non vanishing gradient property and test their efficiency\non various examples.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.04104v1.pdf",
        "similarity": 0.2712487012652433,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-06"
    },
    {
        "new_title": "Multiple Locally Linear Kernel Machines",
        "new_link": "http://arxiv.org/abs/2401.09629v1",
        "new_summary": "  In this paper we propose a new non-linear classifier based on a combination\nof locally linear classifiers. A well known optimization formulation is given\nas we cast the problem in a $\\ell_1$ Multiple Kernel Learning (MKL) problem\nusing many locally linear kernels. Since the number of such kernels is huge, we\nprovide a scalable generic MKL training algorithm handling streaming kernels.\nWith respect to the inference time, the resulting classifier fits the gap\nbetween high accuracy but slow non-linear classifiers (such as classical MKL)\nand fast but low accuracy linear classifiers.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.09629v1.pdf",
        "similarity": 0.27116399056276463,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-17"
    },
    {
        "new_title": "Sparse deep neural networks for nonparametric estimation in\n  high-dimensional sparse regression",
        "new_link": "http://arxiv.org/abs/2406.18137v1",
        "new_summary": "  Generalization theory has been established for sparse deep neural networks\nunder high-dimensional regime. Beyond generalization, parameter estimation is\nalso important since it is crucial for variable selection and interpretability\nof deep neural networks. Current theoretical studies concerning parameter\nestimation mainly focus on two-layer neural networks, which is due to the fact\nthat the convergence of parameter estimation heavily relies on the regularity\nof the Hessian matrix, while the Hessian matrix of deep neural networks is\nhighly singular. To avoid the unidentifiability of deep neural networks in\nparameter estimation, we propose to conduct nonparametric estimation of partial\nderivatives with respect to inputs. We first show that model convergence of\nsparse deep neural networks is guaranteed in that the sample complexity only\ngrows with the logarithm of the number of parameters or the input dimension\nwhen the $\\ell_{1}$-norm of parameters is well constrained. Then by bounding\nthe norm and the divergence of partial derivatives, we establish that the\nconvergence rate of nonparametric estimation of partial derivatives scales as\n$\\mathcal{O}(n^{-1/4})$, a rate which is slower than the model convergence rate\n$\\mathcal{O}(n^{-1/2})$. To the best of our knowledge, this study combines\nnonparametric estimation and parametric sparse deep neural networks for the\nfirst time. As nonparametric estimation of partial derivatives is of great\nsignificance for nonlinear variable selection, the current results show the\npromising future for the interpretability of deep neural networks.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.18137v1.pdf",
        "similarity": 0.2709950730546093,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-26"
    },
    {
        "new_title": "Classification analysis of transition-metal chalcogenides and oxides\n  using quantum machine learning",
        "new_link": "http://arxiv.org/abs/2405.18989v1",
        "new_summary": "  Quantum machine learning (QML) leverages the potential from machine learning\nto explore the subtle patterns in huge datasets of complex nature with quantum\nadvantages. This exponentially reduces the time and resources necessary for\ncomputations. QML accelerates materials research with active screening of\nchemical space, identifying novel materials for practical applications and\nclassifying structurally diverse materials given their measured properties.\nThis study analyzes the performance of three efficient quantum machine learning\nalgorithms viz., variational quantum eigen solver (VQE), quantum support vector\nmachine (QSVM) and quantum neural networks (QNN) for the classification of\ntransition metal chalcogenides and oxides (TMCs &TMOs). The analysis is\nperformed on three datasets of different sizes containing 102, 192 and 350\nmaterials with TMCs and TMOs labelled as +1 and -1 respectively. By employing\nfeature selection, classical machine learning achieves 100% accuracy whereas\nQML achieves the highest performance of 99% and 98% for test and train data\nrespectively on QSVC. This study establishes the competence of QML models in\nmaterials classification and explores the quantum circuits in terms of\nover-fitting using the circuit descriptors expressibility and entangling\ncapability. In addition, the perspectives on QML in materials research with\nnoisy intermediate scale quantum (NISQ) devices is given.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.18989v1.pdf",
        "similarity": 0.2709104061461719,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-29"
    },
    {
        "new_title": "PROSAC: Provably Safe Certification for Machine Learning Models under\n  Adversarial Attacks",
        "new_link": "http://arxiv.org/abs/2402.02629v1",
        "new_summary": "  It is widely known that state-of-the-art machine learning models, including\nvision and language models, can be seriously compromised by adversarial\nperturbations. It is therefore increasingly relevant to develop capabilities to\ncertify their performance in the presence of the most effective adversarial\nattacks. Our paper offers a new approach to certify the performance of machine\nlearning models in the presence of adversarial attacks with population level\nrisk guarantees. In particular, we introduce the notion of $(\\alpha,\\zeta)$\nmachine learning model safety. We propose a hypothesis testing procedure, based\non the availability of a calibration set, to derive statistical guarantees\nproviding that the probability of declaring that the adversarial (population)\nrisk of a machine learning model is less than $\\alpha$ (i.e. the model is\nsafe), while the model is in fact unsafe (i.e. the model adversarial population\nrisk is higher than $\\alpha$), is less than $\\zeta$. We also propose Bayesian\noptimization algorithms to determine efficiently whether a machine learning\nmodel is $(\\alpha,\\zeta)$-safe in the presence of an adversarial attack, along\nwith statistical guarantees. We apply our framework to a range of machine\nlearning models including various sizes of vision Transformer (ViT) and ResNet\nmodels impaired by a variety of adversarial attacks, such as AutoAttack,\nSquareAttack and natural evolution strategy attack, to illustrate the operation\nof our approach. Importantly, we show that ViT's are generally more robust to\nadversarial attacks than ResNets, and ViT-large is more robust than smaller\nmodels. Our approach goes beyond existing empirical adversarial risk-based\ncertification guarantees. It formulates rigorous (and provable) performance\nguarantees that can be used to satisfy regulatory requirements mandating the\nuse of state-of-the-art technical tools.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.02629v1.pdf",
        "similarity": 0.2706464799317666,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-04"
    },
    {
        "new_title": "Estimating the Local Learning Coefficient at Scale",
        "new_link": "http://arxiv.org/abs/2402.03698v1",
        "new_summary": "  The \\textit{local learning coefficient} (LLC) is a principled way of\nquantifying model complexity, originally derived in the context of Bayesian\nstatistics using singular learning theory (SLT). Several methods are known for\nnumerically estimating the local learning coefficient, but so far these methods\nhave not been extended to the scale of modern deep learning architectures or\ndata sets. Using a method developed in {\\tt arXiv:2308.12108 [stat.ML]} we\nempirically show how the LLC may be measured accurately and self-consistently\nfor deep linear networks (DLNs) up to 100M parameters. We also show that the\nestimated LLC has the rescaling invariance that holds for the theoretical\nquantity.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.03698v1.pdf",
        "similarity": 0.27063800949212063,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-06"
    },
    {
        "new_title": "Risk-Sensitive Soft Actor-Critic for Robust Deep Reinforcement Learning\n  under Distribution Shifts",
        "new_link": "http://arxiv.org/abs/2402.09992v1",
        "new_summary": "  We study the robustness of deep reinforcement learning algorithms against\ndistribution shifts within contextual multi-stage stochastic combinatorial\noptimization problems from the operations research domain. In this context,\nrisk-sensitive algorithms promise to learn robust policies. While this field is\nof general interest to the reinforcement learning community, most studies\nup-to-date focus on theoretical results rather than real-world performance.\nWith this work, we aim to bridge this gap by formally deriving a novel\nrisk-sensitive deep reinforcement learning algorithm while providing numerical\nevidence for its efficacy. Specifically, we introduce discrete Soft\nActor-Critic for the entropic risk measure by deriving a version of the Bellman\nequation for the respective Q-values. We establish a corresponding policy\nimprovement result and infer a practical algorithm. We introduce an environment\nthat represents typical contextual multi-stage stochastic combinatorial\noptimization problems and perform numerical experiments to empirically validate\nour algorithm's robustness against realistic distribution shifts, without\ncompromising performance on the training distribution. We show that our\nalgorithm is superior to risk-neutral Soft Actor-Critic as well as to two\nbenchmark approaches for robust deep reinforcement learning. Thereby, we\nprovide the first structured analysis on the robustness of reinforcement\nlearning under distribution shifts in the realm of contextual multi-stage\nstochastic combinatorial optimization problems.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.09992v1.pdf",
        "similarity": 0.27045986908202835,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-15"
    },
    {
        "new_title": "Improving the Fairness of Deep-Learning, Short-term Crime Prediction\n  with Under-reporting-aware Models",
        "new_link": "http://arxiv.org/abs/2406.04382v2",
        "new_summary": "  Deep learning crime predictive tools use past crime data and additional\nbehavioral datasets to forecast future crimes. Nevertheless, these tools have\nbeen shown to suffer from unfair predictions across minority racial and ethnic\ngroups. Current approaches to address this unfairness generally propose either\npre-processing methods that mitigate the bias in the training datasets by\napplying corrections to crime counts based on domain knowledge or in-processing\nmethods that are implemented as fairness regularizers to optimize for both\naccuracy and fairness. In this paper, we propose a novel deep learning\narchitecture that combines the power of these two approaches to increase\nprediction fairness. Our results show that the proposed model improves the\nfairness of crime predictions when compared to models with in-processing\nde-biasing approaches and with models without any type of bias correction,\nalbeit at the cost of reducing accuracy.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.04382v2.pdf",
        "similarity": 0.2702690351041517,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-06"
    },
    {
        "new_title": "Machine Learning Modeling Of SiRNA Structure-Potency Relationship With\n  Applications Against Sars-Cov-2 Spike Gene",
        "new_link": "http://arxiv.org/abs/2401.12232v1",
        "new_summary": "  The pharmaceutical Research and development (R&D) process is lengthy and\ncostly, taking nearly a decade to bring a new drug to the market. However,\nadvancements in biotechnology, computational methods, and machine learning\nalgorithms have the potential to revolutionize drug discovery, speeding up the\nprocess and improving patient outcomes. The COVID-19 pandemic has further\naccelerated and deepened the recognition of the potential of these techniques,\nespecially in the areas of drug repurposing and efficacy predictions.\nMeanwhile, non-small molecule therapeutic modalities such as cell therapies,\nmonoclonal antibodies, and RNA interference (RNAi) technology have gained\nimportance due to their ability to target specific disease pathways and/or\npatient populations. In the field of RNAi, many experiments have been carried\nout to design and select highly efficient siRNAs. However, the established\npatterns for efficient siRNAs are sometimes contradictory and unable to\nconsistently determine the most potent siRNA molecules against a target mRNA.\nThus, this paper focuses on developing machine learning models based on the\ncheminformatics representation of the nucleotide composition (i.e. AUTGC) of\nsiRNA to predict their potency and aid the selection of the most efficient\nsiRNAs for further development. The PLS (Partial Least Square) and SVR (Support\nVector Regression) machine learning models built in this work outperformed\npreviously published models. These models can help in predicting siRNA potency\nand aid in selecting the best siRNA molecules for experimental validation and\nfurther clinical development. The study has demonstrated the potential of\nAI/machine learning models to help expedite siRNA-based drug discovery\nincluding the discovery of potent siRNAs against SARS-CoV-2.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.12232v1.pdf",
        "similarity": 0.2702460016110904,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-18"
    },
    {
        "new_title": "Off-Policy Evaluation from Logged Human Feedback",
        "new_link": "http://arxiv.org/abs/2406.10030v1",
        "new_summary": "  Learning from human feedback has been central to recent advances in\nartificial intelligence and machine learning. Since the collection of human\nfeedback is costly, a natural question to ask is if the new feedback always\nneeds to collected. Or could we evaluate a new model with the human feedback on\nresponses of another model? This motivates us to study off-policy evaluation\nfrom logged human feedback. We formalize the problem, propose both model-based\nand model-free estimators for policy values, and show how to optimize them. We\nanalyze unbiasedness of our estimators and evaluate them empirically. Our\nestimators can predict the absolute values of evaluated policies, rank them,\nand be optimized.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.10030v1.pdf",
        "similarity": 0.27009410693398017,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-14"
    },
    {
        "new_title": "A Deep Learning Approach For Epistemic Uncertainty Quantification Of\n  Turbulent Flow Simulations",
        "new_link": "http://arxiv.org/abs/2405.08148v1",
        "new_summary": "  Simulations of complex turbulent flow are part and parcel of the engineering\ndesign process. Eddy viscosity based turbulence models represent the workhorse\nfor these simulations. The underlying simplifications in eddy viscosity models\nmake them computationally inexpensive but also introduce structural\nuncertainties in their predictions. Currently the Eigenspace Perturbation\nMethod is the only approach to predict these uncertainties. Due to its purely\nphysics based nature this method often leads to unrealistically large\nuncertainty bounds that lead to exceedingly conservative designs. We use a Deep\nLearning based approach to address this issue. We control the perturbations\nusing trained deep learning models that predict how much to perturb the modeled\nReynolds stresses. This is executed using a Convolutional Neural Network that\nlearns the difference between eddy viscosity based model predictions and high\nfidelity data as a mapping of flow features. We show that this approach leads\nto improvements over the Eigenspace Perturbation Method.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.08148v1.pdf",
        "similarity": 0.2699198346117814,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-13"
    },
    {
        "new_title": "Exploring Loss Design Techniques For Decision Tree Robustness To Label\n  Noise",
        "new_link": "http://arxiv.org/abs/2405.17672v1",
        "new_summary": "  In the real world, data is often noisy, affecting not only the quality of\nfeatures but also the accuracy of labels. Current research on mitigating label\nerrors stems primarily from advances in deep learning, and a gap exists in\nexploring interpretable models, particularly those rooted in decision trees. In\nthis study, we investigate whether ideas from deep learning loss design can be\napplied to improve the robustness of decision trees. In particular, we show\nthat loss correction and symmetric losses, both standard approaches, are not\neffective. We argue that other directions need to be explored to improve the\nrobustness of decision trees to label noise.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.17672v1.pdf",
        "similarity": 0.269344464600902,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-27"
    },
    {
        "new_title": "Comparative Evaluation of Learning Models for Bionic Robots: Non-Linear\n  Transfer Function Identifications",
        "new_link": "http://arxiv.org/abs/2407.02428v1",
        "new_summary": "  The control and modeling of bionic robot dynamics have increasingly adopted\nmodel-free control strategies using machine learning methods. Given the\nnon-linear elastic nature of bionic robotic systems, learning-based methods\nprovide reliable alternatives by utilizing numerical data to establish a direct\nmapping from actuation inputs to robot trajectories without complex kinematics\nmodels. However, for developers, the method of identifying an appropriate\nlearning model for their specific bionic robots and further constructing the\ntransfer function has not been thoroughly discussed. Thus, this research trains\nfour types of models, including ensemble learning models, regularization-based\nmodels, kernel-based models, and neural network models, suitable for\nmulti-input multi-output (MIMO) data and non-linear transfer function\nidentification, in order to evaluate their (1) accuracy, (2) computation\ncomplexity, and (3) performance of capturing biological movements. This\nresearch encompasses data collection methods for control inputs and action\noutputs, selection of machine learning models, comparative analysis of training\nresults, and transfer function identifications. The main objective is to\nprovide a comprehensive evaluation strategy and framework for the application\nof model-free control.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.02428v1.pdf",
        "similarity": 0.2691346926063155,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-02"
    },
    {
        "new_title": "MISLEAD: Manipulating Importance of Selected features for Learning\n  Epsilon in Evasion Attack Deception",
        "new_link": "http://arxiv.org/abs/2404.15656v2",
        "new_summary": "  Emerging vulnerabilities in machine learning (ML) models due to adversarial\nattacks raise concerns about their reliability. Specifically, evasion attacks\nmanipulate models by introducing precise perturbations to input data, causing\nerroneous predictions. To address this, we propose a methodology combining\nSHapley Additive exPlanations (SHAP) for feature importance analysis with an\ninnovative Optimal Epsilon technique for conducting evasion attacks. Our\napproach begins with SHAP-based analysis to understand model vulnerabilities,\ncrucial for devising targeted evasion strategies. The Optimal Epsilon\ntechnique, employing a Binary Search algorithm, efficiently determines the\nminimum epsilon needed for successful evasion. Evaluation across diverse\nmachine learning architectures demonstrates the technique's precision in\ngenerating adversarial samples, underscoring its efficacy in manipulating model\noutcomes. This study emphasizes the critical importance of continuous\nassessment and monitoring to identify and mitigate potential security risks in\nmachine learning systems.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.15656v2.pdf",
        "similarity": 0.269118775695589,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-24"
    },
    {
        "new_title": "Differential contributions of machine learning and statistical analysis\n  to language and cognitive sciences",
        "new_link": "http://arxiv.org/abs/2404.14052v1",
        "new_summary": "  Data-driven approaches have revolutionized scientific research. Machine\nlearning and statistical analysis are commonly utilized in this type of\nresearch. Despite their widespread use, these methodologies differ\nsignificantly in their techniques and objectives. Few studies have utilized a\nconsistent dataset to demonstrate these differences within the social sciences,\nparticularly in language and cognitive sciences. This study leverages the\nBuckeye Speech Corpus to illustrate how both machine learning and statistical\nanalysis are applied in data-driven research to obtain distinct insights. This\nstudy significantly enhances our understanding of the diverse approaches\nemployed in data-driven strategies.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.14052v1.pdf",
        "similarity": 0.26897366709601817,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-22"
    },
    {
        "new_title": "Machine Learning Augmented Branch and Bound for Mixed Integer Linear\n  Programming",
        "new_link": "http://arxiv.org/abs/2402.05501v1",
        "new_summary": "  Mixed Integer Linear Programming (MILP) is a pillar of mathematical\noptimization that offers a powerful modeling language for a wide range of\napplications. During the past decades, enormous algorithmic progress has been\nmade in solving MILPs, and many commercial and academic software packages\nexist. Nevertheless, the availability of data, both from problem instances and\nfrom solvers, and the desire to solve new problems and larger (real-life)\ninstances, trigger the need for continuing algorithmic development. MILP\nsolvers use branch and bound as their main component. In recent years, there\nhas been an explosive development in the use of machine learning algorithms for\nenhancing all main tasks involved in the branch-and-bound algorithm, such as\nprimal heuristics, branching, cutting planes, node selection and solver\nconfiguration decisions. This paper presents a survey of such approaches,\naddressing the vision of integration of machine learning and mathematical\noptimization as complementary technologies, and how this integration can\nbenefit MILP solving. In particular, we give detailed attention to machine\nlearning algorithms that automatically optimize some metric of branch-and-bound\nefficiency. We also address how to represent MILPs in the context of applying\nlearning algorithms, MILP benchmarks and software.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.05501v1.pdf",
        "similarity": 0.26882113111486033,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-08"
    },
    {
        "new_title": "Current applications and potential future directions of reinforcement\n  learning-based Digital Twins in agriculture",
        "new_link": "http://arxiv.org/abs/2406.08854v1",
        "new_summary": "  Digital Twins have gained attention in various industries for simulation,\nmonitoring, and decision-making, relying on ever-improving machine learning\nmodels. However, agricultural Digital Twin implementations are limited compared\nto other industries. Meanwhile, machine learning, particularly reinforcement\nlearning, has shown potential in agricultural applications like optimizing\ndecision-making, task automation, and resource management. A key aspect of\nDigital Twins is representing physical assets or systems in a virtual\nenvironment, which aligns well with reinforcement learning's need for\nenvironment representations to learn the best policy for a task. Reinforcement\nlearning in agriculture can thus enable various Digital Twin applications in\nagricultural domains. This review aims to categorize existing research\nemploying reinforcement learning in agricultural settings by application\ndomains like robotics, greenhouse management, irrigation systems, and crop\nmanagement, identifying potential future areas for reinforcement learning-based\nDigital Twins. It also categorizes the reinforcement learning techniques used,\nincluding tabular methods, Deep Q-Networks (DQN), Policy Gradient methods, and\nActor-Critic algorithms, to overview currently employed models. The review\nseeks to provide insights into the state-of-the-art in integrating Digital\nTwins and reinforcement learning in agriculture, identifying gaps and\nopportunities for future research, and exploring synergies to tackle\nagricultural challenges and optimize farming, paving the way for more efficient\nand sustainable farming methodologies.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.08854v1.pdf",
        "similarity": 0.2687247248514684,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-13"
    },
    {
        "new_title": "Solving Partial Differential Equations with Equivariant Extreme Learning\n  Machines",
        "new_link": "http://arxiv.org/abs/2404.18530v4",
        "new_summary": "  We utilize extreme-learning machines for the prediction of partial\ndifferential equations (PDEs). Our method splits the state space into multiple\nwindows that are predicted individually using a single model. Despite requiring\nonly few data points (in some cases, our method can learn from a single\nfull-state snapshot), it still achieves high accuracy and can predict the flow\nof PDEs over long time horizons. Moreover, we show how additional symmetries\ncan be exploited to increase sample efficiency and to enforce equivariance.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.18530v4.pdf",
        "similarity": 0.2684715034913258,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-29"
    },
    {
        "new_title": "Option pricing for Barndorff-Nielsen and Shephard model by supervised\n  deep learning",
        "new_link": "http://arxiv.org/abs/2402.00445v1",
        "new_summary": "  This paper aims to develop a supervised deep-learning scheme to compute call\noption prices for the Barndorff-Nielsen and Shephard model with a\nnon-martingale asset price process having infinite active jumps. In our deep\nlearning scheme, teaching data is generated through the Monte Carlo method\ndeveloped by Arai and Imai (2024). Moreover, the BNS model includes many\nvariables, which makes the deep learning accuracy worse. Therefore, we will\ncreate another input variable using the Black-Scholes formula. As a result, the\naccuracy is improved dramatically.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.00445v1.pdf",
        "similarity": 0.2680391729659943,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-01"
    },
    {
        "new_title": "From Displacements to Distributions: A Machine-Learning Enabled\n  Framework for Quantifying Uncertainties in Parameters of Computational Models",
        "new_link": "http://arxiv.org/abs/2403.03233v1",
        "new_summary": "  This work presents novel extensions for combining two frameworks for\nquantifying both aleatoric (i.e., irreducible) and epistemic (i.e., reducible)\nsources of uncertainties in the modeling of engineered systems. The\ndata-consistent (DC) framework poses an inverse problem and solution for\nquantifying aleatoric uncertainties in terms of pullback and push-forward\nmeasures for a given Quantity of Interest (QoI) map. Unfortunately, a\npre-specified QoI map is not always available a priori to the collection of\ndata associated with system outputs. The data themselves are often polluted\nwith measurement errors (i.e., epistemic uncertainties), which complicates the\nprocess of specifying a useful QoI. The Learning Uncertain Quantities (LUQ)\nframework defines a formal three-step machine-learning enabled process for\ntransforming noisy datasets into samples of a learned QoI map to enable\nDC-based inversion. We develop a robust filtering step in LUQ that can learn\nthe most useful quantitative information present in spatio-temporal datasets.\nThe learned QoI map transforms simulated and observed datasets into\ndistributions to perform DC-based inversion. We also develop a DC-based\ninversion scheme that iterates over time as new spatial datasets are obtained\nand utilizes quantitative diagnostics to identify both the quality and impact\nof inversion at each iteration. Reproducing Kernel Hilbert Space theory is\nleveraged to mathematically analyze the learned QoI map and develop a\nquantitative sufficiency test for evaluating the filtered data. An illustrative\nexample is utilized throughout while the final two examples involve the\nmanufacturing of shells of revolution to demonstrate various aspects of the\npresented frameworks.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.03233v1.pdf",
        "similarity": 0.2676662487581649,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-04"
    },
    {
        "new_title": "A thermodynamically consistent physics-informed deep learning material\n  model for short fiber/polymer nanocomposites",
        "new_link": "http://arxiv.org/abs/2403.18310v1",
        "new_summary": "  This work proposes a physics-informed deep learning (PIDL)-based constitutive\nmodel for investigating the viscoelastic-viscoplastic behavior of short\nfiber-reinforced nanoparticle-filled epoxies under various ambient conditions.\nThe deep-learning model is trained to enforce thermodynamic principles, leading\nto a thermodynamically consistent constitutive model. To accomplish this, a\nlong short-term memory network is combined with a feed-forward neural network\nto predict internal variables required for characterizing the internal\ndissipation of the nanocomposite materials. In addition, another feed-forward\nneural network is used to indicate the free-energy function, which enables\ndefining the thermodynamic state of the entire system. The PIDL model is\ninitially developed for the three-dimensional case by generating synthetic data\nfrom a classical constitutive model. The model is then trained by extracting\nthe data directly from cyclic loading-unloading experimental tests. Numerical\nexamples show that the PIDL model can accurately predict the mechanical\nbehavior of epoxy-based nanocomposites for different volume fractions of fibers\nand nanoparticles under various hygrothermal conditions.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.18310v1.pdf",
        "similarity": 0.2676599932248541,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-27"
    },
    {
        "new_title": "Computer Vision for Multimedia Geolocation in Human Trafficking\n  Investigation: A Systematic Literature Review",
        "new_link": "http://arxiv.org/abs/2402.15448v1",
        "new_summary": "  The task of multimedia geolocation is becoming an increasingly essential\ncomponent of the digital forensics toolkit to effectively combat human\ntrafficking, child sexual exploitation, and other illegal acts. Typically,\nmetadata-based geolocation information is stripped when multimedia content is\nshared via instant messaging and social media. The intricacy of geolocating,\ngeotagging, or finding geographical clues in this content is often overly\nburdensome for investigators. Recent research has shown that contemporary\nadvancements in artificial intelligence, specifically computer vision and deep\nlearning, show significant promise towards expediting the multimedia\ngeolocation task. This systematic literature review thoroughly examines the\nstate-of-the-art leveraging computer vision techniques for multimedia\ngeolocation and assesses their potential to expedite human trafficking\ninvestigation. This includes a comprehensive overview of the application of\ncomputer vision-based approaches to multimedia geolocation, identifies their\napplicability in combating human trafficking, and highlights the potential\nimplications of enhanced multimedia geolocation for prosecuting human\ntrafficking. 123 articles inform this systematic literature review. The\nfindings suggest numerous potential paths for future impactful research on the\nsubject.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.15448v1.pdf",
        "similarity": 0.26743894445440364,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-23"
    },
    {
        "new_title": "AdaFed: Fair Federated Learning via Adaptive Common Descent Direction",
        "new_link": "http://arxiv.org/abs/2401.04993v1",
        "new_summary": "  Federated learning (FL) is a promising technology via which some edge\ndevices/clients collaboratively train a machine learning model orchestrated by\na server. Learning an unfair model is known as a critical problem in federated\nlearning, where the trained model may unfairly advantage or disadvantage some\nof the devices. To tackle this problem, in this work, we propose AdaFed. The\ngoal of AdaFed is to find an updating direction for the server along which (i)\nall the clients' loss functions are decreasing; and (ii) more importantly, the\nloss functions for the clients with larger values decrease with a higher rate.\nAdaFed adaptively tunes this common direction based on the values of local\ngradients and loss functions. We validate the effectiveness of AdaFed on a\nsuite of federated datasets, and demonstrate that AdaFed outperforms\nstate-of-the-art fair FL methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.04993v1.pdf",
        "similarity": 0.26660040369782084,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-10"
    },
    {
        "new_title": "Convolutional Neural Networks for signal detection in real LIGO data",
        "new_link": "http://arxiv.org/abs/2402.07492v1",
        "new_summary": "  Searching the data of gravitational-wave detectors for signals from compact\nbinary mergers is a computationally demanding task. Recently, machine learning\nalgorithms have been proposed to address current and future challenges.\nHowever, the results of these publications often differ greatly due to\ndiffering choices in the evaluation procedure. The Machine Learning\nGravitational-Wave Search Challenge was organized to resolve these issues and\nproduce a unified framework for machine-learning search evaluation. Six teams\nsubmitted contributions, four of which are based on machine learning methods\nand two are state-of-the-art production analyses. This paper describes the\nsubmission from the team TPI FSU Jena and its updated variant. We also apply\nour algorithm to real O3b data and recover the relevant events of the GWTC-3\ncatalog.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.07492v1.pdf",
        "similarity": 0.26637282715738697,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-12"
    },
    {
        "new_title": "Fault Diagnosis on Induction Motor using Machine Learning and Signal\n  Processing",
        "new_link": "http://arxiv.org/abs/2401.15417v1",
        "new_summary": "  The detection and identification of induction motor faults using machine\nlearning and signal processing is a valuable approach to avoiding plant\ndisturbances and shutdowns in the context of Industry 4.0. In this work, we\npresent a study on the detection and identification of induction motor faults\nusing machine learning and signal processing with MATLAB Simulink. We developed\na model of a three-phase induction motor in MATLAB Simulink to generate healthy\nand faulty motor data. The data collected included stator currents, rotor\ncurrents, input power, slip, rotor speed, and efficiency. We generated four\nfaults in the induction motor: open circuit fault, short circuit fault,\noverload, and broken rotor bars. We collected a total of 150,000 data points\nwith a 60-40% ratio of healthy to faulty motor data. We applied Fast Fourier\nTransform (FFT) to detect and identify healthy and unhealthy conditions and\nadded a distinctive feature in our data. The generated dataset was trained\ndifferent machine learning models. On comparing the accuracy of the models on\nthe test set, we concluded that the Decision Tree algorithm performed the best\nwith an accuracy of about 92%. Our study contributes to the literature by\nproviding a valuable approach to fault detection and classification with\nmachine learning models for industrial applications.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.15417v1.pdf",
        "similarity": 0.2663235842994969,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-27"
    },
    {
        "new_title": "Functional Bilevel Optimization for Machine Learning",
        "new_link": "http://arxiv.org/abs/2403.20233v3",
        "new_summary": "  In this paper, we introduce a new functional point of view on bilevel\noptimization problems for machine learning, where the inner objective is\nminimized over a function space. These types of problems are most often solved\nby using methods developed in the parametric setting, where the inner objective\nis strongly convex with respect to the parameters of the prediction function.\nThe functional point of view does not rely on this assumption and notably\nallows using over-parameterized neural networks as the inner prediction\nfunction. We propose scalable and efficient algorithms for the functional\nbilevel optimization problem and illustrate the benefits of our approach on\ninstrumental regression and reinforcement learning tasks.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.20233v3.pdf",
        "similarity": 0.2662473909035507,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-29"
    },
    {
        "new_title": "Time Series Modeling for Heart Rate Prediction: From ARIMA to\n  Transformers",
        "new_link": "http://arxiv.org/abs/2406.12199v2",
        "new_summary": "  Cardiovascular disease (CVD) is a leading cause of death globally,\nnecessitating precise forecasting models for monitoring vital signs like heart\nrate, blood pressure, and ECG. Traditional models, such as ARIMA and Prophet,\nare limited by their need for manual parameter tuning and challenges in\nhandling noisy, sparse, and highly variable medical data. This study\ninvestigates advanced deep learning models, including LSTM, and\ntransformer-based architectures, for predicting heart rate time series from the\nMIT-BIH Database. Results demonstrate that deep learning models, particularly\nPatchTST, significantly outperform traditional models across multiple metrics,\ncapturing complex patterns and dependencies more effectively. This research\nunderscores the potential of deep learning to enhance patient monitoring and\nCVD management, suggesting substantial clinical benefits. Future work should\nextend these findings to larger, more diverse datasets and real-world clinical\napplications to further validate and optimize model performance.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.12199v2.pdf",
        "similarity": 0.26621375985157864,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-18"
    },
    {
        "new_title": "Solving the QAP by Two-Stage Graph Pointer Networks and Reinforcement\n  Learning",
        "new_link": "http://arxiv.org/abs/2404.00539v1",
        "new_summary": "  Quadratic Assignment Problem (QAP) is a practical combinatorial optimization\nproblems that has been studied for several years. Since it is NP-hard, solving\nlarge problem instances of QAP is challenging. Although heuristics can find\nsemi-optimal solutions, the execution time significantly increases as the\nproblem size increases. Recently, solving combinatorial optimization problems\nby deep learning has been attracting attention as a faster solver than\nheuristics. Even with deep learning, however, solving large QAP is still\nchallenging. In this paper, we propose the deep reinforcement learning model\ncalled the two-stage graph pointer network (GPN) for solving QAP. Two-stage GPN\nrelies on GPN, which has been proposed for Euclidean Traveling Salesman Problem\n(TSP). First, we extend GPN for general TSP, and then we add new algorithms to\nthat model for solving QAP. Our experimental results show that our two-stage\nGPN provides semi-optimal solutions for benchmark problem instances from TSPlib\nand QAPLIB.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.00539v1.pdf",
        "similarity": 0.26618905500695783,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-31"
    },
    {
        "new_title": "VQC-Based Reinforcement Learning with Data Re-uploading: Performance and\n  Trainability",
        "new_link": "http://arxiv.org/abs/2401.11555v1",
        "new_summary": "  Reinforcement Learning (RL) consists of designing agents that make\nintelligent decisions without human supervision. When used alongside function\napproximators such as Neural Networks (NNs), RL is capable of solving extremely\ncomplex problems. Deep Q-Learning, a RL algorithm that uses Deep NNs, achieved\nsuper-human performance in some specific tasks. Nonetheless, it is also\npossible to use Variational Quantum Circuits (VQCs) as function approximators\nin RL algorithms. This work empirically studies the performance and\ntrainability of such VQC-based Deep Q-Learning models in classic control\nbenchmark environments. More specifically, we research how data re-uploading\naffects both these metrics. We show that the magnitude and the variance of the\ngradients of these models remain substantial throughout training due to the\nmoving targets of Deep Q-Learning. Moreover, we empirically show that\nincreasing the number of qubits does not lead to an exponential vanishing\nbehavior of the magnitude and variance of the gradients for a PQC approximating\na 2-design, unlike what was expected due to the Barren Plateau Phenomenon. This\nhints at the possibility of VQCs being specially adequate for being used as\nfunction approximators in such a context.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.11555v1.pdf",
        "similarity": 0.26600761410814977,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-21"
    },
    {
        "new_title": "An Innovative Networks in Federated Learning",
        "new_link": "http://arxiv.org/abs/2405.17836v1",
        "new_summary": "  This paper presents the development and application of Wavelet\nKolmogorov-Arnold Networks (Wav-KAN) in federated learning. We implemented\nWav-KAN \\cite{wav-kan} in the clients. Indeed, we have considered both\ncontinuous wavelet transform (CWT) and also discrete wavelet transform (DWT) to\nenable multiresolution capabaility which helps in heteregeneous data\ndistribution across clients. Extensive experiments were conducted on different\ndatasets, demonstrating Wav-KAN's superior performance in terms of\ninterpretability, computational speed, training and test accuracy. Our\nfederated learning algorithm integrates wavelet-based activation functions,\nparameterized by weight, scale, and translation, to enhance local and global\nmodel performance. Results show significant improvements in computational\nefficiency, robustness, and accuracy, highlighting the effectiveness of wavelet\nselection in scalable neural network design.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.17836v1.pdf",
        "similarity": 0.26599598232687816,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-28"
    },
    {
        "new_title": "Deep Learning-based Sentiment Analysis in Persian Language",
        "new_link": "http://arxiv.org/abs/2403.11069v1",
        "new_summary": "  Recently, there has been a growing interest in the use of deep learning\ntechniques for tasks in natural language processing (NLP), with sentiment\nanalysis being one of the most challenging areas, particularly in the Persian\nlanguage. The vast amounts of content generated by Persian users on thousands\nof websites, blogs, and social networks such as Telegram, Instagram, and\nTwitter present a rich resource of information. Deep learning techniques have\nbecome increasingly favored for extracting insights from this extensive pool of\nraw data, although they face several challenges. In this study, we introduced\nand implemented a hybrid deep learning-based model for sentiment analysis,\nusing customer review data from the Digikala Online Retailer website. We\nemployed a variety of deep learning networks and regularization techniques as\nclassifiers. Ultimately, our hybrid approach yielded an impressive performance,\nachieving an F1 score of 78.3 across three sentiment categories: positive,\nnegative, and neutral.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.11069v1.pdf",
        "similarity": 0.26598546172240145,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-17"
    },
    {
        "new_title": "Machine learning-based optimization workflow of the homogeneity of\n  spunbond nonwovens with human validation",
        "new_link": "http://arxiv.org/abs/2404.09604v2",
        "new_summary": "  In the last ten years, the average annual growth rate of nonwoven production\nwas 4%. In 2020 and 2021, nonwoven production has increased even further due to\nthe huge demand for nonwoven products needed for protective clothing such as\nFFP2 masks to combat the COVID19 pandemic. Optimizing the production process is\nstill a challenge due to its high nonlinearity. In this paper, we present a\nmachine learning-based optimization workflow aimed at improving the homogeneity\nof spunbond nonwovens. The optimization workflow is based on a mathematical\nmodel that simulates the microstructures of nonwovens. Based on trainingy data\ncoming from this simulator, different machine learning algorithms are trained\nin order to find a surrogate model for the time-consuming simulator. Human\nvalidation is employed to verify the outputs of machine learning algorithms by\nassessing the aesthetics of the nonwovens. We include scientific and expert\nknowledge into the training data to reduce the computational costs involved in\nthe optimization process. We demonstrate the necessity and effectiveness of our\nworkflow in optimizing the homogeneity of nonwovens.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.09604v2.pdf",
        "similarity": 0.26581128162912276,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-15"
    },
    {
        "new_title": "Towards One Model for Classical Dimensionality Reduction: A\n  Probabilistic Perspective on UMAP and t-SNE",
        "new_link": "http://arxiv.org/abs/2405.17412v1",
        "new_summary": "  This paper shows that the dimensionality reduction methods, UMAP and t-SNE,\ncan be approximately recast as MAP inference methods corresponding to a\ngeneralized Wishart-based model introduced in ProbDR. This interpretation\noffers deeper theoretical insights into these algorithms, while introducing\ntools with which similar dimensionality reduction methods can be studied.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.17412v1.pdf",
        "similarity": 0.2658096329401369,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-27"
    },
    {
        "new_title": "Exploiting Chaotic Dynamics as Deep Neural Networks",
        "new_link": "http://arxiv.org/abs/2406.02580v1",
        "new_summary": "  Chaos presents complex dynamics arising from nonlinearity and a sensitivity\nto initial states. These characteristics suggest a depth of expressivity that\nunderscores their potential for advanced computational applications. However,\nstrategies to effectively exploit chaotic dynamics for information processing\nhave largely remained elusive. In this study, we reveal that the essence of\nchaos can be found in various state-of-the-art deep neural networks. Drawing\ninspiration from this revelation, we propose a novel method that directly\nleverages chaotic dynamics for deep learning architectures. Our approach is\nsystematically evaluated across distinct chaotic systems. In all instances, our\nframework presents superior results to conventional deep neural networks in\nterms of accuracy, convergence speed, and efficiency. Furthermore, we found an\nactive role of transient chaos formation in our scheme. Collectively, this\nstudy offers a new path for the integration of chaos, which has long been\noverlooked in information processing, and provides insights into the\nprospective fusion of chaotic dynamics within the domains of machine learning\nand neuromorphic computation.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.02580v1.pdf",
        "similarity": 0.2657137050101811,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-29"
    },
    {
        "new_title": "Unsupervised Learning of Hybrid Latent Dynamics: A Learn-to-Identify\n  Framework",
        "new_link": "http://arxiv.org/abs/2403.08194v1",
        "new_summary": "  Modern applications increasingly require unsupervised learning of latent\ndynamics from high-dimensional time-series. This presents a significant\nchallenge of identifiability: many abstract latent representations may\nreconstruct observations, yet do they guarantee an adequate identification of\nthe governing dynamics? This paper investigates this challenge from two angles:\nthe use of physics inductive bias specific to the data being modeled, and a\nlearn-to-identify strategy that separates forecasting objectives from the data\nused for the identification. We combine these two strategies in a novel\nframework for unsupervised meta-learning of hybrid latent dynamics (Meta-HyLaD)\nwith: 1) a latent dynamic function that hybridize known mathematical\nexpressions of prior physics with neural functions describing its unknown\nerrors, and 2) a meta-learning formulation to learn to separately identify both\ncomponents of the hybrid dynamics. Through extensive experiments on five\nphysics and one biomedical systems, we provide strong evidence for the benefits\nof Meta-HyLaD to integrate rich prior knowledge while identifying their gap to\nobserved data.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.08194v1.pdf",
        "similarity": 0.2654890002920515,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-13"
    },
    {
        "new_title": "Early Recognition of Parkinson's Disease Through Acoustic Analysis and\n  Machine Learning",
        "new_link": "http://arxiv.org/abs/2407.16091v1",
        "new_summary": "  Parkinson's Disease (PD) is a progressive neurodegenerative disorder that\nsignificantly impacts both motor and non-motor functions, including speech.\nEarly and accurate recognition of PD through speech analysis can greatly\nenhance patient outcomes by enabling timely intervention. This paper provides a\ncomprehensive review of methods for PD recognition using speech data,\nhighlighting advances in machine learning and data-driven approaches. We\ndiscuss the process of data wrangling, including data collection, cleaning,\ntransformation, and exploratory data analysis, to prepare the dataset for\nmachine learning applications. Various classification algorithms are explored,\nincluding logistic regression, SVM, and neural networks, with and without\nfeature selection. Each method is evaluated based on accuracy, precision, and\ntraining time. Our findings indicate that specific acoustic features and\nadvanced machine-learning techniques can effectively differentiate between\nindividuals with PD and healthy controls. The study concludes with a comparison\nof the different models, identifying the most effective approaches for PD\nrecognition, and suggesting potential directions for future research.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.16091v1.pdf",
        "similarity": 0.26537313083294545,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-22"
    },
    {
        "new_title": "Constrained Online Two-stage Stochastic Optimization: Algorithm with\n  (and without) Predictions",
        "new_link": "http://arxiv.org/abs/2401.01077v1",
        "new_summary": "  We consider an online two-stage stochastic optimization with long-term\nconstraints over a finite horizon of $T$ periods. At each period, we take the\nfirst-stage action, observe a model parameter realization and then take the\nsecond-stage action from a feasible set that depends both on the first-stage\ndecision and the model parameter. We aim to minimize the cumulative objective\nvalue while guaranteeing that the long-term average second-stage decision\nbelongs to a set. We develop online algorithms for the online two-stage problem\nfrom adversarial learning algorithms. Also, the regret bound of our algorithm\ncan be reduced to the regret bound of embedded adversarial learning algorithms.\nBased on this framework, we obtain new results under various settings. When the\nmodel parameters are drawn from unknown non-stationary distributions and we are\ngiven machine-learned predictions of the distributions, we develop a new\nalgorithm from our framework with a regret $O(W_T+\\sqrt{T})$, where $W_T$\nmeasures the total inaccuracy of the machine-learned predictions. We then\ndevelop another algorithm that works when no machine-learned predictions are\ngiven and show the performances.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.01077v1.pdf",
        "similarity": 0.2652059972226947,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-02"
    },
    {
        "new_title": "A Mathematical Model of the Hidden Feedback Loop Effect in Machine\n  Learning Systems",
        "new_link": "http://arxiv.org/abs/2405.02726v1",
        "new_summary": "  Widespread deployment of societal-scale machine learning systems necessitates\na thorough understanding of the resulting long-term effects these systems have\non their environment, including loss of trustworthiness, bias amplification,\nand violation of AI safety requirements. We introduce a repeated learning\nprocess to jointly describe several phenomena attributed to unintended hidden\nfeedback loops, such as error amplification, induced concept drift, echo\nchambers and others. The process comprises the entire cycle of obtaining the\ndata, training the predictive model, and delivering predictions to end-users\nwithin a single mathematical model. A distinctive feature of such repeated\nlearning setting is that the state of the environment becomes causally\ndependent on the learner itself over time, thus violating the usual assumptions\nabout the data distribution. We present a novel dynamical systems model of the\nrepeated learning process and prove the limiting set of probability\ndistributions for positive and negative feedback loop modes of the system\noperation. We conduct a series of computational experiments using an exemplary\nsupervised learning problem on two synthetic data sets. The results of the\nexperiments correspond to the theoretical predictions derived from the\ndynamical model. Our results demonstrate the feasibility of the proposed\napproach for studying the repeated learning processes in machine learning\nsystems and open a range of opportunities for further research in the area.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.02726v1.pdf",
        "similarity": 0.2651675123448769,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-04"
    },
    {
        "new_title": "RACH Traffic Prediction in Massive Machine Type Communications",
        "new_link": "http://arxiv.org/abs/2405.05235v1",
        "new_summary": "  Traffic pattern prediction has emerged as a promising approach for\nefficiently managing and mitigating the impacts of event-driven bursty traffic\nin massive machine-type communication (mMTC) networks. However, achieving\naccurate predictions of bursty traffic remains a non-trivial task due to the\ninherent randomness of events, and these challenges intensify within live\nnetwork environments. Consequently, there is a compelling imperative to design\na lightweight and agile framework capable of assimilating continuously\ncollected data from the network and accurately forecasting bursty traffic in\nmMTC networks. This paper addresses these challenges by presenting a machine\nlearning-based framework tailored for forecasting bursty traffic in\nmulti-channel slotted ALOHA networks. The proposed machine learning network\ncomprises long-term short-term memory (LSTM) and a DenseNet with feed-forward\nneural network (FFNN) layers, where the residual connections enhance the\ntraining ability of the machine learning network in capturing complicated\npatterns. Furthermore, we develop a new low-complexity online prediction\nalgorithm that updates the states of the LSTM network by leveraging frequently\ncollected data from the mMTC network. Simulation results and complexity\nanalysis demonstrate the superiority of our proposed algorithm in terms of both\naccuracy and complexity, making it well-suited for time-critical live\nscenarios. We evaluate the performance of the proposed framework in a network\nwith a single base station and thousands of devices organized into groups with\ndistinct traffic-generating characteristics. Comprehensive evaluations and\nsimulations indicate that our proposed machine learning approach achieves a\nremarkable $52\\%$ higher accuracy in long-term predictions compared to\ntraditional methods, without imposing additional processing load on the system.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.05235v1.pdf",
        "similarity": 0.264707171211047,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-08"
    },
    {
        "new_title": "Classification Tree-based Active Learning: A Wrapper Approach",
        "new_link": "http://arxiv.org/abs/2404.09953v1",
        "new_summary": "  Supervised machine learning often requires large training sets to train\naccurate models, yet obtaining large amounts of labeled data is not always\nfeasible. Hence, it becomes crucial to explore active learning methods for\nreducing the size of training sets while maintaining high accuracy. The aim is\nto select the optimal subset of data for labeling from an initial unlabeled\nset, ensuring precise prediction of outcomes. However, conventional active\nlearning approaches are comparable to classical random sampling. This paper\nproposes a wrapper active learning method for classification, organizing the\nsampling process into a tree structure, that improves state-of-the-art\nalgorithms. A classification tree constructed on an initial set of labeled\nsamples is considered to decompose the space into low-entropy regions.\nInput-space based criteria are used thereafter to sub-sample from these\nregions, the total number of points to be labeled being decomposed into each\nregion. This adaptation proves to be a significant enhancement over existing\nactive learning methods. Through experiments conducted on various benchmark\ndata sets, the paper demonstrates the efficacy of the proposed framework by\nbeing effective in constructing accurate classification models, even when\nprovided with a severely restricted labeled data set.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.09953v1.pdf",
        "similarity": 0.2646128344213639,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-15"
    },
    {
        "new_title": "Deep Learning Based Dynamics Identification and Linearization of Orbital\n  Problems using Koopman Theory",
        "new_link": "http://arxiv.org/abs/2403.08965v1",
        "new_summary": "  The study of the Two-Body and Circular Restricted Three-Body Problems in the\nfield of aerospace engineering and sciences is deeply important because they\nhelp describe the motion of both celestial and artificial satellites. With the\ngrowing demand for satellites and satellite formation flying, fast and\nefficient control of these systems is becoming ever more important. Global\nlinearization of these systems allows engineers to employ methods of control in\norder to achieve these desired results. We propose a data-driven framework for\nsimultaneous system identification and global linearization of both the\nTwo-Body Problem and Circular Restricted Three-Body Problem via deep\nlearning-based Koopman Theory, i.e., a framework that can identify the\nunderlying dynamics and globally linearize it into a linear time-invariant\n(LTI) system. The linear Koopman operator is discovered through purely\ndata-driven training of a Deep Neural Network with a custom architecture. This\npaper displays the ability of the Koopman operator to generalize to various\nother Two-Body systems without the need for retraining. We also demonstrate the\ncapability of the same architecture to be utilized to accurately learn a\nKoopman operator that approximates the Circular Restricted Three-Body Problem.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.08965v1.pdf",
        "similarity": 0.26394014448613057,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-13"
    },
    {
        "new_title": "DroneVis: Versatile Computer Vision Library for Drones",
        "new_link": "http://arxiv.org/abs/2406.00447v1",
        "new_summary": "  This paper introduces DroneVis, a novel library designed to automate computer\nvision algorithms on Parrot drones. DroneVis offers a versatile set of features\nand provides a diverse range of computer vision tasks along with a variety of\nmodels to choose from. Implemented in Python, the library adheres to\nhigh-quality code standards, facilitating effortless customization and feature\nexpansion according to user requirements. In addition, comprehensive\ndocumentation is provided, encompassing usage guidelines and illustrative use\ncases. Our documentation, code, and examples are available in\nhttps://github.com/ahmedheakl/drone-vis.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.00447v1.pdf",
        "similarity": 0.26375583810388614,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-01"
    },
    {
        "new_title": "Position: Optimization in SciML Should Employ the Function Space\n  Geometry",
        "new_link": "http://arxiv.org/abs/2402.07318v2",
        "new_summary": "  Scientific machine learning (SciML) is a relatively new field that aims to\nsolve problems from different fields of natural sciences using machine learning\ntools. It is well-documented that the optimizers commonly used in other areas\nof machine learning perform poorly on many SciML problems. We provide an\ninfinite-dimensional view on optimization problems encountered in scientific\nmachine learning and advocate for the paradigm first optimize, then discretize\nfor their solution. This amounts to first choosing an appropriate\ninfinite-dimensional algorithm which is then discretized in a second step. To\nillustrate this point, we show that recently proposed state-of-the-art\nalgorithms for SciML applications can be derived within this framework. As the\ninfinite-dimensional viewpoint is presently underdeveloped in scientific\nmachine learning, we formalize it here and advocate for its use in SciML in the\ndevelopment of efficient optimization algorithms.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.07318v2.pdf",
        "similarity": 0.26375146378243314,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-11"
    },
    {
        "new_title": "Enhancing Automata Learning with Statistical Machine Learning: A Network\n  Security Case Study",
        "new_link": "http://arxiv.org/abs/2405.11141v2",
        "new_summary": "  Intrusion detection systems are crucial for network security. Verification of\nthese systems is complicated by various factors, including the heterogeneity of\nnetwork platforms and the continuously changing landscape of cyber threats. In\nthis paper, we use automata learning to derive state machines from\nnetwork-traffic data with the objective of supporting behavioural verification\nof intrusion detection systems. The most innovative aspect of our work is\naddressing the inability to directly apply existing automata learning\ntechniques to network-traffic data due to the numeric nature of such data.\nSpecifically, we use interpretable machine learning (ML) to partition numeric\nranges into intervals that strongly correlate with a system's decisions\nregarding intrusion detection. These intervals are subsequently used to\nabstract numeric ranges before automata learning. We apply our ML-enhanced\nautomata learning approach to a commercial network intrusion detection system\ndeveloped by our industry partner, RabbitRun Technologies. Our approach results\nin an average 67.5% reduction in the number of states and transitions of the\nlearned state machines, while achieving an average 28% improvement in accuracy\ncompared to using expertise-based numeric data abstraction. Furthermore, the\nresulting state machines help practitioners in verifying system-level security\nrequirements and exploring previously unknown system behaviours through model\nchecking and temporal query checking. We make our implementation and\nexperimental data available online.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.11141v2.pdf",
        "similarity": 0.2636349277857077,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-18"
    },
    {
        "new_title": "DoseGNN: Improving the Performance of Deep Learning Models in Adaptive\n  Dose-Volume Histogram Prediction through Graph Neural Networks",
        "new_link": "http://arxiv.org/abs/2402.01076v1",
        "new_summary": "  Dose-Volume Histogram (DVH) prediction is fundamental in radiation therapy\nthat facilitate treatment planning, dose evaluation, plan comparison and etc.\nIt helps to increase the ability to deliver precise and effective radiation\ntreatments while managing potential toxicities to healthy tissues as needed to\nreduce the risk of complications. This paper extends recently disclosed\nresearch findings presented on AAPM (AAPM 65th Annual Meeting $\\&$ Exhibition)\nand includes necessary technique details. The objective is to design efficient\ndeep learning models for DVH prediction on general radiotherapy platform\nequipped with high performance CBCT system, where input CT images and target\ndose images to predict may have different origins, spacing and sizes. Deep\nlearning models widely-adopted in DVH prediction task are evaluated on the\nnovel radiotherapy platform, and graph neural networks (GNNs) are shown to be\nthe ideal architecture to construct a plug-and-play framework to improve\npredictive performance of base deep learning models in the adaptive setting.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.01076v1.pdf",
        "similarity": 0.26349968307511357,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-02"
    },
    {
        "new_title": "A Likelihood-Based Generative Approach for Spatially Consistent\n  Precipitation Downscaling",
        "new_link": "http://arxiv.org/abs/2407.04724v1",
        "new_summary": "  Deep learning has emerged as a promising tool for precipitation downscaling.\nHowever, current models rely on likelihood-based loss functions to properly\nmodel the precipitation distribution, leading to spatially inconsistent\nprojections when sampling. This work explores a novel approach by fusing the\nstrengths of likelihood-based and adversarial losses used in generative models.\nAs a result, we propose a likelihood-based generative approach for\nprecipitation downscaling, leveraging the benefits of both methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.04724v1.pdf",
        "similarity": 0.26341449236990755,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-26"
    },
    {
        "new_title": "Exploiting Estimation Bias in Deep Double Q-Learning for Actor-Critic\n  Methods",
        "new_link": "http://arxiv.org/abs/2402.09078v1",
        "new_summary": "  This paper introduces innovative methods in Reinforcement Learning (RL),\nfocusing on addressing and exploiting estimation biases in Actor-Critic methods\nfor continuous control tasks, using Deep Double Q-Learning. We propose two\nnovel algorithms: Expectile Delayed Deep Deterministic Policy Gradient (ExpD3)\nand Bias Exploiting - Twin Delayed Deep Deterministic Policy Gradient (BE-TD3).\nExpD3 aims to reduce overestimation bias with a single $Q$ estimate, offering a\nbalance between computational efficiency and performance, while BE-TD3 is\ndesigned to dynamically select the most advantageous estimation bias during\ntraining. Our extensive experiments across various continuous control tasks\ndemonstrate the effectiveness of our approaches. We show that these algorithms\ncan either match or surpass existing methods like TD3, particularly in\nenvironments where estimation biases significantly impact learning. The results\nunderline the importance of bias exploitation in improving policy learning in\nRL.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.09078v1.pdf",
        "similarity": 0.2634139400615114,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-14"
    },
    {
        "new_title": "ActiveDP: Bridging Active Learning and Data Programming",
        "new_link": "http://arxiv.org/abs/2402.06056v1",
        "new_summary": "  Modern machine learning models require large labelled datasets to achieve\ngood performance, but manually labelling large datasets is expensive and\ntime-consuming. The data programming paradigm enables users to label large\ndatasets efficiently but produces noisy labels, which deteriorates the\ndownstream model's performance. The active learning paradigm, on the other\nhand, can acquire accurate labels but only for a small fraction of instances.\nIn this paper, we propose ActiveDP, an interactive framework bridging active\nlearning and data programming together to generate labels with both high\naccuracy and coverage, combining the strengths of both paradigms. Experiments\nshow that ActiveDP outperforms previous weak supervision and active learning\napproaches and consistently performs well under different labelling budgets.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.06056v1.pdf",
        "similarity": 0.26304341404078063,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-08"
    },
    {
        "new_title": "Effect of Random Learning Rate: Theoretical Analysis of SGD Dynamics in\n  Non-Convex Optimization via Stationary Distribution",
        "new_link": "http://arxiv.org/abs/2406.16032v1",
        "new_summary": "  We consider a variant of the stochastic gradient descent (SGD) with a random\nlearning rate and reveal its convergence properties. SGD is a widely used\nstochastic optimization algorithm in machine learning, especially deep\nlearning. Numerous studies reveal the convergence properties of SGD and its\nsimplified variants. Among these, the analysis of convergence using a\nstationary distribution of updated parameters provides generalizable results.\nHowever, to obtain a stationary distribution, the update direction of the\nparameters must not degenerate, which limits the applicable variants of SGD. In\nthis study, we consider a novel SGD variant, Poisson SGD, which has degenerated\nparameter update directions and instead utilizes a random learning rate.\nConsequently, we demonstrate that a distribution of a parameter updated by\nPoisson SGD converges to a stationary distribution under weak assumptions on a\nloss function. Based on this, we further show that Poisson SGD finds global\nminima in non-convex optimization problems and also evaluate the generalization\nerror using this method. As a proof technique, we approximate the distribution\nby Poisson SGD with that of the bouncy particle sampler (BPS) and derive its\nstationary distribution, using the theoretical advance of the piece-wise\ndeterministic Markov process (PDMP).\n",
        "pdf_link": "https://arxiv.org/pdf/2406.16032v1.pdf",
        "similarity": 0.2628808123702486,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-23"
    },
    {
        "new_title": "Research on Education Big Data for Students Academic Performance\n  Analysis based on Machine Learning",
        "new_link": "http://arxiv.org/abs/2407.16907v1",
        "new_summary": "  The application of the Internet in the field of education is becoming more\nand more popular, and a large amount of educational data is generated in the\nprocess. How to effectively use these data has always been a key issue in the\nfield of educational data mining. In this work, a machine learning model based\non Long Short-Term Memory Network (LSTM) was used to conduct an in-depth\nanalysis of educational big data to evaluate student performance. The LSTM\nmodel efficiently processes time series data, allowing us to capture\ntime-dependent and long-term trends in students' learning activities. This\napproach is particularly useful for analyzing student progress, engagement, and\nother behavioral patterns to support personalized education. In an experimental\nanalysis, we verified the effectiveness of the deep learning method in\npredicting student performance by comparing the performance of different\nmodels. Strict cross-validation techniques are used to ensure the accuracy and\ngeneralization of experimental results.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.16907v1.pdf",
        "similarity": 0.2625391262844905,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-25"
    },
    {
        "new_title": "Automating the Training and Deployment of Models in MLOps by Integrating\n  Systems with Machine Learning",
        "new_link": "http://arxiv.org/abs/2405.09819v1",
        "new_summary": "  This article introduces the importance of machine learning in real-world\napplications and explores the rise of MLOps (Machine Learning Operations) and\nits importance for solving challenges such as model deployment and performance\nmonitoring. By reviewing the evolution of MLOps and its relationship to\ntraditional software development methods, the paper proposes ways to integrate\nthe system into machine learning to solve the problems faced by existing MLOps\nand improve productivity. This paper focuses on the importance of automated\nmodel training, and the method to ensure the transparency and repeatability of\nthe training process through version control system. In addition, the\nchallenges of integrating machine learning components into traditional CI/CD\npipelines are discussed, and solutions such as versioning environments and\ncontainerization are proposed. Finally, the paper emphasizes the importance of\ncontinuous monitoring and feedback loops after model deployment to maintain\nmodel performance and reliability. Using case studies and best practices from\nNetflix, the article presents key strategies and lessons learned for successful\nimplementation of MLOps practices, providing valuable references for other\norganizations to build and optimize their own MLOps practices.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.09819v1.pdf",
        "similarity": 0.26204986949477266,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-16"
    },
    {
        "new_title": "Learning Topological Representations for Deep Image Understanding",
        "new_link": "http://arxiv.org/abs/2403.15361v1",
        "new_summary": "  In many scenarios, especially biomedical applications, the correct\ndelineation of complex fine-scaled structures such as neurons, tissues, and\nvessels is critical for downstream analysis. Despite the strong predictive\npower of deep learning methods, they do not provide a satisfactory\nrepresentation of these structures, thus creating significant barriers in\nscalable annotation and downstream analysis. In this dissertation, we tackle\nsuch challenges by proposing novel representations of these topological\nstructures in a deep learning framework. We leverage the mathematical tools\nfrom topological data analysis, i.e., persistent homology and discrete Morse\ntheory, to develop principled methods for better segmentation and uncertainty\nestimation, which will become powerful tools for scalable annotation.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.15361v1.pdf",
        "similarity": 0.2616397701963494,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-22"
    },
    {
        "new_title": "Continual Learning and Catastrophic Forgetting",
        "new_link": "http://arxiv.org/abs/2403.05175v1",
        "new_summary": "  This book chapter delves into the dynamics of continual learning, which is\nthe process of incrementally learning from a non-stationary stream of data.\nAlthough continual learning is a natural skill for the human brain, it is very\nchallenging for artificial neural networks. An important reason is that, when\nlearning something new, these networks tend to quickly and drastically forget\nwhat they had learned before, a phenomenon known as catastrophic forgetting.\nEspecially in the last decade, continual learning has become an extensively\nstudied topic in deep learning. This book chapter reviews the insights that\nthis field has generated.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.05175v1.pdf",
        "similarity": 0.261520615132814,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-08"
    },
    {
        "new_title": "A Systematic Bias of Machine Learning Regression Models and Its\n  Correction: an Application to Imaging-based Brain Age Prediction",
        "new_link": "http://arxiv.org/abs/2405.15950v1",
        "new_summary": "  Machine learning models for continuous outcomes often yield systematically\nbiased predictions, particularly for values that largely deviate from the mean.\nSpecifically, predictions for large-valued outcomes tend to be negatively\nbiased, while those for small-valued outcomes are positively biased. We refer\nto this linear central tendency warped bias as the \"systematic bias of machine\nlearning regression\". In this paper, we first demonstrate that this issue\npersists across various machine learning models, and then delve into its\ntheoretical underpinnings. We propose a general constrained optimization\napproach designed to correct this bias and develop a computationally efficient\nalgorithm to implement our method. Our simulation results indicate that our\ncorrection method effectively eliminates the bias from the predicted outcomes.\nWe apply the proposed approach to the prediction of brain age using\nneuroimaging data. In comparison to competing machine learning models, our\nmethod effectively addresses the longstanding issue of \"systematic bias of\nmachine learning regression\" in neuroimaging-based brain age calculation,\nyielding unbiased predictions of brain age.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.15950v1.pdf",
        "similarity": 0.2611979837797816,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-24"
    },
    {
        "new_title": "Adversarial Consistency and the Uniqueness of the Adversarial Bayes\n  Classifier",
        "new_link": "http://arxiv.org/abs/2404.17358v2",
        "new_summary": "  Adversarial training is a common technique for learning robust classifiers.\nPrior work showed that convex surrogate losses are not statistically consistent\nin the adversarial context -- or in other words, a minimizing sequence of the\nadversarial surrogate risk will not necessarily minimize the adversarial\nclassification error. We connect the consistency of adversarial surrogate\nlosses to properties of minimizers to the adversarial classification risk,\nknown as \\emph{adversarial Bayes classifiers}. Specifically, under reasonable\ndistributional assumptions, a convex loss is statistically consistent for\nadversarial learning iff the adversarial Bayes classifier satisfies a certain\nnotion of uniqueness.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.17358v2.pdf",
        "similarity": 0.26092357148555173,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-26"
    },
    {
        "new_title": "Optimizing Sensor Network Design for Multiple Coverage",
        "new_link": "http://arxiv.org/abs/2405.09096v2",
        "new_summary": "  Sensor placement optimization methods have been studied extensively. They can\nbe applied to a wide range of applications, including surveillance of known\nenvironments, optimal locations for 5G towers, and placement of missile defense\nsystems. However, few works explore the robustness and efficiency of the\nresulting sensor network concerning sensor failure or adversarial attacks. This\npaper addresses this issue by optimizing for the least number of sensors to\nachieve multiple coverage of non-simply connected domains by a prescribed\nnumber of sensors. We introduce a new objective function for the greedy\n(next-best-view) algorithm to design efficient and robust sensor networks and\nderive theoretical bounds on the network's optimality. We further introduce a\nDeep Learning model to accelerate the algorithm for near real-time\ncomputations. The Deep Learning model requires the generation of training\nexamples. Correspondingly, we show that understanding the geometric properties\nof the training data set provides important insights into the performance and\ntraining process of deep learning techniques. Finally, we demonstrate that a\nsimple parallel version of the greedy approach using a simpler objective can be\nhighly competitive.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.09096v2.pdf",
        "similarity": 0.2607886662545653,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-15"
    },
    {
        "new_title": "Modeling Analog Dynamic Range Compressors using Deep Learning and\n  State-space Models",
        "new_link": "http://arxiv.org/abs/2403.16331v1",
        "new_summary": "  We describe a novel approach for developing realistic digital models of\ndynamic range compressors for digital audio production by analyzing their\nanalog prototypes. While realistic digital dynamic compressors are potentially\nuseful for many applications, the design process is challenging because the\ncompressors operate nonlinearly over long time scales. Our approach is based on\nthe structured state space sequence model (S4), as implementing the state-space\nmodel (SSM) has proven to be efficient at learning long-range dependencies and\nis promising for modeling dynamic range compressors. We present in this paper a\ndeep learning model with S4 layers to model the Teletronix LA-2A analog dynamic\nrange compressor. The model is causal, executes efficiently in real time, and\nachieves roughly the same quality as previous deep-learning models but with\nfewer parameters.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.16331v1.pdf",
        "similarity": 0.2607753045257691,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-24"
    },
    {
        "new_title": "Long-Term Fair Decision Making through Deep Generative Models",
        "new_link": "http://arxiv.org/abs/2401.11288v1",
        "new_summary": "  This paper studies long-term fair machine learning which aims to mitigate\ngroup disparity over the long term in sequential decision-making systems. To\ndefine long-term fairness, we leverage the temporal causal graph and use the\n1-Wasserstein distance between the interventional distributions of different\ndemographic groups at a sufficiently large time step as the quantitative\nmetric. Then, we propose a three-phase learning framework where the decision\nmodel is trained on high-fidelity data generated by a deep generative model. We\nformulate the optimization problem as a performative risk minimization and\nadopt the repeated gradient descent algorithm for learning. The empirical\nevaluation shows the efficacy of the proposed method using both synthetic and\nsemi-synthetic datasets.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.11288v1.pdf",
        "similarity": 0.26076227580931244,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-20"
    },
    {
        "new_title": "Local Adaptive Clustering Based Image Matching for Automatic Visual\n  Identification",
        "new_link": "http://arxiv.org/abs/2401.01720v1",
        "new_summary": "  Monitoring cameras are extensively utilized in industrial production to\nmonitor equipment running. With advancements in computer vision, device\nrecognition using image features is viable. This paper presents a\nvision-assisted identification system that implements real-time automatic\nequipment labeling through image matching in surveillance videos. The system\ndeploys the ORB algorithm to extract image features and the GMS algorithm to\nremove incorrect matching points. According to the principles of clustering and\ntemplate locality, a method known as Local Adaptive Clustering (LAC) has been\nestablished to enhance label positioning. This method segments matching\ntemplates using the cluster center, which improves the efficiency and stability\nof labels. The experimental results demonstrate that LAC effectively curtails\nthe label drift.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.01720v1.pdf",
        "similarity": 0.26069808128134075,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-03"
    },
    {
        "new_title": "Towards Bayesian Data Selection",
        "new_link": "http://arxiv.org/abs/2406.12560v2",
        "new_summary": "  A wide range of machine learning algorithms iteratively add data to the\ntraining sample. Examples include semi-supervised learning, active learning,\nmulti-armed bandits, and Bayesian optimization. We embed this kind of data\naddition into decision theory by framing data selection as a decision problem.\nThis paves the way for finding Bayes-optimal selections of data. For the\nillustrative case of self-training in semi-supervised learning, we derive the\nrespective Bayes criterion. We further show that deploying this criterion\nmitigates the issue of confirmation bias by empirically assessing our method\nfor generalized linear models, semi-parametric generalized additive models, and\nBayesian neural networks on simulated and real-world data.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.12560v2.pdf",
        "similarity": 0.26004644623873213,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-18"
    },
    {
        "new_title": "Evaluating Predictive Models in Cybersecurity: A Comparative Analysis of\n  Machine and Deep Learning Techniques for Threat Detection",
        "new_link": "http://arxiv.org/abs/2407.06014v1",
        "new_summary": "  As these attacks become more and more difficult to see, the need for the\ngreat hi-tech models that detect them is undeniable. This paper examines and\ncompares various machine learning as well as deep learning models to choose the\nmost suitable ones for detecting and fighting against cybersecurity risks. The\ntwo datasets are used in the study to assess models like Naive Bayes, SVM,\nRandom Forest, and deep learning architectures, i.e., VGG16, in the context of\naccuracy, precision, recall, and F1-score. Analysis shows that Random Forest\nand Extra Trees do better in terms of accuracy though in different aspects of\nthe dataset characteristics and types of threat. This research not only\nemphasizes the strengths and weaknesses of each predictive model but also\naddresses the difficulties associated with deploying such technologies in the\nreal-world environment, such as data dependency and computational demands. The\nresearch findings are targeted at cybersecurity professionals to help them\nselect appropriate predictive models and configure them to strengthen the\nsecurity measures against cyber threats completely.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.06014v1.pdf",
        "similarity": 0.2599786903402508,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-08"
    },
    {
        "new_title": "On the use of first and second derivative approximations for biometric\n  online signature recognition",
        "new_link": "http://arxiv.org/abs/2406.00512v1",
        "new_summary": "  This paper investigates the impact of different approximation methods in\nfeature extraction for pattern recognition applications, specifically focused\non delta and delta-delta parameters. Using MCYT330 online signature data-base,\nour experiments show that 11-point approximation outperforms 1-point\napproximation, resulting in a 1.4% improvement in identification rate, 36.8%\nreduction in random forgeries and 2.4% reduction in skilled forgeries\n",
        "pdf_link": "https://arxiv.org/pdf/2406.00512v1.pdf",
        "similarity": 0.259851312623356,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-01"
    },
    {
        "new_title": "A Study on Stock Forecasting Using Deep Learning and Statistical Models",
        "new_link": "http://arxiv.org/abs/2402.06689v1",
        "new_summary": "  Predicting a fast and accurate model for stock price forecasting is been a\nchallenging task and this is an active area of research where it is yet to be\nfound which is the best way to forecast the stock price. Machine learning, deep\nlearning and statistical analysis techniques are used here to get the accurate\nresult so the investors can see the future trend and maximize the return of\ninvestment in stock trading. This paper will review many deep learning\nalgorithms for stock price forecasting. We use a record of s&p 500 index data\nfor training and testing. The survey motive is to check various deep learning\nand statistical model techniques for stock price forecasting that are Moving\nAverages, ARIMA which are statistical techniques and LSTM, RNN, CNN, and FULL\nCNN which are deep learning models. It will discuss various models, including\nthe Auto regression integration moving average model, the Recurrent neural\nnetwork model, the long short-term model which is the type of RNN used for long\ndependency for data, the convolutional neural network model, and the full\nconvolutional neural network model, in terms of error calculation or percentage\nof accuracy that how much it is accurate which measures by the function like\nRoot mean square error, mean absolute error, mean squared error. The model can\nbe used to predict the stock price by checking the low MAE value as lower the\nMAE value the difference between the predicting and the actual value will be\nless and this model will predict the price more accurately than other models.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.06689v1.pdf",
        "similarity": 0.2597155559744888,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-08"
    },
    {
        "new_title": "Enhancing Wildfire Forecasting Through Multisource Spatio-Temporal Data,\n  Deep Learning, Ensemble Models and Transfer Learning",
        "new_link": "http://arxiv.org/abs/2407.15878v1",
        "new_summary": "  This paper presents a novel approach in wildfire prediction through the\nintegration of multisource spatiotemporal data, including satellite data, and\nthe application of deep learning techniques. Specifically, we utilize an\nensemble model built on transfer learning algorithms to forecast wildfires. The\nkey focus is on understanding the significance of weather sequences, human\nactivities, and specific weather parameters in wildfire prediction. The study\nencounters challenges in acquiring real-time data for training the network,\nespecially in Moroccan wildlands. The future work intends to develop a global\nmodel capable of processing multichannel, multidimensional, and unformatted\ndata sources to enhance our understanding of the future entropy of surface\ntiles.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.15878v1.pdf",
        "similarity": 0.25958132395992367,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-20"
    },
    {
        "new_title": "Rigor with Machine Learning from Field Theory to the Poincar\u00e9\n  Conjecture",
        "new_link": "http://arxiv.org/abs/2402.13321v1",
        "new_summary": "  Machine learning techniques are increasingly powerful, leading to many\nbreakthroughs in the natural sciences, but they are often stochastic,\nerror-prone, and blackbox. How, then, should they be utilized in fields such as\ntheoretical physics and pure mathematics that place a premium on rigor and\nunderstanding? In this Perspective we discuss techniques for obtaining rigor in\nthe natural sciences with machine learning. Non-rigorous methods may lead to\nrigorous results via conjecture generation or verification by reinforcement\nlearning. We survey applications of these techniques-for-rigor ranging from\nstring theory to the smooth $4$d Poincar\\'e conjecture in low-dimensional\ntopology. One can also imagine building direct bridges between machine learning\ntheory and either mathematics or theoretical physics. As examples, we describe\na new approach to field theory motivated by neural network theory, and a theory\nof Riemannian metric flows induced by neural network gradient descent, which\nencompasses Perelman's formulation of the Ricci flow that was utilized to\nresolve the $3$d Poincar\\'e conjecture.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.13321v1.pdf",
        "similarity": 0.2595625779229482,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-20"
    },
    {
        "new_title": "Reconstructing Visual Stimulus Images from EEG Signals Based on Deep\n  Visual Representation Model",
        "new_link": "http://arxiv.org/abs/2403.06532v1",
        "new_summary": "  Reconstructing visual stimulus images is a significant task in neural\ndecoding, and up to now, most studies consider the functional magnetic\nresonance imaging (fMRI) as the signal source. However, the fMRI-based image\nreconstruction methods are difficult to widely applied because of the\ncomplexity and high cost of the acquisition equipments. Considering the\nadvantages of low cost and easy portability of the electroencephalogram (EEG)\nacquisition equipments, we propose a novel image reconstruction method based on\nEEG signals in this paper. Firstly, to satisfy the high recognizability of\nvisual stimulus images in fast switching manner, we build a visual stimuli\nimage dataset, and obtain the EEG dataset by a corresponding EEG signals\ncollection experiment. Secondly, the deep visual representation model(DVRM)\nconsisting of a primary encoder and a subordinate decoder is proposed to\nreconstruct visual stimuli. The encoder is designed based on the\nresidual-in-residual dense blocks to learn the distribution characteristics\nbetween EEG signals and visual stimulus images, while the decoder is designed\nbased on the deep neural network to reconstruct the visual stimulus image from\nthe learned deep visual representation. The DVRM can fit the deep and multiview\nvisual features of human natural state and make the reconstructed images more\nprecise. Finally, we evaluate the DVRM in the quality of the generated images\non our EEG dataset. The results show that the DVRM have good performance in the\ntask of learning deep visual representation from EEG signals and generating\nreconstructed images that are realistic and highly resemble the original\nimages.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.06532v1.pdf",
        "similarity": 0.259504845414336,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-11"
    },
    {
        "new_title": "Reinforcement Learning for Collision-free Flight Exploiting Deep\n  Collision Encoding",
        "new_link": "http://arxiv.org/abs/2402.03947v1",
        "new_summary": "  This work contributes a novel deep navigation policy that enables\ncollision-free flight of aerial robots based on a modular approach exploiting\ndeep collision encoding and reinforcement learning. The proposed solution\nbuilds upon a deep collision encoder that is trained on both simulated and real\ndepth images using supervised learning such that it compresses the\nhigh-dimensional depth data to a low-dimensional latent space encoding\ncollision information while accounting for the robot size. This compressed\nencoding is combined with an estimate of the robot's odometry and the desired\ntarget location to train a deep reinforcement learning navigation policy that\noffers low-latency computation and robust sim2real performance. A set of\nsimulation and experimental studies in diverse environments are conducted and\ndemonstrate the efficiency of the emerged behavior and its resilience in\nreal-life deployments.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.03947v1.pdf",
        "similarity": 0.25894892645584494,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-06"
    },
    {
        "new_title": "\"Forgetting\" in Machine Learning and Beyond: A Survey",
        "new_link": "http://arxiv.org/abs/2405.20620v1",
        "new_summary": "  This survey investigates the multifaceted nature of forgetting in machine\nlearning, drawing insights from neuroscientific research that posits forgetting\nas an adaptive function rather than a defect, enhancing the learning process\nand preventing overfitting. This survey focuses on the benefits of forgetting\nand its applications across various machine learning sub-fields that can help\nimprove model performance and enhance data privacy. Moreover, the paper\ndiscusses current challenges, future directions, and ethical considerations\nregarding the integration of forgetting mechanisms into machine learning\nmodels.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.20620v1.pdf",
        "similarity": 0.2585584104510705,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-31"
    },
    {
        "new_title": "Logifold: A Geometrical Foundation of Ensemble Machine Learning",
        "new_link": "http://arxiv.org/abs/2407.16177v1",
        "new_summary": "  We present a local-to-global and measure-theoretical approach to\nunderstanding datasets. The core idea is to formulate a logifold structure and\nto interpret network models with restricted domains as local charts of\ndatasets. In particular, this provides a mathematical foundation for ensemble\nmachine learning. Our experiments demonstrate that logifolds can be implemented\nto identify fuzzy domains and improve accuracy compared to taking average of\nmodel outputs. Additionally, we provide a theoretical example of a logifold,\nhighlighting the importance of restricting to domains of classifiers in an\nensemble.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.16177v1.pdf",
        "similarity": 0.258449455553519,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-23"
    },
    {
        "new_title": "Machine learning for modular multiplication",
        "new_link": "http://arxiv.org/abs/2402.19254v1",
        "new_summary": "  Motivated by cryptographic applications, we investigate two machine learning\napproaches to modular multiplication: namely circular regression and a\nsequence-to-sequence transformer model. The limited success of both methods\ndemonstrated in our results gives evidence for the hardness of tasks involving\nmodular multiplication upon which cryptosystems are based.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.19254v1.pdf",
        "similarity": 0.25822621580315247,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-29"
    },
    {
        "new_title": "Accurate nuclear quantum statistics on machine-learned classical\n  effective potentials",
        "new_link": "http://arxiv.org/abs/2407.03448v1",
        "new_summary": "  The contribution of nuclear quantum effects (NQEs) to the properties of\nvarious hydrogen-bound systems, including biomolecules, is increasingly\nrecognized. Despite the development of many acceleration techniques, the\ncomputational overhead of incorporating NQEs in complex systems is sizable,\nparticularly at low temperatures. In this work, we leverage deep learning and\nmultiscale coarse-graining techniques to mitigate the computational burden of\npath integral molecular dynamics (PIMD). Specifically, we employ a\nmachine-learned potential to accurately represent corrections to classical\npotentials, thereby significantly reducing the computational cost of simulating\nNQEs. We validate our approach using four distinct systems: Morse potential,\nZundel cation, single water molecule, and bulk water. Our framework allows us\nto accurately compute position-dependent static properties, as demonstrated by\nthe excellent agreement obtained between the machine-learned potential and\ncomputationally intensive PIMD calculations, even in the presence of strong\nNQEs. This approach opens the way to the development of transferable\nmachine-learned potentials capable of accurately reproducing NQEs in a wide\nrange of molecular systems.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.03448v1.pdf",
        "similarity": 0.25822064387155186,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-03"
    },
    {
        "new_title": "Machine Learning Applications of Quantum Computing: A Review",
        "new_link": "http://arxiv.org/abs/2406.13262v2",
        "new_summary": "  At the intersection of quantum computing and machine learning, this review\npaper explores the transformative impact these technologies are having on the\ncapabilities of data processing and analysis, far surpassing the bounds of\ntraditional computational methods. Drawing upon an in-depth analysis of 32\nseminal papers, this review delves into the interplay between quantum computing\nand machine learning, focusing on transcending the limitations of classical\ncomputing in advanced data processing and applications. This review emphasizes\nthe potential of quantum-enhanced methods in enhancing cybersecurity, a\ncritical sector that stands to benefit significantly from these advancements.\nThe literature review, primarily leveraging Science Direct as an academic\ndatabase, delves into the transformative effects of quantum technologies on\nmachine learning, drawing insights from a diverse collection of studies and\nscholarly articles. While the focus is primarily on the growing significance of\nquantum computing in cybersecurity, the review also acknowledges the promising\nimplications for other sectors as the field matures. Our systematic approach\ncategorizes sources based on quantum machine learning algorithms, applications,\nchallenges, and potential future developments, uncovering that quantum\ncomputing is increasingly being implemented in practical machine learning\nscenarios. The review highlights advancements in quantum-enhanced machine\nlearning algorithms and their potential applications in sectors such as\ncybersecurity, emphasizing the need for industry-specific solutions while\nconsidering ethical and security concerns. By presenting an overview of the\ncurrent state and projecting future directions, the paper sets a foundation for\nongoing research and strategic advancement in quantum machine learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.13262v2.pdf",
        "similarity": 0.258133340579884,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-19"
    },
    {
        "new_title": "Deep Learning-Based Weather-Related Power Outage Prediction with\n  Socio-Economic and Power Infrastructure Data",
        "new_link": "http://arxiv.org/abs/2404.03115v1",
        "new_summary": "  This paper presents a deep learning-based approach for hourly power outage\nprobability prediction within census tracts encompassing a utility company's\nservice territory. Two distinct deep learning models, conditional Multi-Layer\nPerceptron (MLP) and unconditional MLP, were developed to forecast power outage\nprobabilities, leveraging a rich array of input features gathered from publicly\navailable sources including weather data, weather station locations, power\ninfrastructure maps, socio-economic and demographic statistics, and power\noutage records. Given a one-hour-ahead weather forecast, the models predict the\npower outage probability for each census tract, taking into account both the\nweather prediction and the location's characteristics. The deep learning models\nemployed different loss functions to optimize prediction performance. Our\nexperimental results underscore the significance of socio-economic factors in\nenhancing the accuracy of power outage predictions at the census tract level.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.03115v1.pdf",
        "similarity": 0.2580480214259472,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-03"
    },
    {
        "new_title": "Improving density matrix electronic structure method by deep learning",
        "new_link": "http://arxiv.org/abs/2406.17561v1",
        "new_summary": "  The combination of deep learning and ab initio materials calculations is\nemerging as a trending frontier of materials science research, with\ndeep-learning density functional theory (DFT) electronic structure being\nparticularly promising. In this work, we introduce a neural-network method for\nmodeling the DFT density matrix, a fundamental yet previously unexplored\nquantity in deep-learning electronic structure. Utilizing an advanced neural\nnetwork framework that leverages the nearsightedness and equivariance\nproperties of the density matrix, the method demonstrates high accuracy and\nexcellent generalizability in multiple example studies, as well as capability\nto precisely predict charge density and reproduce other electronic structure\nproperties. Given the pivotal role of the density matrix in DFT as well as\nother computational methods, the current research introduces a novel approach\nto the deep-learning study of electronic structure properties, opening up new\nopportunities for deep-learning enhanced computational materials study.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.17561v1.pdf",
        "similarity": 0.25790244967715426,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-25"
    },
    {
        "new_title": "Light-cone feature selection for quantum machine learning",
        "new_link": "http://arxiv.org/abs/2403.18733v2",
        "new_summary": "  Feature selection plays an essential role in improving the predictive\nperformance and interpretability of trained models in classical machine\nlearning. On the other hand, the usability of conventional feature selection\ncould be limited for quantum machine learning tasks; the technique might not\nprovide a clear interpretation on embedding quantum circuits for classical data\ntasks and, more importantly, is not applicable to quantum data tasks. In this\nwork, we propose a feature selection method with a specific focus on quantum\nmachine learning. Our scheme treats the light-cones (i.e., subspace) of quantum\nmodels as features and then select relevant ones through training of the\ncorresponding local quantum kernels. We numerically demonstrate its versatility\nfor four different applications using toy tasks: (1) feature selection of\nclassical inputs, (2) circuit architecture search for data embedding, (3)\ncompression of quantum machine learning models and (4) subspace selection for\nquantum data. The proposed framework paves the way towards applications of\nquantum machine learning to practical tasks. Also, this technique could be used\nto practically test if the quantum machine learning tasks really need\nquantumness, while it is beyond the scope of this work.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.18733v2.pdf",
        "similarity": 0.2577893588261455,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-27"
    },
    {
        "new_title": "General Distribution Learning: A theoretical framework for Deep Learning",
        "new_link": "http://arxiv.org/abs/2406.05666v5",
        "new_summary": "  This paper introduces General Distribution Learning (GD learning), a novel\ntheoretical learning framework designed to address a comprehensive range of\nmachine learning and statistical tasks, including classification, regression,\nand parameter estimation. GD learning focuses on estimating the true underlying\nprobability distribution of dataset and using models to fit the estimated\nparameters of the distribution. The learning error in GD learning is thus\ndecomposed into two distinct categories: estimation error and fitting error.\nThe estimation error, which stems from the constraints of finite sampling,\nlimited prior knowledge, and the estimation algorithm's inherent limitations,\nquantifies the discrepancy between the true distribution and its estimate. The\nfitting error can be attributed to model's capacity limitation and the\nperformance limitation of the optimization algorithm, which evaluates the\ndeviation of the model output from the fitted objective. To address the\nchallenge of non-convexity in the optimization of learning error, we introduce\nthe standard loss function and demonstrate that, when employing this function,\nglobal optimal solutions in non-convex optimization can be approached by\nminimizing the gradient norm and the structural error. Moreover, we demonstrate\nthat the estimation error is determined by the uncertainty of the estimate $q$,\nand propose the minimum uncertainty principle to obtain an optimal estimate of\nthe true distribution. We further provide upper bounds for the estimation\nerror, fitting error, and learning error within the GD learning framework.\nUltimately, our findings are applied to offer theoretical explanations for\nseveral unanswered questions on deep learning, including overparameterization,\nnon-convex optimization, flat minima, dynamic isometry condition and other\ntechniques in deep learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.05666v5.pdf",
        "similarity": 0.2576635267543698,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-09"
    },
    {
        "new_title": "Extrinsicaly Rewarded Soft Q Imitation Learning with Discriminator",
        "new_link": "http://arxiv.org/abs/2401.16772v1",
        "new_summary": "  Imitation learning is often used in addition to reinforcement learning in\nenvironments where reward design is difficult or where the reward is sparse,\nbut it is difficult to be able to imitate well in unknown states from a small\namount of expert data and sampling data. Supervised learning methods such as\nBehavioral Cloning do not require sampling data, but usually suffer from\ndistribution shift. The methods based on reinforcement learning, such as\ninverse reinforcement learning and Generative Adversarial imitation learning\n(GAIL), can learn from only a few expert data. However, they often need to\ninteract with the environment. Soft Q imitation learning (SQIL) addressed the\nproblems, and it was shown that it could learn efficiently by combining\nBehavioral Cloning and soft Q-learning with constant rewards. In order to make\nthis algorithm more robust to distribution shift, we propose more efficient and\nrobust algorithm by adding to this method a reward function based on\nadversarial inverse reinforcement learning that rewards the agent for\nperforming actions in status similar to the demo. We call this algorithm\nDiscriminator Soft Q Imitation Learning (DSQIL). We evaluated it on MuJoCo\nenvironments.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.16772v1.pdf",
        "similarity": 0.25753624400123715,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-30"
    },
    {
        "new_title": "Zero-shot Microclimate Prediction with Deep Learning",
        "new_link": "http://arxiv.org/abs/2401.02665v1",
        "new_summary": "  Weather station data is a valuable resource for climate prediction, however,\nits reliability can be limited in remote locations. To compound the issue,\nmaking local predictions often relies on sensor data that may not be accessible\nfor a new, previously unmonitored location. In response to these challenges, we\npropose a novel zero-shot learning approach designed to forecast various\nclimate measurements at new and unmonitored locations. Our method surpasses\nconventional weather forecasting techniques in predicting microclimate\nvariables by leveraging knowledge extracted from other geographic locations.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.02665v1.pdf",
        "similarity": 0.2574031045540699,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-05"
    },
    {
        "new_title": "Quadruplet Loss For Improving the Robustness to Face Morphing Attacks",
        "new_link": "http://arxiv.org/abs/2402.14665v1",
        "new_summary": "  Recent advancements in deep learning have revolutionized technology and\nsecurity measures, necessitating robust identification methods. Biometric\napproaches, leveraging personalized characteristics, offer a promising\nsolution. However, Face Recognition Systems are vulnerable to sophisticated\nattacks, notably face morphing techniques, enabling the creation of fraudulent\ndocuments. In this study, we introduce a novel quadruplet loss function for\nincreasing the robustness of face recognition systems against morphing attacks.\nOur approach involves specific sampling of face image quadruplets, combined\nwith face morphs, for network training. Experimental results demonstrate the\nefficiency of our strategy in improving the robustness of face recognition\nnetworks against morphing attacks.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.14665v1.pdf",
        "similarity": 0.2572964838122034,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-22"
    },
    {
        "new_title": "Comparative Analysis of Predicting Subsequent Steps in H\u00e9non Map",
        "new_link": "http://arxiv.org/abs/2405.10190v2",
        "new_summary": "  This paper explores the prediction of subsequent steps in H\\'enon Map using\nvarious machine learning techniques. The H\\'enon map, well known for its\nchaotic behaviour, finds applications in various fields including cryptography,\nimage encryption, and pattern recognition. Machine learning methods,\nparticularly deep learning, are increasingly essential for understanding and\npredicting chaotic phenomena. This study evaluates the performance of different\nmachine learning models including Random Forest, Recurrent Neural Network\n(RNN), Long Short-Term Memory (LSTM) networks, Support Vector Machines (SVM),\nand Feed Forward Neural Networks (FNN) in predicting the evolution of the\nH\\'enon map. Results indicate that LSTM network demonstrate superior predictive\naccuracy, particularly in extreme event prediction. Furthermore, a comparison\nbetween LSTM and FNN models reveals the LSTM's advantage, especially for longer\nprediction horizons and larger datasets. This research underscores the\nsignificance of machine learning in elucidating chaotic dynamics and highlights\nthe importance of model selection and dataset size in forecasting subsequent\nsteps in chaotic systems.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.10190v2.pdf",
        "similarity": 0.2569981665677371,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-15"
    },
    {
        "new_title": "Investigating Reproducibility in Deep Learning-Based Software Fault\n  Prediction",
        "new_link": "http://arxiv.org/abs/2402.05645v1",
        "new_summary": "  Over the past few years, deep learning methods have been applied for a wide\nrange of Software Engineering (SE) tasks, including in particular for the\nimportant task of automatically predicting and localizing faults in software.\nWith the rapid adoption of increasingly complex machine learning models, it\nhowever becomes more and more difficult for scholars to reproduce the results\nthat are reported in the literature. This is in particular the case when the\napplied deep learning models and the evaluation methodology are not properly\ndocumented and when code and data are not shared. Given some recent -- and very\nworrying -- findings regarding reproducibility and progress in other areas of\napplied machine learning, the goal of this work is to analyze to what extent\nthe field of software engineering, in particular in the area of software fault\nprediction, is plagued by similar problems. We have therefore conducted a\nsystematic review of the current literature and examined the level of\nreproducibility of 56 research articles that were published between 2019 and\n2022 in top-tier software engineering conferences. Our analysis revealed that\nscholars are apparently largely aware of the reproducibility problem, and about\ntwo thirds of the papers provide code for their proposed deep learning models.\nHowever, it turned out that in the vast majority of cases, crucial elements for\nreproducibility are missing, such as the code of the compared baselines, code\nfor data pre-processing or code for hyperparameter tuning. In these cases, it\ntherefore remains challenging to exactly reproduce the results in the current\nresearch literature. Overall, our meta-analysis therefore calls for improved\nresearch practices to ensure the reproducibility of machine-learning based\nresearch.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.05645v1.pdf",
        "similarity": 0.25692052139255667,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-08"
    },
    {
        "new_title": "Learning Staged Trees from Incomplete Data",
        "new_link": "http://arxiv.org/abs/2405.18306v1",
        "new_summary": "  Staged trees are probabilistic graphical models capable of representing any\nclass of non-symmetric independence via a coloring of its vertices. Several\nstructural learning routines have been defined and implemented to learn staged\ntrees from data, under the frequentist or Bayesian paradigm. They assume a data\nset has been observed fully and, in practice, observations with missing entries\nare either dropped or imputed before learning the model. Here, we introduce the\nfirst algorithms for staged trees that handle missingness within the learning\nof the model. To this end, we characterize the likelihood of staged tree models\nin the presence of missing data and discuss pseudo-likelihoods that approximate\nit. A structural expectation-maximization algorithm estimating the model\ndirectly from the full likelihood is also implemented and evaluated. A\ncomputational experiment showcases the performance of the novel learning\nalgorithms, demonstrating that it is feasible to account for different\nmissingness patterns when learning staged trees.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.18306v1.pdf",
        "similarity": 0.2566874676609986,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-28"
    },
    {
        "new_title": "On the Convergence of Locally Adaptive and Scalable Diffusion-Based\n  Sampling Methods for Deep Bayesian Neural Network Posteriors",
        "new_link": "http://arxiv.org/abs/2403.08609v2",
        "new_summary": "  Achieving robust uncertainty quantification for deep neural networks\nrepresents an important requirement in many real-world applications of deep\nlearning such as medical imaging where it is necessary to assess the\nreliability of a neural network's prediction. Bayesian neural networks are a\npromising approach for modeling uncertainties in deep neural networks.\nUnfortunately, generating samples from the posterior distribution of neural\nnetworks is a major challenge. One significant advance in that direction would\nbe the incorporation of adaptive step sizes, similar to modern neural network\noptimizers, into Monte Carlo Markov chain sampling algorithms without\nsignificantly increasing computational demand. Over the past years, several\npapers have introduced sampling algorithms with claims that they achieve this\nproperty. However, do they indeed converge to the correct distribution? In this\npaper, we demonstrate that these methods can have a substantial bias in the\ndistribution they sample, even in the limit of vanishing step sizes and at full\nbatch size.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.08609v2.pdf",
        "similarity": 0.2564737870468684,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-13"
    },
    {
        "new_title": "Aprendizado de m\u00e1quina aplicado na eletroqu\u00edmica",
        "new_link": "http://arxiv.org/abs/2401.14413v1",
        "new_summary": "  This systematic review focuses on analyzing the use of machine learning\ntechniques for identifying and quantifying analytes in various electrochemical\napplications, presenting the available applications in the literature. Machine\nlearning is a tool that can facilitate the analysis and enhance the\nunderstanding of processes involving various analytes. In electrochemical\nbiosensors, it increases the precision of medical diagnostics, improving the\nidentification of biomarkers and pathogens with high reliability. It can be\neffectively used for the classification of complex chemical products; in\nenvironmental monitoring, using low-cost sensors; in portable devices and\nwearable systems; among others. Currently, the analysis of some analytes is\nstill performed manually, requiring the expertise of a specialist in the field\nand thus hindering the generalization of results. In light of the advancements\nin artificial intelligence today, this work proposes to carry out a systematic\nreview of the literature on the applications of artificial intelligence\ntechniques. A set of articles has been identified that address electrochemical\nproblems using machine learning techniques, more specifically, supervised\nlearning.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.14413v1.pdf",
        "similarity": 0.2563908965127747,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-20"
    },
    {
        "new_title": "Pre-Sorted Tsetlin Machine (The Genetic K-Medoid Method)",
        "new_link": "http://arxiv.org/abs/2403.09680v2",
        "new_summary": "  This paper proposes a machine learning pre-sort stage to traditional\nsupervised learning using Tsetlin Machines. Initially, K data-points are\nidentified from the dataset using an expedited genetic algorithm to solve the\nmaximum dispersion problem. These are then used as the initial placement to run\nthe K-Medoid clustering algorithm. Finally, an expedited genetic algorithm is\nused to align K independent Tsetlin Machines by maximising hamming distance.\nFor MNIST level classification problems, results demonstrate up to 10%\nimprovement in accuracy, approx. 383X reduction in training time and approx.\n86X reduction in inference time.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.09680v2.pdf",
        "similarity": 0.2561756458096745,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-07"
    },
    {
        "new_title": "From Model Performance to Claim: How a Change of Focus in Machine\n  Learning Replicability Can Help Bridge the Responsibility Gap",
        "new_link": "http://arxiv.org/abs/2404.13131v1",
        "new_summary": "  Two goals - improving replicability and accountability of Machine Learning\nresearch respectively, have accrued much attention from the AI ethics and the\nMachine Learning community. Despite sharing the measures of improving\ntransparency, the two goals are discussed in different registers -\nreplicability registers with scientific reasoning whereas accountability\nregisters with ethical reasoning. Given the existing challenge of the\nResponsibility Gap - holding Machine Learning scientists accountable for\nMachine Learning harms due to them being far from sites of application, this\npaper posits that reconceptualizing replicability can help bridge the gap.\nThrough a shift from model performance replicability to claim replicability,\nMachine Learning scientists can be held accountable for producing\nnon-replicable claims that are prone to eliciting harm due to misuse and\nmisinterpretation. In this paper, I make the following contributions. First, I\ndefine and distinguish two forms of replicability for ML research that can aid\nconstructive conversations around replicability. Second, I formulate an\nargument for claim-replicability's advantage over model performance\nreplicability in justifying assigning accountability to Machine Learning\nscientists for producing non-replicable claims and show how it enacts a sense\nof responsibility that is actionable. In addition, I characterize the\nimplementation of claim replicability as more of a social project than a\ntechnical one by discussing its competing epistemological principles, practical\nimplications on Circulating Reference, Interpretative Labor, and research\ncommunication.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.13131v1.pdf",
        "similarity": 0.25587987593038947,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-19"
    },
    {
        "new_title": "Active Learning for Derivative-Based Global Sensitivity Analysis with\n  Gaussian Processes",
        "new_link": "http://arxiv.org/abs/2407.09739v1",
        "new_summary": "  We consider the problem of active learning for global sensitivity analysis of\nexpensive black-box functions. Our aim is to efficiently learn the importance\nof different input variables, e.g., in vehicle safety experimentation, we study\nthe impact of the thickness of various components on safety objectives. Since\nfunction evaluations are expensive, we use active learning to prioritize\nexperimental resources where they yield the most value. We propose novel active\nlearning acquisition functions that directly target key quantities of\nderivative-based global sensitivity measures (DGSMs) under Gaussian process\nsurrogate models. We showcase the first application of active learning directly\nto DGSMs, and develop tractable uncertainty reduction and information gain\nacquisition functions for these measures. Through comprehensive evaluation on\nsynthetic and real-world problems, our study demonstrates how these active\nlearning acquisition strategies substantially enhance the sample efficiency of\nDGSM estimation, particularly with limited evaluation budgets. Our work paves\nthe way for more efficient and accurate sensitivity analysis in various\nscientific and engineering applications.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.09739v1.pdf",
        "similarity": 0.25561917220102615,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-13"
    },
    {
        "new_title": "Deep Learning-Based Speech and Vision Synthesis to Improve Phishing\n  Attack Detection through a Multi-layer Adaptive Framework",
        "new_link": "http://arxiv.org/abs/2402.17249v1",
        "new_summary": "  The ever-evolving ways attacker continues to im prove their phishing\ntechniques to bypass existing state-of-the-art phishing detection methods pose\na mountain of challenges to researchers in both industry and academia research\ndue to the inability of current approaches to detect complex phishing attack.\nThus, current anti-phishing methods remain vulnerable to complex phishing\nbecause of the increasingly sophistication tactics adopted by attacker coupled\nwith the rate at which new tactics are being developed to evade detection. In\nthis research, we proposed an adaptable framework that combines Deep learning\nand Randon Forest to read images, synthesize speech from deep-fake videos, and\nnatural language processing at various predictions layered to significantly\nincrease the performance of machine learning models for phishing attack\ndetection.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.17249v1.pdf",
        "similarity": 0.2554338906676901,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-27"
    },
    {
        "new_title": "Is Epistemic Uncertainty Faithfully Represented by Evidential Deep\n  Learning Methods?",
        "new_link": "http://arxiv.org/abs/2402.09056v2",
        "new_summary": "  Trustworthy ML systems should not only return accurate predictions, but also\na reliable representation of their uncertainty. Bayesian methods are commonly\nused to quantify both aleatoric and epistemic uncertainty, but alternative\napproaches, such as evidential deep learning methods, have become popular in\nrecent years. The latter group of methods in essence extends empirical risk\nminimization (ERM) for predicting second-order probability distributions over\noutcomes, from which measures of epistemic (and aleatoric) uncertainty can be\nextracted. This paper presents novel theoretical insights of evidential deep\nlearning, highlighting the difficulties in optimizing second-order loss\nfunctions and interpreting the resulting epistemic uncertainty measures. With a\nsystematic setup that covers a wide range of approaches for classification,\nregression and counts, it provides novel insights into issues of\nidentifiability and convergence in second-order loss minimization, and the\nrelative (rather than absolute) nature of epistemic uncertainty measures.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.09056v2.pdf",
        "similarity": 0.2554114139534669,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-14"
    },
    {
        "new_title": "Dynamic Byzantine-Robust Learning: Adapting to Switching Byzantine\n  Workers",
        "new_link": "http://arxiv.org/abs/2402.02951v2",
        "new_summary": "  Byzantine-robust learning has emerged as a prominent fault-tolerant\ndistributed machine learning framework. However, most techniques focus on the\nstatic setting, wherein the identity of Byzantine workers remains unchanged\nthroughout the learning process. This assumption fails to capture real-world\ndynamic Byzantine behaviors, which may include intermittent malfunctions or\ntargeted, time-limited attacks. Addressing this limitation, we propose DynaBRO\n-- a new method capable of withstanding any sub-linear number of identity\nchanges across rounds. Specifically, when the number of such changes is\n$\\mathcal{O}(\\sqrt{T})$ (where $T$ is the total number of training rounds),\nDynaBRO nearly matches the state-of-the-art asymptotic convergence rate of the\nstatic setting. Our method utilizes a multi-level Monte Carlo (MLMC) gradient\nestimation technique applied at the server to robustly aggregated worker\nupdates. By additionally leveraging an adaptive learning rate, we circumvent\nthe need for prior knowledge of the fraction of Byzantine workers.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.02951v2.pdf",
        "similarity": 0.25532802790475934,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-05"
    },
    {
        "new_title": "Efficient Normalized Conformal Prediction and Uncertainty Quantification\n  for Anti-Cancer Drug Sensitivity Prediction with Deep Regression Forests",
        "new_link": "http://arxiv.org/abs/2402.14080v1",
        "new_summary": "  Deep learning models are being adopted and applied on various critical\ndecision-making tasks, yet they are trained to provide point predictions\nwithout providing degrees of confidence. The trustworthiness of deep learning\nmodels can be increased if paired with uncertainty estimations. Conformal\nPrediction has emerged as a promising method to pair machine learning models\nwith prediction intervals, allowing for a view of the model's uncertainty.\nHowever, popular uncertainty estimation methods for conformal prediction fail\nto provide heteroskedastic intervals that are equally accurate for all samples.\nIn this paper, we propose a method to estimate the uncertainty of each sample\nby calculating the variance obtained from a Deep Regression Forest. We show\nthat the deep regression forest variance improves the efficiency and coverage\nof normalized inductive conformal prediction on a drug response prediction\ntask.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.14080v1.pdf",
        "similarity": 0.2552139114938621,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-21"
    },
    {
        "new_title": "Research on Autonomous Robots Navigation based on Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2407.02539v2",
        "new_summary": "  Reinforcement learning continuously optimizes decision-making based on\nreal-time feedback reward signals through continuous interaction with the\nenvironment, demonstrating strong adaptive and self-learning capabilities. In\nrecent years, it has become one of the key methods to achieve autonomous\nnavigation of robots. In this work, an autonomous robot navigation method based\non reinforcement learning is introduced. We use the Deep Q Network (DQN) and\nProximal Policy Optimization (PPO) models to optimize the path planning and\ndecision-making process through the continuous interaction between the robot\nand the environment, and the reward signals with real-time feedback. By\ncombining the Q-value function with the deep neural network, deep Q network can\nhandle high-dimensional state space, so as to realize path planning in complex\nenvironments. Proximal policy optimization is a strategy gradient-based method,\nwhich enables robots to explore and utilize environmental information more\nefficiently by optimizing policy functions. These methods not only improve the\nrobot's navigation ability in the unknown environment, but also enhance its\nadaptive and self-learning capabilities. Through multiple training and\nsimulation experiments, we have verified the effectiveness and robustness of\nthese models in various complex scenarios.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.02539v2.pdf",
        "similarity": 0.25517368372719856,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-02"
    },
    {
        "new_title": "A Cutting-Edge Deep Learning Method For Enhancing IoT Security",
        "new_link": "http://arxiv.org/abs/2406.12400v1",
        "new_summary": "  There have been significant issues given the IoT, with heterogeneity of\nbillions of devices and with a large amount of data. This paper proposed an\ninnovative design of the Internet of Things (IoT) Environment Intrusion\nDetection System (or IDS) using Deep Learning-integrated Convolutional Neural\nNetworks (CNN) and Long Short-Term Memory (LSTM) networks. Our model, based on\nthe CICIDS2017 dataset, achieved an accuracy of 99.52% in classifying network\ntraffic as either benign or malicious. The real-time processing capability,\nscalability, and low false alarm rate in our model surpass some traditional IDS\napproaches and, therefore, prove successful for application in today's IoT\nnetworks. The development and the performance of the model, with possible\napplications that may extend to other related fields of adaptive learning\ntechniques and cross-domain applicability, are discussed. The research\ninvolving deep learning for IoT cybersecurity offers a potent solution for\nsignificantly improving network security.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.12400v1.pdf",
        "similarity": 0.2548793623963168,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-18"
    },
    {
        "new_title": "How Deep is your Guess? A Fresh Perspective on Deep Learning for Medical\n  Time-Series Imputation",
        "new_link": "http://arxiv.org/abs/2407.08442v1",
        "new_summary": "  We introduce a novel classification framework for time-series imputation\nusing deep learning, with a particular focus on clinical data. By identifying\nconceptual gaps in the literature and existing reviews, we devise a taxonomy\ngrounded on the inductive bias of neural imputation frameworks, resulting in a\nclassification of existing deep imputation strategies based on their\nsuitability for specific imputation scenarios and data-specific properties. Our\nreview further examines the existing methodologies employed to benchmark deep\nimputation models, evaluating their effectiveness in capturing the missingness\nscenarios found in clinical data and emphasising the importance of reconciling\nmathematical abstraction with clinical insights. Our classification aims to\nserve as a guide for researchers to facilitate the selection of appropriate\ndeep learning imputation techniques tailored to their specific clinical data.\nOur novel perspective also highlights the significance of bridging the gap\nbetween computational methodologies and medical insights to achieve clinically\nsound imputation models.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.08442v1.pdf",
        "similarity": 0.25465130491546967,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-11"
    },
    {
        "new_title": "Complexity Reduction in Machine Learning-Based Wireless Positioning:\n  Minimum Description Features",
        "new_link": "http://arxiv.org/abs/2402.09580v1",
        "new_summary": "  A recent line of research has been investigating deep learning approaches to\nwireless positioning (WP). Although these WP algorithms have demonstrated high\naccuracy and robust performance against diverse channel conditions, they also\nhave a major drawback: they require processing high-dimensional features, which\ncan be prohibitive for mobile applications. In this work, we design a\npositioning neural network (P-NN) that substantially reduces the complexity of\ndeep learning-based WP through carefully crafted minimum description features.\nOur feature selection is based on maximum power measurements and their temporal\nlocations to convey information needed to conduct WP. We also develop a novel\nmethodology for adaptively selecting the size of feature space, which optimizes\nover balancing the expected amount of useful information and classification\ncapability, quantified using information-theoretic measures on the signal bin\nselection. Numerical results show that P-NN achieves a significant advantage in\nperformance-complexity tradeoff over deep learning baselines that leverage the\nfull power delay profile (PDP).\n",
        "pdf_link": "https://arxiv.org/pdf/2402.09580v1.pdf",
        "similarity": 0.25458202590955553,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-14"
    },
    {
        "new_title": "DeepLINK-T: deep learning inference for time series data using knockoffs\n  and LSTM",
        "new_link": "http://arxiv.org/abs/2404.04317v1",
        "new_summary": "  High-dimensional longitudinal time series data is prevalent across various\nreal-world applications. Many such applications can be modeled as regression\nproblems with high-dimensional time series covariates. Deep learning has been a\npopular and powerful tool for fitting these regression models. Yet, the\ndevelopment of interpretable and reproducible deep-learning models is\nchallenging and remains underexplored. This study introduces a novel method,\nDeep Learning Inference using Knockoffs for Time series data (DeepLINK-T),\nfocusing on the selection of significant time series variables in regression\nwhile controlling the false discovery rate (FDR) at a predetermined level.\nDeepLINK-T combines deep learning with knockoff inference to control FDR in\nfeature selection for time series models, accommodating a wide variety of\nfeature distributions. It addresses dependencies across time and features by\nleveraging a time-varying latent factor structure in time series covariates.\nThree key ingredients for DeepLINK-T are 1) a Long Short-Term Memory (LSTM)\nautoencoder for generating time series knockoff variables, 2) an LSTM\nprediction network using both original and knockoff variables, and 3) the\napplication of the knockoffs framework for variable selection with FDR control.\nExtensive simulation studies have been conducted to evaluate DeepLINK-T's\nperformance, showing its capability to control FDR effectively while\ndemonstrating superior feature selection power for high-dimensional\nlongitudinal time series data compared to its non-time series counterpart.\nDeepLINK-T is further applied to three metagenomic data sets, validating its\npractical utility and effectiveness, and underscoring its potential in\nreal-world applications.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.04317v1.pdf",
        "similarity": 0.25456787708288997,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-05"
    },
    {
        "new_title": "Resource Constrained U-Net for Extraction of Retinal Vascular Trees",
        "new_link": "http://arxiv.org/abs/2407.04940v1",
        "new_summary": "  This paper demonstrates the efficacy of a modified U-Net structure for the\nextraction of vascular tree masks for human fundus photographs. On limited\ncompute resources and training data, the proposed model only slightly\nunderperforms when compared to state of the art methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.04940v1.pdf",
        "similarity": 0.25423909366655795,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-06"
    },
    {
        "new_title": "How to beat a Bayesian adversary",
        "new_link": "http://arxiv.org/abs/2407.08678v1",
        "new_summary": "  Deep neural networks and other modern machine learning models are often\nsusceptible to adversarial attacks. Indeed, an adversary may often be able to\nchange a model's prediction through a small, directed perturbation of the\nmodel's input - an issue in safety-critical applications. Adversarially robust\nmachine learning is usually based on a minmax optimisation problem that\nminimises the machine learning loss under maximisation-based adversarial\nattacks.\n  In this work, we study adversaries that determine their attack using a\nBayesian statistical approach rather than maximisation. The resulting Bayesian\nadversarial robustness problem is a relaxation of the usual minmax problem. To\nsolve this problem, we propose Abram - a continuous-time particle system that\nshall approximate the gradient flow corresponding to the underlying learning\nproblem. We show that Abram approximates a McKean-Vlasov process and justify\nthe use of Abram by giving assumptions under which the McKean-Vlasov process\nfinds the minimiser of the Bayesian adversarial robustness problem. We discuss\ntwo ways to discretise Abram and show its suitability in benchmark adversarial\ndeep learning experiments.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.08678v1.pdf",
        "similarity": 0.25420541936411034,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-11"
    },
    {
        "new_title": "FedQNN: Federated Learning using Quantum Neural Networks",
        "new_link": "http://arxiv.org/abs/2403.10861v1",
        "new_summary": "  In this study, we explore the innovative domain of Quantum Federated Learning\n(QFL) as a framework for training Quantum Machine Learning (QML) models via\ndistributed networks. Conventional machine learning models frequently grapple\nwith issues about data privacy and the exposure of sensitive information. Our\nproposed Federated Quantum Neural Network (FedQNN) framework emerges as a\ncutting-edge solution, integrating the singular characteristics of QML with the\nprinciples of classical federated learning. This work thoroughly investigates\nQFL, underscoring its capability to secure data handling in a distributed\nenvironment and facilitate cooperative learning without direct data sharing.\nOur research corroborates the concept through experiments across varied\ndatasets, including genomics and healthcare, thereby validating the versatility\nand efficacy of our FedQNN framework. The results consistently exceed 86%\naccuracy across three distinct datasets, proving its suitability for conducting\nvarious QML tasks. Our research not only identifies the limitations of\nclassical paradigms but also presents a novel framework to propel the field of\nQML into a new era of secure and collaborative innovation.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.10861v1.pdf",
        "similarity": 0.2541957182147775,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-16"
    },
    {
        "new_title": "Semi-Supervised Multi-Task Learning Based Framework for Power System\n  Security Assessment",
        "new_link": "http://arxiv.org/abs/2407.08886v1",
        "new_summary": "  This paper develops a novel machine learning-based framework using\nSemi-Supervised Multi-Task Learning (SS-MTL) for power system dynamic security\nassessment that is accurate, reliable, and aware of topological changes. The\nlearning algorithm underlying the proposed framework integrates conditional\nmasked encoders and employs multi-task learning for classification-aware\nfeature representation, which improves the accuracy and scalability to larger\nsystems. Additionally, this framework incorporates a confidence measure for its\npredictions, enhancing its reliability and interpretability. A topological\nsimilarity index has also been incorporated to add topological awareness to the\nframework. Various experiments on the IEEE 68-bus system were conducted to\nvalidate the proposed method, employing two distinct database generation\ntechniques to generate the required data to train the machine learning\nalgorithm. The results demonstrate that our algorithm outperforms existing\nstate-of-the-art machine learning based techniques for security assessment in\nterms of accuracy and robustness. Finally, our work underscores the value of\nemploying auto-encoders for security assessment, highlighting improvements in\naccuracy, reliability, and robustness. All datasets and codes used have been\nmade publicly available to ensure reproducibility and transparency.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.08886v1.pdf",
        "similarity": 0.25404160902093786,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-11"
    },
    {
        "new_title": "Robust deep learning from weakly dependent data",
        "new_link": "http://arxiv.org/abs/2405.05081v1",
        "new_summary": "  Recent developments on deep learning established some theoretical properties\nof deep neural networks estimators. However, most of the existing works on this\ntopic are restricted to bounded loss functions or (sub)-Gaussian or bounded\ninput. This paper considers robust deep learning from weakly dependent\nobservations, with unbounded loss function and unbounded input/output. It is\nonly assumed that the output variable has a finite $r$ order moment, with $r\n>1$. Non asymptotic bounds for the expected excess risk of the deep neural\nnetwork estimator are established under strong mixing, and $\\psi$-weak\ndependence assumptions on the observations. We derive a relationship between\nthese bounds and $r$, and when the data have moments of any order (that is\n$r=\\infty$), the convergence rate is close to some well-known results. When the\ntarget predictor belongs to the class of H\\\"older smooth functions with\nsufficiently large smoothness index, the rate of the expected excess risk for\nexponentially strongly mixing data is close to or as same as those for obtained\nwith i.i.d. samples. Application to robust nonparametric regression and robust\nnonparametric autoregression are considered. The simulation study for models\nwith heavy-tailed errors shows that, robust estimators with absolute loss and\nHuber loss function outperform the least squares method.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.05081v1.pdf",
        "similarity": 0.25372490952417615,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-08"
    },
    {
        "new_title": "Deep Learning for Multivariate Time Series Imputation: A Survey",
        "new_link": "http://arxiv.org/abs/2402.04059v1",
        "new_summary": "  The ubiquitous missing values cause the multivariate time series data to be\npartially observed, destroying the integrity of time series and hindering the\neffective time series data analysis. Recently deep learning imputation methods\nhave demonstrated remarkable success in elevating the quality of corrupted time\nseries data, subsequently enhancing performance in downstream tasks. In this\npaper, we conduct a comprehensive survey on the recently proposed deep learning\nimputation methods. First, we propose a taxonomy for the reviewed methods, and\nthen provide a structured review of these methods by highlighting their\nstrengths and limitations. We also conduct empirical experiments to study\ndifferent methods and compare their enhancement for downstream tasks. Finally,\nthe open issues for future research on multivariate time series imputation are\npointed out. All code and configurations of this work, including a regularly\nmaintained multivariate time series imputation paper list, can be found in the\nGitHub repository~\\url{https://github.com/WenjieDu/Awesome\\_Imputation}.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.04059v1.pdf",
        "similarity": 0.2536669388055693,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-06"
    },
    {
        "new_title": "Black-box Adversarial Transferability: An Empirical Study in\n  Cybersecurity Perspective",
        "new_link": "http://arxiv.org/abs/2404.10796v1",
        "new_summary": "  The rapid advancement of artificial intelligence within the realm of\ncybersecurity raises significant security concerns. The vulnerability of deep\nlearning models in adversarial attacks is one of the major issues. In\nadversarial machine learning, malicious users try to fool the deep learning\nmodel by inserting adversarial perturbation inputs into the model during its\ntraining or testing phase. Subsequently, it reduces the model confidence score\nand results in incorrect classifications. The novel key contribution of the\nresearch is to empirically test the black-box adversarial transferability\nphenomena in cyber attack detection systems. It indicates that the adversarial\nperturbation input generated through the surrogate model has a similar impact\non the target model in producing the incorrect classification. To empirically\nvalidate this phenomenon, surrogate and target models are used. The adversarial\nperturbation inputs are generated based on the surrogate-model for which the\nhacker has complete information. Based on these adversarial perturbation\ninputs, both surrogate and target models are evaluated during the inference\nphase. We have done extensive experimentation over the CICDDoS-2019 dataset,\nand the results are classified in terms of various performance metrics like\naccuracy, precision, recall, and f1-score. The findings indicate that any deep\nlearning model is highly susceptible to adversarial attacks, even if the\nattacker does not have access to the internal details of the target model. The\nresults also indicate that white-box adversarial attacks have a severe impact\ncompared to black-box adversarial attacks. There is a need to investigate and\nexplore adversarial defence techniques to increase the robustness of the deep\nlearning models against adversarial attacks.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.10796v1.pdf",
        "similarity": 0.2533152830858564,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-15"
    },
    {
        "new_title": "Improving Local Training in Federated Learning via Temperature Scaling",
        "new_link": "http://arxiv.org/abs/2401.09986v2",
        "new_summary": "  Federated learning is inherently hampered by data heterogeneity: non-i.i.d.\ntraining data over local clients. We propose a novel model training approach\nfor federated learning, FLex&Chill, which exploits the Logit Chilling method.\nThrough extensive evaluations, we demonstrate that, in the presence of\nnon-i.i.d. data characteristics inherent in federated learning systems, this\napproach can expedite model convergence and improve inference accuracy.\nQuantitatively, from our experiments, we observe up to 6X improvement in the\nglobal federated learning model convergence time, and up to 3.37% improvement\nin inference accuracy.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.09986v2.pdf",
        "similarity": 0.252818965265365,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-18"
    },
    {
        "new_title": "SUPClust: Active Learning at the Boundaries",
        "new_link": "http://arxiv.org/abs/2403.03741v1",
        "new_summary": "  Active learning is a machine learning paradigm designed to optimize model\nperformance in a setting where labeled data is expensive to acquire. In this\nwork, we propose a novel active learning method called SUPClust that seeks to\nidentify points at the decision boundary between classes. By targeting these\npoints, SUPClust aims to gather information that is most informative for\nrefining the model's prediction of complex decision regions. We demonstrate\nexperimentally that labeling these points leads to strong model performance.\nThis improvement is observed even in scenarios characterized by strong class\nimbalance.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.03741v1.pdf",
        "similarity": 0.2526117986197551,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-06"
    },
    {
        "new_title": "WindDragon: Enhancing wind power forecasting with Automated Deep\n  Learning",
        "new_link": "http://arxiv.org/abs/2402.14385v1",
        "new_summary": "  Achieving net zero carbon emissions by 2050 requires the integration of\nincreasing amounts of wind power into power grids. This energy source poses a\nchallenge to system operators due to its variability and uncertainty.\nTherefore, accurate forecasting of wind power is critical for grid operation\nand system balancing. This paper presents an innovative approach to short-term\n(1 to 6 hour horizon) windpower forecasting at a national level. The method\nleverages Automated Deep Learning combined with Numerical Weather Predictions\nwind speed maps to accurately forecast wind power.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.14385v1.pdf",
        "similarity": 0.25143188508741876,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-22"
    },
    {
        "new_title": "A Short Survey on Importance Weighting for Machine Learning",
        "new_link": "http://arxiv.org/abs/2403.10175v2",
        "new_summary": "  Importance weighting is a fundamental procedure in statistics and machine\nlearning that weights the objective function or probability distribution based\non the importance of the instance in some sense. The simplicity and usefulness\nof the idea has led to many applications of importance weighting. For example,\nit is known that supervised learning under an assumption about the difference\nbetween the training and test distributions, called distribution shift, can\nguarantee statistically desirable properties through importance weighting by\ntheir density ratio. This survey summarizes the broad applications of\nimportance weighting in machine learning and related research.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.10175v2.pdf",
        "similarity": 0.25077341665617336,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-15"
    },
    {
        "new_title": "A Machine Learning Approach to Detect Customer Satisfaction From\n  Multiple Tweet Parameters",
        "new_link": "http://arxiv.org/abs/2402.15992v1",
        "new_summary": "  Since internet technologies have advanced, one of the primary factors in\ncompany development is customer happiness. Online platforms have become\nprominent places for sharing reviews. Twitter is one of these platforms where\ncustomers frequently post their thoughts. Reviews of flights on these platforms\nhave become a concern for the airline business. A positive review can help the\ncompany grow, while a negative one can quickly ruin its revenue and reputation.\nSo it's vital for airline businesses to examine the feedback and experiences of\ntheir customers and enhance their services to remain competitive. But studying\nthousands of tweets and analyzing them to find the satisfaction of the customer\nis quite a difficult task. This tedious process can be made easier by using a\nmachine learning approach to analyze tweets to determine client satisfaction\nlevels. Some work has already been done on this strategy to automate the\nprocedure using machine learning and deep learning techniques. However, they\nare all purely concerned with assessing the text's sentiment. In addition to\nthe text, the tweet also includes the time, location, username, airline name,\nand so on. This additional information can be crucial for improving the model's\noutcome. To provide a machine learning based solution, this work has broadened\nits perspective to include these qualities. And it has come as no surprise that\nthe additional features beyond text sentiment analysis produce better outcomes\nin machine learning based models.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.15992v1.pdf",
        "similarity": 0.25072479506918727,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-25"
    },
    {
        "new_title": "MoveLight: Enhancing Traffic Signal Control through Movement-Centric\n  Deep Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2407.17303v1",
        "new_summary": "  This paper introduces MoveLight, a novel traffic signal control system that\nenhances urban traffic management through movement-centric deep reinforcement\nlearning. By leveraging detailed real-time data and advanced machine learning\ntechniques, MoveLight overcomes the limitations of traditional traffic signal\ncontrol methods. It employs a lane-level control approach using the FRAP\nalgorithm to achieve dynamic and adaptive traffic signal control, optimizing\ntraffic flow, reducing congestion, and improving overall efficiency. Our\nresearch demonstrates the scalability and effectiveness of MoveLight across\nsingle intersections, arterial roads, and network levels. Experimental results\nusing real-world datasets from Cologne and Hangzhou show significant\nimprovements in metrics such as queue length, delay, and throughput compared to\nexisting methods. This study highlights the transformative potential of deep\nreinforcement learning in intelligent traffic signal control, setting a new\nstandard for sustainable and efficient urban transportation systems.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.17303v1.pdf",
        "similarity": 0.2506674122889864,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-24"
    },
    {
        "new_title": "Machine Learning and Data Analysis Using Posets: A Survey",
        "new_link": "http://arxiv.org/abs/2404.03082v2",
        "new_summary": "  Posets are discrete mathematical structures which are ubiquitous in a broad\nrange of data analysis and machine learning applications. Research connecting\nposets to the data science domain has been ongoing for many years. In this\npaper, a comprehensive review of a wide range of studies on data analysis and\nmachine learning using posets are examined in terms of their theory, algorithms\nand applications. In addition, the applied lattice theory domain of formal\nconcept analysis will also be highlighted in terms of its machine learning\napplications.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.03082v2.pdf",
        "similarity": 0.25062849247551056,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-03"
    },
    {
        "new_title": "Advances in Machine Learning, Statistical Methods, and AI for\n  Single-Cell RNA Annotation Using Raw Count Matrices in scRNA-seq Data",
        "new_link": "http://arxiv.org/abs/2406.05258v1",
        "new_summary": "  Single-cell RNA sequencing (scRNA-seq) has revolutionized our ability to\nanalyze gene expression at the resolution of individual cells, providing\nunprecedented insights into cellular heterogeneity and complex biological\nsystems. This paper reviews various advanced computational and machine learning\ntechniques tailored for the analysis of scRNA-seq data, emphasizing their roles\nin different stages of the data processing pipeline.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.05258v1.pdf",
        "similarity": 0.2501710644931525,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-07"
    },
    {
        "new_title": "DeepVARMA: A Hybrid Deep Learning and VARMA Model for Chemical Industry\n  Index Forecasting",
        "new_link": "http://arxiv.org/abs/2404.17615v1",
        "new_summary": "  Since the chemical industry index is one of the important indicators to\nmeasure the development of the chemical industry, forecasting it is critical\nfor understanding the economic situation and trends of the industry. Taking the\nmultivariable nonstationary series-synthetic material index as the main\nresearch object, this paper proposes a new prediction model: DeepVARMA, and its\nvariants Deep-VARMA-re and DeepVARMA-en, which combine LSTM and VARMAX models.\nThe new model firstly uses the deep learning model such as the LSTM remove the\ntrends of the target time series and also learn the representation of\nendogenous variables, and then uses the VARMAX model to predict the detrended\ntarget time series with the embeddings of endogenous variables, and finally\ncombines the trend learned by the LSTM and dependency learned by the VARMAX\nmodel to obtain the final predictive values. The experimental results show that\n(1) the new model achieves the best prediction accuracy by combining the LSTM\nencoding of the exogenous variables and the VARMAX model. (2) In multivariate\nnon-stationary series prediction, DeepVARMA uses a phased processing strategy\nto show higher adaptability and accuracy compared to the traditional VARMA\nmodel as well as the machine learning models LSTM, RF and XGBoost. (3) Compared\nwith smooth sequence prediction, the traditional VARMA and VARMAX models\nfluctuate more in predicting non-smooth sequences, while DeepVARMA shows more\nflexibility and robustness. This study provides more accurate tools and methods\nfor future development and scientific decision-making in the chemical industry.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.17615v1.pdf",
        "similarity": 0.2498802521484383,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-26"
    },
    {
        "new_title": "A Review and Comparison of AI Enhanced Side Channel Analysis",
        "new_link": "http://arxiv.org/abs/2402.02299v1",
        "new_summary": "  Side Channel Analysis (SCA) presents a clear threat to privacy and security\nin modern computing systems. The vast majority of communications are secured\nthrough cryptographic algorithms. These algorithms are often provably-secure\nfrom a cryptographical perspective, but their implementation on real hardware\nintroduces vulnerabilities. Adversaries can exploit these vulnerabilities to\nconduct SCA and recover confidential information, such as secret keys or\ninternal states. The threat of SCA has greatly increased as machine learning,\nand in particular deep learning, enhanced attacks become more common. In this\nwork, we will examine the latest state-of-the-art deep learning techniques for\nside channel analysis, the theory behind them, and how they are conducted. Our\nfocus will be on profiling attacks using deep learning techniques, but we will\nalso examine some new and emerging methodologies enhanced by deep learning\ntechniques, such as non-profiled attacks, artificial trace generation, and\nothers. Finally, different deep learning enhanced SCA schemes attempted against\nthe ANSSI SCA Database (ASCAD) and their relative performance will be evaluated\nand compared. This will lead to new research directions to secure cryptographic\nimplementations against the latest SCA attacks.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.02299v1.pdf",
        "similarity": 0.2496514301687538,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-03"
    },
    {
        "new_title": "Comparison of machine learning and statistical approaches for digital\n  elevation model (DEM) correction: interim results",
        "new_link": "http://arxiv.org/abs/2402.06688v1",
        "new_summary": "  Several methods have been proposed for correcting the elevation bias in\ndigital elevation models (DEMs) for example, linear regression. Nowadays,\nsupervised machine learning enables the modelling of complex relationships\nbetween variables, and has been deployed by researchers in a variety of fields.\nIn the existing literature, several studies have adopted either machine\nlearning or statistical approaches in the task of DEM correction. However, to\nour knowledge, none of these studies have compared the performance of both\napproaches, especially with regard to open-access global DEMs. Our previous\nwork has already shown the potential of machine learning approaches,\nspecifically gradient boosted decision trees (GBDTs) for DEM correction. In\nthis study, we share some results from the comparison of three recent\nimplementations of gradient boosted decision trees (XGBoost, LightGBM and\nCatBoost), versus multiple linear regression (MLR) for enhancing the vertical\naccuracy of 30 m Copernicus and AW3D global DEMs in Cape Town, South Africa.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.06688v1.pdf",
        "similarity": 0.24951897388476701,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-08"
    },
    {
        "new_title": "Is In-Context Learning in Large Language Models Bayesian? A Martingale\n  Perspective",
        "new_link": "http://arxiv.org/abs/2406.00793v1",
        "new_summary": "  In-context learning (ICL) has emerged as a particularly remarkable\ncharacteristic of Large Language Models (LLM): given a pretrained LLM and an\nobserved dataset, LLMs can make predictions for new data points from the same\ndistribution without fine-tuning. Numerous works have postulated ICL as\napproximately Bayesian inference, rendering this a natural hypothesis. In this\nwork, we analyse this hypothesis from a new angle through the martingale\nproperty, a fundamental requirement of a Bayesian learning system for\nexchangeable data. We show that the martingale property is a necessary\ncondition for unambiguous predictions in such scenarios, and enables a\nprincipled, decomposed notion of uncertainty vital in trustworthy,\nsafety-critical systems. We derive actionable checks with corresponding theory\nand test statistics which must hold if the martingale property is satisfied. We\nalso examine if uncertainty in LLMs decreases as expected in Bayesian learning\nwhen more data is observed. In three experiments, we provide evidence for\nviolations of the martingale property, and deviations from a Bayesian scaling\nbehaviour of uncertainty, falsifying the hypothesis that ICL is Bayesian.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.00793v1.pdf",
        "similarity": 0.24926333992980812,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-02"
    },
    {
        "new_title": "Metric Learning to Accelerate Convergence of Operator Splitting Methods\n  for Differentiable Parametric Programming",
        "new_link": "http://arxiv.org/abs/2404.00882v1",
        "new_summary": "  Recent work has shown a variety of ways in which machine learning can be used\nto accelerate the solution of constrained optimization problems. Increasing\ndemand for real-time decision-making capabilities in applications such as\nartificial intelligence and optimal control has led to a variety of approaches,\nbased on distinct strategies. This work proposes a novel approach to learning\noptimization, in which the underlying metric space of a proximal operator\nsplitting algorithm is learned so as to maximize its convergence rate. While\nprior works in optimization theory have derived optimal metrics for limited\nclasses of problems, the results do not extend to many practical problem forms\nincluding general Quadratic Programming (QP). This paper shows how\ndifferentiable optimization can enable the end-to-end learning of proximal\nmetrics, enhancing the convergence of proximal algorithms for QP problems\nbeyond what is possible based on known theory. Additionally, the results\nillustrate a strong connection between the learned proximal metrics and active\nconstraints at the optima, leading to an interpretation in which the learning\nof proximal metrics can be viewed as a form of active set learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.00882v1.pdf",
        "similarity": 0.24925644666729785,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-01"
    },
    {
        "new_title": "Quantum Supervised Learning",
        "new_link": "http://arxiv.org/abs/2407.17161v1",
        "new_summary": "  Recent advancements in quantum computing have positioned it as a prospective\nsolution for tackling intricate computational challenges, with supervised\nlearning emerging as a promising domain for its application. Despite this\npotential, the field of quantum machine learning is still in its early stages,\nand there persists a level of skepticism regarding a possible near-term quantum\nadvantage. This paper aims to provide a classical perspective on current\nquantum algorithms for supervised learning, effectively bridging traditional\nmachine learning principles with advancements in quantum machine learning.\nSpecifically, this study charts a research trajectory that diverges from the\npredominant focus of quantum machine learning literature, originating from the\nprerequisites of classical methodologies and elucidating the potential impact\nof quantum approaches. Through this exploration, our objective is to deepen the\nunderstanding of the convergence between classical and quantum methods, thereby\nlaying the groundwork for future advancements in both domains and fostering the\ninvolvement of classical practitioners in the field of quantum machine\nlearning.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.17161v1.pdf",
        "similarity": 0.24922874758341804,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-24"
    },
    {
        "new_title": "Research on the Application of Deep Learning-based BERT Model in\n  Sentiment Analysis",
        "new_link": "http://arxiv.org/abs/2403.08217v1",
        "new_summary": "  This paper explores the application of deep learning techniques, particularly\nfocusing on BERT models, in sentiment analysis. It begins by introducing the\nfundamental concept of sentiment analysis and how deep learning methods are\nutilized in this domain. Subsequently, it delves into the architecture and\ncharacteristics of BERT models. Through detailed explanation, it elucidates the\napplication effects and optimization strategies of BERT models in sentiment\nanalysis, supported by experimental validation. The experimental findings\nindicate that BERT models exhibit robust performance in sentiment analysis\ntasks, with notable enhancements post fine-tuning. Lastly, the paper concludes\nby summarizing the potential applications of BERT models in sentiment analysis\nand suggests directions for future research and practical implementations.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.08217v1.pdf",
        "similarity": 0.249221526692601,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-13"
    },
    {
        "new_title": "Machine Learning for Quantum Computing Specialists",
        "new_link": "http://arxiv.org/abs/2404.18555v1",
        "new_summary": "  Quantum machine learning (QML) is a promising early use case for quantum\ncomputing. There has been progress in the last five years from theoretical\nstudies and numerical simulations to proof of concepts. Use cases demonstrated\non contemporary quantum devices include classifying medical images and items\nfrom the Iris dataset, classifying and generating handwritten images, toxicity\nscreening, and learning a probability distribution. Potential benefits of QML\ninclude faster training and identification of feature maps not found\nclassically. Although, these examples lack the scale for commercial\nexploitation, and it may be several years before QML algorithms replace the\nclassical solutions, QML is an exciting area.\n  This article is written for those who already have a sound knowledge of\nquantum computing and now wish to gain a basic overview of the terminology and\nsome applications of classical machine learning ready to study quantum machine\nlearning. The reader will already understand the relevant relevant linear\nalgebra, including Hilbert spaces, a vector space with an inner product.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.18555v1.pdf",
        "similarity": 0.2485613617405159,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-29"
    },
    {
        "new_title": "Is machine learning good or bad for the natural sciences?",
        "new_link": "http://arxiv.org/abs/2405.18095v2",
        "new_summary": "  Machine learning (ML) methods are having a huge impact across all of the\nsciences. However, ML has a strong ontology - in which only the data exist -\nand a strong epistemology - in which a model is considered good if it performs\nwell on held-out training data. These philosophies are in strong conflict with\nboth standard practices and key philosophies in the natural sciences. Here we\nidentify some locations for ML in the natural sciences at which the ontology\nand epistemology are valuable. For example, when an expressive machine learning\nmodel is used in a causal inference to represent the effects of confounders,\nsuch as foregrounds, backgrounds, or instrument calibration parameters, the\nmodel capacity and loose philosophy of ML can make the results more\ntrustworthy. We also show that there are contexts in which the introduction of\nML introduces strong, unwanted statistical biases. For one, when ML models are\nused to emulate physical (or first-principles) simulations, they amplify\nconfirmation biases. For another, when expressive regressions are used to label\ndatasets, those labels cannot be used in downstream joint or ensemble analyses\nwithout taking on uncontrolled biases. The question in the title is being asked\nof all of the natural sciences; that is, we are calling on the scientific\ncommunities to take a step back and consider the role and value of ML in their\nfields; the (partial) answers we give here come from the particular perspective\nof physics.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.18095v2.pdf",
        "similarity": 0.2475921870820886,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-28"
    },
    {
        "new_title": "Deep Reinforcement Learning Behavioral Mode Switching Using Optimal\n  Control Based on a Latent Space Objective",
        "new_link": "http://arxiv.org/abs/2406.01178v1",
        "new_summary": "  In this work, we use optimal control to change the behavior of a deep\nreinforcement learning policy by optimizing directly in the policy's latent\nspace. We hypothesize that distinct behavioral patterns, termed behavioral\nmodes, can be identified within certain regions of a deep reinforcement\nlearning policy's latent space, meaning that specific actions or strategies are\npreferred within these regions. We identify these behavioral modes using latent\nspace dimension-reduction with \\ac*{pacmap}. Using the actions generated by the\noptimal control procedure, we move the system from one behavioral mode to\nanother. We subsequently utilize these actions as a filter for interpreting the\nneural network policy. The results show that this approach can impose desired\nbehavioral modes in the policy, demonstrated by showing how a failed episode\ncan be made successful and vice versa using the lunar lander reinforcement\nlearning environment.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.01178v1.pdf",
        "similarity": 0.24742723939995953,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-03"
    },
    {
        "new_title": "Utilizing Deep Learning for Enhancing Network Resilience in Finance",
        "new_link": "http://arxiv.org/abs/2402.09820v2",
        "new_summary": "  In the age of the Internet, people's lives are increasingly dependent on\ntoday's network technology. Maintaining network integrity and protecting the\nlegitimate interests of users is at the heart of network construction. Threat\ndetection is an important part of a complete and effective defense system. How\nto effectively detect unknown threats is one of the concerns of network\nprotection. Currently, network threat detection is usually based on rules and\ntraditional machine learning methods, which create artificial rules or extract\ncommon spatiotemporal features, which cannot be applied to large-scale data\napplications, and the emergence of unknown risks causes the detection accuracy\nof the original model to decline. With this in mind, this paper uses deep\nlearning for advanced threat detection to improve protective measures in the\nfinancial industry. Many network researchers have shifted their focus to\nexception-based intrusion detection techniques. The detection technology mainly\nuses statistical machine learning methods - collecting normal program and\nnetwork behavior data, extracting multidimensional features, and training\ndecision machine learning models on this basis (commonly used include naive\nBayes, decision trees, support vector machines, random forests, etc.).\n",
        "pdf_link": "https://arxiv.org/pdf/2402.09820v2.pdf",
        "similarity": 0.24706970471943537,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-15"
    },
    {
        "new_title": "Expert-Driven Monitoring of Operational ML Models",
        "new_link": "http://arxiv.org/abs/2401.11993v1",
        "new_summary": "  We propose Expert Monitoring, an approach that leverages domain expertise to\nenhance the detection and mitigation of concept drift in machine learning (ML)\nmodels. Our approach supports practitioners by consolidating domain expertise\nrelated to concept drift-inducing events, making this expertise accessible to\non-call personnel, and enabling automatic adaptability with expert oversight.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.11993v1.pdf",
        "similarity": 0.24673568543832652,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-22"
    },
    {
        "new_title": "Learning minimal volume uncertainty ellipsoids",
        "new_link": "http://arxiv.org/abs/2405.02441v1",
        "new_summary": "  We consider the problem of learning uncertainty regions for parameter\nestimation problems. The regions are ellipsoids that minimize the average\nvolumes subject to a prescribed coverage probability. As expected, under the\nassumption of jointly Gaussian data, we prove that the optimal ellipsoid is\ncentered around the conditional mean and shaped as the conditional covariance\nmatrix. In more practical cases, we propose a differentiable optimization\napproach for approximately computing the optimal ellipsoids using a neural\nnetwork with proper calibration. Compared to existing methods, our network\nrequires less storage and less computations in inference time, leading to\naccurate yet smaller ellipsoids. We demonstrate these advantages on four\nreal-world localization datasets.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.02441v1.pdf",
        "similarity": 0.24643052592415957,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-03"
    },
    {
        "new_title": "An Over Complete Deep Learning Method for Inverse Problems",
        "new_link": "http://arxiv.org/abs/2402.04653v1",
        "new_summary": "  Obtaining meaningful solutions for inverse problems has been a major\nchallenge with many applications in science and engineering. Recent machine\nlearning techniques based on proximal and diffusion-based methods have shown\npromising results. However, as we show in this work, they can also face\nchallenges when applied to some exemplary problems. We show that similar to\nprevious works on over-complete dictionaries, it is possible to overcome these\nshortcomings by embedding the solution into higher dimensions. The novelty of\nthe work proposed is that we jointly design and learn the embedding and the\nregularizer for the embedding vector. We demonstrate the merit of this approach\non several exemplary and common inverse problems.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.04653v1.pdf",
        "similarity": 0.24614913678444095,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-07"
    },
    {
        "new_title": "Deep Learning-Based Operators for Evolutionary Algorithms",
        "new_link": "http://arxiv.org/abs/2407.10477v1",
        "new_summary": "  We present two novel domain-independent genetic operators that harness the\ncapabilities of deep learning: a crossover operator for genetic algorithms and\na mutation operator for genetic programming. Deep Neural Crossover leverages\nthe capabilities of deep reinforcement learning and an encoder-decoder\narchitecture to select offspring genes. BERT mutation masks multiple gp-tree\nnodes and then tries to replace these masks with nodes that will most likely\nimprove the individual's fitness. We show the efficacy of both operators\nthrough experimentation.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.10477v1.pdf",
        "similarity": 0.2451389422038998,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-15"
    },
    {
        "new_title": "Robust Counterfactual Explanations in Machine Learning: A Survey",
        "new_link": "http://arxiv.org/abs/2402.01928v1",
        "new_summary": "  Counterfactual explanations (CEs) are advocated as being ideally suited to\nproviding algorithmic recourse for subjects affected by the predictions of\nmachine learning models. While CEs can be beneficial to affected individuals,\nrecent work has exposed severe issues related to the robustness of\nstate-of-the-art methods for obtaining CEs. Since a lack of robustness may\ncompromise the validity of CEs, techniques to mitigate this risk are in order.\nIn this survey, we review works in the rapidly growing area of robust CEs and\nperform an in-depth analysis of the forms of robustness they consider. We also\ndiscuss existing solutions and their limitations, providing a solid foundation\nfor future developments.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.01928v1.pdf",
        "similarity": 0.24509241677936264,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-02"
    },
    {
        "new_title": "Chaos-based reinforcement learning with TD3",
        "new_link": "http://arxiv.org/abs/2405.09086v1",
        "new_summary": "  Chaos-based reinforcement learning (CBRL) is a method in which the agent's\ninternal chaotic dynamics drives exploration. This approach offers a model for\nconsidering how the biological brain can create variability in its behavior and\nlearn in an exploratory manner. At the same time, it is a learning model that\nhas the ability to automatically switch between exploration and exploitation\nmodes and the potential to realize higher explorations that reflect what it has\nlearned so far. However, the learning algorithms in CBRL have not been\nwell-established in previous studies and have yet to incorporate recent\nadvances in reinforcement learning. This study introduced Twin Delayed Deep\nDeterministic Policy Gradients (TD3), which is one of the state-of-the-art deep\nreinforcement learning algorithms that can treat deterministic and continuous\naction spaces, to CBRL. The validation results provide several insights. First,\nTD3 works as a learning algorithm for CBRL in a simple goal-reaching task.\nSecond, CBRL agents with TD3 can autonomously suppress their exploratory\nbehavior as learning progresses and resume exploration when the environment\nchanges. Finally, examining the effect of the agent's chaoticity on learning\nshows that extremely strong chaos negatively impacts the flexible switching\nbetween exploration and exploitation.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.09086v1.pdf",
        "similarity": 0.24435275844046006,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-15"
    },
    {
        "new_title": "Comparative Evaluation of Weather Forecasting using Machine Learning\n  Models",
        "new_link": "http://arxiv.org/abs/2402.01206v1",
        "new_summary": "  Gaining a deeper understanding of weather and being able to predict its\nfuture conduct have always been considered important endeavors for the growth\nof our society. This research paper explores the advancements in understanding\nand predicting nature's behavior, particularly in the context of weather\nforecasting, through the application of machine learning algorithms. By\nleveraging the power of machine learning, data mining, and data analysis\ntechniques, significant progress has been made in this field. This study\nfocuses on analyzing the contributions of various machine learning algorithms\nin predicting precipitation and temperature patterns using a 20-year dataset\nfrom a single weather station in Dhaka city. Algorithms such as Gradient\nBoosting, AdaBoosting, Artificial Neural Network, Stacking Random Forest,\nStacking Neural Network, and Stacking KNN are evaluated and compared based on\ntheir performance metrics, including Confusion matrix measurements. The\nfindings highlight remarkable achievements and provide valuable insights into\ntheir performances and features correlation.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.01206v1.pdf",
        "similarity": 0.24369770710458769,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-02"
    },
    {
        "new_title": "Deep Mamba Multi-modal Learning",
        "new_link": "http://arxiv.org/abs/2406.18007v1",
        "new_summary": "  Inspired by the excellent performance of Mamba networks, we propose a novel\nDeep Mamba Multi-modal Learning (DMML). It can be used to achieve the fusion of\nmulti-modal features. We apply DMML to the field of multimedia retrieval and\npropose an innovative Deep Mamba Multi-modal Hashing (DMMH) method. It combines\nthe advantages of algorithm accuracy and inference speed. We validated the\neffectiveness of DMMH on three public datasets and achieved state-of-the-art\nresults.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.18007v1.pdf",
        "similarity": 0.24347583435294168,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-09"
    },
    {
        "new_title": "DeepSafeMPC: Deep Learning-Based Model Predictive Control for Safe\n  Multi-Agent Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2403.06397v2",
        "new_summary": "  Safe Multi-agent reinforcement learning (safe MARL) has increasingly gained\nattention in recent years, emphasizing the need for agents to not only optimize\nthe global return but also adhere to safety requirements through behavioral\nconstraints. Some recent work has integrated control theory with multi-agent\nreinforcement learning to address the challenge of ensuring safety. However,\nthere have been only very limited applications of Model Predictive Control\n(MPC) methods in this domain, primarily due to the complex and implicit\ndynamics characteristic of multi-agent environments. To bridge this gap, we\npropose a novel method called Deep Learning-Based Model Predictive Control for\nSafe Multi-Agent Reinforcement Learning (DeepSafeMPC). The key insight of\nDeepSafeMPC is leveraging a entralized deep learning model to well predict\nenvironmental dynamics. Our method applies MARL principles to search for\noptimal solutions. Through the employment of MPC, the actions of agents can be\nrestricted within safe states concurrently. We demonstrate the effectiveness of\nour approach using the Safe Multi-agent MuJoCo environment, showcasing\nsignificant advancements in addressing safety concerns in MARL.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.06397v2.pdf",
        "similarity": 0.24239720980352952,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-11"
    },
    {
        "new_title": "Multiple Realizability and the Rise of Deep Learning",
        "new_link": "http://arxiv.org/abs/2405.13231v1",
        "new_summary": "  The multiple realizability thesis holds that psychological states may be\nimplemented in a diversity of physical systems. The deep learning revolution\nseems to be bringing this possibility to life, offering the most plausible\nexamples of man-made realizations of sophisticated cognitive functions to date.\nThis paper explores the implications of deep learning models for the multiple\nrealizability thesis. Among other things, it challenges the widely held view\nthat multiple realizability entails that the study of the mind can and must be\npursued independently of the study of its implementation in the brain or in\nartificial analogues. Although its central contribution is philosophical, the\npaper has substantial methodological upshots for contemporary cognitive\nscience, suggesting that deep neural networks may play a crucial role in\nformulating and evaluating hypotheses about cognition, even if they are\ninterpreted as implementation-level models. In the age of deep learning,\nmultiple realizability possesses a renewed significance.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.13231v1.pdf",
        "similarity": 0.24231920298315107,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-21"
    },
    {
        "new_title": "Emotion Classification in Short English Texts using Deep Learning\n  Techniques",
        "new_link": "http://arxiv.org/abs/2402.16034v2",
        "new_summary": "  Detecting emotions in limited text datasets from under-resourced languages\npresents a formidable obstacle, demanding specialized frameworks and\ncomputational strategies. This study conducts a thorough examination of deep\nlearning techniques for discerning emotions in short English texts. Deep\nlearning approaches employ transfer learning and word embedding, notably BERT,\nto attain superior accuracy. To evaluate these methods, we introduce the\n\"SmallEnglishEmotions\" dataset, comprising 6372 varied short English texts\nannotated with five primary emotion categories. Our experiments reveal that\ntransfer learning and BERT-based text embedding outperform alternative methods\nin accurately categorizing the text in the dataset.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.16034v2.pdf",
        "similarity": 0.24196630114977954,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-25"
    },
    {
        "new_title": "Measuring machine learning harms from stereotypes: requires\n  understanding who is being harmed by which errors in what ways",
        "new_link": "http://arxiv.org/abs/2402.04420v1",
        "new_summary": "  As machine learning applications proliferate, we need an understanding of\ntheir potential for harm. However, current fairness metrics are rarely grounded\nin human psychological experiences of harm. Drawing on the social psychology of\nstereotypes, we use a case study of gender stereotypes in image search to\nexamine how people react to machine learning errors. First, we use survey\nstudies to show that not all machine learning errors reflect stereotypes nor\nare equally harmful. Then, in experimental studies we randomly expose\nparticipants to stereotype-reinforcing, -violating, and -neutral machine\nlearning errors. We find stereotype-reinforcing errors induce more\nexperientially (i.e., subjectively) harmful experiences, while having minimal\nchanges to cognitive beliefs, attitudes, or behaviors. This experiential harm\nimpacts women more than men. However, certain stereotype-violating errors are\nmore experientially harmful for men, potentially due to perceived threats to\nmasculinity. We conclude that harm cannot be the sole guide in fairness\nmitigation, and propose a nuanced perspective depending on who is experiencing\nwhat harm and why.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.04420v1.pdf",
        "similarity": 0.24162559215680585,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-06"
    },
    {
        "new_title": "Analysis of Total Variation Minimization for Clustered Federated\n  Learning",
        "new_link": "http://arxiv.org/abs/2403.06298v1",
        "new_summary": "  A key challenge in federated learning applications is the statistical\nheterogeneity of local datasets. Clustered federated learning addresses this\nchallenge by identifying clusters of local datasets that are approximately\nhomogeneous. One recent approach to clustered federated learning is generalized\ntotal variation minimization (GTVMin). This approach requires a similarity\ngraph which can be obtained by domain expertise or in a data-driven fashion via\ngraph learning techniques. Under a widely applicable clustering assumption, we\nderive an upper bound the deviation between GTVMin solutions and their\ncluster-wise averages. This bound provides valuable insights into the\neffectiveness and robustness of GTVMin in addressing statistical heterogeneity\nwithin federated learning environments.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.06298v1.pdf",
        "similarity": 0.2416113575925668,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-10"
    },
    {
        "new_title": "Learning System Dynamics without Forgetting",
        "new_link": "http://arxiv.org/abs/2407.00717v1",
        "new_summary": "  Predicting the trajectories of systems with unknown dynamics (\\textit{i.e.}\nthe governing rules) is crucial in various research fields, including physics\nand biology. This challenge has gathered significant attention from diverse\ncommunities. Most existing works focus on learning fixed system dynamics within\none single system. However, real-world applications often involve multiple\nsystems with different types of dynamics or evolving systems with\nnon-stationary dynamics (dynamics shifts). When data from those systems are\ncontinuously collected and sequentially fed to machine learning models for\ntraining, these models tend to be biased toward the most recently learned\ndynamics, leading to catastrophic forgetting of previously observed/learned\nsystem dynamics. To this end, we aim to learn system dynamics via continual\nlearning. Specifically, we present a novel framework of Mode-switching Graph\nODE (MS-GODE), which can continually learn varying dynamics and encode the\nsystem-specific dynamics into binary masks over the model parameters. During\nthe inference stage, the model can select the most confident mask based on the\nobservational data to identify the system and predict future trajectories\naccordingly. Empirically, we systematically investigate the task configurations\nand compare the proposed MS-GODE with state-of-the-art techniques. More\nimportantly, we construct a novel benchmark of biological dynamic systems,\nfeaturing diverse systems with disparate dynamics and significantly enriching\nthe research field of machine learning for dynamic systems.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.00717v1.pdf",
        "similarity": 0.24116231617565573,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-30"
    },
    {
        "new_title": "Enhancing Kubernetes Automated Scheduling with Deep Learning and\n  Reinforcement Techniques for Large-Scale Cloud Computing Optimization",
        "new_link": "http://arxiv.org/abs/2403.07905v1",
        "new_summary": "  With the continuous expansion of the scale of cloud computing applications,\nartificial intelligence technologies such as Deep Learning and Reinforcement\nLearning have gradually become the key tools to solve the automated task\nscheduling of large-scale cloud computing systems. Aiming at the complexity and\nreal-time requirement of task scheduling in large-scale cloud computing system,\nthis paper proposes an automatic task scheduling scheme based on deep learning\nand reinforcement learning. Firstly, the deep learning technology is used to\nmonitor and predict the parameters in the cloud computing system in real time\nto obtain the system status information. Then, combined with reinforcement\nlearning algorithm, the task scheduling strategy is dynamically adjusted\naccording to the real-time system state and task characteristics to achieve the\noptimal utilization of system resources and the maximum of task execution\nefficiency. This paper verifies the effectiveness and performance advantages of\nthe proposed scheme in experiments, and proves the potential and application\nprospect of deep learning and reinforcement learning in automatic task\nscheduling in large-scale cloud computing systems.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.07905v1.pdf",
        "similarity": 0.24039999310434076,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-26"
    },
    {
        "new_title": "Efficient machine learning for motion sensing for lighting applications",
        "new_link": "http://arxiv.org/abs/2406.16723v1",
        "new_summary": "  The use of machine learning for building a classifier in signal processing\nfor motion sensing presents unique challenges. This paper proposes a novel\nmethod that effectively addresses the combination of skewed data sets and\noptimization requirements. By utilizing a customized loss function and a\nproduct of probability models, our approach achieves a fully automated and\nefficient machine learning process. Additionally, our resulting probability\nmodels offer reduced complexity, making them ideal for embedded applications.\nOur method offers a promising solution for motion sensing applications that\nrequire accurate and efficient classification.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.16723v1.pdf",
        "similarity": 0.24012544819659584,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-21"
    },
    {
        "new_title": "The Definitive Guide to Policy Gradients in Deep Reinforcement Learning:\n  Theory, Algorithms and Implementations",
        "new_link": "http://arxiv.org/abs/2401.13662v2",
        "new_summary": "  In recent years, various powerful policy gradient algorithms have been\nproposed in deep reinforcement learning. While all these algorithms build on\nthe Policy Gradient Theorem, the specific design choices differ significantly\nacross algorithms. We provide a holistic overview of on-policy policy gradient\nalgorithms to facilitate the understanding of both their theoretical\nfoundations and their practical implementations. In this overview, we include a\ndetailed proof of the continuous version of the Policy Gradient Theorem,\nconvergence results and a comprehensive discussion of practical algorithms. We\ncompare the most prominent algorithms on continuous control environments and\nprovide insights on the benefits of regularization. All code is available at\nhttps://github.com/Matt00n/PolicyGradientsJax.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.13662v2.pdf",
        "similarity": 0.23997684610852235,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-24"
    },
    {
        "new_title": "Fast and Scalable Network Slicing by Integrating Deep Learning with\n  Lagrangian Methods",
        "new_link": "http://arxiv.org/abs/2401.11731v1",
        "new_summary": "  Network slicing is a key technique in 5G and beyond for efficiently\nsupporting diverse services. Many network slicing solutions rely on deep\nlearning to manage complex and high-dimensional resource allocation problems.\nHowever, deep learning models suffer limited generalization and adaptability to\ndynamic slicing configurations. In this paper, we propose a novel framework\nthat integrates constrained optimization methods and deep learning models,\nresulting in strong generalization and superior approximation capability. Based\non the proposed framework, we design a new neural-assisted algorithm to\nallocate radio resources to slices to maximize the network utility under\ninter-slice resource constraints. The algorithm exhibits high scalability,\naccommodating varying numbers of slices and slice configurations with ease. We\nimplement the proposed solution in a system-level network simulator and\nevaluate its performance extensively by comparing it to state-of-the-art\nsolutions including deep reinforcement learning approaches. The numerical\nresults show that our solution obtains near-optimal quality-of-service\nsatisfaction and promising generalization performance under different network\nslicing scenarios.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.11731v1.pdf",
        "similarity": 0.23976436013984473,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-22"
    },
    {
        "new_title": "Single-shot quantum machine learning",
        "new_link": "http://arxiv.org/abs/2406.13812v1",
        "new_summary": "  Quantum machine learning aims to improve learning methods through the use of\nquantum computers. If it is to ever realize its potential, many obstacles need\nto be overcome. A particularly pressing one arises at the prediction stage\nbecause the outputs of quantum learning models are inherently random. This\ncreates an often considerable overhead, as many executions of a quantum\nlearning model have to be aggregated to obtain an actual prediction. In this\nwork, we analyze when quantum learning models can evade this issue and produce\npredictions in a near-deterministic way -- paving the way to single-shot\nquantum machine learning. We give a rigorous definition of single-shotness in\nquantum classifiers and show that the degree to which a quantum learning model\nis near-deterministic is constrained by the distinguishability of the embedded\nquantum states used in the model. Opening the black box of the embedding, we\nshow that if the embedding is realized by quantum circuits, a certain depth is\nnecessary for single-shotness to be even possible. We conclude by showing that\nquantum learning models cannot be single-shot in a generic way and trainable at\nthe same time.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.13812v1.pdf",
        "similarity": 0.23947432945020697,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-19"
    },
    {
        "new_title": "A Fast Learning-Based Surrogate of Electrical Machines using a Reduced\n  Basis",
        "new_link": "http://arxiv.org/abs/2406.18990v1",
        "new_summary": "  A surrogate model approximates the outputs of a solver of Partial\nDifferential Equations (PDEs) with a low computational cost. In this article,\nwe propose a method to build learning-based surrogates in the context of\nparameterized PDEs, which are PDEs that depend on a set of parameters but are\nalso temporal and spatial processes. Our contribution is a method hybridizing\nthe Proper Orthogonal Decomposition and several Support Vector Regression\nmachines. This method is conceived to work in real-time, thus aimed for being\nused in the context of digital twins, where a user can perform an interactive\nanalysis of results based on the proposed surrogate. We present promising\nresults on two use cases concerning electrical machines. These use cases are\nnot toy examples but are produced an industrial computational code, they use\nmeshes representing non-trivial geometries and contain non-linearities.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.18990v1.pdf",
        "similarity": 0.23943726833999793,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-27"
    },
    {
        "new_title": "A Deep Reinforcement Learning Approach to Battery Management in Dairy\n  Farming via Proximal Policy Optimization",
        "new_link": "http://arxiv.org/abs/2407.01653v1",
        "new_summary": "  Dairy farms consume a significant amount of electricity for their operations,\nand this research focuses on enhancing energy efficiency and minimizing the\nimpact on the environment in the sector by maximizing the utilization of\nrenewable energy sources. This research investigates the application of\nProximal Policy Optimization (PPO), a deep reinforcement learning algorithm\n(DRL), to enhance dairy farming battery management. We evaluate the algorithm's\neffectiveness based on its ability to reduce reliance on the electricity grid,\nhighlighting the potential of DRL to enhance energy management in dairy\nfarming. Using real-world data our results demonstrate how the PPO approach\noutperforms Q-learning by 1.62% for reducing electricity import from the grid.\nThis significant improvement highlights the potential of the Deep Reinforcement\nLearning algorithm for improving energy efficiency and sustainability in dairy\nfarms.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.01653v1.pdf",
        "similarity": 0.23942782435678905,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-01"
    },
    {
        "new_title": "Database-assisted automata learning",
        "new_link": "http://arxiv.org/abs/2406.07208v1",
        "new_summary": "  This paper presents DAALder (Database-Assisted Automata Learning, with Dutch\nsuffix from leerder), a new algorithm for learning state machines, or automata,\nspecifically deterministic finite-state automata (DFA). When learning state\nmachines from log data originating from software systems, the large amount of\nlog data can pose a challenge. Conventional state merging algorithms cannot\nefficiently deal with this, as they require a large amount of memory. To solve\nthis, we utilized database technologies to efficiently query a big trace\ndataset and construct a state machine from it, as databases allow to save large\namounts of data on disk while still being able to query it efficiently.\nBuilding on research in both active learning and passive learning, the proposed\nalgorithm is a combination of the two. It can quickly find a characteristic set\nof traces from a database using heuristics from a state merging algorithm.\nExperiments show that our algorithm has similar performance to conventional\nstate merging algorithms on large datasets, but requires far less memory.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.07208v1.pdf",
        "similarity": 0.2393625412481458,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-11"
    },
    {
        "new_title": "Window to Wall Ratio Detection using SegFormer",
        "new_link": "http://arxiv.org/abs/2406.02706v1",
        "new_summary": "  Window to Wall Ratios (WWR) are key to assessing the energy, daylight and\nventilation performance of buildings. Studies have shown that window area has a\nlarge impact on building performance and simulation. However, data to set up\nthese environmental models and simulations is typically not available. Instead,\na standard 40% WWR is typically assumed for all buildings. This paper leverages\nexisting computer vision window detection methods to predict WWR of buildings\nfrom external street view images using semantic segmentation, demonstrating the\npotential for adapting established computer vision technique in architectural\napplications\n",
        "pdf_link": "https://arxiv.org/pdf/2406.02706v1.pdf",
        "similarity": 0.23924657075130634,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-04"
    },
    {
        "new_title": "Data-Error Scaling in Machine Learning on Natural Discrete Combinatorial\n  Mutation-prone Sets: Case Studies on Peptides and Small Molecules",
        "new_link": "http://arxiv.org/abs/2405.05167v1",
        "new_summary": "  We investigate trends in the data-error scaling behavior of machine learning\n(ML) models trained on discrete combinatorial spaces that are\nprone-to-mutation, such as proteins or organic small molecules. We trained and\nevaluated kernel ridge regression machines using variable amounts of\ncomputationally generated training data. Our synthetic datasets comprise i) two\nna\\\"ive functions based on many-body theory; ii) binding energy estimates\nbetween a protein and a mutagenised peptide; and iii) solvation energies of two\n6-heavy atom structural graphs. In contrast to typical data-error scaling, our\nresults showed discontinuous monotonic phase transitions during learning,\nobserved as rapid drops in the test error at particular thresholds of training\ndata. We observed two learning regimes, which we call saturated and asymptotic\ndecay, and found that they are conditioned by the level of complexity (i.e.\nnumber of mutations) enclosed in the training set. We show that during training\non this class of problems, the predictions were clustered by the ML models\nemployed in the calibration plots. Furthermore, we present an alternative\nstrategy to normalize learning curves (LCs) and the concept of mutant based\nshuffling. This work has implications for machine learning on mutagenisable\ndiscrete spaces such as chemical properties or protein phenotype prediction,\nand improves basic understanding of concepts in statistical learning theory.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.05167v1.pdf",
        "similarity": 0.23923718215529338,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-08"
    },
    {
        "new_title": "Gradient directions and relative inexactness in optimization and machine\n  learning",
        "new_link": "http://arxiv.org/abs/2407.00667v1",
        "new_summary": "  In this paper, we investigate the influence of noise giving an estimate of\nthe gradient having a acute angle with the original. Noise amplitude has a\nrelative model. The work offers both theoretical calculations and theorems, as\nwell as experimental results. Classic machine learning problems were chosen as\nexperiments -- linear and logistic regression, computer vision and natural\nlanguage processing.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.00667v1.pdf",
        "similarity": 0.2392095976826652,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-30"
    },
    {
        "new_title": "Large Deviations of Gaussian Neural Networks with ReLU activation",
        "new_link": "http://arxiv.org/abs/2405.16958v1",
        "new_summary": "  We prove a large deviation principle for deep neural networks with Gaussian\nweights and (at most linearly growing) activation functions. This generalises\nearlier work, in which bounded and continuous activation functions were\nconsidered. In practice, linearly growing activation functions such as ReLU are\nmost commonly used. We furthermore simplify previous expressions for the rate\nfunction and a give power-series expansions for the ReLU case.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.16958v1.pdf",
        "similarity": 0.23915705120235195,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-27"
    },
    {
        "new_title": "Exotic and physics-informed support vector machines for high energy\n  physics",
        "new_link": "http://arxiv.org/abs/2407.03538v1",
        "new_summary": "  In this article, we explore machine learning techniques using support vector\nmachines with two novel approaches: exotic and physics-informed support vector\nmachines. Exotic support vector machines employ unconventional techniques such\nas genetic algorithms and boosting. Physics-informed support vector machines\nintegrate the physics dynamics of a given high-energy physics process in a\nstraightforward manner. The goal is to efficiently distinguish signal and\nbackground events in high-energy physics collision data. To test our\nalgorithms, we perform computational experiments with simulated Drell-Yan\nevents in proton-proton collisions. Our results highlight the superiority of\nthe physics-informed support vector machines, emphasizing their potential in\nhigh-energy physics and promoting the inclusion of physics information in\nmachine learning algorithms for future research.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.03538v1.pdf",
        "similarity": 0.2389183644085109,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-03"
    },
    {
        "new_title": "Leveraging KANs For Enhanced Deep Koopman Operator Discovery",
        "new_link": "http://arxiv.org/abs/2406.02875v2",
        "new_summary": "  Multi-layer perceptrons (MLP's) have been extensively utilized in discovering\nDeep Koopman operators for linearizing nonlinear dynamics. With the emergence\nof Kolmogorov-Arnold Networks (KANs) as a more efficient and accurate\nalternative to the MLP Neural Network, we propose a comparison of the\nperformance of each network type in the context of learning Koopman operators\nwith control. In this work, we propose a KANs-based deep Koopman framework with\napplications to an orbital Two-Body Problem (2BP) and the pendulum for\ndata-driven discovery of linear system dynamics. KANs were found to be superior\nin nearly all aspects of training; learning 31 times faster, being 15 times\nmore parameter efficiency, and predicting 1.25 times more accurately as\ncompared to the MLP Deep Neural Networks (DNNs) in the case of the 2BP. Thus,\nKANs shows potential for being an efficient tool in the development of Deep\nKoopman Theory.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.02875v2.pdf",
        "similarity": 0.2385888750646701,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-05"
    },
    {
        "new_title": "Quantum Visual Feature Encoding Revisited",
        "new_link": "http://arxiv.org/abs/2405.19725v1",
        "new_summary": "  Although quantum machine learning has been introduced for a while, its\napplications in computer vision are still limited. This paper, therefore,\nrevisits the quantum visual encoding strategies, the initial step in quantum\nmachine learning. Investigating the root cause, we uncover that the existing\nquantum encoding design fails to ensure information preservation of the visual\nfeatures after the encoding process, thus complicating the learning process of\nthe quantum machine learning models. In particular, the problem, termed\n\"Quantum Information Gap\" (QIG), leads to a gap of information between\nclassical and corresponding quantum features. We provide theoretical proof and\npractical demonstrations of that found and underscore the significance of QIG,\nas it directly impacts the performance of quantum machine learning algorithms.\nTo tackle this challenge, we introduce a simple but efficient new loss function\nnamed Quantum Information Preserving (QIP) to minimize this gap, resulting in\nenhanced performance of quantum machine learning algorithms. Extensive\nexperiments validate the effectiveness of our approach, showcasing superior\nperformance compared to current methodologies and consistently achieving\nstate-of-the-art results in quantum modeling.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.19725v1.pdf",
        "similarity": 0.23849583294164095,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-30"
    },
    {
        "new_title": "Single-Task Continual Offline Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2404.12639v2",
        "new_summary": "  In this paper, we study the continual learning problem of single-task offline\nreinforcement learning. In the past, continual reinforcement learning usually\nonly dealt with multitasking, that is, learning multiple related or unrelated\ntasks in a row, but once each learned task was learned, it was not relearned,\nbut only used in subsequent processes. However, offline reinforcement learning\ntasks require the continuously learning of multiple different datasets for the\nsame task. Existing algorithms will try their best to achieve the best results\nin each offline dataset they have learned and the skills of the network will\noverwrite the high-quality datasets that have been learned after learning the\nsubsequent poor datasets. On the other hand, if too much emphasis is placed on\nstability, the network will learn the subsequent better dataset after learning\nthe poor offline dataset, and the problem of insufficient plasticity and\nnon-learning will occur. How to design a strategy that can always preserve the\nbest performance for each state in the data that has been learned is a new\nchallenge and the focus of this study. Therefore, this study proposes a new\nalgorithm, called Ensemble Offline Reinforcement Learning Based on Experience\nReplay, which introduces multiple value networks to learn the same dataset and\njudge whether the strategy has been learned by the discrete degree of the value\nnetwork, to improve the performance of the network in single-task offline\nreinforcement learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.12639v2.pdf",
        "similarity": 0.23827202261785452,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-19"
    },
    {
        "new_title": "Harnessing Quantum Support Vector Machines for Cross-Domain\n  Classification of Quantum States",
        "new_link": "http://arxiv.org/abs/2407.00774v2",
        "new_summary": "  In the present study, we use cross-domain classification using quantum\nmachine learning for quantum advantages to readdress the entanglement versus\nseparability paradigm. The inherent structure of quantum states and its\nrelation to a particular class of quantum states are used to intuitively\nclassify testing states from domains different from training states, called\n\\textit{cross-domain classification}. Using our quantum machine learning\nalgorithm, we demonstrate efficient classifications of two-qubit mixed states\ninto entangled and separable classes. For analyzing the quantumness of\ncorrelations, our model adequately classifies Bell diagonal states as zero and\nnon-zero discord states. In addition, we also extend our analysis to evaluate\nthe robustness of our model using random local unitary transformations. Our\nresults demonstrate the potential of the quantum support vector machine for\nclassifying quantum states across the multi-dimensional Hilbert space in\ncomparison to classical support vector machines and neural networks.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.00774v2.pdf",
        "similarity": 0.23783576761918637,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-30"
    },
    {
        "new_title": "Compensating for charge sharing by a deep-learning method: a preliminary\n  experimental study",
        "new_link": "http://arxiv.org/abs/2403.17375v1",
        "new_summary": "  Photon counting detectors (PCDs) bring valuable advantages to diagnostic\ncomputed tomography (CT), including lower noise and higher resolution than\nenergy integrating detectors. However, there are still several nonideal factors\npreventing PCDs from meeting people's expectations, for example, charge sharing\nand pile up. In this paper, we did some preliminary work on charge sharing and\nconducted an experimental study using an XCounter PCD to compare the effects of\nno anti-coincidence, anti-coincidence by hardware and charge sharing\ncompensation by a deep learning method. In our results, a smaller bias and\nstandard deviation are obtained from deep learning method than directly from\nno-anti-coincidence mode of the detector. Our network also outperforms the\nanti-coincidence mode of the detector in the low energy bin and has smaller\nstandard deviation in the high energy bin. The results validate that a deep\nlearning method is suitable to compensate for charge sharing.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.17375v1.pdf",
        "similarity": 0.23734094669864106,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-26"
    },
    {
        "new_title": "What can machine learning help with microstructure-informed materials\n  modeling and design?",
        "new_link": "http://arxiv.org/abs/2405.18396v1",
        "new_summary": "  Machine learning techniques have been widely employed as effective tools in\naddressing various engineering challenges in recent years, particularly for the\nchallenging task of microstructure-informed materials modeling. This work\nprovides a comprehensive review of the current machine learning-assisted and\ndata-driven advancements in this field, including microstructure\ncharacterization and reconstruction, multiscale simulation, correlations among\nprocess, microstructure, and properties, as well as microstructure optimization\nand inverse design. It outlines the achievements of existing research through\nbest practices and suggests potential avenues for future investigations.\nMoreover, it prepares the readers with educative instructions of basic\nknowledge and an overview on machine learning, microstructure descriptors and\nmachine learning-assisted material modeling, lowering the interdisciplinary\nhurdles. It should help to stimulate and attract more research attention to the\nrapidly growing field of machine learning-based modeling and design of\nmicrostructured materials.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.18396v1.pdf",
        "similarity": 0.23724067329816276,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-28"
    },
    {
        "new_title": "BARMPy: Bayesian Additive Regression Models Python Package",
        "new_link": "http://arxiv.org/abs/2404.04738v1",
        "new_summary": "  We make Bayesian Additive Regression Networks (BARN) available as a Python\npackage, \\texttt{barmpy}, with documentation at\n\\url{https://dvbuntu.github.io/barmpy/} for general machine learning\npractitioners. Our object-oriented design is compatible with SciKit-Learn,\nallowing usage of their tools like cross-validation. To ease learning to use\n\\texttt{barmpy}, we produce a companion tutorial that expands on reference\ninformation in the documentation. Any interested user can \\texttt{pip install\nbarmpy} from the official PyPi repository. \\texttt{barmpy} also serves as a\nbaseline Python library for generic Bayesian Additive Regression Models.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.04738v1.pdf",
        "similarity": 0.23720090184334594,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-06"
    },
    {
        "new_title": "Deep Learning Approach for Enhanced Transferability and Learning\n  Capacity in Tool Wear Estimation",
        "new_link": "http://arxiv.org/abs/2407.01200v1",
        "new_summary": "  As an integral part of contemporary manufacturing, monitoring systems obtain\nvaluable information during machining to oversee the condition of both the\nprocess and the machine. Recently, diverse algorithms have been employed to\ndetect tool wear using single or multiple sources of measurements. In this\nstudy, a deep learning approach is proposed for estimating tool wear,\nconsidering cutting parameters. The model's accuracy and transferability in\ntool wear estimation were assessed with milling experiments conducted under\nvarying cutting parameters. The results indicate that the proposed method\noutperforms conventional methods in terms of both transferability and rapid\nlearning capabilities.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.01200v1.pdf",
        "similarity": 0.23705392331122152,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-01"
    },
    {
        "new_title": "Streaming IoT Data and the Quantum Edge: A Classic/Quantum Machine\n  Learning Use Case",
        "new_link": "http://arxiv.org/abs/2402.15542v1",
        "new_summary": "  With the advent of the Post-Moore era, the scientific community is faced with\nthe challenge of addressing the demands of current data-intensive machine\nlearning applications, which are the cornerstone of urgent analytics in\ndistributed computing. Quantum machine learning could be a solution for the\nincreasing demand of urgent analytics, providing potential theoretical speedups\nand increased space efficiency. However, challenges such as (1) the encoding of\ndata from the classical to the quantum domain, (2) hyperparameter tuning, and\n(3) the integration of quantum hardware into a distributed computing continuum\nlimit the adoption of quantum machine learning for urgent analytics. In this\nwork, we investigate the use of Edge computing for the integration of quantum\nmachine learning into a distributed computing continuum, identifying the main\nchallenges and possible solutions. Furthermore, exploring the data encoding and\nhyperparameter tuning challenges, we present preliminary results for quantum\nmachine learning analytics on an IoT scenario.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.15542v1.pdf",
        "similarity": 0.23702694088916515,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-23"
    },
    {
        "new_title": "Deep Modeling of Non-Gaussian Aleatoric Uncertainty",
        "new_link": "http://arxiv.org/abs/2405.20513v1",
        "new_summary": "  Deep learning offers promising new ways to accurately model aleatoric\nuncertainty in robotic estimation systems, particularly when the uncertainty\ndistributions do not conform to traditional assumptions of being fixed and\nGaussian. In this study, we formulate and evaluate three fundamental deep\nlearning approaches for conditional probability density modeling to quantify\nnon-Gaussian aleatoric uncertainty: parametric, discretized, and generative\nmodeling. We systematically compare the respective strengths and weaknesses of\nthese three methods on simulated non-Gaussian densities as well as on\nreal-world terrain-relative navigation data. Our results show that these deep\nlearning methods can accurately capture complex uncertainty patterns,\nhighlighting their potential for improving the reliability and robustness of\nestimation systems.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.20513v1.pdf",
        "similarity": 0.2369802655075663,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-30"
    },
    {
        "new_title": "Accurate adaptive deep learning method for solving elliptic problems",
        "new_link": "http://arxiv.org/abs/2404.18838v1",
        "new_summary": "  Deep learning method is of great importance in solving partial differential\nequations. In this paper, inspired by the failure-informed idea proposed by Gao\net.al. (SIAM Journal on Scientific Computing 45(4)(2023)) and as an\nimprovement, a new accurate adaptive deep learning method is proposed for\nsolving elliptic problems, including the interface problems and the\nconvection-dominated problems. Based on the failure probability framework, the\npiece-wise uniform distribution is used to approximate the optimal proposal\ndistribution and an kernel-based method is proposed for efficient sampling.\nTogether with the improved Levenberg-Marquardt optimization method, the\nproposed adaptive deep learning method shows great potential in improving\nsolution accuracy. Numerical tests on the elliptic problems without interface\nconditions, on the elliptic interface problem, and on the convection-dominated\nproblems demonstrate the effectiveness of the proposed method, as it reduces\nthe relative errors by a factor varying from $10^2$ to $10^4$ for different\ncases.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.18838v1.pdf",
        "similarity": 0.23686437056908988,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-29"
    },
    {
        "new_title": "Replication Study: Enhancing Hydrological Modeling with Physics-Guided\n  Machine Learning",
        "new_link": "http://arxiv.org/abs/2402.13911v1",
        "new_summary": "  Current hydrological modeling methods combine data-driven Machine Learning\n(ML) algorithms and traditional physics-based models to address their\nrespective limitations incorrect parameter estimates from rigid physics-based\nmodels and the neglect of physical process constraints by ML algorithms.\nDespite the accuracy of ML in outcome prediction, the integration of scientific\nknowledge is crucial for reliable predictions. This study introduces a Physics\nInformed Machine Learning (PIML) model, which merges the process understanding\nof conceptual hydrological models with the predictive efficiency of ML\nalgorithms. Applied to the Anandapur sub-catchment, the PIML model demonstrates\nsuperior performance in forecasting monthly streamflow and actual\nevapotranspiration over both standalone conceptual models and ML algorithms,\nensuring physical consistency of the outputs. This study replicates the\nmethodologies of Bhasme, P., Vagadiya, J., & Bhatia, U. (2022) from their\npivotal work on Physics Informed Machine Learning for hydrological processes,\nutilizing their shared code and datasets to further explore the predictive\ncapabilities in hydrological modeling.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.13911v1.pdf",
        "similarity": 0.2363405460778369,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-21"
    },
    {
        "new_title": "Exciting Action: Investigating Efficient Exploration for Learning\n  Musculoskeletal Humanoid Locomotion",
        "new_link": "http://arxiv.org/abs/2407.11658v1",
        "new_summary": "  Learning a locomotion controller for a musculoskeletal system is challenging\ndue to over-actuation and high-dimensional action space. While many\nreinforcement learning methods attempt to address this issue, they often\nstruggle to learn human-like gaits because of the complexity involved in\nengineering an effective reward function. In this paper, we demonstrate that\nadversarial imitation learning can address this issue by analyzing key problems\nand providing solutions using both current literature and novel techniques. We\nvalidate our methodology by learning walking and running gaits on a simulated\nhumanoid model with 16 degrees of freedom and 92 Muscle-Tendon Units, achieving\nnatural-looking gaits with only a few demonstrations.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.11658v1.pdf",
        "similarity": 0.23568446524631476,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-16"
    },
    {
        "new_title": "Towards Deep Active Learning in Avian Bioacoustics",
        "new_link": "http://arxiv.org/abs/2406.18621v1",
        "new_summary": "  Passive acoustic monitoring (PAM) in avian bioacoustics enables\ncost-effective and extensive data collection with minimal disruption to natural\nhabitats. Despite advancements in computational avian bioacoustics, deep\nlearning models continue to encounter challenges in adapting to diverse\nenvironments in practical PAM scenarios. This is primarily due to the scarcity\nof annotations, which requires labor-intensive efforts from human experts.\nActive learning (AL) reduces annotation cost and speed ups adaption to diverse\nscenarios by querying the most informative instances for labeling. This paper\noutlines a deep AL approach, introduces key challenges, and conducts a\nsmall-scale pilot study.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.18621v1.pdf",
        "similarity": 0.23560094803450582,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-26"
    },
    {
        "new_title": "Beyond Expectations: Learning with Stochastic Dominance Made Practical",
        "new_link": "http://arxiv.org/abs/2402.02698v1",
        "new_summary": "  Stochastic dominance models risk-averse preferences for decision making with\nuncertain outcomes, which naturally captures the intrinsic structure of the\nunderlying uncertainty, in contrast to simply resorting to the expectations.\nDespite theoretically appealing, the application of stochastic dominance in\nmachine learning has been scarce, due to the following challenges:\n$\\textbf{i)}$, the original concept of stochastic dominance only provides a\n$\\textit{partial order}$, therefore, is not amenable to serve as an optimality\ncriterion; and $\\textbf{ii)}$, an efficient computational recipe remains\nlacking due to the continuum nature of evaluating stochastic dominance.%, which\nbarriers its application for machine learning.\n  In this work, we make the first attempt towards establishing a general\nframework of learning with stochastic dominance. We first generalize the\nstochastic dominance concept to enable feasible comparisons between any\narbitrary pair of random variables. We next develop a simple and\ncomputationally efficient approach for finding the optimal solution in terms of\nstochastic dominance, which can be seamlessly plugged into many learning tasks.\nNumerical experiments demonstrate that the proposed method achieves comparable\nperformance as standard risk-neutral strategies and obtains better trade-offs\nagainst risk across a variety of applications including supervised learning,\nreinforcement learning, and portfolio optimization.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.02698v1.pdf",
        "similarity": 0.23527389993996173,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-05"
    },
    {
        "new_title": "Analytical results for uncertainty propagation through trained machine\n  learning regression models",
        "new_link": "http://arxiv.org/abs/2404.11224v2",
        "new_summary": "  Machine learning (ML) models are increasingly being used in metrology\napplications. However, for ML models to be credible in a metrology context they\nshould be accompanied by principled uncertainty quantification. This paper\naddresses the challenge of uncertainty propagation through trained/fixed\nmachine learning (ML) regression models. Analytical expressions for the mean\nand variance of the model output are obtained/presented for certain input data\ndistributions and for a variety of ML models. Our results cover several popular\nML models including linear regression, penalised linear regression, kernel\nridge regression, Gaussian Processes (GPs), support vector machines (SVMs) and\nrelevance vector machines (RVMs). We present numerical experiments in which we\nvalidate our methods and compare them with a Monte Carlo approach from a\ncomputational efficiency point of view. We also illustrate our methods in the\ncontext of a metrology application, namely modelling the state-of-health of\nlithium-ion cells based upon Electrical Impedance Spectroscopy (EIS) data\n",
        "pdf_link": "https://arxiv.org/pdf/2404.11224v2.pdf",
        "similarity": 0.2350440923605584,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-17"
    },
    {
        "new_title": "Deep learning for quadratic hedging in incomplete jump market",
        "new_link": "http://arxiv.org/abs/2407.13688v1",
        "new_summary": "  We propose a deep learning approach to study the minimal variance pricing and\nhedging problem in an incomplete jump diffusion market. It is based upon a\nrigorous stochastic calculus derivation of the optimal hedging portfolio,\noptimal option price, and the corresponding equivalent martingale measure\nthrough the means of the Stackelberg game approach. A deep learning algorithm\nbased on the combination of the feedforward and LSTM neural networks is tested\non three different market models, two of which are incomplete. In contrast, the\ncomplete market Black-Scholes model serves as a benchmark for the algorithm's\nperformance. The results that indicate the algorithm's good performance are\npresented and discussed.\n  In particular, we apply our results to the special incomplete market model\nstudied by Merton and give a detailed comparison between our results based on\nthe minimal variance principle and the results obtained by Merton based on a\ndifferent pricing principle. Using deep learning, we find that the minimal\nvariance principle leads to typically higher option prices than those deduced\nfrom the Merton principle. On the other hand, the minimal variance principle\nleads to lower losses than the Merton principle.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.13688v1.pdf",
        "similarity": 0.23503858731503918,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-12"
    },
    {
        "new_title": "AI-Driven Anonymization: Protecting Personal Data Privacy While\n  Leveraging Machine Learning",
        "new_link": "http://arxiv.org/abs/2402.17191v1",
        "new_summary": "  The development of artificial intelligence has significantly transformed\npeople's lives. However, it has also posed a significant threat to privacy and\nsecurity, with numerous instances of personal information being exposed online\nand reports of criminal attacks and theft. Consequently, the need to achieve\nintelligent protection of personal information through machine learning\nalgorithms has become a paramount concern. Artificial intelligence leverages\nadvanced algorithms and technologies to effectively encrypt and anonymize\npersonal data, enabling valuable data analysis and utilization while\nsafeguarding privacy. This paper focuses on personal data privacy protection\nand the promotion of anonymity as its core research objectives. It achieves\npersonal data privacy protection and detection through the use of machine\nlearning's differential privacy protection algorithm. The paper also addresses\nexisting challenges in machine learning related to privacy and personal data\nprotection, offers improvement suggestions, and analyzes factors impacting\ndatasets to enable timely personal data privacy detection and protection.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.17191v1.pdf",
        "similarity": 0.234583543975683,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-27"
    },
    {
        "new_title": "Preparing for Black Swans: The Antifragility Imperative for Machine\n  Learning",
        "new_link": "http://arxiv.org/abs/2405.11397v1",
        "new_summary": "  Operating safely and reliably despite continual distribution shifts is vital\nfor high-stakes machine learning applications. This paper builds upon the\ntransformative concept of ``antifragility'' introduced by (Taleb, 2014) as a\nconstructive design paradigm to not just withstand but benefit from volatility.\nWe formally define antifragility in the context of online decision making as\ndynamic regret's strictly concave response to environmental variability,\nrevealing limitations of current approaches focused on resisting rather than\nbenefiting from nonstationarity. Our contribution lies in proposing potential\ncomputational pathways for engineering antifragility, grounding the concept in\nonline learning theory and drawing connections to recent advancements in areas\nsuch as meta-learning, safe exploration, continual learning,\nmulti-objective/quality-diversity optimization, and foundation models. By\nidentifying promising mechanisms and future research directions, we aim to put\nantifragility on a rigorous theoretical foundation in machine learning. We\nfurther emphasize the need for clear guidelines, risk assessment frameworks,\nand interdisciplinary collaboration to ensure responsible application.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.11397v1.pdf",
        "similarity": 0.2344663037172213,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-18"
    },
    {
        "new_title": "Handling Device Heterogeneity for Deep Learning-based Localization",
        "new_link": "http://arxiv.org/abs/2407.16923v1",
        "new_summary": "  Deep learning-based fingerprinting is one of the current promising\ntechnologies for outdoor localization in cellular networks. However, deploying\nsuch localization systems for heterogeneous phones affects their accuracy as\nthe cellular received signal strength (RSS) readings vary for different types\nof phones. In this paper, we introduce a number of techniques for addressing\nthe phones heterogeneity problem in the deep-learning based localization\nsystems. The basic idea is either to approximate a function that maps the\ncellular RSS measurements between different devices or to transfer the\nknowledge across them.\n  Evaluation of the proposed techniques using different Android phones on four\nindependent testbeds shows that our techniques can improve the localization\naccuracy by more than 220% for the four testbeds as compared to the\nstate-of-the-art systems. This highlights the promise of the proposed device\nheterogeneity handling techniques for enabling a wide deployment of deep\nlearning-based localization systems over different devices.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.16923v1.pdf",
        "similarity": 0.23411308066551445,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-24"
    },
    {
        "new_title": "Lessons from a human-in-the-loop machine learning approach for\n  identifying vacant, abandoned, and deteriorated properties in Savannah,\n  Georgia",
        "new_link": "http://arxiv.org/abs/2407.11138v2",
        "new_summary": "  Addressing strategies for managing vacant, abandoned, and deteriorated (VAD)\nproperties is important for maintaining healthy communities. Yet, the process\nof identifying these properties can be difficult. Here, we create a\nhuman-in-the-loop machine learning (HITLML) model called VADecide and apply it\nto a parcel-level case study in Savannah, Georgia. The results show a higher\nprediction accuracy than was achieved when using a machine learning model\nwithout human input in the training. The HITLML approach also reveals\ndifferences between machine vs. human-generated results. Our findings\ncontribute to knowledge about the advantages and challenges of HITLML in urban\nplanning.\n  [Accepted for Publication at a Peer Review Journal]\n",
        "pdf_link": "https://arxiv.org/pdf/2407.11138v2.pdf",
        "similarity": 0.23393537038927398,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-15"
    },
    {
        "new_title": "A Survey of Privacy Threats and Defense in Vertical Federated Learning:\n  From Model Life Cycle Perspective",
        "new_link": "http://arxiv.org/abs/2402.03688v1",
        "new_summary": "  Vertical Federated Learning (VFL) is a federated learning paradigm where\nmultiple participants, who share the same set of samples but hold different\nfeatures, jointly train machine learning models. Although VFL enables\ncollaborative machine learning without sharing raw data, it is still\nsusceptible to various privacy threats. In this paper, we conduct the first\ncomprehensive survey of the state-of-the-art in privacy attacks and defenses in\nVFL. We provide taxonomies for both attacks and defenses, based on their\ncharacterizations, and discuss open challenges and future research directions.\nSpecifically, our discussion is structured around the model's life cycle, by\ndelving into the privacy threats encountered during different stages of machine\nlearning and their corresponding countermeasures. This survey not only serves\nas a resource for the research community but also offers clear guidance and\nactionable insights for practitioners to safeguard data privacy throughout the\nmodel's life cycle.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.03688v1.pdf",
        "similarity": 0.23344969375425711,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-06"
    },
    {
        "new_title": "A Novel Review of Stability Techniques for Improved Privacy-Preserving\n  Machine Learning",
        "new_link": "http://arxiv.org/abs/2406.00073v1",
        "new_summary": "  Machine learning models have recently enjoyed a significant increase in size\nand popularity. However, this growth has created concerns about dataset\nprivacy. To counteract data leakage, various privacy frameworks guarantee that\nthe output of machine learning models does not compromise their training data.\nHowever, this privatization comes at a cost by adding random noise to the\ntraining process, which reduces model performance. By making models more\nresistant to small changes in input and thus more stable, the necessary amount\nof noise can be decreased while still protecting privacy. This paper\ninvestigates various techniques to enhance stability, thereby minimizing the\nnegative effects of privatization in machine learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.00073v1.pdf",
        "similarity": 0.23331593482498691,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-31"
    },
    {
        "new_title": "Differential Equations for Continuous-Time Deep Learning",
        "new_link": "http://arxiv.org/abs/2401.03965v1",
        "new_summary": "  This short, self-contained article seeks to introduce and survey\ncontinuous-time deep learning approaches that are based on neural ordinary\ndifferential equations (neural ODEs). It primarily targets readers familiar\nwith ordinary and partial differential equations and their analysis who are\ncurious to see their role in machine learning. Using three examples from\nmachine learning and applied mathematics, we will see how neural ODEs can\nprovide new insights into deep learning and a foundation for more efficient\nalgorithms.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.03965v1.pdf",
        "similarity": 0.23319958676857394,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-08"
    },
    {
        "new_title": "Provably Stable Feature Rankings with SHAP and LIME",
        "new_link": "http://arxiv.org/abs/2401.15800v2",
        "new_summary": "  Feature attributions are ubiquitous tools for understanding the predictions\nof machine learning models. However, the calculation of popular methods for\nscoring input variables such as SHAP and LIME suffers from high instability due\nto random sampling. Leveraging ideas from multiple hypothesis testing, we\ndevise attribution methods that ensure the most important features are ranked\ncorrectly with high probability. Given SHAP estimates from KernelSHAP or\nShapley Sampling, we demonstrate how to retrospectively verify the number of\nstable rankings. Further, we introduce efficient sampling algorithms for SHAP\nand LIME that guarantee the $K$ highest-ranked features have the proper\nordering. Finally, we show how to adapt these local feature attribution methods\nfor the global importance setting.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.15800v2.pdf",
        "similarity": 0.23316702667322128,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-28"
    },
    {
        "new_title": "Guiding drones by information gain",
        "new_link": "http://arxiv.org/abs/2401.03947v1",
        "new_summary": "  The accurate estimation of locations and emission rates of gas sources is\ncrucial across various domains, including environmental monitoring and\ngreenhouse gas emission analysis. This study investigates two drone sampling\nstrategies for inferring source term parameters of gas plumes from atmospheric\nmeasurements. Both strategies are guided by the goal of maximizing information\ngain attained from observations at sequential locations. Our research compares\nthe myopic approach of infotaxis to a far-sighted navigation strategy trained\nthrough deep reinforcement learning. We demonstrate the superior performance of\ndeep reinforcement learning over infotaxis in environments with non-isotropic\ngas plumes.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.03947v1.pdf",
        "similarity": 0.23310163714533932,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-08"
    },
    {
        "new_title": "Bias Correction in Machine Learning-based Classification of Rare Events",
        "new_link": "http://arxiv.org/abs/2407.06212v1",
        "new_summary": "  Online platform businesses can be identified by using web-scraped texts. This\nis a classification problem that combines elements of natural language\nprocessing and rare event detection. Because online platforms are rare,\naccurately identifying them with Machine Learning algorithms is challenging.\nHere, we describe the development of a Machine Learning-based text\nclassification approach that reduces the number of false positives as much as\npossible. It greatly reduces the bias in the estimates obtained by using\ncalibrated probabilities and ensembles.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.06212v1.pdf",
        "similarity": 0.23289664517861708,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-04"
    },
    {
        "new_title": "Philosophy of Cognitive Science in the Age of Deep Learning",
        "new_link": "http://arxiv.org/abs/2405.04048v1",
        "new_summary": "  Deep learning has enabled major advances across most areas of artificial\nintelligence research. This remarkable progress extends beyond mere engineering\nachievements and holds significant relevance for the philosophy of cognitive\nscience. Deep neural networks have made significant strides in overcoming the\nlimitations of older connectionist models that once occupied the centre stage\nof philosophical debates about cognition. This development is directly relevant\nto long-standing theoretical debates in the philosophy of cognitive science.\nFurthermore, ongoing methodological challenges related to the comparative\nevaluation of deep neural networks stand to benefit greatly from\ninterdisciplinary collaboration with philosophy and cognitive science. The time\nis ripe for philosophers to explore foundational issues related to deep\nlearning and cognition; this perspective paper surveys key areas where their\ncontributions can be especially fruitful.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.04048v1.pdf",
        "similarity": 0.23219792345649562,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-07"
    },
    {
        "new_title": "Generalizing Machine Learning Evaluation through the Integration of\n  Shannon Entropy and Rough Set Theory",
        "new_link": "http://arxiv.org/abs/2404.12511v1",
        "new_summary": "  This research paper delves into the innovative integration of Shannon entropy\nand rough set theory, presenting a novel approach to generalize the evaluation\napproach in machine learning. The conventional application of entropy,\nprimarily focused on information uncertainty, is extended through its\ncombination with rough set theory to offer a deeper insight into data's\nintrinsic structure and the interpretability of machine learning models. We\nintroduce a comprehensive framework that synergizes the granularity of rough\nset theory with the uncertainty quantification of Shannon entropy, applied\nacross a spectrum of machine learning algorithms. Our methodology is rigorously\ntested on various datasets, showcasing its capability to not only assess\npredictive performance but also to illuminate the underlying data complexity\nand model robustness. The results underscore the utility of this integrated\napproach in enhancing the evaluation landscape of machine learning, offering a\nmulti-faceted perspective that balances accuracy with a profound understanding\nof data attributes and model dynamics. This paper contributes a groundbreaking\nperspective to machine learning evaluation, proposing a method that\nencapsulates a holistic view of model performance, thereby facilitating more\ninformed decision-making in model selection and application.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.12511v1.pdf",
        "similarity": 0.23198613883325425,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-18"
    },
    {
        "new_title": "Coordinated Multi-Neighborhood Learning on a Directed Acyclic Graph",
        "new_link": "http://arxiv.org/abs/2405.15358v1",
        "new_summary": "  Learning the structure of causal directed acyclic graphs (DAGs) is useful in\nmany areas of machine learning and artificial intelligence, with wide\napplications. However, in the high-dimensional setting, it is challenging to\nobtain good empirical and theoretical results without strong and often\nrestrictive assumptions. Additionally, it is questionable whether all of the\nvariables purported to be included in the network are observable. It is of\ninterest then to restrict consideration to a subset of the variables for\nrelevant and reliable inferences. In fact, researchers in various disciplines\ncan usually select a set of target nodes in the network for causal discovery.\nThis paper develops a new constraint-based method for estimating the local\nstructure around multiple user-specified target nodes, enabling coordination in\nstructure learning between neighborhoods. Our method facilitates causal\ndiscovery without learning the entire DAG structure. We establish consistency\nresults for our algorithm with respect to the local neighborhood structure of\nthe target nodes in the true graph. Experimental results on synthetic and\nreal-world data show that our algorithm is more accurate in learning the\nneighborhood structures with much less computational cost than standard methods\nthat estimate the entire DAG. An R package implementing our methods may be\naccessed at https://github.com/stephenvsmith/CML.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.15358v1.pdf",
        "similarity": 0.23168333041520386,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-24"
    },
    {
        "new_title": "Analyses and Concerns in Precision Medicine: A Statistical Perspective",
        "new_link": "http://arxiv.org/abs/2401.06899v1",
        "new_summary": "  This article explores the critical role of statistical analysis in precision\nmedicine. It discusses how personalized healthcare is enhanced by statistical\nmethods that interpret complex, multidimensional datasets, focusing on\npredictive modeling, machine learning algorithms, and data visualization\ntechniques. The paper addresses challenges in data integration and\ninterpretation, particularly with diverse data sources like electronic health\nrecords (EHRs) and genomic data. It also delves into ethical considerations\nsuch as patient privacy and data security. In addition, the paper highlights\nthe evolution of statistical analysis in medicine, core statistical\nmethodologies in precision medicine, and future directions in the field,\nemphasizing the integration of artificial intelligence (AI) and machine\nlearning (ML).\n",
        "pdf_link": "https://arxiv.org/pdf/2401.06899v1.pdf",
        "similarity": 0.2316720259269001,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-12"
    },
    {
        "new_title": "The Role of Learning Algorithms in Collective Action",
        "new_link": "http://arxiv.org/abs/2405.06582v3",
        "new_summary": "  Collective action in machine learning is the study of the control that a\ncoordinated group can have over machine learning algorithms. While previous\nresearch has concentrated on assessing the impact of collectives against Bayes\n(sub-)optimal classifiers, this perspective is limited in that it does not\naccount for the choice of learning algorithm. Since classifiers seldom behave\nlike Bayes classifiers and are influenced by the choice of learning algorithms\nalong with their inherent biases, in this work we initiate the study of how the\nchoice of the learning algorithm plays a role in the success of a collective in\npractical settings. Specifically, we focus on distributionally robust\noptimization (DRO), popular for improving a worst group error, and on the\nubiquitous stochastic gradient descent (SGD), due to its inductive bias for\n\"simpler\" functions. Our empirical results, supported by a theoretical\nfoundation, show that the effective size and success of the collective are\nhighly dependent on properties of the learning algorithm. This highlights the\nnecessity of taking the learning algorithm into account when studying the\nimpact of collective action in machine learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.06582v3.pdf",
        "similarity": 0.2312827160634107,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-10"
    },
    {
        "new_title": "Novel clustered federated learning based on local loss",
        "new_link": "http://arxiv.org/abs/2407.09360v1",
        "new_summary": "  This paper proposes LCFL, a novel clustering metric for evaluating clients'\ndata distributions in federated learning. LCFL aligns with federated learning\nrequirements, accurately assessing client-to-client variations in data\ndistribution. It offers advantages over existing clustered federated learning\nmethods, addressing privacy concerns, improving applicability to non-convex\nmodels, and providing more accurate classification results. LCFL does not\nrequire prior knowledge of clients' data distributions. We provide a rigorous\nmathematical analysis, demonstrating the correctness and feasibility of our\nframework. Numerical experiments with neural network instances highlight the\nsuperior performance of LCFL over baselines on several clustered federated\nlearning benchmarks.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.09360v1.pdf",
        "similarity": 0.23093332160168928,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-12"
    },
    {
        "new_title": "Bridging deep learning force fields and electronic structures with a\n  physics-informed approach",
        "new_link": "http://arxiv.org/abs/2403.13675v2",
        "new_summary": "  This work presents a physics-informed neural network approach bridging\ndeep-learning force field and electronic structure simulations, illustrated\nthrough twisted two-dimensional large-scale material systems. The deep\npotential molecular dynamics model is adopted as the backbone, and electronic\nstructure simulation is integrated. Using Wannier functions as the basis, we\ncategorize Wannier Hamiltonian elements based on physical principles to\nincorporate diverse information from a deep-learning force field model. This\ninformation-sharing mechanism streamlines the architecture of our\nmultifunctional model, enhancing its efficiency and effectiveness. Utilizing\nWannier functions as the basis lays the groundwork for predicting more physical\nquantities. This approach serves as a powerful tool to explore both the\nstructural and electronic properties of large-scale systems characterized by\nlow periodicities. By endowing an existing well-developed machine-learning\nforce field with electronic structure simulation capabilities, the study marks\na significant advancement in developing multimodal machine-learning-based\ncomputational methods that can achieve multiple functionalities traditionally\nexclusive to first-principles calculations.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.13675v2.pdf",
        "similarity": 0.23039948467984725,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-20"
    },
    {
        "new_title": "Is Exploration All You Need? Effective Exploration Characteristics for\n  Transfer in Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2404.02235v1",
        "new_summary": "  In deep reinforcement learning (RL) research, there has been a concerted\neffort to design more efficient and productive exploration methods while\nsolving sparse-reward problems. These exploration methods often share common\nprinciples (e.g., improving diversity) and implementation details (e.g.,\nintrinsic reward). Prior work found that non-stationary Markov decision\nprocesses (MDPs) require exploration to efficiently adapt to changes in the\nenvironment with online transfer learning. However, the relationship between\nspecific exploration characteristics and effective transfer learning in deep RL\nhas not been characterized. In this work, we seek to understand the\nrelationships between salient exploration characteristics and improved\nperformance and efficiency in transfer learning. We test eleven popular\nexploration algorithms on a variety of transfer types -- or ``novelties'' -- to\nidentify the characteristics that positively affect online transfer learning.\nOur analysis shows that some characteristics correlate with improved\nperformance and efficiency across a wide range of transfer tasks, while others\nonly improve transfer performance with respect to specific environment changes.\nFrom our analysis, make recommendations about which exploration algorithm\ncharacteristics are best suited to specific transfer situations.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.02235v1.pdf",
        "similarity": 0.22965605749686072,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-02"
    },
    {
        "new_title": "Physics-Informed Machine Learning for Smart Additive Manufacturing",
        "new_link": "http://arxiv.org/abs/2407.10761v1",
        "new_summary": "  Compared to physics-based computational manufacturing, data-driven models\nsuch as machine learning (ML) are alternative approaches to achieve smart\nmanufacturing. However, the data-driven ML's \"black box\" nature has presented a\nchallenge to interpreting its outcomes. On the other hand, governing physical\nlaws are not effectively utilized to develop data-efficient ML algorithms. To\nleverage the advantages of ML and physical laws of advanced manufacturing, this\npaper focuses on the development of a physics-informed machine learning (PIML)\nmodel by integrating neural networks and physical laws to improve model\naccuracy, transparency, and generalization with case studies in laser metal\ndeposition (LMD).\n",
        "pdf_link": "https://arxiv.org/pdf/2407.10761v1.pdf",
        "similarity": 0.22958219667223229,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-15"
    },
    {
        "new_title": "ESFL: Efficient Split Federated Learning over Resource-Constrained\n  Heterogeneous Wireless Devices",
        "new_link": "http://arxiv.org/abs/2402.15903v2",
        "new_summary": "  Federated learning (FL) allows multiple parties (distributed devices) to\ntrain a machine learning model without sharing raw data. How to effectively and\nefficiently utilize the resources on devices and the central server is a highly\ninteresting yet challenging problem. In this paper, we propose an efficient\nsplit federated learning algorithm (ESFL) to take full advantage of the\npowerful computing capabilities at a central server under a split federated\nlearning framework with heterogeneous end devices (EDs). By splitting the model\ninto different submodels between the server and EDs, our approach jointly\noptimizes user-side workload and server-side computing resource allocation by\nconsidering users' heterogeneity. We formulate the whole optimization problem\nas a mixed-integer non-linear program, which is an NP-hard problem, and develop\nan iterative approach to obtain an approximate solution efficiently. Extensive\nsimulations have been conducted to validate the significantly increased\nefficiency of our ESFL approach compared with standard federated learning,\nsplit learning, and splitfed learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.15903v2.pdf",
        "similarity": 0.2295182451090901,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-24"
    },
    {
        "new_title": "Machine Learning-Aided Cooperative Localization under Dense Urban\n  Environment",
        "new_link": "http://arxiv.org/abs/2404.04096v1",
        "new_summary": "  Future wireless network technology provides automobiles with the connectivity\nfeature to consolidate the concept of vehicular networks that collaborate on\nconducting cooperative driving tasks. The full potential of connected vehicles,\nwhich promises road safety and quality driving experience, can be leveraged if\nmachine learning models guarantee the robustness in performing core functions\nincluding localization and controls. Location awareness, in particular, lends\nitself to the deployment of location-specific services and the improvement of\nthe operation performance. The localization entails direct communication to the\nnetwork infrastructure, and the resulting centralized positioning solutions\nreadily become intractable as the network scales up. As an alternative to the\ncentralized solutions, this article addresses decentralized principle of\nvehicular localization reinforced by machine learning techniques in dense urban\nenvironments with frequent inaccessibility to reliable measurement. As such,\nthe collaboration of multiple vehicles enhances the positioning performance of\nmachine learning approaches. A virtual testbed is developed to validate this\nmachine learning model for real-map vehicular networks. Numerical results\ndemonstrate universal feasibility of cooperative localization, in particular,\nfor dense urban area configurations.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.04096v1.pdf",
        "similarity": 0.22921629559891954,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-05"
    },
    {
        "new_title": "Connecting Algorithmic Fairness to Quality Dimensions in Machine\n  Learning in Official Statistics and Survey Production",
        "new_link": "http://arxiv.org/abs/2402.09328v1",
        "new_summary": "  National Statistical Organizations (NSOs) increasingly draw on Machine\nLearning (ML) to improve the timeliness and cost-effectiveness of their\nproducts. When introducing ML solutions, NSOs must ensure that high standards\nwith respect to robustness, reproducibility, and accuracy are upheld as\ncodified, e.g., in the Quality Framework for Statistical Algorithms (QF4SA;\nYung et al. 2022). At the same time, a growing body of research focuses on\nfairness as a pre-condition of a safe deployment of ML to prevent disparate\nsocial impacts in practice. However, fairness has not yet been explicitly\ndiscussed as a quality aspect in the context of the application of ML at NSOs.\nWe employ Yung et al. (2022)'s QF4SA quality framework and present a mapping of\nits quality dimensions to algorithmic fairness. We thereby extend the QF4SA\nframework in several ways: we argue for fairness as its own quality dimension,\nwe investigate the interaction of fairness with other dimensions, and we\nexplicitly address data, both on its own and its interaction with applied\nmethodology. In parallel with empirical illustrations, we show how our mapping\ncan contribute to methodology in the domains of official statistics,\nalgorithmic fairness, and trustworthy machine learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.09328v1.pdf",
        "similarity": 0.22892629068896111,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-14"
    },
    {
        "new_title": "Quantum Machine Learning with Application to Progressive Supranuclear\n  Palsy Network Classification",
        "new_link": "http://arxiv.org/abs/2407.06226v1",
        "new_summary": "  Machine learning and quantum computing are being progressively explored to\nshed light on possible computational approaches to deal with hitherto\nunsolvable problems. Classical methods for machine learning are ubiquitous in\npattern recognition, with support vector machines (SVMs) being a prominent\ntechnique for network classification. However, there are limitations to the\nsuccessful resolution of such classification instances when the input feature\nspace becomes large, and the successive evaluation of so-called kernel\nfunctions becomes computationally exorbitant. The use of principal component\nanalysis (PCA) substantially minimizes the dimensionality of feature space\nthereby enabling computational speed-ups of supervised learning: the creation\nof a classifier. Further, the application of quantum-based learning to the PCA\nreduced input feature space might offer an exponential speedup with fewer\nparameters. The present learning model is evaluated on a real clinical\napplication: the diagnosis of Progressive Supranuclear Palsy (PSP) disorder.\nThe results suggest that quantum machine learning has led to noticeable\nadvancement and outperforms classical frameworks. The optimized variational\nquantum classifier classifies the PSP dataset with 86% accuracy as compared to\nconventional SVM. The other technique, a quantum kernel estimator, approximates\nthe kernel function on the quantum machine and optimizes a classical SVM. In\nparticular, we have demonstrated the successful application of the present\nmodel on both a quantum simulator and real chips of the IBM quantum platform.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.06226v1.pdf",
        "similarity": 0.22876838176035325,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-06"
    },
    {
        "new_title": "Language Evolution with Deep Learning",
        "new_link": "http://arxiv.org/abs/2403.11958v1",
        "new_summary": "  Computational modeling plays an essential role in the study of language\nemergence. It aims to simulate the conditions and learning processes that could\ntrigger the emergence of a structured language within a simulated controlled\nenvironment. Several methods have been used to investigate the origin of our\nlanguage, including agent-based systems, Bayesian agents, genetic algorithms,\nand rule-based systems. This chapter explores another class of computational\nmodels that have recently revolutionized the field of machine learning: deep\nlearning models. The chapter introduces the basic concepts of deep and\nreinforcement learning methods and summarizes their helpfulness for simulating\nlanguage emergence. It also discusses the key findings, limitations, and recent\nattempts to build realistic simulations. This chapter targets linguists and\ncognitive scientists seeking an introduction to deep learning as a tool to\ninvestigate language evolution.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.11958v1.pdf",
        "similarity": 0.22834975338407135,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-18"
    },
    {
        "new_title": "Can Machines Learn the True Probabilities?",
        "new_link": "http://arxiv.org/abs/2407.05526v1",
        "new_summary": "  When there exists uncertainty, AI machines are designed to make decisions so\nas to reach the best expected outcomes. Expectations are based on true facts\nabout the objective environment the machines interact with, and those facts can\nbe encoded into AI models in the form of true objective probability functions.\nAccordingly, AI models involve probabilistic machine learning in which the\nprobabilities should be objectively interpreted. We prove under some basic\nassumptions when machines can learn the true objective probabilities, if any,\nand when machines cannot learn them.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.05526v1.pdf",
        "similarity": 0.22785207462133758,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-08"
    },
    {
        "new_title": "Making Better Use of Unlabelled Data in Bayesian Active Learning",
        "new_link": "http://arxiv.org/abs/2404.17249v1",
        "new_summary": "  Fully supervised models are predominant in Bayesian active learning. We argue\nthat their neglect of the information present in unlabelled data harms not just\npredictive performance but also decisions about what data to acquire. Our\nproposed solution is a simple framework for semi-supervised Bayesian active\nlearning. We find it produces better-performing models than either conventional\nBayesian active learning or semi-supervised learning with randomly acquired\ndata. It is also easier to scale up than the conventional approach. As well as\nsupporting a shift towards semi-supervised models, our findings highlight the\nimportance of studying models and acquisition methods in conjunction.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.17249v1.pdf",
        "similarity": 0.22769737260724715,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-26"
    },
    {
        "new_title": "Deep Reinforcement Learning with Enhanced PPO for Safe Mobile Robot\n  Navigation",
        "new_link": "http://arxiv.org/abs/2405.16266v1",
        "new_summary": "  Collision-free motion is essential for mobile robots. Most approaches to\ncollision-free and efficient navigation with wheeled robots require parameter\ntuning by experts to obtain good navigation behavior. This study investigates\nthe application of deep reinforcement learning to train a mobile robot for\nautonomous navigation in a complex environment. The robot utilizes LiDAR sensor\ndata and a deep neural network to generate control signals guiding it toward a\nspecified target while avoiding obstacles. We employ two reinforcement learning\nalgorithms in the Gazebo simulation environment: Deep Deterministic Policy\nGradient and proximal policy optimization. The study introduces an enhanced\nneural network structure in the Proximal Policy Optimization algorithm to boost\nperformance, accompanied by a well-designed reward function to improve\nalgorithm efficacy. Experimental results conducted in both obstacle and\nobstacle-free environments underscore the effectiveness of the proposed\napproach. This research significantly contributes to the advancement of\nautonomous robotics in complex environments through the application of deep\nreinforcement learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.16266v1.pdf",
        "similarity": 0.22767080585362146,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-25"
    },
    {
        "new_title": "Scaling up the Banded Matrix Factorization Mechanism for Differentially\n  Private ML",
        "new_link": "http://arxiv.org/abs/2405.15913v1",
        "new_summary": "  DP-BandMF offers a powerful approach to differentially private machine\nlearning, balancing privacy amplification with noise correlation for optimal\nnoise reduction. However, its scalability has been limited to settings where\nthe number of training iterations is less than $10^4$. In this work, we present\ntechniques that significantly extend DP-BandMF's reach, enabling use in\nsettings with and over $10^6$ training iterations. Our enhanced implementation,\ncoupled with extensive experiments, provides clear guidelines on selecting the\noptimal number of bands. These insights offer practitioners a deeper\nunderstanding of DP-BandMF's performance and how to maximize its utility for\nprivacy-preserving machine learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.15913v1.pdf",
        "similarity": 0.22731675380411423,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-24"
    },
    {
        "new_title": "Position: Why We Must Rethink Empirical Research in Machine Learning",
        "new_link": "http://arxiv.org/abs/2405.02200v2",
        "new_summary": "  We warn against a common but incomplete understanding of empirical research\nin machine learning that leads to non-replicable results, makes findings\nunreliable, and threatens to undermine progress in the field. To overcome this\nalarming situation, we call for more awareness of the plurality of ways of\ngaining knowledge experimentally but also of some epistemic limitations. In\nparticular, we argue most current empirical machine learning research is\nfashioned as confirmatory research while it should rather be considered\nexploratory.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.02200v2.pdf",
        "similarity": 0.22685378662500072,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-03"
    },
    {
        "new_title": "Deep Learning for Prediction and Classifying the Dynamical behaviour of\n  Piecewise Smooth Maps",
        "new_link": "http://arxiv.org/abs/2406.17001v1",
        "new_summary": "  This paper explores the prediction of the dynamics of piecewise smooth maps\nusing various deep learning models. We have shown various novel ways of\npredicting the dynamics of piecewise smooth maps using deep learning models.\nMoreover, we have used machine learning models such as Decision Tree\nClassifier, Logistic Regression, K-Nearest Neighbor, Random Forest, and Support\nVector Machine for predicting the border collision bifurcation in the 1D normal\nform map and the 1D tent map. Further, we classified the regular and chaotic\nbehaviour of the 1D tent map and the 2D Lozi map using deep learning models\nlike Convolutional Neural Network (CNN), ResNet50, and ConvLSTM via cobweb\ndiagram and phase portraits. We also classified the chaotic and hyperchaotic\nbehaviour of the 3D piecewise smooth map using deep learning models such as the\nFeed Forward Neural Network (FNN), Long Short-Term Memory (LSTM), and Recurrent\nNeural Network (RNN). Finally, deep learning models such as Long Short-Term\nMemory (LSTM) and Recurrent Neural Network (RNN) are used for reconstructing\nthe two parametric charts of 2D border collision bifurcation normal form map.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.17001v1.pdf",
        "similarity": 0.22684488288783758,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-24"
    },
    {
        "new_title": "Learning Human-like Locomotion Based on Biological Actuation and Rewards",
        "new_link": "http://arxiv.org/abs/2401.15664v1",
        "new_summary": "  We propose a method of learning a policy for human-like locomotion via deep\nreinforcement learning based on a human anatomical model, muscle actuation, and\nbiologically inspired rewards, without any inherent control rules or reference\nmotions. Our main ideas involve providing a dense reward using metabolic energy\nconsumption at every step during the initial stages of learning and then\ntransitioning to a sparse reward as learning progresses, and adjusting the\ninitial posture of the human model to facilitate the exploration of locomotion.\nAdditionally, we compared and analyzed differences in learning outcomes across\nvarious settings other than the proposed method.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.15664v1.pdf",
        "similarity": 0.2263168170531956,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-28"
    },
    {
        "new_title": "Bounds on the price of feedback for mistake-bounded online learning",
        "new_link": "http://arxiv.org/abs/2401.05794v2",
        "new_summary": "  We improve several worst-case bounds for various online learning scenarios\nfrom (Auer and Long, Machine Learning, 1999). In particular, we sharpen an\nupper bound for delayed ambiguous reinforcement learning by a factor of 2 and\nan upper bound for learning compositions of families of functions by a factor\nof 2.41. We also improve a lower bound from the same paper for learning\ncompositions of $k$ families of functions by a factor of $\\Theta(\\ln{k})$,\nmatching the upper bound up to a constant factor. In addition, we solve a\nproblem from (Long, Theoretical Computer Science, 2020) on the price of bandit\nfeedback with respect to standard feedback for multiclass learning, and we\nimprove an upper bound from (Feng et al., Theoretical Computer Science, 2023)\non the price of $r$-input delayed ambiguous reinforcement learning by a factor\nof $r$, matching a lower bound from the same paper up to the leading term.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.05794v2.pdf",
        "similarity": 0.22611620448718497,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-11"
    },
    {
        "new_title": "Machine Learning in Short-Reach Optical Systems: A Comprehensive Survey",
        "new_link": "http://arxiv.org/abs/2405.09557v2",
        "new_summary": "  In recent years, extensive research has been conducted to explore the\nutilization of machine learning algorithms in various direct-detected and\nself-coherent short-reach communication applications. These applications\nencompass a wide range of tasks, including bandwidth request prediction, signal\nquality monitoring, fault detection, traffic prediction, and digital signal\nprocessing (DSP)-based equalization. As a versatile approach, machine learning\ndemonstrates the ability to address stochastic phenomena in optical systems\nnetworks where deterministic methods may fall short. However, when it comes to\nDSP equalization algorithms, their performance improvements are often marginal,\nand their complexity is prohibitively high, especially in cost-sensitive\nshort-reach communications scenarios such as passive optical networks (PONs).\nThey excel in capturing temporal dependencies, handling irregular or nonlinear\npatterns effectively, and accommodating variable time intervals. Within this\nextensive survey, we outline the application of machine learning techniques in\nshort-reach communications, specifically emphasizing their utilization in\nhigh-bandwidth demanding PONs. Notably, we introduce a novel taxonomy for\ntime-series methods employed in machine learning signal processing, providing a\nstructured classification framework. Our taxonomy categorizes current time\nseries methods into four distinct groups: traditional methods, Fourier\nconvolution-based methods, transformer-based models, and time-series\nconvolutional networks. Finally, we highlight prospective research directions\nwithin this rapidly evolving field and outline specific solutions to mitigate\nthe complexity associated with hardware implementations. We aim to pave the way\nfor more practical and efficient deployment of machine learning approaches in\nshort-reach optical communication systems by addressing complexity concerns.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.09557v2.pdf",
        "similarity": 0.22507484803581584,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-02"
    },
    {
        "new_title": "A Brief Overview of Optimization-Based Algorithms for MRI Reconstruction\n  Using Deep Learning",
        "new_link": "http://arxiv.org/abs/2406.02626v1",
        "new_summary": "  Magnetic resonance imaging (MRI) is renowned for its exceptional soft tissue\ncontrast and high spatial resolution, making it a pivotal tool in medical\nimaging. The integration of deep learning algorithms offers significant\npotential for optimizing MRI reconstruction processes. Despite the growing body\nof research in this area, a comprehensive survey of optimization-based deep\nlearning models tailored for MRI reconstruction has yet to be conducted. This\nreview addresses this gap by presenting a thorough examination of the latest\noptimization-based algorithms in deep learning specifically designed for MRI\nreconstruction. The goal of this paper is to provide researchers with a\ndetailed understanding of these advancements, facilitating further innovation\nand application within the MRI community.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.02626v1.pdf",
        "similarity": 0.22507210205002198,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-03"
    },
    {
        "new_title": "Imitation Game: A Model-based and Imitation Learning Deep Reinforcement\n  Learning Hybrid",
        "new_link": "http://arxiv.org/abs/2404.01794v1",
        "new_summary": "  Autonomous and learning systems based on Deep Reinforcement Learning have\nfirmly established themselves as a foundation for approaches to creating\nresilient and efficient Cyber-Physical Energy Systems. However, most current\napproaches suffer from two distinct problems: Modern model-free algorithms such\nas Soft Actor Critic need a high number of samples to learn a meaningful\npolicy, as well as a fallback to ward against concept drifts (e. g.,\ncatastrophic forgetting). In this paper, we present the work in progress\ntowards a hybrid agent architecture that combines model-based Deep\nReinforcement Learning with imitation learning to overcome both problems.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.01794v1.pdf",
        "similarity": 0.2248203280134341,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-02"
    },
    {
        "new_title": "On the consistency of hyper-parameter selection in value-based deep\n  reinforcement learning",
        "new_link": "http://arxiv.org/abs/2406.17523v2",
        "new_summary": "  Deep reinforcement learning (deep RL) has achieved tremendous success on\nvarious domains through a combination of algorithmic design and careful\nselection of hyper-parameters. Algorithmic improvements are often the result of\niterative enhancements built upon prior approaches, while hyper-parameter\nchoices are typically inherited from previous methods or fine-tuned\nspecifically for the proposed technique. Despite their crucial impact on\nperformance, hyper-parameter choices are frequently overshadowed by algorithmic\nadvancements. This paper conducts an extensive empirical study focusing on the\nreliability of hyper-parameter selection for value-based deep reinforcement\nlearning agents, including the introduction of a new score to quantify the\nconsistency and reliability of various hyper-parameters. Our findings not only\nhelp establish which hyper-parameters are most critical to tune, but also help\nclarify which tunings remain consistent across different training regimes.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.17523v2.pdf",
        "similarity": 0.22448800093837556,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-25"
    },
    {
        "new_title": "Animal Behavior Analysis Methods Using Deep Learning: A Survey",
        "new_link": "http://arxiv.org/abs/2405.14002v1",
        "new_summary": "  Animal behavior serves as a reliable indicator of the adaptation of organisms\nto their environment and their overall well-being. Through rigorous observation\nof animal actions and interactions, researchers and observers can glean\nvaluable insights into diverse facets of their lives, encompassing health,\nsocial dynamics, ecological relationships, and neuroethological dimensions.\nAlthough state-of-the-art deep learning models have demonstrated remarkable\naccuracy in classifying various forms of animal data, their adoption in animal\nbehavior studies remains limited. This survey article endeavors to\ncomprehensively explore deep learning architectures and strategies applied to\nthe identification of animal behavior, spanning auditory, visual, and\naudiovisual methodologies. Furthermore, the manuscript scrutinizes extant\nanimal behavior datasets, offering a detailed examination of the principal\nchallenges confronting this research domain. The article culminates in a\ncomprehensive discussion of key research directions within deep learning that\nhold potential for advancing the field of animal behavior studies.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.14002v1.pdf",
        "similarity": 0.22444885680010512,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-22"
    },
    {
        "new_title": "Fully Unconstrained Online Learning",
        "new_link": "http://arxiv.org/abs/2405.20540v1",
        "new_summary": "  We provide an online learning algorithm that obtains regret\n$G\\|w_\\star\\|\\sqrt{T\\log(\\|w_\\star\\|G\\sqrt{T})} + \\|w_\\star\\|^2 + G^2$ on\n$G$-Lipschitz convex losses for any comparison point $w_\\star$ without knowing\neither $G$ or $\\|w_\\star\\|$. Importantly, this matches the optimal bound\n$G\\|w_\\star\\|\\sqrt{T}$ available with such knowledge (up to logarithmic\nfactors), unless either $\\|w_\\star\\|$ or $G$ is so large that even\n$G\\|w_\\star\\|\\sqrt{T}$ is roughly linear in $T$. Thus, it matches the optimal\nbound in all cases in which one can achieve sublinear regret, which arguably\nmost \"interesting\" scenarios.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.20540v1.pdf",
        "similarity": 0.2241857952897152,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-30"
    },
    {
        "new_title": "Vulnerability Detection in Smart Contracts: A Comprehensive Survey",
        "new_link": "http://arxiv.org/abs/2407.07922v1",
        "new_summary": "  In the growing field of blockchain technology, smart contracts exist as\ntransformative digital agreements that execute transactions autonomously in\ndecentralised networks. However, these contracts face challenges in the form of\nsecurity vulnerabilities, posing significant financial and operational risks.\nWhile traditional methods to detect and mitigate vulnerabilities in smart\ncontracts are limited due to a lack of comprehensiveness and effectiveness,\nintegrating advanced machine learning technologies presents an attractive\napproach to increasing effective vulnerability countermeasures. We endeavour to\nfill an important gap in the existing literature by conducting a rigorous\nsystematic review, exploring the intersection between machine learning and\nsmart contracts. Specifically, the study examines the potential of machine\nlearning techniques to improve the detection and mitigation of vulnerabilities\nin smart contracts. We analysed 88 articles published between 2018 and 2023\nfrom the following databases: IEEE, ACM, ScienceDirect, Scopus, and Google\nScholar. The findings reveal that classical machine learning techniques,\nincluding KNN, RF, DT, XG-Boost, and SVM, outperform static tools in\nvulnerability detection. Moreover, multi-model approaches integrating deep\nlearning and classical machine learning show significant improvements in\nprecision and recall, while hybrid models employing various techniques achieve\nnear-perfect performance in vulnerability detection accuracy.\n  By integrating state-of-the-art solutions, this work synthesises current\nmethods, thoroughly investigates research gaps, and suggests directions for\nfuture studies. The insights gathered from this study are intended to serve as\na seminal reference for academics, industry experts, and bodies interested in\nleveraging machine learning to enhance smart contract security.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.07922v1.pdf",
        "similarity": 0.2241827893826854,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-08"
    },
    {
        "new_title": "Modeling Freight Mode Choice Using Machine Learning Classifiers: A\n  Comparative Study Using the Commodity Flow Survey (CFS) Data",
        "new_link": "http://arxiv.org/abs/2402.00659v1",
        "new_summary": "  This study explores the usefulness of machine learning classifiers for\nmodeling freight mode choice. We investigate eight commonly used machine\nlearning classifiers, namely Naive Bayes, Support Vector Machine, Artificial\nNeural Network, K-Nearest Neighbors, Classification and Regression Tree, Random\nForest, Boosting and Bagging, along with the classical Multinomial Logit model.\nUS 2012 Commodity Flow Survey data are used as the primary data source; we\naugment it with spatial attributes from secondary data sources. The performance\nof the classifiers is compared based on prediction accuracy results. The\ncurrent research also examines the role of sample size and training-testing\ndata split ratios on the predictive ability of the various approaches. In\naddition, the importance of variables is estimated to determine how the\nvariables influence freight mode choice. The results show that the tree-based\nensemble classifiers perform the best. Specifically, Random Forest produces the\nmost accurate predictions, closely followed by Boosting and Bagging. With\nregard to variable importance, shipment characteristics, such as shipment\ndistance, industry classification of the shipper and shipment size, are the\nmost significant factors for freight mode choice decisions.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.00659v1.pdf",
        "similarity": 0.22416344309012548,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-01"
    },
    {
        "new_title": "Data-Driven Estimation of Conditional Expectations, Application to\n  Optimal Stopping and Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2407.13189v1",
        "new_summary": "  When the underlying conditional density is known, conditional expectations\ncan be computed analytically or numerically. When, however, such knowledge is\nnot available and instead we are given a collection of training data, the goal\nof this work is to propose simple and purely data-driven means for estimating\ndirectly the desired conditional expectation. Because conditional expectations\nappear in the description of a number of stochastic optimization problems with\nthe corresponding optimal solution satisfying a system of nonlinear equations,\nwe extend our data-driven method to cover such cases as well. We test our\nmethodology by applying it to Optimal Stopping and Optimal Action Policy in\nReinforcement Learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.13189v1.pdf",
        "similarity": 0.2238330184381533,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-18"
    },
    {
        "new_title": "Mode Estimation with Partial Feedback",
        "new_link": "http://arxiv.org/abs/2402.13079v1",
        "new_summary": "  The combination of lightly supervised pre-training and online fine-tuning has\nplayed a key role in recent AI developments. These new learning pipelines call\nfor new theoretical frameworks. In this paper, we formalize core aspects of\nweakly supervised and active learning with a simple problem: the estimation of\nthe mode of a distribution using partial feedback. We show how entropy coding\nallows for optimal information acquisition from partial feedback, develop\ncoarse sufficient statistics for mode identification, and adapt bandit\nalgorithms to our new setting. Finally, we combine those contributions into a\nstatistically and computationally efficient solution to our problem.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.13079v1.pdf",
        "similarity": 0.22379979790017981,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-20"
    },
    {
        "new_title": "Deep Contextual Bandit and Reinforcement Learning for IRS-Assisted\n  MU-MIMO Systems",
        "new_link": "http://arxiv.org/abs/2401.16901v1",
        "new_summary": "  The combination of multiple-input multiple-output (MIMO) systems and\nintelligent reflecting surfaces (IRSs) is foreseen as a critical enabler of\nbeyond 5G (B5G) and 6G. In this work, two different approaches are considered\nfor the joint optimization of the IRS phase-shift matrix and MIMO precoders of\nan IRS-assisted multi-stream (MS) multi-user MIMO (MU-MIMO) system. Both\napproaches aim to maximize the system sum-rate for every channel realization.\nThe first proposed solution is a novel contextual bandit (CB) framework with\ncontinuous state and action spaces called deep contextual bandit-oriented deep\ndeterministic policy gradient (DCB-DDPG). The second is an innovative deep\nreinforcement learning (DRL) formulation where the states, actions, and rewards\nare selected such that the Markov decision process (MDP) property of\nreinforcement learning (RL) is appropriately met. Both proposals perform\nremarkably better than state-of-the-art heuristic methods in scenarios with\nhigh multi-user interference.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.16901v1.pdf",
        "similarity": 0.2236617712292705,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-30"
    },
    {
        "new_title": "A Pontryagin Perspective on Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2405.18100v1",
        "new_summary": "  Reinforcement learning has traditionally focused on learning state-dependent\npolicies to solve optimal control problems in a closed-loop fashion. In this\nwork, we introduce the paradigm of open-loop reinforcement learning where a\nfixed action sequence is learned instead. We present three new algorithms: one\nrobust model-based method and two sample-efficient model-free methods. Rather\nthan basing our algorithms on Bellman's equation from dynamic programming, our\nwork builds on Pontryagin's principle from the theory of open-loop optimal\ncontrol. We provide convergence guarantees and evaluate all methods empirically\non a pendulum swing-up task, as well as on two high-dimensional MuJoCo tasks,\ndemonstrating remarkable performance compared to existing baselines.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.18100v1.pdf",
        "similarity": 0.22357976330139248,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-28"
    },
    {
        "new_title": "Deep Learning Method for Computing Committor Functions with Adaptive\n  Sampling",
        "new_link": "http://arxiv.org/abs/2404.06206v1",
        "new_summary": "  The committor function is a central object for quantifying the transitions\nbetween metastable states of dynamical systems. Recently, a number of\ncomputational methods based on deep neural networks have been developed for\ncomputing the high-dimensional committor function. The success of the methods\nrelies on sampling adequate data for the transition, which still is a\nchallenging task for complex systems at low temperatures. In this work, we\npropose a deep learning method with two novel adaptive sampling schemes (I and\nII). In the two schemes, the data are generated actively with a modified\npotential where the bias potential is constructed from the learned committor\nfunction. We theoretically demonstrate the advantages of the sampling schemes\nand show that the data in sampling scheme II are uniformly distributed along\nthe transition tube. This makes a promising method for studying the transition\nof complex systems. The efficiency of the method is illustrated in\nhigh-dimensional systems including the alanine dipeptide and a solvated dimer\nsystem.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.06206v1.pdf",
        "similarity": 0.22350269043093568,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-09"
    },
    {
        "new_title": "Towards Quantum-Safe Federated Learning via Homomorphic Encryption:\n  Learning with Gradients",
        "new_link": "http://arxiv.org/abs/2402.01154v1",
        "new_summary": "  This paper introduces a privacy-preserving distributed learning framework via\nprivate-key homomorphic encryption. Thanks to the randomness of the\nquantization of gradients, our learning with error (LWE) based encryption can\neliminate the error terms, thus avoiding the issue of error expansion in\nconventional LWE-based homomorphic encryption. The proposed system allows a\nlarge number of learning participants to engage in neural network-based deep\nlearning collaboratively over an honest-but-curious server, while ensuring the\ncryptographic security of participants' uploaded gradients.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.01154v1.pdf",
        "similarity": 0.22341974028676084,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-02"
    },
    {
        "new_title": "opML: Optimistic Machine Learning on Blockchain",
        "new_link": "http://arxiv.org/abs/2401.17555v2",
        "new_summary": "  The integration of machine learning with blockchain technology has witnessed\nincreasing interest, driven by the vision of decentralized, secure, and\ntransparent AI services. In this context, we introduce opML (Optimistic Machine\nLearning on chain), an innovative approach that empowers blockchain systems to\nconduct AI model inference. opML lies a interactive fraud proof protocol,\nreminiscent of the optimistic rollup systems. This mechanism ensures\ndecentralized and verifiable consensus for ML services, enhancing trust and\ntransparency. Unlike zkML (Zero-Knowledge Machine Learning), opML offers\ncost-efficient and highly efficient ML services, with minimal participation\nrequirements. Remarkably, opML enables the execution of extensive language\nmodels, such as 7B-LLaMA, on standard PCs without GPUs, significantly expanding\naccessibility. By combining the capabilities of blockchain and AI through opML,\nwe embark on a transformative journey toward accessible, secure, and efficient\non-chain machine learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.17555v2.pdf",
        "similarity": 0.22339356599590676,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-31"
    },
    {
        "new_title": "Detecting Security-Relevant Methods using Multi-label Machine Learning",
        "new_link": "http://arxiv.org/abs/2403.07501v1",
        "new_summary": "  To detect security vulnerabilities, static analysis tools need to be\nconfigured with security-relevant methods. Current approaches can automatically\nidentify such methods using binary relevance machine learning approaches.\nHowever, they ignore dependencies among security-relevant methods,\nover-generalize and perform poorly in practice. Additionally, users have to\nnevertheless manually configure static analysis tools using the detected\nmethods. Based on feedback from users and our observations, the excessive\nmanual steps can often be tedious, error-prone and counter-intuitive.\n  In this paper, we present Dev-Assist, an IntelliJ IDEA plugin that detects\nsecurity-relevant methods using a multi-label machine learning approach that\nconsiders dependencies among labels. The plugin can automatically generate\nconfigurations for static analysis tools, run the static analysis, and show the\nresults in IntelliJ IDEA. Our experiments reveal that Dev-Assist's machine\nlearning approach has a higher F1-Measure than related approaches. Moreover,\nthe plugin reduces and simplifies the manual effort required when configuring\nand using static analysis tools.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.07501v1.pdf",
        "similarity": 0.22330442538404371,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-12"
    },
    {
        "new_title": "Structured Prediction in Online Learning",
        "new_link": "http://arxiv.org/abs/2406.12366v1",
        "new_summary": "  We study a theoretical and algorithmic framework for structured prediction in\nthe online learning setting. The problem of structured prediction, i.e.\nestimating function where the output space lacks a vectorial structure, is well\nstudied in the literature of supervised statistical learning. We show that our\nalgorithm is a generalisation of optimal algorithms from the supervised\nlearning setting, and achieves the same excess risk upper bound also when data\nare not i.i.d. Moreover, we consider a second algorithm designed especially for\nnon-stationary data distributions, including adversarial data. We bound its\nstochastic regret in function of the variation of the data distributions.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.12366v1.pdf",
        "similarity": 0.22309812536936963,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-18"
    },
    {
        "new_title": "Machine learning approach to detect dynamical states from recurrence\n  measures",
        "new_link": "http://arxiv.org/abs/2401.10298v2",
        "new_summary": "  We integrate machine learning approaches with nonlinear time series analysis,\nspecifically utilizing recurrence measures to classify various dynamical states\nemerging from time series. We implement three machine learning algorithms\nLogistic Regression, Random Forest, and Support Vector Machine for this study.\nThe input features are derived from the recurrence quantification of nonlinear\ntime series and characteristic measures of the corresponding recurrence\nnetworks. For training and testing we generate synthetic data from standard\nnonlinear dynamical systems and evaluate the efficiency and performance of the\nmachine learning algorithms in classifying time series into periodic, chaotic,\nhyper-chaotic, or noisy categories. Additionally, we explore the significance\nof input features in the classification scheme and find that the features\nquantifying the density of recurrence points are the most relevant.\nFurthermore, we illustrate how the trained algorithms can successfully predict\nthe dynamical states of two variable stars, SX Her and AC Her from the data of\ntheir light curves.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.10298v2.pdf",
        "similarity": 0.2230888492682363,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-18"
    },
    {
        "new_title": "Explaining the Contributing Factors for Vulnerability Detection in\n  Machine Learning",
        "new_link": "http://arxiv.org/abs/2406.03577v1",
        "new_summary": "  There is an increasing trend to mine vulnerabilities from software\nrepositories and use machine learning techniques to automatically detect\nsoftware vulnerabilities. A fundamental but unresolved research question is:\nhow do different factors in the mining and learning process impact the accuracy\nof identifying vulnerabilities in software projects of varying characteristics?\nSubstantial research has been dedicated in this area, including source code\nstatic analysis, software repository mining, and NLP-based machine learning.\nHowever, practitioners lack experience regarding the key factors for building a\nbaseline model of the state-of-the-art. In addition, there lacks of experience\nregarding the transferability of the vulnerability signatures from project to\nproject. This study investigates how the combination of different vulnerability\nfeatures and three representative machine learning models impact the accuracy\nof vulnerability detection in 17 real-world projects. We examine two types of\nvulnerability representations: 1) code features extracted through NLP with\nvarying tokenization strategies and three different embedding techniques\n(bag-of-words, word2vec, and fastText) and 2) a set of eight architectural\nmetrics that capture the abstract design of the software systems. The three\nmachine learning algorithms include a random forest model, a support vector\nmachines model, and a residual neural network model. The analysis shows a\nrecommended baseline model with signatures extracted through bag-of-words\nembedding, combined with the random forest, consistently increases the\ndetection accuracy by about 4% compared to other combinations in all 17\nprojects. Furthermore, we observe the limitation of transferring vulnerability\nsignatures across domains based on our experiments.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.03577v1.pdf",
        "similarity": 0.22290302515900345,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-05"
    },
    {
        "new_title": "Two-level overlapping additive Schwarz preconditioner for training\n  scientific machine learning applications",
        "new_link": "http://arxiv.org/abs/2406.10997v1",
        "new_summary": "  We introduce a novel two-level overlapping additive Schwarz preconditioner\nfor accelerating the training of scientific machine learning applications. The\ndesign of the proposed preconditioner is motivated by the nonlinear two-level\noverlapping additive Schwarz preconditioner. The neural network parameters are\ndecomposed into groups (subdomains) with overlapping regions. In addition, the\nnetwork's feed-forward structure is indirectly imposed through a novel\nsubdomain-wise synchronization strategy and a coarse-level training step.\nThrough a series of numerical experiments, which consider physics-informed\nneural networks and operator learning approaches, we demonstrate that the\nproposed two-level preconditioner significantly speeds up the convergence of\nthe standard (LBFGS) optimizer while also yielding more accurate machine\nlearning models. Moreover, the devised preconditioner is designed to take\nadvantage of model-parallel computations, which can further reduce the training\ntime.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.10997v1.pdf",
        "similarity": 0.22274479233196734,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-16"
    },
    {
        "new_title": "Variational Bayesian Last Layers",
        "new_link": "http://arxiv.org/abs/2404.11599v1",
        "new_summary": "  We introduce a deterministic variational formulation for training Bayesian\nlast layer neural networks. This yields a sampling-free, single-pass model and\nloss that effectively improves uncertainty estimation. Our variational Bayesian\nlast layer (VBLL) can be trained and evaluated with only quadratic complexity\nin last layer width, and is thus (nearly) computationally free to add to\nstandard architectures. We experimentally investigate VBLLs, and show that they\nimprove predictive accuracy, calibration, and out of distribution detection\nover baselines across both regression and classification. Finally, we\ninvestigate combining VBLL layers with variational Bayesian feature learning,\nyielding a lower variance collapsed variational inference method for Bayesian\nneural networks.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.11599v1.pdf",
        "similarity": 0.22220411603366083,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-17"
    },
    {
        "new_title": "The impact of model size on catastrophic forgetting in Online Continual\n  Learning",
        "new_link": "http://arxiv.org/abs/2407.00176v1",
        "new_summary": "  This study investigates the impact of model size on Online Continual Learning\nperformance, with a focus on catastrophic forgetting. Employing ResNet\narchitectures of varying sizes, the research examines how network depth and\nwidth affect model performance in class-incremental learning using the\nSplitCIFAR-10 dataset. Key findings reveal that larger models do not guarantee\nbetter Continual Learning performance; in fact, they often struggle more in\nadapting to new tasks, particularly in online settings. These results challenge\nthe notion that larger models inherently mitigate catastrophic forgetting,\nhighlighting the nuanced relationship between model size and Continual Learning\nefficacy. This study contributes to a deeper understanding of model scalability\nand its practical implications in Continual Learning scenarios.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.00176v1.pdf",
        "similarity": 0.22189600277863614,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-28"
    },
    {
        "new_title": "Discovering Multiple Solutions from a Single Task in Offline\n  Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2406.05993v1",
        "new_summary": "  Recent studies on online reinforcement learning (RL) have demonstrated the\nadvantages of learning multiple behaviors from a single task, as in the case of\nfew-shot adaptation to a new environment. Although this approach is expected to\nyield similar benefits in offline RL, appropriate methods for learning multiple\nsolutions have not been fully investigated in previous studies. In this study,\nwe therefore addressed the problem of finding multiple solutions from a single\ntask in offline RL. We propose algorithms that can learn multiple solutions in\noffline RL, and empirically investigate their performance. Our experimental\nresults show that the proposed algorithm learns multiple qualitatively and\nquantitatively distinctive solutions in offline RL.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.05993v1.pdf",
        "similarity": 0.22169022464762422,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-10"
    },
    {
        "new_title": "Jet Discrimination with Quantum Complete Graph Neural Network",
        "new_link": "http://arxiv.org/abs/2403.04990v2",
        "new_summary": "  Machine learning, particularly deep neural networks, has been widely utilized\nin high energy physics and has shown remarkable results in various\napplications. Moreover, the concept of machine learning has been extended to\nquantum computers, giving rise to a new research area known as quantum machine\nlearning. In this paper, we propose a novel variational quantum circuit model,\nQuantum Complete Graph Neural Network (QCGNN), designed for learning complete\ngraphs. We argue that QCGNN has a polynomial speedup against its classical\ncounterpart, due to the property of quantum parallelism. In this paper, we\nstudy the application of QCGNN through the challenging jet discrimination,\nwhere the jets are represented with complete graphs. Subsequently, we conduct a\ncomparative analysis with classical graph neural networks to establish a\nbenchmark.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.04990v2.pdf",
        "similarity": 0.22162031971325694,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-08"
    },
    {
        "new_title": "Machine Learning on Dynamic Graphs: A Survey on Applications",
        "new_link": "http://arxiv.org/abs/2401.08147v1",
        "new_summary": "  Dynamic graph learning has gained significant attention as it offers a\npowerful means to model intricate interactions among entities across various\nreal-world and scientific domains. Notably, graphs serve as effective\nrepresentations for diverse networks such as transportation, brain, social, and\ninternet networks. Furthermore, the rapid advancements in machine learning have\nexpanded the scope of dynamic graph applications beyond the aforementioned\ndomains. In this paper, we present a review of lesser-explored applications of\ndynamic graph learning. This study revealed the potential of machine learning\non dynamic graphs in addressing challenges across diverse domains, including\nthose with limited levels of association with the field.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.08147v1.pdf",
        "similarity": 0.2214577486715016,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-16"
    },
    {
        "new_title": "Negative impact of heavy-tailed uncertainty and error distributions on\n  the reliability of calibration statistics for machine learning regression\n  tasks",
        "new_link": "http://arxiv.org/abs/2402.10043v4",
        "new_summary": "  Average calibration of the (variance-based) prediction uncertainties of\nmachine learning regression tasks can be tested in two ways: one is to estimate\nthe calibration error (CE) as the difference between the mean absolute error\n(MSE) and the mean variance (MV); the alternative is to compare the mean\nsquared z-scores (ZMS) to 1. The problem is that both approaches might lead to\ndifferent conclusions, as illustrated in this study for an ensemble of datasets\nfrom the recent machine learning uncertainty quantification (ML-UQ) literature.\nIt is shown that the estimation of MV, MSE and their confidence intervals\nbecomes unreliable for heavy-tailed uncertainty and error distributions, which\nseems to be a frequent feature of ML-UQ datasets. By contrast, the ZMS\nstatistic is less sensitive and offers the most reliable approach in this\ncontext. Unfortunately, the same problem is expected to affect also conditional\ncalibrations statistics, such as the popular ENCE, and very likely post-hoc\ncalibration methods based on similar statistics. Several solutions to\ncircumvent the outlined problems are proposed.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.10043v4.pdf",
        "similarity": 0.2214479240964094,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-15"
    },
    {
        "new_title": "Statistical learning for constrained functional parameters in\n  infinite-dimensional models with applications in fair machine learning",
        "new_link": "http://arxiv.org/abs/2404.09847v1",
        "new_summary": "  Constrained learning has become increasingly important, especially in the\nrealm of algorithmic fairness and machine learning. In these settings,\npredictive models are developed specifically to satisfy pre-defined notions of\nfairness. Here, we study the general problem of constrained statistical machine\nlearning through a statistical functional lens. We consider learning a\nfunction-valued parameter of interest under the constraint that one or several\npre-specified real-valued functional parameters equal zero or are otherwise\nbounded. We characterize the constrained functional parameter as the minimizer\nof a penalized risk criterion using a Lagrange multiplier formulation. We show\nthat closed-form solutions for the optimal constrained parameter are often\navailable, providing insight into mechanisms that drive fairness in predictive\nmodels. Our results also suggest natural estimators of the constrained\nparameter that can be constructed by combining estimates of unconstrained\nparameters of the data generating distribution. Thus, our estimation procedure\nfor constructing fair machine learning algorithms can be applied in conjunction\nwith any statistical learning approach and off-the-shelf software. We\ndemonstrate the generality of our method by explicitly considering a number of\nexamples of statistical fairness constraints and implementing the approach\nusing several popular learning approaches.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.09847v1.pdf",
        "similarity": 0.2212330103526234,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-15"
    },
    {
        "new_title": "Research Experience of an Undergraduate Student in Computer Vision and\n  Robotics",
        "new_link": "http://arxiv.org/abs/2407.10044v1",
        "new_summary": "  This paper focuses on the educational journey of a computer engineering\nundergraduate student venturing into the domain of computer vision and\nrobotics. It explores how optical flow and its applications can be used to\ndetect moving objects when a camera undergoes translational motion,\nhighlighting the challenges encountered and the strategies used to overcome\nthem. Furthermore, the paper discusses not only the technical skills acquired\nby the student but also interpersonal skills as related to teamwork and\ndiversity. In this paper, we detail the learning process, including the\nacquisition of technical and problem-solving skills, as well as out-of-the-box\nthinking.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.10044v1.pdf",
        "similarity": 0.220834195859363,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-14"
    },
    {
        "new_title": "A Survey of Deep Learning Audio Generation Methods",
        "new_link": "http://arxiv.org/abs/2406.00146v1",
        "new_summary": "  This article presents a review of typical techniques used in three distinct\naspects of deep learning model development for audio generation. In the first\npart of the article, we provide an explanation of audio representations,\nbeginning with the fundamental audio waveform. We then progress to the\nfrequency domain, with an emphasis on the attributes of human hearing, and\nfinally introduce a relatively recent development. The main part of the article\nfocuses on explaining basic and extended deep learning architecture variants,\nalong with their practical applications in the field of audio generation. The\nfollowing architectures are addressed: 1) Autoencoders 2) Generative\nadversarial networks 3) Normalizing flows 4) Transformer networks 5) Diffusion\nmodels. Lastly, we will examine four distinct evaluation metrics that are\ncommonly employed in audio generation. This article aims to offer novice\nreaders and beginners in the field a comprehensive understanding of the current\nstate of the art in audio generation methods as well as relevant studies that\ncan be explored for future research.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.00146v1.pdf",
        "similarity": 0.22080865266908245,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-31"
    },
    {
        "new_title": "Predict Click-Through Rates with Deep Interest Network Model in\n  E-commerce Advertising",
        "new_link": "http://arxiv.org/abs/2406.10239v1",
        "new_summary": "  This paper proposes new methods to enhance click-through rate (CTR)\nprediction models using the Deep Interest Network (DIN) model, specifically\napplied to the advertising system of Alibaba's Taobao platform. Unlike\ntraditional deep learning approaches, this research focuses on localized user\nbehavior activation for tailored ad targeting by leveraging extensive user\nbehavior data. Compared to traditional models, this method demonstrates\nsuperior ability to handle diverse and dynamic user data, thereby improving the\nefficiency of ad systems and increasing revenue.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.10239v1.pdf",
        "similarity": 0.21971433947124733,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-04"
    },
    {
        "new_title": "Machine Learning for Blockchain Data Analysis: Progress and\n  Opportunities",
        "new_link": "http://arxiv.org/abs/2404.18251v1",
        "new_summary": "  Blockchain technology has rapidly emerged to mainstream attention, while its\npublicly accessible, heterogeneous, massive-volume, and temporal data are\nreminiscent of the complex dynamics encountered during the last decade of big\ndata. Unlike any prior data source, blockchain datasets encompass multiple\nlayers of interactions across real-world entities, e.g., human users,\nautonomous programs, and smart contracts. Furthermore, blockchain's integration\nwith cryptocurrencies has introduced financial aspects of unprecedented scale\nand complexity such as decentralized finance, stablecoins, non-fungible tokens,\nand central bank digital currencies. These unique characteristics present both\nopportunities and challenges for machine learning on blockchain data.\n  On one hand, we examine the state-of-the-art solutions, applications, and\nfuture directions associated with leveraging machine learning for blockchain\ndata analysis critical for the improvement of blockchain technology such as\ne-crime detection and trends prediction. On the other hand, we shed light on\nthe pivotal role of blockchain by providing vast datasets and tools that can\ncatalyze the growth of the evolving machine learning ecosystem. This paper\nserves as a comprehensive resource for researchers, practitioners, and\npolicymakers, offering a roadmap for navigating this dynamic and transformative\nfield.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.18251v1.pdf",
        "similarity": 0.21969576934385068,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-28"
    },
    {
        "new_title": "Deep Learning Based Simulators for the Phosphorus Removal Process\n  Control in Wastewater Treatment via Deep Reinforcement Learning Algorithms",
        "new_link": "http://arxiv.org/abs/2401.12822v1",
        "new_summary": "  Phosphorus removal is vital in wastewater treatment to reduce reliance on\nlimited resources. Deep reinforcement learning (DRL) is a machine learning\ntechnique that can optimize complex and nonlinear systems, including the\nprocesses in wastewater treatment plants, by learning control policies through\ntrial and error. However, applying DRL to chemical and biological processes is\nchallenging due to the need for accurate simulators. This study trained six\nmodels to identify the phosphorus removal process and used them to create a\nsimulator for the DRL environment. Although the models achieved high accuracy\n(>97%), uncertainty and incorrect prediction behavior limited their performance\nas simulators over longer horizons. Compounding errors in the models'\npredictions were identified as one of the causes of this problem. This approach\nfor improving process control involves creating simulation environments for DRL\nalgorithms, using data from supervisory control and data acquisition (SCADA)\nsystems with a sufficient historical horizon without complex system modeling or\nparameter estimation.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.12822v1.pdf",
        "similarity": 0.21951993076053278,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-23"
    },
    {
        "new_title": "Machine Learning Predictors for Min-Entropy Estimation",
        "new_link": "http://arxiv.org/abs/2406.19983v1",
        "new_summary": "  This study investigates the application of machine learning predictors for\nmin-entropy estimation in Random Number Generators (RNGs), a key component in\ncryptographic applications where accurate entropy assessment is essential for\ncybersecurity. Our research indicates that these predictors, and indeed any\npredictor that leverages sequence correlations, primarily estimate average\nmin-entropy, a metric not extensively studied in this context. We explore the\nrelationship between average min-entropy and the traditional min-entropy,\nfocusing on their dependence on the number of target bits being predicted.\nUtilizing data from Generalized Binary Autoregressive Models, a subset of\nMarkov processes, we demonstrate that machine learning models (including a\nhybrid of convolutional and recurrent Long Short-Term Memory layers and the\ntransformer-based GPT-2 model) outperform traditional NIST SP 800-90B\npredictors in certain scenarios. Our findings underscore the importance of\nconsidering the number of target bits in min-entropy assessment for RNGs and\nhighlight the potential of machine learning approaches in enhancing entropy\nestimation techniques for improved cryptographic security.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.19983v1.pdf",
        "similarity": 0.2194785055985797,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-28"
    },
    {
        "new_title": "Federated learning in food research",
        "new_link": "http://arxiv.org/abs/2406.06202v1",
        "new_summary": "  Research in the food domain is at times limited due to data sharing\nobstacles, such as data ownership, privacy requirements, and regulations. While\nimportant, these obstacles can restrict data-driven methods such as machine\nlearning. Federated learning, the approach of training models on locally kept\ndata and only sharing the learned parameters, is a potential technique to\nalleviate data sharing obstacles. This systematic review investigates the use\nof federated learning within the food domain, structures included papers in a\nfederated learning framework, highlights knowledge gaps, and discusses\npotential applications. A total of 41 papers were included in the review. The\ncurrent applications include solutions to water and milk quality assessment,\ncybersecurity of water processing, pesticide residue risk analysis, weed\ndetection, and fraud detection, focusing on centralized horizontal federated\nlearning. One of the gaps found was the lack of vertical or transfer federated\nlearning and decentralized architectures.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.06202v1.pdf",
        "similarity": 0.21933751304264984,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-10"
    },
    {
        "new_title": "Quantum-Inspired Machine Learning for Molecular Docking",
        "new_link": "http://arxiv.org/abs/2401.12999v2",
        "new_summary": "  Molecular docking is an important tool for structure-based drug design,\naccelerating the efficiency of drug development. Complex and dynamic binding\nprocesses between proteins and small molecules require searching and sampling\nover a wide spatial range. Traditional docking by searching for possible\nbinding sites and conformations is computationally complex and results poorly\nunder blind docking. Quantum-inspired algorithms combining quantum properties\nand annealing show great advantages in solving combinatorial optimization\nproblems. Inspired by this, we achieve an improved in blind docking by using\nquantum-inspired combined with gradients learned by deep learning in the\nencoded molecular space. Numerical simulation shows that our method outperforms\ntraditional docking algorithms and deep learning-based algorithms over 10\\%.\nCompared to the current state-of-the-art deep learning-based docking algorithm\nDiffDock, the success rate of Top-1 (RMSD<2) achieves an improvement from 33\\%\nto 35\\% in our same setup. In particular, a 6\\% improvement is realized in the\nhigh-precision region(RMSD<1) on molecules data unseen in DiffDock, which\ndemonstrates the well-generalized of our method.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.12999v2.pdf",
        "similarity": 0.21904419147971382,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-22"
    },
    {
        "new_title": "A Brief Introduction to Causal Inference in Machine Learning",
        "new_link": "http://arxiv.org/abs/2405.08793v1",
        "new_summary": "  This is a lecture note produced for DS-GA 3001.003 \"Special Topics in DS -\nCausal Inference in Machine Learning\" at the Center for Data Science, New York\nUniversity in Spring, 2024. This course was created to target master's and PhD\nlevel students with basic background in machine learning but who were not\nexposed to causal inference or causal reasoning in general previously. In\nparticular, this course focuses on introducing such students to expand their\nview and knowledge of machine learning to incorporate causal reasoning, as this\naspect is at the core of so-called out-of-distribution generalization (or lack\nthereof.)\n",
        "pdf_link": "https://arxiv.org/pdf/2405.08793v1.pdf",
        "similarity": 0.21887065695945068,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-14"
    },
    {
        "new_title": "Misconduct in Post-Selections and Deep Learning",
        "new_link": "http://arxiv.org/abs/2403.00773v1",
        "new_summary": "  This is a theoretical paper on \"Deep Learning\" misconduct in particular and\nPost-Selection in general. As far as the author knows, the first peer-reviewed\npapers on Deep Learning misconduct are [32], [37], [36]. Regardless of learning\nmodes, e.g., supervised, reinforcement, adversarial, and evolutional, almost\nall machine learning methods (except for a few methods that train a sole\nsystem) are rooted in the same misconduct -- cheating and hiding -- (1)\ncheating in the absence of a test and (2) hiding bad-looking data. It was\nreasoned in [32], [37], [36] that authors must report at least the average\nerror of all trained networks, good and bad, on the validation set (called\ngeneral cross-validation in this paper). Better, report also five percentage\npositions of ranked errors. From the new analysis here, we can see that the\nhidden culprit is Post-Selection. This is also true for Post-Selection on\nhand-tuned or searched hyperparameters, because they are random, depending on\nrandom observation data. Does cross-validation on data splits rescue\nPost-Selections from the Misconducts (1) and (2)? The new result here says: No.\nSpecifically, this paper reveals that using cross-validation for data splits is\ninsufficient to exonerate Post-Selections in machine learning. In general,\nPost-Selections of statistical learners based on their errors on the validation\nset are statistically invalid.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.00773v1.pdf",
        "similarity": 0.21803507764051433,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-13"
    },
    {
        "new_title": "Evaluating Bayesian deep learning for radio galaxy classification",
        "new_link": "http://arxiv.org/abs/2405.18351v1",
        "new_summary": "  The radio astronomy community is rapidly adopting deep learning techniques to\ndeal with the huge data volumes expected from the next generation of radio\nobservatories. Bayesian neural networks (BNNs) provide a principled way to\nmodel uncertainty in the predictions made by such deep learning models and will\nplay an important role in extracting well-calibrated uncertainty estimates on\ntheir outputs. In this work, we evaluate the performance of different BNNs\nagainst the following criteria: predictive performance, uncertainty calibration\nand distribution-shift detection for the radio galaxy classification problem.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.18351v1.pdf",
        "similarity": 0.21769973058303985,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-28"
    },
    {
        "new_title": "Bayesian Outcome Weighted Learning",
        "new_link": "http://arxiv.org/abs/2406.11573v1",
        "new_summary": "  One of the primary goals of statistical precision medicine is to learn\noptimal individualized treatment rules (ITRs). The classification-based, or\nmachine learning-based, approach to estimating optimal ITRs was first\nintroduced in outcome-weighted learning (OWL). OWL recasts the optimal ITR\nlearning problem into a weighted classification problem, which can be solved\nusing machine learning methods, e.g., support vector machines. In this paper,\nwe introduce a Bayesian formulation of OWL. Starting from the OWL objective\nfunction, we generate a pseudo-likelihood which can be expressed as a scale\nmixture of normal distributions. A Gibbs sampling algorithm is developed to\nsample the posterior distribution of the parameters. In addition to providing a\nstrategy for learning an optimal ITR, Bayesian OWL provides a natural,\nprobabilistic approach to estimate uncertainty in ITR treatment recommendations\nthemselves. We demonstrate the performance of our method through several\nsimulation studies.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.11573v1.pdf",
        "similarity": 0.21736692998158513,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-17"
    },
    {
        "new_title": "Model Averaging and Double Machine Learning",
        "new_link": "http://arxiv.org/abs/2401.01645v1",
        "new_summary": "  This paper discusses pairing double/debiased machine learning (DDML) with\nstacking, a model averaging method for combining multiple candidate learners,\nto estimate structural parameters. We introduce two new stacking approaches for\nDDML: short-stacking exploits the cross-fitting step of DDML to substantially\nreduce the computational burden and pooled stacking enforces common stacking\nweights over cross-fitting folds. Using calibrated simulation studies and two\napplications estimating gender gaps in citations and wages, we show that DDML\nwith stacking is more robust to partially unknown functional forms than common\nalternative approaches based on single pre-selected learners. We provide Stata\nand R software implementing our proposals.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.01645v1.pdf",
        "similarity": 0.2172265193379753,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-03"
    },
    {
        "new_title": "Recurrent Natural Policy Gradient for POMDPs",
        "new_link": "http://arxiv.org/abs/2405.18221v1",
        "new_summary": "  In this paper, we study a natural policy gradient method based on recurrent\nneural networks (RNNs) for partially-observable Markov decision processes,\nwhereby RNNs are used for policy parameterization and policy evaluation to\naddress curse of dimensionality in non-Markovian reinforcement learning. We\npresent finite-time and finite-width analyses for both the critic (recurrent\ntemporal difference learning), and correspondingly-operated recurrent natural\npolicy gradient method in the near-initialization regime. Our analysis\ndemonstrates the efficiency of RNNs for problems with short-term memory with\nexplicit bounds on the required network widths and sample complexity, and\npoints out the challenges in the case of long-term dependencies.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.18221v1.pdf",
        "similarity": 0.21719115253659352,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-28"
    },
    {
        "new_title": "Forecasting Four Business Cycle Phases Using Machine Learning: A Case\n  Study of US and EuroZone",
        "new_link": "http://arxiv.org/abs/2405.17170v2",
        "new_summary": "  Understanding the business cycle is crucial for building economic stability,\nguiding business planning, and informing investment decisions. The business\ncycle refers to the recurring pattern of expansion and contraction in economic\nactivity over time. Economic analysis is inherently complex, incorporating a\nmyriad of factors (such as macroeconomic indicators, political decisions). This\ncomplexity makes it challenging to fully account for all variables when\ndetermining the current state of the economy and predicting its future\ntrajectory in the upcoming months. The objective of this study is to\ninvestigate the capacity of machine learning models in automatically analyzing\nthe state of the economic, with the goal of forecasting business phases\n(expansion, slowdown, recession and recovery) in the United States and the\nEuroZone. We compared three different machine learning approaches to classify\nthe phases of the business cycle, and among them, the Multinomial Logistic\nRegression (MLR) achieved the best results. Specifically, MLR got the best\nresults by achieving the accuracy of 65.25% (Top1) and 84.74% (Top2) for the\nEuroZone and 75% (Top1) and 92.14% (Top2) for the United States. These results\ndemonstrate the potential of machine learning techniques to predict business\ncycles accurately, which can aid in making informed decisions in the fields of\neconomics and finance.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.17170v2.pdf",
        "similarity": 0.21683292391884665,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-27"
    },
    {
        "new_title": "Fast Value Tracking for Deep Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2403.13178v1",
        "new_summary": "  Reinforcement learning (RL) tackles sequential decision-making problems by\ncreating agents that interacts with their environment. However, existing\nalgorithms often view these problem as static, focusing on point estimates for\nmodel parameters to maximize expected rewards, neglecting the stochastic\ndynamics of agent-environment interactions and the critical role of uncertainty\nquantification. Our research leverages the Kalman filtering paradigm to\nintroduce a novel and scalable sampling algorithm called Langevinized Kalman\nTemporal-Difference (LKTD) for deep reinforcement learning. This algorithm,\ngrounded in Stochastic Gradient Markov Chain Monte Carlo (SGMCMC), efficiently\ndraws samples from the posterior distribution of deep neural network\nparameters. Under mild conditions, we prove that the posterior samples\ngenerated by the LKTD algorithm converge to a stationary distribution. This\nconvergence not only enables us to quantify uncertainties associated with the\nvalue function and model parameters but also allows us to monitor these\nuncertainties during policy updates throughout the training phase. The LKTD\nalgorithm paves the way for more robust and adaptable reinforcement learning\napproaches.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.13178v1.pdf",
        "similarity": 0.21642367322795306,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-19"
    },
    {
        "new_title": "RMF: A Risk Measurement Framework for Machine Learning Models",
        "new_link": "http://arxiv.org/abs/2406.12929v1",
        "new_summary": "  Machine learning (ML) models are used in many safety- and security-critical\napplications nowadays. It is therefore important to measure the security of a\nsystem that uses ML as a component. This paper focuses on the field of ML,\nparticularly the security of autonomous vehicles. For this purpose, a technical\nframework will be described, implemented, and evaluated in a case study. Based\non ISO/IEC 27004:2016, risk indicators are utilized to measure and evaluate the\nextent of damage and the effort required by an attacker. It is not possible,\nhowever, to determine a single risk value that represents the attacker's\neffort. Therefore, four different values must be interpreted individually.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.12929v1.pdf",
        "similarity": 0.21631344946278644,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-15"
    },
    {
        "new_title": "Causal Learning in Biomedical Applications",
        "new_link": "http://arxiv.org/abs/2406.15189v1",
        "new_summary": "  We present a benchmark for methods in causal learning. Specifically, we\nconsider training a rich class of causal models from time-series data, and we\nsuggest the use of the Krebs cycle and models of metabolism more broadly.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.15189v1.pdf",
        "similarity": 0.21623511442914495,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-21"
    },
    {
        "new_title": "Investigating Data Usage for Inductive Conformal Predictors",
        "new_link": "http://arxiv.org/abs/2406.12262v1",
        "new_summary": "  Inductive conformal predictors (ICPs) are algorithms that are able to\ngenerate prediction sets, instead of point predictions, which are valid at a\nuser-defined confidence level, only assuming exchangeability. These algorithms\nare useful for reliable machine learning and are increasing in popularity. The\nICP development process involves dividing development data into three parts:\ntraining, calibration and test. With access to limited or expensive development\ndata, it is an open question regarding the most efficient way to divide the\ndata. This study provides several experiments to explore this question and\nconsider the case for allowing overlap of examples between training and\ncalibration sets. Conclusions are drawn that will be of value to academics and\npractitioners planning to use ICPs.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.12262v1.pdf",
        "similarity": 0.21590328952104643,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-18"
    },
    {
        "new_title": "Detection of Sleep Oxygen Desaturations from Electroencephalogram\n  Signals",
        "new_link": "http://arxiv.org/abs/2405.09566v1",
        "new_summary": "  In this work, we leverage machine learning techniques to identify potential\nbiomarkers of oxygen desaturation during sleep exclusively from\nelectroencephalogram (EEG) signals in pediatric patients with sleep apnea.\nDevelopment of a machine learning technique which can successfully identify EEG\nsignals from patients with sleep apnea as well as identify latent EEG signals\nwhich come from subjects who experience oxygen desaturations but do not\nthemselves occur during oxygen desaturation events would provide a strong step\ntowards developing a brain-based biomarker for sleep apnea in order to aid with\neasier diagnosis of this disease. We leverage a large corpus of data, and show\nthat machine learning enables us to classify EEG signals as occurring during\noxygen desaturations or not occurring during oxygen desaturations with an\naverage 66.8% balanced accuracy. We furthermore investigate the ability of\nmachine learning models to identify subjects who experience oxygen\ndesaturations from EEG data that does not occur during oxygen desaturations. We\nconclude that there is a potential biomarker for oxygen desaturation in EEG\ndata.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.09566v1.pdf",
        "similarity": 0.2158954211809117,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-08"
    },
    {
        "new_title": "Optimal Blackjack Strategy Recommender: A Comprehensive Study on\n  Computer Vision Integration for Enhanced Gameplay",
        "new_link": "http://arxiv.org/abs/2404.00191v1",
        "new_summary": "  This research project investigates the application of several computer vision\ntechniques for playing card detection and recognition in the context of the\npopular casino game, blackjack. The primary objective is to develop a robust\nsystem that is capable of detecting and accurately classifying playing cards in\nreal-time, and displaying the optimal move recommendation based on the given\nimage of the current game. The proposed methodology involves using K-Means for\nimage segmentation, card reprojection and feature extraction, training of the\nKNN classifier using a labeled dataset, and integration of the detection system\ninto a Blackjack Basic Strategy recommendation algorithm. Further, the study\naims to observe the effectiveness of this approach in detecting various card\ndesigns under different lighting conditions and occlusions. Overall, the\nproject examines the potential benefits of incorporating computer vision\ntechniques, with a specific focus on card detection, into commonly played games\naiming to enhance player decision-making and optimize strategic outcomes. The\nresults obtained from our experimental evaluations with models developed under\nconsiderable time constraints, highlight the potential for practical\nimplementation in real-world casino environments and across other similarly\nstructured games.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.00191v1.pdf",
        "similarity": 0.21561064698630902,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-29"
    },
    {
        "new_title": "Extending the Scope of Inference About Predictive Ability to Machine\n  Learning Methods",
        "new_link": "http://arxiv.org/abs/2402.12838v2",
        "new_summary": "  Though out-of-sample forecast evaluation is systematically employed with\nmodern machine learning methods and there exists a well-established classic\ninference theory for predictive ability, see, e.g., West (1996, Asymptotic\nInference About Predictive Ability, Econometrica, 64, 1067-1084), such theory\nis not directly applicable to modern machine learners such as the Lasso in the\nhigh dimensional setting. We investigate under which conditions such extensions\nare possible. Two key properties for standard out-of-sample asymptotic\ninference to be valid with machine learning are (i) a zero-mean condition for\nthe score of the prediction loss function; and (ii) a fast rate of convergence\nfor the machine learner. Monte Carlo simulations confirm our theoretical\nfindings. We recommend a small out-of-sample vs in-sample size ratio for\naccurate finite sample inferences with machine learning. We illustrate the wide\napplicability of our results with a new out-of-sample test for the Martingale\nDifference Hypothesis (MDH). We obtain the asymptotic null distribution of our\ntest and use it to evaluate the MDH of some major exchange rates at daily and\nhigher frequencies.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.12838v2.pdf",
        "similarity": 0.21553923185363452,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-20"
    },
    {
        "new_title": "Time-Efficient Light-Field Acquisition Using Coded Aperture and Events",
        "new_link": "http://arxiv.org/abs/2403.07244v1",
        "new_summary": "  We propose a computational imaging method for time-efficient light-field\nacquisition that combines a coded aperture with an event-based camera.\nDifferent from the conventional coded-aperture imaging method, our method\napplies a sequence of coding patterns during a single exposure for an image\nframe. The parallax information, which is related to the differences in coding\npatterns, is recorded as events. The image frame and events, all of which are\nmeasured in a single exposure, are jointly used to computationally reconstruct\na light field. We also designed an algorithm pipeline for our method that is\nend-to-end trainable on the basis of deep optics and compatible with real\ncamera hardware. We experimentally showed that our method can achieve more\naccurate reconstruction than several other imaging methods with a single\nexposure. We also developed a hardware prototype with the potential to complete\nthe measurement on the camera within 22 msec and demonstrated that light fields\nfrom real 3-D scenes can be obtained with convincing visual quality. Our\nsoftware and supplementary video are available from our project website.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.07244v1.pdf",
        "similarity": 0.21534553586669128,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-12"
    },
    {
        "new_title": "SMLP: Symbolic Machine Learning Prover",
        "new_link": "http://arxiv.org/abs/2402.01415v1",
        "new_summary": "  Symbolic Machine Learning Prover (SMLP) is a tool and a library for system\nexploration based on data samples obtained by simulating or executing the\nsystem on a number of input vectors. SMLP aims at exploring the system based on\nthis data by taking a grey-box approach: SMLP combines statistical methods of\ndata exploration with building and exploring machine learning models in close\nfeedback loop with the system's response, and exploring these models by\ncombining probabilistic and formal methods. SMLP has been applied in industrial\nsetting at Intel for analyzing and optimizing hardware designs at the analog\nlevel. SMLP is a general purpose tool and can be applied to systems that can be\nsampled and modeled by machine learning models.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.01415v1.pdf",
        "similarity": 0.21525985126293548,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-02"
    },
    {
        "new_title": "Multi-Agent Deep Reinforcement Learning for Distributed Satellite\n  Routing",
        "new_link": "http://arxiv.org/abs/2402.17666v1",
        "new_summary": "  This paper introduces a Multi-Agent Deep Reinforcement Learning (MA-DRL)\napproach for routing in Low Earth Orbit Satellite Constellations (LSatCs). Each\nsatellite is an independent decision-making agent with a partial knowledge of\nthe environment, and supported by feedback received from the nearby agents.\nBuilding on our previous work that introduced a Q-routing solution, the\ncontribution of this paper is to extend it to a deep learning framework able to\nquickly adapt to the network and traffic changes, and based on two phases: (1)\nAn offline exploration learning phase that relies on a global Deep Neural\nNetwork (DNN) to learn the optimal paths at each possible position and\ncongestion level; (2) An online exploitation phase with local, on-board,\npre-trained DNNs. Results show that MA-DRL efficiently learns optimal routes\noffline that are then loaded for an efficient distributed routing online.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.17666v1.pdf",
        "similarity": 0.21525162470199224,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-27"
    },
    {
        "new_title": "To Pool or Not To Pool: Analyzing the Regularizing Effects of Group-Fair\n  Training on Shared Models",
        "new_link": "http://arxiv.org/abs/2402.18803v1",
        "new_summary": "  In fair machine learning, one source of performance disparities between\ngroups is over-fitting to groups with relatively few training samples. We\nderive group-specific bounds on the generalization error of welfare-centric\nfair machine learning that benefit from the larger sample size of the majority\ngroup. We do this by considering group-specific Rademacher averages over a\nrestricted hypothesis class, which contains the family of models likely to\nperform well with respect to a fair learning objective (e.g., a power-mean).\nOur simulations demonstrate these bounds improve over a naive method, as\nexpected by theory, with particularly significant improvement for smaller group\nsizes.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.18803v1.pdf",
        "similarity": 0.21497907760181226,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-29"
    },
    {
        "new_title": "Optimizing Wireless Networks with Deep Unfolding: Comparative Study on\n  Two Deep Unfolding Mechanisms",
        "new_link": "http://arxiv.org/abs/2403.18930v1",
        "new_summary": "  In this work, we conduct a comparative study on two deep unfolding mechanisms\nto efficiently perform power control in the next generation wireless networks.\nThe power control problem is formulated as energy efficiency over multiple\ninterference links. The problem is nonconvex. We employ fractional programming\ntransformation to design two solutions for the problem. The first solution is a\nnumerical solution while the second solution is a closed-form solution. Based\non the first solution, we design a semi-unfolding deep learning model where we\ncombine the domain knowledge of the wireless communications and the recent\nadvances in the data-driven deep learning. Moreover, on the highlights of the\nclosed-form solution, fully deep unfolded deep learning model is designed in\nwhich we fully leveraged the expressive closed-form power control solution and\ndeep learning advances. In the simulation results, we compare the performance\nof the proposed deep learning models and the iterative solutions in terms of\naccuracy and inference speed to show their suitability for the real-time\napplication in next generation networks.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.18930v1.pdf",
        "similarity": 0.21491811153408563,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-17"
    },
    {
        "new_title": "Tverberg's theorem and multi-class support vector machines",
        "new_link": "http://arxiv.org/abs/2404.16724v1",
        "new_summary": "  We show how, using linear-algebraic tools developed to prove Tverberg's\ntheorem in combinatorial geometry, we can design new models of multi-class\nsupport vector machines (SVMs). These supervised learning protocols require\nfewer conditions to classify sets of points, and can be computed using existing\nbinary SVM algorithms in higher-dimensional spaces, including soft-margin SVM\nalgorithms. We describe how the theoretical guarantees of standard support\nvector machines transfer to these new classes of multi-class support vector\nmachines. We give a new simple proof of a geometric characterization of support\nvectors for largest margin SVMs by Veelaert.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.16724v1.pdf",
        "similarity": 0.21456598024655585,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-25"
    },
    {
        "new_title": "Model-Based Deep Learning for Music Information Research",
        "new_link": "http://arxiv.org/abs/2406.11540v1",
        "new_summary": "  In this article, we investigate the notion of model-based deep learning in\nthe realm of music information research (MIR). Loosely speaking, we refer to\nthe term model-based deep learning for approaches that combine traditional\nknowledge-based methods with data-driven techniques, especially those based on\ndeep learning, within a diff erentiable computing framework. In music, prior\nknowledge for instance related to sound production, music perception or music\ncomposition theory can be incorporated into the design of neural networks and\nassociated loss functions. We outline three specifi c scenarios to illustrate\nthe application of model-based deep learning in MIR, demonstrating the\nimplementation of such concepts and their potential.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.11540v1.pdf",
        "similarity": 0.2143555081262396,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-17"
    },
    {
        "new_title": "Accelerating Computer Architecture Simulation through Machine Learning",
        "new_link": "http://arxiv.org/abs/2402.18746v1",
        "new_summary": "  This paper presents our approach to accelerate computer architecture\nsimulation by leveraging machine learning techniques. Traditional computer\narchitecture simulations are time-consuming, making it challenging to explore\ndifferent design choices efficiently. Our proposed model utilizes a combination\nof application features and micro-architectural features to predict the\nperformance of an application. These features are derived from simulations of a\nsmall portion of the application. We demonstrate the effectiveness of our\napproach by building and evaluating a machine learning model that offers\nsignificant speedup in architectural exploration. This model demonstrates the\nability to predict IPC values for the testing data with a root mean square\nerror of less than 0.1.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.18746v1.pdf",
        "similarity": 0.21427898373355175,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-28"
    },
    {
        "new_title": "Signal Recovery with Proximal Comixtures",
        "new_link": "http://arxiv.org/abs/2403.09610v1",
        "new_summary": "  In variational signal processing and machine learning problems, loss\nfunctions and linear operators are typically aggregated as an average of\ncomposite terms. We propose an alternative formulation using proximal\ncomixtures, an operation that combines functions and linear operators in such a\nway that the proximity operator of the resulting function is computable\nexplicitly. The benefits of comixture formulations are illustrated through\nimage recovery and machine learning applications.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.09610v1.pdf",
        "similarity": 0.21417762450673955,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-14"
    },
    {
        "new_title": "On Maximum Entropy Linear Feature Inversion",
        "new_link": "http://arxiv.org/abs/2407.14166v1",
        "new_summary": "  We revisit the classical problem of inverting dimension-reducing linear\nmappings using the maximum entropy (MaxEnt) criterion. In the literature,\nsolutions are problem-dependent, inconsistent, and use different entropy\nmeasures. We propose a new unified approach that not only specializes to the\nexisting approaches, but offers solutions to new cases, such as when data\nvalues are constrained to [0, 1], which has new applications in machine\nlearning.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.14166v1.pdf",
        "similarity": 0.2141324091975343,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-19"
    },
    {
        "new_title": "Online learning of quantum processes",
        "new_link": "http://arxiv.org/abs/2406.04250v1",
        "new_summary": "  Among recent insights into learning quantum states, online learning and\nshadow tomography procedures are notable for their ability to accurately\npredict expectation values even of adaptively chosen observables. In contrast\nto the state case, quantum process learning tasks with a similarly adaptive\nnature have received little attention. In this work, we investigate online\nlearning tasks for quantum processes. Whereas online learning is infeasible for\ngeneral quantum channels, we show that channels of bounded gate complexity as\nwell as Pauli channels can be online learned in the regret and mistake-bounded\nmodels of online learning. In fact, we can online learn probabilistic mixtures\nof any exponentially large set of known channels. We also provide a provably\nsample-efficient shadow tomography procedure for Pauli channels. Our results\nextend beyond quantum channels to non-Markovian multi-time processes, with\nfavorable regret and mistake bounds, as well as a shadow tomography procedure.\nWe complement our online learning upper bounds with mistake as well as\ncomputational lower bounds. On the technical side, we make use of the\nmultiplicative weights update algorithm, classical adaptive data analysis, and\nBell sampling, as well as tools from the theory of quantum combs for multi-time\nquantum processes. Our work initiates a study of online learning for classes of\nquantum channels and, more generally, non-Markovian quantum processes. Given\nthe importance of online learning for state shadow tomography, this may serve\nas a step towards quantum channel variants of adaptive shadow tomography.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.04250v1.pdf",
        "similarity": 0.21401663051116485,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-06"
    },
    {
        "new_title": "Asynchronous Online Adaptation via Modular Drift Detection for Deep\n  Receivers",
        "new_link": "http://arxiv.org/abs/2407.09134v1",
        "new_summary": "  Deep learning is envisioned to facilitate the operation of wireless\nreceivers, with emerging architectures integrating deep neural networks (DNNs)\nwith traditional modular receiver processing. While deep receivers were shown\nto operate reliably in complex settings for which they were trained, the\ndynamic nature of wireless communications gives rise to the need to repeatedly\nadapt deep receivers to channel variations. However, frequent re-training is\ncostly and ineffective, while in practice, not every channel variation\nnecessitates adaptation of the entire DNN. In this paper, we study concept\ndrift detection for identifying when does a deep receiver no longer match the\nchannel, enabling asynchronous adaptation, i.e., re-training only when\nnecessary. We identify existing drift detection schemes from the machine\nlearning literature that can be adapted for deep receivers in dynamic channels,\nand propose a novel soft-output detection mechanism tailored to the\ncommunication domain. Moreover, for deep receivers that preserve conventional\nmodular receiver processing, we design modular drift detection mechanisms, that\nsimultaneously identify when and which sub-module to re-train. The provided\nnumerical studies show that even in a rapidly time-varying scenarios,\nasynchronous adaptation via modular drift detection dramatically reduces the\nnumber of trained parameters and re-training times, with little compromise on\nperformance.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.09134v1.pdf",
        "similarity": 0.2137916546102961,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-12"
    },
    {
        "new_title": "Coprocessor Actor Critic: A Model-Based Reinforcement Learning Approach\n  For Adaptive Brain Stimulation",
        "new_link": "http://arxiv.org/abs/2406.06714v1",
        "new_summary": "  Adaptive brain stimulation can treat neurological conditions such as\nParkinson's disease and post-stroke motor deficits by influencing abnormal\nneural activity. Because of patient heterogeneity, each patient requires a\nunique stimulation policy to achieve optimal neural responses. Model-free\nreinforcement learning (MFRL) holds promise in learning effective policies for\na variety of similar control tasks, but is limited in domains like brain\nstimulation by a need for numerous costly environment interactions. In this\nwork we introduce Coprocessor Actor Critic, a novel, model-based reinforcement\nlearning (MBRL) approach for learning neural coprocessor policies for brain\nstimulation. Our key insight is that coprocessor policy learning is a\ncombination of learning how to act optimally in the world and learning how to\ninduce optimal actions in the world through stimulation of an injured brain. We\nshow that our approach overcomes the limitations of traditional MFRL methods in\nterms of sample efficiency and task success and outperforms baseline MBRL\napproaches in a neurologically realistic model of an injured brain.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.06714v1.pdf",
        "similarity": 0.21352818890754016,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-10"
    },
    {
        "new_title": "Machine Learning for Economic Forecasting: An Application to China's GDP\n  Growth",
        "new_link": "http://arxiv.org/abs/2407.03595v1",
        "new_summary": "  This paper aims to explore the application of machine learning in forecasting\nChinese macroeconomic variables. Specifically, it employs various machine\nlearning models to predict the quarterly real GDP growth of China, and analyzes\nthe factors contributing to the performance differences among these models. Our\nfindings indicate that the average forecast errors of machine learning models\nare generally lower than those of traditional econometric models or expert\nforecasts, particularly in periods of economic stability. However, during\ncertain inflection points, although machine learning models still outperform\ntraditional econometric models, expert forecasts may exhibit greater accuracy\nin some instances due to experts' more comprehensive understanding of the\nmacroeconomic environment and real-time economic variables. In addition to\nmacroeconomic forecasting, this paper employs interpretable machine learning\nmethods to identify the key attributive variables from different machine\nlearning models, aiming to enhance the understanding and evaluation of their\ncontributions to macroeconomic fluctuations.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.03595v1.pdf",
        "similarity": 0.2132667826010871,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-04"
    },
    {
        "new_title": "Semi-parametric Expert Bayesian Network Learning with Gaussian Processes\n  and Horseshoe Priors",
        "new_link": "http://arxiv.org/abs/2401.16419v1",
        "new_summary": "  This paper proposes a model learning Semi-parametric relationships in an\nExpert Bayesian Network (SEBN) with linear parameter and structure constraints.\nWe use Gaussian Processes and a Horseshoe prior to introduce minimal nonlinear\ncomponents. To prioritize modifying the expert graph over adding new edges, we\noptimize differential Horseshoe scales. In real-world datasets with unknown\ntruth, we generate diverse graphs to accommodate user input, addressing\nidentifiability issues and enhancing interpretability. Evaluation on synthetic\nand UCI Liver Disorders datasets, using metrics like structural Hamming\nDistance and test likelihood, demonstrates our models outperform\nstate-of-the-art semi-parametric Bayesian Network model.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.16419v1.pdf",
        "similarity": 0.21319550330457368,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-29"
    },
    {
        "new_title": "Collaborative Optimization of Wireless Communication and Computing\n  Resource Allocation based on Multi-Agent Federated Weighting Deep\n  Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2404.01638v1",
        "new_summary": "  As artificial intelligence (AI)-enabled wireless communication systems\ncontinue their evolution, distributed learning has gained widespread attention\nfor its ability to offer enhanced data privacy protection, improved resource\nutilization, and enhanced fault tolerance within wireless communication\napplications. Federated learning further enhances the ability of resource\ncoordination and model generalization across nodes based on the above\nfoundation, enabling the realization of an AI-driven communication and\ncomputing integrated wireless network. This paper proposes a novel wireless\ncommunication system to cater to a personalized service needs of both\nprivacy-sensitive and privacy-insensitive users. We design the system based on\nbased on multi-agent federated weighting deep reinforcement learning (MAFWDRL).\nThe system, while fulfilling service requirements for users, facilitates\nreal-time optimization of local communication resources allocation and\nconcurrent decision-making concerning computing resources. Additionally,\nexploration noise is incorporated to enhance the exploration process of\noff-policy deep reinforcement learning (DRL) for wireless channels. Federated\nweighting (FedWgt) effectively compensates for heterogeneous differences in\nchannel status between communication nodes. Extensive simulation experiments\ndemonstrate that the proposed scheme outperforms baseline methods significantly\nin terms of throughput, calculation latency, and energy consumption\nimprovement.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.01638v1.pdf",
        "similarity": 0.21278767017253314,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-02"
    },
    {
        "new_title": "Machine Learning Driven Global Optimisation Framework for Analog Circuit\n  Design",
        "new_link": "http://arxiv.org/abs/2404.02911v1",
        "new_summary": "  We propose a machine learning-driven optimisation framework for analog\ncircuit design in this paper. The primary objective is to determine the device\nsizes for the optimal performance of analog circuits for a given set of\nspecifications. Our methodology entails employing machine learning models and\nspice simulations to direct the optimisation algorithm towards achieving the\noptimal design for analog circuits. Machine learning based global offline\nsurrogate models, with the circuit design parameters as the input, are built in\nthe design space for the analog circuits under study and is used to guide the\noptimisation algorithm, resulting in faster convergence and a reduced number of\nspice simulations. Multi-layer perceptron and random forest regressors are\nemployed to predict the required design specifications of the analog circuit.\nSince the saturation condition of transistors is vital in the proper working of\nanalog circuits, multi-layer perceptron classifiers are used to predict the\nsaturation condition of each transistor in the circuit. The feasibility of the\ncandidate solutions is verified using machine learning models before invoking\nspice simulations. We validate the proposed framework using three circuit\ntopologies--a bandgap reference, a folded cascode operational amplifier, and a\ntwo-stage operational amplifier. The simulation results show better optimum\nvalues and lower standard deviations for fitness functions after convergence.\nIncorporating the machine learning-based predictions proposed in the\noptimisation method has resulted in the reduction of spice calls by 56%, 59%,\nand 83% when compared with standard approaches in the three test cases\nconsidered in the study.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.02911v1.pdf",
        "similarity": 0.21271943806742594,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-27"
    },
    {
        "new_title": "Explainable AI for survival analysis: a median-SHAP approach",
        "new_link": "http://arxiv.org/abs/2402.00072v1",
        "new_summary": "  With the adoption of machine learning into routine clinical practice comes\nthe need for Explainable AI methods tailored to medical applications. Shapley\nvalues have sparked wide interest for locally explaining models. Here, we\ndemonstrate their interpretation strongly depends on both the summary statistic\nand the estimator for it, which in turn define what we identify as an 'anchor\npoint'. We show that the convention of using a mean anchor point may generate\nmisleading interpretations for survival analysis and introduce median-SHAP, a\nmethod for explaining black-box models predicting individual survival times.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.00072v1.pdf",
        "similarity": 0.2121634768867859,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-30"
    },
    {
        "new_title": "Privacy Preserving Machine Learning for Electronic Health Records using\n  Federated Learning and Differential Privacy",
        "new_link": "http://arxiv.org/abs/2406.15962v1",
        "new_summary": "  An Electronic Health Record (EHR) is an electronic database used by\nhealthcare providers to store patients' medical records which may include\ndiagnoses, treatments, costs, and other personal information. Machine learning\n(ML) algorithms can be used to extract and analyze patient data to improve\npatient care. Patient records contain highly sensitive information, such as\nsocial security numbers (SSNs) and residential addresses, which introduces a\nneed to apply privacy-preserving techniques for these ML models using federated\nlearning and differential privacy.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.15962v1.pdf",
        "similarity": 0.21168393107867337,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-23"
    },
    {
        "new_title": "Projection Methods for Operator Learning and Universal Approximation",
        "new_link": "http://arxiv.org/abs/2406.12264v1",
        "new_summary": "  We obtain a new universal approximation theorem for continuous operators on\narbitrary Banach spaces using the Leray-Schauder mapping. Moreover, we\nintroduce and study a method for operator learning in Banach spaces $L^p$ of\nfunctions with multiple variables, based on orthogonal projections on\npolynomial bases. We derive a universal approximation result for operators\nwhere we learn a linear projection and a finite dimensional mapping under some\nadditional assumptions. For the case of $p=2$, we give some sufficient\nconditions for the approximation results to hold. This article serves as the\ntheoretical framework for a deep learning methodology whose implementation will\nbe provided in subsequent work.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.12264v1.pdf",
        "similarity": 0.2110952407187159,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-18"
    },
    {
        "new_title": "Learning to Cover: Online Learning and Optimization with Irreversible\n  Decisions",
        "new_link": "http://arxiv.org/abs/2406.14777v1",
        "new_summary": "  We define an online learning and optimization problem with irreversible\ndecisions contributing toward a coverage target. At each period, a\ndecision-maker selects facilities to open, receives information on the success\nof each one, and updates a machine learning model to guide future decisions.\nThe goal is to minimize costs across a finite horizon under a chance constraint\nreflecting the coverage target. We derive an optimal algorithm and a tight\nlower bound in an asymptotic regime characterized by a large target number of\nfacilities $m\\to\\infty$ but a finite horizon $T\\in\\mathbb{Z}_+$. We find that\nthe regret grows sub-linearly at a rate\n$\\Theta\\left(m^{\\frac{1}{2}\\cdot\\frac{1}{1-2^{-T}}}\\right)$, thus converging\nexponentially fast to $\\Theta(\\sqrt{m})$. We establish the robustness of this\nresult to the learning environment; we also extend it to a more complicated\nfacility location setting in a bipartite facility-customer graph with a target\non customer coverage. Throughout, constructive proofs identify a policy\nfeaturing limited exploration initially for learning purposes, and fast\nexploitation later on for optimization purposes once uncertainty gets\nmitigated. These findings underscore the benefits of limited online learning\nand optimization, in that even a few rounds can provide significant benefits as\ncompared to a no-learning baseline.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.14777v1.pdf",
        "similarity": 0.2109271975771114,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-20"
    },
    {
        "new_title": "Conformal Predictions for Probabilistically Robust Scalable Machine\n  Learning Classification",
        "new_link": "http://arxiv.org/abs/2403.10368v1",
        "new_summary": "  Conformal predictions make it possible to define reliable and robust learning\nalgorithms. But they are essentially a method for evaluating whether an\nalgorithm is good enough to be used in practice. To define a reliable learning\nframework for classification from the very beginning of its design, the concept\nof scalable classifier was introduced to generalize the concept of classical\nclassifier by linking it to statistical order theory and probabilistic learning\ntheory. In this paper, we analyze the similarities between scalable classifiers\nand conformal predictions by introducing a new definition of a score function\nand defining a special set of input variables, the conformal safety set, which\ncan identify patterns in the input space that satisfy the error coverage\nguarantee, i.e., that the probability of observing the wrong (possibly unsafe)\nlabel for points belonging to this set is bounded by a predefined $\\varepsilon$\nerror level. We demonstrate the practical implications of this framework\nthrough an application in cybersecurity for identifying DNS tunneling attacks.\nOur work contributes to the development of probabilistically robust and\nreliable machine learning models.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.10368v1.pdf",
        "similarity": 0.2107923124867418,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-15"
    },
    {
        "new_title": "Error Exponent in Agnostic PAC Learning",
        "new_link": "http://arxiv.org/abs/2405.00792v1",
        "new_summary": "  Statistical learning theory and the Probably Approximately Correct (PAC)\ncriterion are the common approach to mathematical learning theory. PAC is\nwidely used to analyze learning problems and algorithms, and have been studied\nthoroughly. Uniform worst case bounds on the convergence rate have been well\nestablished using, e.g., VC theory or Radamacher complexity. However, in a\ntypical scenario the performance could be much better. In this paper, we\nconsider PAC learning using a somewhat different tradeoff, the error exponent -\na well established analysis method in Information Theory - which describes the\nexponential behavior of the probability that the risk will exceed a certain\nthreshold as function of the sample size. We focus on binary classification and\nfind, under some stability assumptions, an improved distribution dependent\nerror exponent for a wide range of problems, establishing the exponential\nbehavior of the PAC error probability in agnostic learning. Interestingly,\nunder these assumptions, agnostic learning may have the same error exponent as\nrealizable learning. The error exponent criterion can be applied to analyze\nknowledge distillation, a problem that so far lacks a theoretical analysis.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.00792v1.pdf",
        "similarity": 0.20984604876342475,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-01"
    },
    {
        "new_title": "Encoding Binary Events from Continuous Time Series in Rooted Trees using\n  Contrastive Learning",
        "new_link": "http://arxiv.org/abs/2401.01242v1",
        "new_summary": "  Broadband infrastructure owners do not always know how their customers are\nconnected in the local networks, which are structured as rooted trees. A recent\nstudy is able to infer the topology of a local network using discrete time\nseries data from the leaves of the tree (customers). In this study we propose a\ncontrastive approach for learning a binary event encoder from continuous time\nseries data. As a preliminary result, we show that our approach has some\npotential in learning a valuable encoder.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.01242v1.pdf",
        "similarity": 0.20905278059035096,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-02"
    },
    {
        "new_title": "Label-free detection of exosomes from different cellular sources based\n  on surface-enhanced Raman spectroscopy combined with machine learning models",
        "new_link": "http://arxiv.org/abs/2401.14104v2",
        "new_summary": "  Exosomes are significant facilitators of inter-cellular communication that\ncan unveil cell-cell interactions, signaling pathways, regulatory mechanisms\nand disease diagnostics. Nonetheless, current analysis required large amount of\ndata for exosome identification that it hampers efficient and timely mechanism\nstudy and diagnostics. Here, we used a machine-learning assisted\nSurface-enhanced Raman spectroscopy (SERS) method to detect exosomes derived\nfrom six distinct cell lines (HepG2, Hela, 143B, LO-2, BMSC, and H8) with small\namount of data. By employing sodium borohydride-reduced silver nanoparticles\nand sodium borohydride solution as an aggregating agent, 100 SERS spectra of\nthe each types of exosomes were collected and then subjected to multivariate\nand machine learning analysis. By integrating Principal Component Analysis with\nSupport Vector Machine (PCA-SVM) models, our analysis achieved a high accuracy\nrate of 94.4% in predicting exosomes originating from various cellular sources.\nIn comparison to other machine learning analysis, our method used small amount\nof SERS data to allow a simple and rapid exosome detection, which enables a\ntimely subsequent study of cell-cell interactions, communication mechanisms,\nand disease mechanisms in life sciences.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.14104v2.pdf",
        "similarity": 0.20885543183059818,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-25"
    },
    {
        "new_title": "Deep Learning Based Joint Multi-User MISO Power Allocation and\n  Beamforming Design",
        "new_link": "http://arxiv.org/abs/2406.08373v1",
        "new_summary": "  The evolution of fifth generation (5G) wireless communication networks has\nled to an increased need for wireless resource management solutions that\nprovide higher data rates, wide coverage, low latency, and power efficiency.\nYet, many of existing traditional approaches remain non-practical due to\ncomputational limitations, and unrealistic presumptions of static network\nconditions and algorithm initialization dependencies. This creates an important\ngap between theoretical analysis and real-time processing of algorithms. To\nbridge this gap, deep learning based techniques offer promising solutions with\ntheir representational capabilities for universal function approximation. We\npropose a novel unsupervised deep learning based joint power allocation and\nbeamforming design for multi-user multiple-input single-output (MU-MISO)\nsystem. The objective is to enhance the spectral efficiency by maximizing the\nsum-rate with the proposed joint design framework, NNBF-P while also offering\ncomputationally efficient solution in contrast to conventional approaches. We\nconduct experiments for diverse settings to compare the performance of NNBF-P\nwith zero-forcing beamforming (ZFBF), minimum mean square error (MMSE)\nbeamforming, and NNBF, which is also our deep learning based beamforming design\nwithout joint power allocation scheme. Experiment results demonstrate the\nsuperiority of NNBF-P compared to ZFBF, and MMSE while NNBF can have lower\nperformances than MMSE and ZFBF in some experiment settings. It can also\ndemonstrate the effectiveness of joint design framework with respect to NNBF.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.08373v1.pdf",
        "similarity": 0.20884019078466537,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-12"
    },
    {
        "new_title": "Feature Importance and Explainability in Quantum Machine Learning",
        "new_link": "http://arxiv.org/abs/2405.08917v1",
        "new_summary": "  Many Machine Learning (ML) models are referred to as black box models,\nproviding no real insights into why a prediction is made. Feature importance\nand explainability are important for increasing transparency and trust in ML\nmodels, particularly in settings such as healthcare and finance. With quantum\ncomputing's unique capabilities, such as leveraging quantum mechanical\nphenomena like superposition, which can be combined with ML techniques to\ncreate the field of Quantum Machine Learning (QML), and such techniques may be\napplied to QML models. This article explores feature importance and\nexplainability insights in QML compared to Classical ML models. Utilizing the\nwidely recognized Iris dataset, classical ML algorithms such as SVM and Random\nForests, are compared against hybrid quantum counterparts, implemented via\nIBM's Qiskit platform: the Variational Quantum Classifier (VQC) and Quantum\nSupport Vector Classifier (QSVC). This article aims to provide a comparison of\nthe insights generated in ML by employing permutation and leave one out feature\nimportance methods, alongside ALE (Accumulated Local Effects) and SHAP (SHapley\nAdditive exPlanations) explainers.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.08917v1.pdf",
        "similarity": 0.20831080478122566,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-14"
    },
    {
        "new_title": "A PNP ion channel deep learning solver with local neural network and\n  finite element input data",
        "new_link": "http://arxiv.org/abs/2401.17513v2",
        "new_summary": "  In this paper, a deep learning method for solving an improved one-dimensional\nPoisson-Nernst-Planck ion channel (PNPic) model, called the PNPic deep learning\nsolver, is presented. In particular, it combines a novel local neural network\nscheme with an effective PNPic finite element solver. Since the input data of\nthe neural network scheme only involves a small local patch of coarse grid\nsolutions, which the finite element solver can quickly produce, the PNPic deep\nlearning solver can be trained much faster than any corresponding conventional\nglobal neural network solvers. After properly trained, it can output a\npredicted PNPic solution in a much higher degree of accuracy than the low cost\ncoarse grid solutions and can reflect different perturbation cases on the\nparameters, ion channel subregions, and interface and boundary values, etc.\nConsequently, the PNPic deep learning solver can generate a numerical solution\nwith high accuracy for a family of PNPic models. As an initial study, two types\nof numerical tests were done by perturbing one and two parameters of the PNPic\nmodel, respectively, as well as the tests done by using a few perturbed\ninterface positions of the model as training samples. These tests demonstrate\nthat the PNPic deep learning solver can generate highly accurate PNPic\nnumerical solutions.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.17513v2.pdf",
        "similarity": 0.2074044850728991,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-31"
    },
    {
        "new_title": "A chaotic maps-based privacy-preserving distributed deep learning for\n  incomplete and Non-IID datasets",
        "new_link": "http://arxiv.org/abs/2402.10145v1",
        "new_summary": "  Federated Learning is a machine learning approach that enables the training\nof a deep learning model among several participants with sensitive data that\nwish to share their own knowledge without compromising the privacy of their\ndata. In this research, the authors employ a secured Federated Learning method\nwith an additional layer of privacy and proposes a method for addressing the\nnon-IID challenge. Moreover, differential privacy is compared with\nchaotic-based encryption as layer of privacy. The experimental approach\nassesses the performance of the federated deep learning model with differential\nprivacy using both IID and non-IID data. In each experiment, the Federated\nLearning process improves the average performance metrics of the deep neural\nnetwork, even in the case of non-IID data.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.10145v1.pdf",
        "similarity": 0.20739058711160893,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-15"
    },
    {
        "new_title": "Swarm Learning: A Survey of Concepts, Applications, and Trends",
        "new_link": "http://arxiv.org/abs/2405.00556v1",
        "new_summary": "  Deep learning models have raised privacy and security concerns due to their\nreliance on large datasets on central servers. As the number of Internet of\nThings (IoT) devices increases, artificial intelligence (AI) will be crucial\nfor resource management, data processing, and knowledge acquisition. To address\nthose issues, federated learning (FL) has introduced a novel approach to\nbuilding a versatile, large-scale machine learning framework that operates in a\ndecentralized and hardware-agnostic manner. However, FL faces network bandwidth\nlimitations and data breaches. To reduce the central dependency in FL and\nincrease scalability, swarm learning (SL) has been proposed in collaboration\nwith Hewlett Packard Enterprise (HPE). SL represents a decentralized machine\nlearning framework that leverages blockchain technology for secure, scalable,\nand private data management. A blockchain-based network enables the exchange\nand aggregation of model parameters among participants, thus mitigating the\nrisk of a single point of failure and eliminating communication bottlenecks. To\nthe best of our knowledge, this survey is the first to introduce the principles\nof Swarm Learning, its architectural design, and its fields of application. In\naddition, it highlights numerous research avenues that require further\nexploration by academic and industry communities to unlock the full potential\nand applications of SL.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.00556v1.pdf",
        "similarity": 0.20679055580344963,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-01"
    },
    {
        "new_title": "Utilizing Deep Learning to Optimize Software Development Processes",
        "new_link": "http://arxiv.org/abs/2404.13630v2",
        "new_summary": "  This study explores the application of deep learning technologies in software\ndevelopment processes, particularly in automating code reviews, error\nprediction, and test generation to enhance code quality and development\nefficiency. Through a series of empirical studies, experimental groups using\ndeep learning tools and control groups using traditional methods were compared\nin terms of code error rates and project completion times. The results\ndemonstrated significant improvements in the experimental group, validating the\neffectiveness of deep learning technologies. The research also discusses\npotential optimization points, methodologies, and technical challenges of deep\nlearning in software development, as well as how to integrate these\ntechnologies into existing software development workflows.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.13630v2.pdf",
        "similarity": 0.20673824896923393,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-21"
    },
    {
        "new_title": "Gaussian Plane-Wave Neural Operator for Electron Density Estimation",
        "new_link": "http://arxiv.org/abs/2402.04278v2",
        "new_summary": "  This work studies machine learning for electron density prediction, which is\nfundamental for understanding chemical systems and density functional theory\n(DFT) simulations. To this end, we introduce the Gaussian plane-wave neural\noperator (GPWNO), which operates in the infinite-dimensional functional space\nusing the plane-wave and Gaussian-type orbital bases, widely recognized in the\ncontext of DFT. In particular, both high- and low-frequency components of the\ndensity can be effectively represented due to the complementary nature of the\ntwo bases. Extensive experiments on QM9, MD, and material project datasets\ndemonstrate GPWNO's superior performance over ten baselines.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.04278v2.pdf",
        "similarity": 0.20641409063738456,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-05"
    },
    {
        "new_title": "Neyman Meets Causal Machine Learning: Experimental Evaluation of\n  Individualized Treatment Rules",
        "new_link": "http://arxiv.org/abs/2404.17019v1",
        "new_summary": "  A century ago, Neyman showed how to evaluate the efficacy of treatment using\na randomized experiment under a minimal set of assumptions. This classical\nrepeated sampling framework serves as a basis of routine experimental analyses\nconducted by today's scientists across disciplines. In this paper, we\ndemonstrate that Neyman's methodology can also be used to experimentally\nevaluate the efficacy of individualized treatment rules (ITRs), which are\nderived by modern causal machine learning algorithms. In particular, we show\nhow to account for additional uncertainty resulting from a training process\nbased on cross-fitting. The primary advantage of Neyman's approach is that it\ncan be applied to any ITR regardless of the properties of machine learning\nalgorithms that are used to derive the ITR. We also show, somewhat\nsurprisingly, that for certain metrics, it is more efficient to conduct this\nex-post experimental evaluation of an ITR than to conduct an ex-ante\nexperimental evaluation that randomly assigns some units to the ITR. Our\nanalysis demonstrates that Neyman's repeated sampling framework is as relevant\nfor causal inference today as it has been since its inception.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.17019v1.pdf",
        "similarity": 0.2062949075457783,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-25"
    },
    {
        "new_title": "MCMC-driven learning",
        "new_link": "http://arxiv.org/abs/2402.09598v1",
        "new_summary": "  This paper is intended to appear as a chapter for the Handbook of Markov\nChain Monte Carlo. The goal of this chapter is to unify various problems at the\nintersection of Markov chain Monte Carlo (MCMC) and machine\nlearning$\\unicode{x2014}$which includes black-box variational inference,\nadaptive MCMC, normalizing flow construction and transport-assisted MCMC,\nsurrogate-likelihood MCMC, coreset construction for MCMC with big data, Markov\nchain gradient descent, Markovian score climbing, and\nmore$\\unicode{x2014}$within one common framework. By doing so, the theory and\nmethods developed for each may be translated and generalized.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.09598v1.pdf",
        "similarity": 0.20622522663717074,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-14"
    },
    {
        "new_title": "Deep Reinforcement Learning-aided Transmission Design for\n  Energy-efficient Link Optimization in Vehicular Communications",
        "new_link": "http://arxiv.org/abs/2404.12595v1",
        "new_summary": "  This letter presents a deep reinforcement learning (DRL) approach for\ntransmission design to optimize the energy efficiency in vehicle-to-vehicle\n(V2V) communication links. Considering the dynamic environment of vehicular\ncommunications, the optimization problem is non-convex and mathematically\ndifficult to solve. Hence, we propose scenario identification-based double and\nDueling deep Q-Network (SI-D3QN), a DRL algorithm integrating both double deep\nQ-Network and Dueling deep Q-Network, for the joint design of modulation and\ncoding scheme (MCS) selection and power control. To be more specific, we employ\nSI techique to enhance link performance and assit the D3QN agent in refining\nits decision-making processes. The experiment results demonstrate that, across\nvarious optimization tasks, our proposed SI-D3QN agent outperforms the\nbenchmark algorithms in terms of the valid actions and link performance\nmetrics. Particularly, while ensuring significant improvement in energy\nefficiency, the agent facilitates a 29.6% enhancement in the link throughput\nunder the same energy consumption.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.12595v1.pdf",
        "similarity": 0.20576999115775793,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-19"
    },
    {
        "new_title": "Dynamic Online Ensembles of Basis Expansions",
        "new_link": "http://arxiv.org/abs/2405.01365v1",
        "new_summary": "  Practical Bayesian learning often requires (1) online inference, (2) dynamic\nmodels, and (3) ensembling over multiple different models. Recent advances have\nshown how to use random feature approximations to achieve scalable, online\nensembling of Gaussian processes with desirable theoretical properties and\nfruitful applications. One key to these methods' success is the inclusion of a\nrandom walk on the model parameters, which makes models dynamic. We show that\nthese methods can be generalized easily to any basis expansion model and that\nusing alternative basis expansions, such as Hilbert space Gaussian processes,\noften results in better performance. To simplify the process of choosing a\nspecific basis expansion, our method's generality also allows the ensembling of\nseveral entirely different models, for example, a Gaussian process and\npolynomial regression. Finally, we propose a novel method to ensemble static\nand dynamic models together.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.01365v1.pdf",
        "similarity": 0.20526808554702117,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-02"
    },
    {
        "new_title": "Collaborative real-time vision-based device for olive oil production\n  monitoring",
        "new_link": "http://arxiv.org/abs/2407.13285v1",
        "new_summary": "  This paper proposes an innovative approach to improving quality control of\nolive oil manufacturing and preventing damage to the machinery caused by\nforeign objects. We developed a computer-vision-based system that monitors the\ninput of an olive grinder and promptly alerts operators if a foreign object is\ndetected, indicating it by using guided lasers, audio, and visual cues.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.13285v1.pdf",
        "similarity": 0.20482688412747402,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-18"
    },
    {
        "new_title": "Reflective Policy Optimization",
        "new_link": "http://arxiv.org/abs/2406.03678v1",
        "new_summary": "  On-policy reinforcement learning methods, like Trust Region Policy\nOptimization (TRPO) and Proximal Policy Optimization (PPO), often demand\nextensive data per update, leading to sample inefficiency. This paper\nintroduces Reflective Policy Optimization (RPO), a novel on-policy extension\nthat amalgamates past and future state-action information for policy\noptimization. This approach empowers the agent for introspection, allowing\nmodifications to its actions within the current state. Theoretical analysis\nconfirms that policy performance is monotonically improved and contracts the\nsolution space, consequently expediting the convergence procedure. Empirical\nresults demonstrate RPO's feasibility and efficacy in two reinforcement\nlearning benchmarks, culminating in superior sample efficiency. The source code\nof this work is available at https://github.com/Edgargan/RPO.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.03678v1.pdf",
        "similarity": 0.2047930359382487,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-06"
    },
    {
        "new_title": "Evaluations of Machine Learning Privacy Defenses are Misleading",
        "new_link": "http://arxiv.org/abs/2404.17399v1",
        "new_summary": "  Empirical defenses for machine learning privacy forgo the provable guarantees\nof differential privacy in the hope of achieving higher utility while resisting\nrealistic adversaries. We identify severe pitfalls in existing empirical\nprivacy evaluations (based on membership inference attacks) that result in\nmisleading conclusions. In particular, we show that prior evaluations fail to\ncharacterize the privacy leakage of the most vulnerable samples, use weak\nattacks, and avoid comparisons with practical differential privacy baselines.\nIn 5 case studies of empirical privacy defenses, we find that prior evaluations\nunderestimate privacy leakage by an order of magnitude. Under our stronger\nevaluation, none of the empirical defenses we study are competitive with a\nproperly tuned, high-utility DP-SGD baseline (with vacuous provable\nguarantees).\n",
        "pdf_link": "https://arxiv.org/pdf/2404.17399v1.pdf",
        "similarity": 0.20472861804178893,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-26"
    },
    {
        "new_title": "Machine Learning for Windows Malware Detection and Classification:\n  Methods, Challenges and Ongoing Research",
        "new_link": "http://arxiv.org/abs/2404.18541v1",
        "new_summary": "  In this chapter, readers will explore how machine learning has been applied\nto build malware detection systems designed for the Windows operating system.\nThis chapter starts by introducing the main components of a Machine Learning\npipeline, highlighting the challenges of collecting and maintaining up-to-date\ndatasets. Following this introduction, various state-of-the-art malware\ndetectors are presented, encompassing both feature-based and deep\nlearning-based detectors. Subsequent sections introduce the primary challenges\nencountered by machine learning-based malware detectors, including concept\ndrift and adversarial attacks. Lastly, this chapter concludes by providing a\nbrief overview of the ongoing research on adversarial defenses.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.18541v1.pdf",
        "similarity": 0.20393629734248658,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-29"
    },
    {
        "new_title": "Facebook Report on Privacy of fNIRS data",
        "new_link": "http://arxiv.org/abs/2401.00973v1",
        "new_summary": "  The primary goal of this project is to develop privacy-preserving machine\nlearning model training techniques for fNIRS data. This project will build a\nlocal model in a centralized setting with both differential privacy (DP) and\ncertified robustness. It will also explore collaborative federated learning to\ntrain a shared model between multiple clients without sharing local fNIRS\ndatasets. To prevent unintentional private information leakage of such clients'\nprivate datasets, we will also implement DP in the federated learning setting.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.00973v1.pdf",
        "similarity": 0.20381919820341696,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-01"
    },
    {
        "new_title": "A tutorial on learning from preferences and choices with Gaussian\n  Processes",
        "new_link": "http://arxiv.org/abs/2403.11782v4",
        "new_summary": "  Preference modelling lies at the intersection of economics, decision theory,\nmachine learning and statistics. By understanding individuals' preferences and\nhow they make choices, we can build products that closely match their\nexpectations, paving the way for more efficient and personalised applications\nacross a wide range of domains. The objective of this tutorial is to present a\ncohesive and comprehensive framework for preference learning with Gaussian\nProcesses (GPs), demonstrating how to seamlessly incorporate rationality\nprinciples (from economics and decision theory) into the learning process. By\nsuitably tailoring the likelihood function, this framework enables the\nconstruction of preference learning models that encompass random utility\nmodels, limits of discernment, and scenarios with multiple conflicting\nutilities for both object- and label-preference. This tutorial builds upon\nestablished research while simultaneously introducing some novel GP-based\nmodels to address specific gaps in the existing literature.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.11782v4.pdf",
        "similarity": 0.20331789711940182,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-18"
    },
    {
        "new_title": "Recent and Upcoming Developments in Randomized Numerical Linear Algebra\n  for Machine Learning",
        "new_link": "http://arxiv.org/abs/2406.11151v2",
        "new_summary": "  Large matrices arise in many machine learning and data analysis applications,\nincluding as representations of datasets, graphs, model weights, and first and\nsecond-order derivatives. Randomized Numerical Linear Algebra (RandNLA) is an\narea which uses randomness to develop improved algorithms for ubiquitous matrix\nproblems. The area has reached a certain level of maturity; but recent hardware\ntrends, efforts to incorporate RandNLA algorithms into core numerical\nlibraries, and advances in machine learning, statistics, and random matrix\ntheory, have lead to new theoretical and practical challenges. This article\nprovides a self-contained overview of RandNLA, in light of these developments.\n",
        "pdf_link": "https://arxiv.org/pdf/2406.11151v2.pdf",
        "similarity": 0.20308746525800012,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-06-17"
    },
    {
        "new_title": "A Quick Introduction to Quantum Machine Learning for Non-Practitioners",
        "new_link": "http://arxiv.org/abs/2402.14694v1",
        "new_summary": "  This paper provides an introduction to quantum machine learning, exploring\nthe potential benefits of using quantum computing principles and algorithms\nthat may improve upon classical machine learning approaches. Quantum computing\nutilizes particles governed by quantum mechanics for computational purposes,\nleveraging properties like superposition and entanglement for information\nrepresentation and manipulation. Quantum machine learning applies these\nprinciples to enhance classical machine learning models, potentially reducing\nnetwork size and training time on quantum hardware. The paper covers basic\nquantum mechanics principles, including superposition, phase space, and\nentanglement, and introduces the concept of quantum gates that exploit these\nproperties. It also reviews classical deep learning concepts, such as\nartificial neural networks, gradient descent, and backpropagation, before\ndelving into trainable quantum circuits as neural networks. An example problem\ndemonstrates the potential advantages of quantum neural networks, and the\nappendices provide detailed derivations. The paper aims to help researchers new\nto quantum mechanics and machine learning develop their expertise more\nefficiently.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.14694v1.pdf",
        "similarity": 0.2028647495740768,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-22"
    },
    {
        "new_title": "On the Role of Similarity in Detecting Masquerading Files",
        "new_link": "http://arxiv.org/abs/2402.11227v1",
        "new_summary": "  Similarity has been applied to a wide range of security applications,\ntypically used in machine learning models. We examine the problem posed by\nmasquerading samples; that is samples crafted by bad actors to be similar or\nnear identical to legitimate samples. We find that these samples potentially\ncreate significant problems for machine learning solutions. The primary problem\nbeing that bad actors can circumvent machine learning solutions by using\nmasquerading samples.\n  We then examine the interplay between digital signatures and machine learning\nsolutions. In particular, we focus on executable files and code signing. We\noffer a taxonomy for masquerading files. We use a combination of similarity and\nclustering to find masquerading files. We use the insights gathered in this\nprocess to offer improvements to similarity based and machine learning security\nsolutions.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.11227v1.pdf",
        "similarity": 0.20258430812197123,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-17"
    },
    {
        "new_title": "Output-decomposed Learning of Mealy Machines",
        "new_link": "http://arxiv.org/abs/2405.08647v1",
        "new_summary": "  We present an active automata learning algorithm which learns a decomposition\nof a finite state machine, based on projecting onto individual outputs. This is\ndual to a recent compositional learning algorithm by Labbaf et al. (2023). When\nprojecting the outputs to a smaller set, the model itself is reduced in size.\nBy having several such projections, we do not lose any information and the full\nsystem can be reconstructed. Depending on the structure of the system this\nreduces the number of queries drastically, as shown by a preliminary evaluation\nof the algorithm.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.08647v1.pdf",
        "similarity": 0.2020076544142079,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-14"
    },
    {
        "new_title": "Architectural Blueprint For Heterogeneity-Resilient Federated Learning",
        "new_link": "http://arxiv.org/abs/2403.04546v2",
        "new_summary": "  This paper proposes a novel three tier architecture for federated learning to\noptimize edge computing environments. The proposed architecture addresses the\nchallenges associated with client data heterogeneity and computational\nconstraints. It introduces a scalable, privacy preserving framework that\nenhances the efficiency of distributed machine learning. Through\nexperimentation, the paper demonstrates the architecture capability to manage\nnon IID data sets more effectively than traditional federated learning models.\nAdditionally, the paper highlights the potential of this innovative approach to\nsignificantly improve model accuracy, reduce communication overhead, and\nfacilitate broader adoption of federated learning technologies.\n",
        "pdf_link": "https://arxiv.org/pdf/2403.04546v2.pdf",
        "similarity": 0.20194905901283605,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-03-07"
    },
    {
        "new_title": "Potential of quantum scientific machine learning applied to weather\n  modelling",
        "new_link": "http://arxiv.org/abs/2404.08737v1",
        "new_summary": "  In this work we explore how quantum scientific machine learning can be used\nto tackle the challenge of weather modelling. Using parameterised quantum\ncircuits as machine learning models, we consider two paradigms: supervised\nlearning from weather data and physics-informed solving of the underlying\nequations of atmospheric dynamics. In the first case, we demonstrate how a\nquantum model can be trained to accurately reproduce real-world global stream\nfunction dynamics at a resolution of 4{\\deg}. We detail a number of\nproblem-specific classical and quantum architecture choices used to achieve\nthis result. Subsequently, we introduce the barotropic vorticity equation (BVE)\nas our model of the atmosphere, which is a $3^{\\text{rd}}$ order partial\ndifferential equation (PDE) in its stream function formulation. Using the\ndifferentiable quantum circuits algorithm, we successfully solve the BVE under\nappropriate boundary conditions and use the trained model to predict unseen\nfuture dynamics to high accuracy given an artificial initial weather state.\nWhilst challenges remain, our results mark an advancement in terms of the\ncomplexity of PDEs solved with quantum scientific machine learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.08737v1.pdf",
        "similarity": 0.20185475598428243,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-12"
    },
    {
        "new_title": "Real-time multichannel deep speech enhancement in hearing aids:\n  Comparing monaural and binaural processing in complex acoustic scenarios",
        "new_link": "http://arxiv.org/abs/2405.01967v1",
        "new_summary": "  Deep learning has the potential to enhance speech signals and increase their\nintelligibility for users of hearing aids. Deep models suited for real-world\napplication should feature a low computational complexity and low processing\ndelay of only a few milliseconds. In this paper, we explore deep speech\nenhancement that matches these requirements and contrast monaural and binaural\nprocessing algorithms in two complex acoustic scenes. Both algorithms are\nevaluated with objective metrics and in experiments with hearing-impaired\nlisteners performing a speech-in-noise test. Results are compared to two\ntraditional enhancement strategies, i.e., adaptive differential microphone\nprocessing and binaural beamforming. While in diffuse noise, all algorithms\nperform similarly, the binaural deep learning approach performs best in the\npresence of spatial interferers. Through a post-analysis, this can be\nattributed to improvements at low SNRs and to precise spatial filtering.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.01967v1.pdf",
        "similarity": 0.20176742154006722,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-03"
    },
    {
        "new_title": "Dual Interior-Point Optimization Learning",
        "new_link": "http://arxiv.org/abs/2402.02596v1",
        "new_summary": "  This paper introduces Dual Interior Point Learning (DIPL) and Dual\nSupergradient Learning (DSL) to learn dual feasible solutions to parametric\nlinear programs with bounded variables, which are pervasive across many\nindustries. DIPL mimics a novel dual interior point algorithm while DSL mimics\nclassical dual supergradient ascent. DIPL and DSL ensure dual feasibility by\npredicting dual variables associated with the constraints then exploiting the\nflexibility of the duals of the bound constraints. DIPL and DSL complement\nexisting primal learning methods by providing a certificate of quality. They\nare shown to produce high-fidelity dual-feasible solutions to large-scale\noptimal power flow problems providing valid dual bounds under 0.5% optimality\ngap.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.02596v1.pdf",
        "similarity": 0.2017116893790234,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-04"
    },
    {
        "new_title": "Federated Deep Q-Learning and 5G load balancing",
        "new_link": "http://arxiv.org/abs/2403.08813v1",
        "new_summary": "  Despite advances in cellular network technology, base station (BS) load\nbalancing remains a persistent problem. Although centralized resource\nallocation methods can address the load balancing problem, it still remains an\nNP-hard problem. In this research, we study how federated deep Q learning can\nbe used to inform each user equipment (UE) of the each BS's load conditions.\nFederated deep Q learning's load balancing enables intelligent UEs to\nindependently select the best BS while also limiting the amount of private\ninformation exposed to the network.\n  In this study, we propose and analyze a federated deep Q learning load\nbalancing system, which is implemented using the Open-RAN xAPP framework and\nthe near-Real Time Radio Interface Controller (near-RT RIC). Our simulation\nresults indicate that compared to the maximum Signal-To-Noise-Ratio (MAX-SINR)\nmethod currently used by UEs, our proposed deep Q learning model can\nconsistently provide better High average UE quality of service\n",
        "pdf_link": "https://arxiv.org/pdf/2403.08813v1.pdf",
        "similarity": 0.2016282486368691,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-10"
    },
    {
        "new_title": "Advances in Differential Privacy and Differentially Private Machine\n  Learning",
        "new_link": "http://arxiv.org/abs/2404.04706v1",
        "new_summary": "  There has been an explosion of research on differential privacy (DP) and its\nvarious applications in recent years, ranging from novel variants and\naccounting techniques in differential privacy to the thriving field of\ndifferentially private machine learning (DPML) to newer implementations in\npractice, like those by various companies and organisations such as census\nbureaus. Most recent surveys focus on the applications of differential privacy\nin particular contexts like data publishing, specific machine learning tasks,\nanalysis of unstructured data, location privacy, etc. This work thus seeks to\nfill the gap for a survey that primarily discusses recent developments in the\ntheory of differential privacy along with newer DP variants, viz. Renyi DP and\nConcentrated DP, novel mechanisms and techniques, and the theoretical\ndevelopments in differentially private machine learning in proper detail. In\naddition, this survey discusses its applications to privacy-preserving machine\nlearning in practice and a few practical implementations of DP.\n",
        "pdf_link": "https://arxiv.org/pdf/2404.04706v1.pdf",
        "similarity": 0.20123531690983432,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-04-06"
    },
    {
        "new_title": "Application of Machine Learning and Convex Limiting to Subgrid Flux\n  Modeling in the Shallow-Water Equations",
        "new_link": "http://arxiv.org/abs/2407.17214v1",
        "new_summary": "  We propose a combination of machine learning and flux limiting for\nproperty-preserving subgrid scale modeling in the context of flux-limited\nfinite volume methods for the one-dimensional shallow-water equations. The\nnumerical fluxes of a conservative target scheme are fitted to the coarse-mesh\naverages of a monotone fine-grid discretization using a neural network to\nparametrize the subgrid scale components. To ensure positivity preservation and\nthe validity of local maximum principles, we use a flux limiter that constrains\nthe intermediate states of an equivalent fluctuation form to stay in a convex\nadmissible set. The results of our numerical studies confirm that the proposed\ncombination of machine learning with monolithic convex limiting produces\nmeaningful closures even in scenarios for which the network was not trained.\n",
        "pdf_link": "https://arxiv.org/pdf/2407.17214v1.pdf",
        "similarity": 0.20070068301915625,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-07-24"
    },
    {
        "new_title": "GenFormer: A Deep-Learning-Based Approach for Generating Multivariate\n  Stochastic Processes",
        "new_link": "http://arxiv.org/abs/2402.02010v1",
        "new_summary": "  Stochastic generators are essential to produce synthetic realizations that\npreserve target statistical properties. We propose GenFormer, a stochastic\ngenerator for spatio-temporal multivariate stochastic processes. It is\nconstructed using a Transformer-based deep learning model that learns a mapping\nbetween a Markov state sequence and time series values. The synthetic data\ngenerated by the GenFormer model preserves the target marginal distributions\nand approximately captures other desired statistical properties even in\nchallenging applications involving a large number of spatial locations and a\nlong simulation horizon. The GenFormer model is applied to simulate synthetic\nwind speed data at various stations in Florida to calculate exceedance\nprobabilities for risk management.\n",
        "pdf_link": "https://arxiv.org/pdf/2402.02010v1.pdf",
        "similarity": 0.2003825449810351,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-02-03"
    },
    {
        "new_title": "Exploring the Truth and Beauty of Theory Landscapes with Machine\n  Learning",
        "new_link": "http://arxiv.org/abs/2401.11513v1",
        "new_summary": "  Theoretical physicists describe nature by i) building a theory model and ii)\ndetermining the model parameters. The latter step involves the dual aspect of\nboth fitting to the existing experimental data and satisfying abstract criteria\nlike beauty, naturalness, etc. We use the Yukawa quark sector as a toy example\nto demonstrate how both of those tasks can be accomplished with machine\nlearning techniques. We propose loss functions whose minimization results in\ntrue models that are also beautiful as measured by three different criteria -\nuniformity, sparsity, or symmetry.\n",
        "pdf_link": "https://arxiv.org/pdf/2401.11513v1.pdf",
        "similarity": 0.20014314218024978,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-01-21"
    },
    {
        "new_title": "A Hybrid Deep Learning Framework for Stock Price Prediction Considering\n  the Investor Sentiment of Online Forum Enhanced by Popularity",
        "new_link": "http://arxiv.org/abs/2405.10584v1",
        "new_summary": "  Stock price prediction has always been a difficult task for forecasters.\nUsing cutting-edge deep learning techniques, stock price prediction based on\ninvestor sentiment extracted from online forums has become feasible. We propose\na novel hybrid deep learning framework for predicting stock prices. The\nframework leverages the XLNET model to analyze the sentiment conveyed in user\nposts on online forums, combines these sentiments with the post popularity\nfactor to compute daily group sentiments, and integrates this information with\nstock technical indicators into an improved BiLSTM-highway model for stock\nprice prediction. Through a series of comparative experiments involving four\nstocks on the Chinese stock market, it is demonstrated that the hybrid\nframework effectively predicts stock prices. This study reveals the necessity\nof analyzing investors' textual views for stock price prediction.\n",
        "pdf_link": "https://arxiv.org/pdf/2405.10584v1.pdf",
        "similarity": 0.20007137703289352,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2403.04822",
        "published": "2024-05-17"
    }
]